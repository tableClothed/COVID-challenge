title,abstract,text,similarity
Mass gatherings medicine: public health issues arising from mass gathering religious and sporting events,"Mass gathering events are associated with major public health challenges. The 2014 Lancet Series on the new discipline of mass gatherings medicine was launched at the World Health Assembly of Ministers of Health in Geneva in May, 2014. The Series covered the planning and surveillance systems used to monitor public health risks, public health threats, and experiences of health-care providers from mass gathering events in 2012 and 2013. This follow-up Review focuses on the main public health issues arising from planned mass gathering events held between 2013 and 2018. We highlight public health and research data on transmission of infectious diseases and antibioticresistant bacteria, mass casualty incidents, and non-communicable diseases, including thermal disorders. In the events discussed in this Review, the combination of a large influx of people, many from countries with outbreakprone infectious diseases, with a high degree of crowd interactions imposed substantial burdens on host countries' health systems. The detection and transmission of antibiotic-resistant bacteria in pilgrims attending the Kumbh Mela and the Hajj raise concern of possible globalisation from mass-gathering religious events. Priorities for further investments and opportunities for research into prevention, surveillance, and management of these public health issues are discussed.","A mass gathering is defined by WHO as a planned or spontaneous event that gathers substantial numbers of attendees who might strain the health planning and response capacities of the host community, city, or country. 1 Mass gatherings can present important public health challenges related to the health of attendees and of the host country population and health services. 2 The concept of mass gathering medicine as a specialty emanated from discourse on the 2009 Hajj, which was held during the 2009 HIN1 influenza pandemic (panel 1). 3 The first International Conference on Mass Gatherings Medicine was held in Jeddah, Saudi Arabia, in October, 2010, where The Lancet Infectious Diseases Series on mass gatherings was launched (panel 1). 4 This conference led to a coalition of experts from virtual WHO mass gathering collaborating centres and global academic and public health faculty 5 to guide develop ment of, and update, optimal public health and medical prevention and treatment guidelines at mass gathering events. 6, 7 Mass gathering medicine as a new discipline was highlighted at the World Health Assembly of Ministers of Health in Geneva in May, 2014, where The Lancet Series on mass gathering medicine was launched. 7 These stateoftheart reviews covered the planning and surveillance systems used to monitor public health risks, public health threats, and experiences of health care providers from three mass gathering events: the London 2012 Summer Olympics and Paralympics, 8 the 2012 Union of European Football Associations European Championship finals, 9 and the Hajj pilgrimages of 2012 and 2013. 10 They set out the planning and surveillance systems used to monitor public health risks and described existing and potential public health threats (panel 2) and the experiences of healthcare providers.

Since the launch of The Lancet Series in 2014, various public health threats have continued to pose public health challenges for the planning of events and during events at various mass gathering events. This followup Review focuses on the main public health issues arising from planned religious, sporting, scouting, cultural, and musical mass gathering events held between 2013 and 2018: the Kumbh Mela 2013 and 2016 pilgrimages in India, the 12th Festival of Pacific Arts, the UN's Third Conference on Small Island Developing States, and the Micronesian Games in the Pacific Island Countries and Territories (PICTs), the Rio de Janeiro 2016 Summer Olympics and Paralympics, Russia 2018 FIFA World Cup, the 23rd World Scout Jamboree in Japan in 2015, and the annual Hajj pilgrimages of 2015, 2016, and 2017 in Saudi Arabia. We highlight public health and research data on transmission of infectious diseases, emergence of antibioticresistant bacteria, mass casualty incidents arising from stampedes and terrorist attacks, and noncommunicable diseases, including heatrelated

The Kumbh Mela, the Hindu religious pilgrimage festival, is the largest mass gathering event in the world, [11] [12] [13] [14] [15] [16] [17] [18] it attracts more than 120 million pilgrims from across the world for up to 2 months, and is held every 3 years alternately along the banks of four holy rivers: the Ganga, or Ganges, River (Haridwar district), Godavari River (Nasik district), Kshipra River (Ujjain district), and Sangam River (Prayag district, Allahabad) at a confluence of Ganga, Yamuna, and Saraswati. The 2013 Kumbh Mela event was attended by 70 million Hindu pilgrims over 55 days at the confluence of the Yamuna and Ganga Rivers. 19 The Kumbh Mela is a highly coordinated and organised event, where WHO recomendations for mass gatherings are implemented (panel 3). 1, 2 The Indian Government and local authorities plan well ahead of each Kumbh Mela event and have established plans for physical and public health infrastructure to be in place. Socalled temporary popup cities that include tents for pilgrims, water pipelines and clean water supply, toilets, sanitation and sewage disposal facilities, vector control and sur veillance teams, security services, and administrative assistance for pilgrims are set up along the river. [20] [21] [22] A range of healthcare facilities are put in place and hospitals are prepared to receive pilgrims requiring inpatient treatment and intensive care for every Kumbh Mela festival. 21 Internal roads and pontoon bridges are built for these popup cities and are designed to streamline and facilitate pilgrim crowd movement and flow. 22 State government irrigation and water resources departments and the Central Pollution Control Board work closely together to increase the capacity of sewage treatment plants and to ensure sufficient flow to remove contamination by releasing water into rivers. 19, 20 90 000 kL/day of potable water were supplied for the 2013 Kumbh Mela for drinking and cooking, with 550 km of water pipelines were laid for distribution through 20 000 taps. 19 Approximately 45 000 toilets were provided, although there was little health promotion messaging. 21 Similarly to other mass gatherings, random inspection and vaccinations were done for immunocompromised individuals. 21

The Kumbh Mela is unique in that it involves frequent and prolonged bathing by pilgrims in the holy river, a ritual which is believed to break the cycle of reincarnations and convey immortality. 17, 18 Contamination of river water with faeces, urine, saliva, and sputa is inevitable and leads to transmission of waterborne and respiratory tract infections. 12, 14, 20, 21 A notable example of such a cycle of transmission is the 1817-24 Asia cholera pandemic, which was associated with the Kumbh Mela. 23 urination. 21 Bacterial load in the river increased by approximately 130 fold. 12, [24] [25] [26] [27] [28] [29] At the 2013 Kumbh Mela, a study 15,16,20 that followed 30 000 pilgrims in four sector hospitals found a 5% incidence of diarrhoeal diseases over a 23day period. A rise in nonbloody diarrhoea cases was seen just 2 days after the main bathing day on Jan 29, 2013, and a peak in upper respiratory tract infections coincided with the peak in nonbloody diarrhoeal disease. The use by pilgrims of fires fuelled by cow dung or firewood resulted in more than 23% seeking cough medicine and smoke from choolahs using wood or coal was the commonest cause of respiratory illnesses among 15 000 patients. 13 The observation that a socalled shaving ritual could increase the risk of bloodborne disease transmission has not yet been confirmed, because the practice is surrounded by secrecy and there is little of information on how widespread it is. 13

The presence of extendedspectrum βlactamasepro ducing Escherichia coli in municipal sewer systems and waste water in Hyderabad, India, have sparked concerns that antibioticresistant bacteria could be spreading at massgathering religious events. 26 The rivers of India are known to be polluted owing to inadequately treated household and industrial effluents. 26, 27, 29 A 20 times increase in blaNDM-1 gene carrying bacteria during the pilgrimage on the bank of the Ganges was observed during the 2015 Kumbh Mela. 27 A novel antibioticresistant bacterial species, Corynebacterium godavarianum, has been isolated from the bathing site of the Godavari River. 25 In response to this growing global concern, 28 an expert panel concluded that use of antibiotics should be restricted to acute travellers whose diarrhoea is incapacitating. 30

Since the stampede at the Kumbh Mela in 2015, 22 the risk of crush injuries, stampedes, and other mass casualty incidents, such as fires, have been minimised by close surveillance and constant evaluation of crowd flow. Site preparation and negotiations with the various akharas (sects) to predetermine the order of ritual baths has resulted in a reduction in the number of deaths due to stampedes from about 500 people in 1954 to 37 in 2015 (table). 22

The Festival of Pacific Arts and the Micronesian Games are a feature of PICTs. 38, 39 Although several of the event host countries have a population under 200 000, such as Yap State (population 11 000) in the Federated States of Micronesia that hosted the 9th Micronesia games in July 2018, the health security risks remain the same, and can be proportionally greater, than similar events in larger countries, due to the influx of people and its effect on the fragile health infrastructure of PICTs. Three enhanced surveillance activities for mass gathering events in PICTs occurred during national or regional disease outbreaks that had the potential to severely affect the mass gatherings, place greater strain on health services, and cause substantial tourismrelated economic and reputation loss.

The Eighth Micronesian Games took place in Pohnpei State, Federated States of Micronesia, in July, 2014. 38 Pohnpei is a small island state of around 36 000 people in the western Pacific. The Games attracted about 1700 athletes and officials from several Micronesian countris and territories. In June, 2014, 6 weeks before the opening of the Games, Pohnpei reported the first case of measles in an outbreak that spanned the duration of the Games and the following 3 months and resulted in a total of 251 cases. 40 The UN's Third Conference on Small Island Developing States was held in Apia, Samoa on Sept 1-4, 2014. 41 Attracting more than 3000 delegates from 115 countries, it was the largest event ever to be hosted by Samoa, a Polynesian island nation in the south Pacific with a population of around 187 000. In late July, 2014, an extensive chikungunya virus disease outbreak began in Samoa that resulted in more than 4000 cases 4 months later. 42 The 12th Festival of Pacific Arts was hosted by the Micronesian island of Guam (population 163 000) in 2016. 43 The Festival was a large event, attracting more than 2500 artists and performers from 27 countries and territories across the Pacific. These island communities (some very small, such as the Pitcairn Islands with fewer than 50 residents) would have been greatly affected by the introduction of a novel disease. The Festival was held Panel 3: Key considerations related to communicable disease alert, response, and operation plans for mass gatherings • Risk assessment and management, surveillance and alert systems, and outbreak alert and response twinned to effective communication strategies • Rapid identification of attendees with communicable diseases and their contacts for potential quarantining and instituting preventive infection control measures • Medical care planning and guidelines (preventive medicine, mandatory vaccinations, food management, water and waste management, and identification of physical and fire hazards) • Public health measures (water protection and provision, food protection and provision, and syndromic surveillance) • Emergency planning and response (rapid access to injured or ill patients, provision of triage in the field and at aid stations, provision of on-site care for minor injuries and illnesses, effective and timely stabilisation and transport of patients requiring evacuation) • Disaster medicine, trauma, and triage and level of care • Mobile units, health stations, and designated hospitals • Use of modern technologies for proactive surveillance, early diagnosis and rapid detection, data collection, analyses, sharing, and communication. • Updating travel guidelines, including vaccinations, for each specific mass gathering event while Zika, dengue, chikungunya, and measles outbreaks were occurring in several participating countries. 40, 42, 44, 45 At all these mass gathering events, the combination of large influxes of people and high degrees of social mixing in the presence of highly infectious pathogens, emerging nonvaccinepreventable diseases, and diseases with incurable long term consequences (eg, Zika birth defects) could have introduced substantial population burdens and consequences for health systems. To counter these threats, enhanced surveillance for these mass gathering events was essential to ensure health security. 41, 46 The three events used enhanced syndromic surveillance that built on the existing panPacific WHOled weekly syndromic surveillance, 47 ensuring host countries were prepared. However, although short, the period of intense activity that characterises mass gathering surveillance is a substantial burden to stretched local public health resources. To alleviate this burden additional support was gained from within host countries and from external partners provided by the Pacific Community, which has extensive experience of mass gathering surveillance, and the Pacific Island Health Officers Association.

Massgathering surveillance in the Pacific has shown the value of a strong foundation in existing disease surveillance and of regional partnerships. These two aspects were essential in host countries' ability to implement comprehensive and intensive massgathering surveillance. However, although the benefit of early warning disease surveillance was established, longer term sustainable surveillance improvements were harder to realise. For example, local public health departments borrowed personnel from other departments and external support, all of whom returned to their previous duties once the massgathering surveillance finished.

The massgathering surveillance was successful 46,47 on all three occasions and showed that even small nations can (with the right support) provide health security in the face of substantial health risks. In doing so, these events proved the value of the mass gathering surveillance in providing health security assurance to event organisers, participants, and local and regional communities as well as showing that small island nations can meet International Health Regulations requirements and implement enhanced massgathering surveillance.

Various public health concerns were raised before the Rio de Janeiro Olympics and Paralympics in August, 2016. [48] [49] [50] [51] In February, 2016, WHO declared the Zika virus a public health emergency of international concern owing to the Zika outbreak in Brazil and its association with birth defects. 50 There was global pressure for the Games to be either postponed or moved to another country. 52 The WHO Emergency Committee on Zika virus decided that postponing or cancelling the Games had no public health justification, [49] [50] [51] 53 stating that ""indi vidual risks in areas of transmission are the same whether or not a mass gathering is conducted, and can be minimised by good public health measures"". 50 Because Brazil is a dengueendemic country, the risk of dengue to nonimmune visitors was also a concern, 54 based on the experience from a pilgrimage in Senegal 55 and from GeoSentinel data. GeoSentinel is a global network of travel medicine providers focused on sentinel surveillance, which generates evidencebased assessments of infectious disease risks for attendees of future mass gatherings events. 56, 57 As with the 2012 Olympics and Paralympics in London, 18 WHO provided technical support to the Brazilian Ministry of Health and International Olympic Committee to ensure public health safety during the Games. This support included improving the quality of water for recreation and preventing the spread of Zika virus by increasing vector control efforts. In spite of intensive media coverage, no cases of Zika virus were confirmed among people who attended, both during the Games and since returning to their home countries. 50 gathering events during infectious disease outbreaks with epidemic potential. 60 Before the 2016 Summer Olympics, another concern was related to water sports and athletes being exposed to polluted waters. 61 However, a study 61 done during tests in 2015 showed no increased incidence of diarrhoea among exposed individuals compared with controls. When travellers to the 2014 FIFA World Cup in Brazil were compared with other travellers to Brazil during the same period, the majority in both groups were bitten by insects and had sunburns as environmental risk factors, and every third traveller suffered from diarrhoea. 58, 61 Men travelling to the World Cup tended to have more sexual contacts outside of a relationship than a control group of other travellers to Brazil and also had more alcohol intake. 48

The 2018 FIFA World Cup was hosted by Russia between June 14 and July 15, 2018. 62 The event was attended by 3 million people with 32 international teams participating in 64 football matches in 12 stadiums across 11 cities: Ekaterinburg, Kaliningrad, Kazan, Moscow, Nizhny Novgorod, RostovonDon, Saint Petersburg, Samara, Saransk, Sochi, and Volgograd. 62 Data from WHO's Regional Office for Europe 62 showed that the Russian Federation had reported 454 isolates from blood and cerebrospinal fluid samples from urban tertiary care hospitals across the country before the event. Carba penem resistance occurred in 74% of Acinetobacter spp isolates, 49% of Pseudomonas aeruginosa isolates, and 12% of Klebsiella pneumoniae isolates. 23% of Staphylococcus aureus isolates were methicillinresistant and 91% of K pneumoniae isolates were resistant to third generation cephalosporins. Although no reported major public health incidences were reported, the 2018 FIFA World Cup created heightened awareness of the threat of the transmission and globalisation of antibioticresistant bacteria. No crosssectional or longitudinal cohort studies from the event have been published.

The 23rd World Scout Jamboree was hosted by Japan between July 28 and Aug 8, 2015, and was attended by 33 000 scouts of up to 17 years of age from 162 countries. 63 The World Scout Jamboree is an international Scout Movement camp. It takes place every 4 years, with the site of the Jamboree rotating between continents. The daily schedule includes arduous activities, such as climbing, woodchopping, and knife handling, and preparing one's own meals on gas stoves. Apart from risk of transmission of infectious diseases, injuries, and burns, it induces a range of psychological disorders, inc luding homesickness and behavioural and neuropsychological disorders. 64 No major infectious diseases outbreaks were reported during the Jamboree. However, within 9 days of the end of the event, six cases of laboratoryconfirmed invasive meningococcal disease caused by Neisseria meningitides capsular serogroup W occurred among scouts and their close contacts in Scotland and Sweden. [65] [66] [67] Soon after return to Scotland, three scouts and one relative were diagnosed with invasive meningo coccal disease. These cases were all associated with one scout unit. Two confirmed cases of invasive meningo coccal disease were reported from Sweden. 65 Molecular analysis [65] [66] [67] showed that the same N meningitides isolates were responsible for the six cases and that they belonged to the ST11 clonal complex, which is usually associated with large outbreaks. No other cases were reported from scouts who attended from other European countries or Japan. All participants were made aware of the outbreak and of signs and symptoms of invasive meningococcal disease through an information campaign and were offered ciprofloxacin chemoprophylaxis. 66, 67 To make a decision on the need for prophylactic treatment for close contacts of returning scouts, throat or nasopharyngeal swabs from participants receiving antibiotic prophylaxis were taken. The carrier state in Swedish teenagers was studied comparing sensitivity of throat versus nasopharyngeal swabs from 1020 of the 1890 Swedish teenagers who participated in the Jamboree. The overall positivity for N meningitidis in this group was 83 (8%) of 1020, of which 61 were nongroupable. 67 Further car riage studies are required during mass gatherings to determine the epidemiology and association between carrier isolates and diseasecausing isolates in the population and the dynamics of globali sation from mass gathering events.

Every year an increasing number of people travel to Saudi Arabia for the Hajj and Umrah pilgrimages. 68 The total number of pilgrims for both the Hajj and Umrah is about 10 million pilgrims annually from more than 180 countries. The Hajj pilgrimages that took place in 2015, 2016, and 2017 each attracted more than 2 million people from outside Saudi Arabia. 68 Several major public health challenges are associated with each Hajj, 10 in cluding transmission of infectious diseases, exacerbation of noncommunicable diseases, mental health disorders, and stampedes (appendix), among others. Infectious diseases surveillance systems are operational during the annual Hajj, and they have evolved from paperbased reporting tools to automated electronic systems, re cording and storing large datasets, and reporting from mobile units, clinics, primary health facilities, and hospitals that serve pilgrims. 10 All these data are fed directly to a central command and control unit, enabling rapid amalgamation and analyses of data and necessary public health interventions. These advances in realtime surveillance have improved public health security for the mass gatherings at the Hajj. 69 See Online for appendix

In the past 5 years, concern has grown with regard to the threat to global health security posed by several emerging and reemerging infectious diseases. Because of the Ebola outbreaks in West Africa (2013-16), 69 74 concerns were expressed by the global public health fraternity regarding pilgrims from affected countries travelling to Saudi Arabia for the Hajj or Umrah. 70 The Saudi government implemented restrictions only to countries affected by the Ebola outbreak. For pilgrims from Yemen and other countries, intensified screening at points of entry into Saudi Arabia and close surveillance during their stay throughout the Hajj, was implemented, with no cases detected. 68 At the 2016 Hajj, The Indian Medical Mission provided health care to about 400 000 pilgrim patients from August to October, 2016, through a team of 144 doctors, including 50 specialists, 146 paramedics, and 74 ancillary staff. 75 They coordinated a tiered healthcare network, including primarycare static clinics, tent clinics, and mobile medical task forces, secondarycare hospitals, and referral and evacuation capabilities at Mecca, Medina, and Jeddah. Secondarycare referral hospitals catered for critical care, internal medicine, general surgery, ortho paedics, gynaecology, paediatrics, psychiatry, dermatology, isolation, lab medicine, and radiology. Infectious diseases consituted 53% of outpatient diagnoses, with upper and lower respiratory infections, gastroenteritis, and diabetes related severe infections, particularly cellulitis and pneumonia the most common. Urinary tract infections were common in female pilgrims. 75 Nearly all pilgrims develop a respiratory tract infection during the Hajj, known as the pilgrims' cough. 10 Overcrowding during the Hajj increases the risk of transmission of respiratory pathogens, such as Middle East respiratory syndrome coronavirus (MERSCoV), rhino virus, respiratory syncytial virus, influenza A H1N1, influenza B, parainfluenza virus, adenovirus, meta pneumovirus, enterovirus, multidrugresistant tu ber culosis, and Streptococcus pneumoniae. Ebola, MERSCoV, Alkhumra viral haemorrhagic fever, and Rift Valley fever have high outbreak potential during Hajj. 10, 76, 77 Although the threat of coronaviruses (MERSCoV and severe acute respiratory syndromerelated coronavirus) with epidemic potential remains, [77] [78] [79] [80] [81] no cases of MERSCoV have yet been identified in pilgrims during Hajj or upon return to their home countries. 79 Other corona viruses identified at the Hajj include alphacoronavirus and betacoronavirus, of which the 229E strain is most common cause of upper respiratory tract illnesses. 82 A systematic review of 31 studies on the prevalence of respiratory viruses in Hajj pilgrims 76 showed influenza, rhinovirus, and parainfluenza continue to be the most common viral infections among pilgrims. At the 2016 Hajj, a study 83 of 266 pilgrims admitted to hospitals with community acquired pneumonia showed that 96 (36%) cases had diabetes, 26 (10%) were smokers, and 108 (45%) cases required intensive care. 48 (18%) cases had invasive S pneumoniae (pneumococcal) infection.

Every year the Saudi Arabian Ministry of Health issues updates on travel immunisation recommendations for pilgrims. 10 They are classified as mandatory (required) and voluntary (recommended) before performing Hajj and Umrah. The three mandatory vaccines are the quadrivalent meningococcal vaccine for all pilgrims and the Yellow fever and Polio vaccines for pilgrims coming from countries with active polio transmission. 10, 68 Recom mended vaccines include influenza vaccine and pneumococcal vaccine. 84, 85 Since the inclusion of the quadrivalent meningococcal vaccine in 2001, 85 no major meningo coccal meningitis outbreaks related to hajj have been identified. [85] [86] [87] However, concern that new serogroups of N meningitidis (eg, B and X), which are not covered in the current quadrivalent vaccine targeting serogroups A, C, W135, and Y, could be a cause of future epidemics is increasing. 88, 89 Owing to the high incidence of pertussis seen among Hajj pilgrims in 2003, 90 Bordetella pertussis is considered a risk in pilgrims, especially those who have not completed their immuni sation schedule. 91 Tuberculosis is the commonest cause of death from an infectious disease worldwide. 37 A large percentage of Hajj pilgrims come from high tuberculosisendemic coun tries. The burden of undiagnosed active pulmonary tuberculosis in pilgrims attending the 2015 Hajj from five high tuberculosisendemic countries was evaluated by a study randomly screening 1164 pilgrims, 36 of which 15 had previously undiagnosed active pulmonary tuberculosis. With millions of pilgrims visiting Saudi Arabia from high tuberculosis endemic areas, cases of undiagnosed active pulmonary tuberculosis will continue to pose a risk to other pilgrims. Further studies are required to define the scale of the tuberculosis during the Hajj and to develop proactive screening, treatment, and prevention guidelines. 92 Several studies also done on enteric pathogens during the Hajj show the emergence of antibioticresistant bacteria (appendix). 33, 34, 93 To prevent emergence of and spread of antibioticresistant bacteria, antibiotic pre scription and consumption by pilgrims at mass gathering events should be rationalised and regulated. Antibiotic stewardship and good prescribing practices should be promoted amongst healthcare providers.

Noncommunicable diseases constitute a large burden on health services at the Hajj. 10, 68, 94 They include diabetes, respiratory failure, myocardial infarction, cardiac failure, renal failure, chronic obstructive airways disease, prostate hypertrophy and urine retention, thyroid disorders, strokes, neuropsychiatric disorders, heatrelated disorders, traumatic injuries, ear, nose, and throat disorders, and eye ailments. The Indian Medical Mission studied 495 Indian pilgrims admitted for intensive care management during the 2016 Hajj and found that 262 (53%) had respiratory tract infections (53%) and 199 (24%) had traumatic injuries. 68 Risk factors associated with high morbidity were old age and preexisting comorbidities. A substantial number of pilgrims participating in the Hajj are elderly with preexisting chronic medical conditions. 10,75,95 These pilgrims might be unable to cope with the arduous rituals condensed into a 5day period and thus exacerbation of existing noncommunicable diseases occurs. 95

In contrast to mass gathering religious events, sport and music concerts 96 attract younger individuals of 15-25 years of age. Excessive alcohol consumption and recreational drug use increases the risk of intoxication and injury, extreme behaviours, such as fire jumping (jumping and dancing through flames), 97 sexual activity resulting in transmission of sexually transmitted disease and sexual assault. Additionally, loud noise at music festivals can lead to deafness, hence legislation is needed to keep to certain noise thresholds. 96

At mass gathering events the threat of mass casualty incidents from crush injuries due to stampedes, fires, accidents (eg, airplane crashes, motor vehicles, boat collisions, crane collapse), structural failures (eg, building or bridges collapses, terrorist attacks, and toxic exposures) is ever present, and their occurrences are unpredictable. Several mass casualty incidents have occurred between 2015 and 2018 (table; appendix).

For example, during the 2015 Hajj pilgrimage, two major disasters occurred, resulting in deaths of pilgrims (table) . The first occurred on Sept 11, 2015, in which a crane being used for construction to expand the area around the Grand Mosque (Masjid alHaram) in Mecca toppled over, killing 107 people and injuring 394. 31 The victims were from 12 countries. The second, on Sept 24, 2015, was a major stampede that occurred in Mina at the intersection leading up to the Jamaraat Bridge. This incident resulted in crush injuries and suffocation, with 769 pilgrim deaths and 934 injured. 32

Owing to the sudden and unexpected nature of mass casualty incidents, they pose major challenges to health services. Preparations for mass casualty incidents are mandated by the WHO and followed by the organising authorities. When these incidences occur, local investi gations and reviews are done to assess the underlying factors leading to the mass casualty incident, assess the effectiveness of the response, and identify lessons for inclusion in future mass gathering planning. 93, [98] [99] [100] [101] [102] Training and simulation exercises for healthcare workers in management of mass casualties and injuries should be done to assess preparedness and identify areas for improvement.

A 30year literature review 100 of 290 mass casualty events at mass gatherings showed that the most frequent mechanism of injury involved the movement of people under crowded conditions (162 [56%] of 290). Organisers of the Kumbh Mela and Hajj have used experiences of their mass casualty incidents to identify improvements to reduce risk at future events (appendix).

The Saudi Arabian Government has also put in place measure to reduce mordbidity and mortality from heat related disorders during the Hajj, as has the Japanese Government for the upcoming Olympics (appendix). mandatory and optional vaccines for prevention of infections are issued by local public health authorities and WHO. Occasionally authorities in countries of origin face hurdles in the implementation of these measures 87, 108 or issues regarding reduced immunogenicity of vaccines owing to interactions between vaccines. 109 Although no global outbreaks of meningococcal disease have occurred after the Hajj for decades, a high level of awareness of the possibility of outbreaks at all mass gatherings remains. 87, [110] [111] [112] Increasingly, attention is being focused on vaccinepreventable diseases, such as cholera, polio, measles, and pertussis, which are increasing in the eastern Mediterranean owing to conflict and large refugee populations with poor access to preventive public health services. 113 The highly lethal MERSCoV causes increasing concern, as it continues to circulate in Saudi Arabia 78, 79 and remains in the top ten of the WHO Research and Development Blueprint list of infectious diseases likely to cause major epidemics (appendix). 112

More countryspecific religious mass gathering events exist that attract smaller numbers of pilgrims from neighbouring countries or overseas. Several religious mass gatherings occur on a yearly basis in Iraq, drawing millions of pilgrims from across the country. The Arbaeen is one of the largest, during which pilgrims from Iraqi provinces visit the holy shrine in Karbala, and poses major public health challenges. 114 In West Africa, each year up to 5 million Muslim pilgrims from the Mouride community in Senegal, and from neighbouring countries, assemble in the holy city of Touba in Senegal for the Grand Magal religious pilgrimage, 115 the largest religious mass gathering in West Africa. This pilgrimage attracts individuals from outside Africa and has the potential for globalisation of local endemic infectious diseases. 115 Research on communicable diseases and non communicable diseases, including heatrelated disorders and disasters, with a more collaborative approach between local and international researchers and organisers of mass gathering events is required. The creation of the Africa Centres for Diseases Control (Africa CDC) on Jan 31, 2017, in Addis Ababa by the African Union, with five regional CDCs in Gabon, Egypt, Nigeria, Kenya, and Zambia, 116 creates major opportunities for improving coordination and public health capacitybuilding initiatives in partner ship with organisers of sporting and religious mass gathering events. For example, the majority of the live stock for ritual sacrifices during the Hajj are exported from Africa to the Middle East-the One Human EnvironmentalAnimal Health approach 101, 117, 118 should be adopted in partnership with Africa CDC. This initiative should aim to define the risk and threats to global health security and help reduce the risk of outbreaks and spread of zoonotic infections, such as Rift Valley fever, MERS CoV, Ebola, and other viral haemorrhagic fevers, across Mass gathering events provide unique opportunities for crosscontinental multidisciplinary collaborations on public health and basic science research, which will allow for the development of a strong evidence base for public health planning and health services at mass gatherings (panel 5). Although the formalisation of mass gatherings medicine has led to an increase in research studies into the specific health issues affecting pilgrims at the Hajj, they have not been forthcoming for the Kumbh Mela and other mass gatherings. Research output from mass gathering events is focused on small studies of pilgrims from individual countries and the data are not generalisable and no outputs have changed global policy. A need remains for more coordinated action by a global coalition of interested partners to share experiences from various mass gathering events, gather and translate appropriate evidence base into public health policy, and push for the best health promotion and educational policies. The creation of a mass gathering medicine specialist society or formal network, with a dedicated journal, might generate more frequent dialogue and enhance international collaborations on mass gathering medicine. A need also remains for high quality studies that are appropriately designed and adequately powered using participants from several geographical regions to provide data that stand up to rigorous scientific review. Advances in technology and closer monitoring of pilgrims at the Hajj provide an opportunity for large cohort studies of attendees of mass gathering events. These studies could focus on infectious diseases priorities, inc luding neglected tropical diseases, 119 noncom municable diseases, mental health, and heatrelated disorders (panels 4 and 5). The widespread availability of next generation sequencing, genotyping and phenotyping, and other molecular methods also provides an opportunity for analyses of health risks among large population cohorts and for appropriate research to obtain an evidence base for the production of WHOapproved guidelines, which will be useful for countries that host mass gathering events.

Structured, realtime interoperable surveillance and reporting systems are required for active surveillance of communicable and noncommunicable diseases during mass gatherings. 120 Given the continuing threat of new emerging and reemerging infectious diseases with epidemic potential, regularly recurring mass gathering events, can act as important global sentinel surveillance platforms. 120 Additionally, diseases targeted by WHO for elimination, such as polio, are still endemic in some countries from where individuals attending mass gathering events originate, offering opportunities for research and evalu ation of public health portfolios. 121 Increased investments in the field of mass gathering medicine are required to obtain an accurate evidence base for the development of accurate prevention, management, and control guide lines and optimal public health services to protect the health of attendees of mass gatherings and of host country populations.

ZAM and AZ ideated the review. ZAM and AZ developed the first draft outline. All authors contributed equally to the writing and finalisation of the manuscript.

ZAM, RS, OD, EIA, and AZ were members of the group that formalised mass gathering medicine as a discipline. ZAM and AZ were authors for and coordinated the 2014 Lancet Series on mass gatherings. ZAM was founding director of the Global Center for Mass Gatherings Medicine and the WHO Collaborating Center for Mass Gatherings, Saudi Arabian Ministry of Health. In the past 3 years, RS has received support to attend advisory boards or present paid lectures by GlaxoSmithKline, Mérieux (Fondation), PaxVax and Emergent BioSolutions, Pfizer, Sanofi Pasteur, Takeda, and Valneva, and he has been principal investigator or adviser to Aries, Clasado and Host Therabiomics, and Dr Falk Pharma. PW and AS declare no conflicts of interest.

",0.7040770343337763
Mass Gatherings and Public Health: Case Studies from the Hajj to Mecca,"B A C K G R O U N D Many new and challenging risks can be introduced during mass gatherings. The Hajj, as one of the largest mass gatherings, provides an excellent annual opportunity to reflect on the public health risk posed by international and multicultural crowds and the value of mitigation strategies.","The World Health Organization describes a mass gathering as ""An organized or unplanned event where the number of people attending is sufficient to strain the planning and response resources of the community, state or nation hosting the event,"" whereas the US-based National Association of Emergency Medical Service Physicians defines it as ""Spectators and participants at events in which at least 1000 persons are gathered at a specific location for a defined period of time."" 1 The Hajj, one of the world's largest mass gatherings, takes annually place in Mecca, Saudi Arabia. 2 In 2015, approximately 2.8 million Muslims from more than 183 countries visited this holy place for 5 days of rituals compulsory for all Muslims capable of undertaking the journey. 3 This challenge to the country perhaps is no less than organizing an Olympic every year. Saudi Arabia, being the custodian of the 2 holy cities (Mecca and Medina), provides extensive, multifaceted programs to serve these ""Guests of God."" However, the annual event is confronted with public health challenges besides immigration, crowd, and crime management. In addition to the formidable logistics, the risk for spread of infectious diseases is well recognized in the era of globalization. Contemporary outbreaks of Middle East respiratory syndrome coronavirus (MERS-CoV), Ebola virus, and Zika virus have drawn much attention to global health. The annual Hajj event definitely calls for precautions. This paper is informed by a review of the literature and case studies among 2015 Hajj pilgrims.

Infectious Diseases. The 5-day ritual causes extensive physical and mental stresses. Heat, sun exposure, thirst, crowding, traffic congestions, steep inclines, rough terrain, and cultural diversity present extensive stressors. During Hajj, pilgrims are also challenged by various prevention and infection control measures. Weather conditions, along with overcrowding within a confined area, make pilgrims also more susceptible to acquiring and spreading infections, particularly acute respiratory infections. 4 These major challenges have been witnessed when outbreaks of epidemics have occurred; for example, outbreaks of plague and cholera in ancient times involved large numbers of pilgrims, when quarantine was the prime means of control 5,6 ; a global meningococcal serotype W135 outbreak in 2000 was widely linked to either a recent return from the Hajj or contact with returned pilgrims 2,7 ; in recent years notable threats were avian influenza viruses, severe acute respiratory syndrome coronavirus, Middle East respiratory syndrome coronavirus (MERS-CoV), and multidrug-resistant tuberculosis. 8, 9 In 2012, the Hajj received political and scientific attention when MERS cases were detected in France, Germany, Italy, and the United Kingdom. MERS-CoV was first isolated from a patient who died of a severe respiratory illness in Jeddah, Saudi Arabia. 8 According to Bakhsh et al, 10 in 2 health care centers near the Holy Mosque, most of the patients were suffering from respiratory problems followed by skin and gastrointestinal problems during the 2013 Hajj season. ""Hajj cough"" was considered by pilgrims the most reported complaint, and 1 in 3 pilgrims were found to be affected. The severity and clinical spectrum of respiratory disease varies from mild sickness to severe pneumonia, hospitalization, and even death. 11 Unfortunately there are no comprehensive studies on the epidemiology of respiratory infections during Hajj; most investigations involved cross-sectional studies on a relatively small number of cases. 12, 13 Although governments of Hajji's countries of origin recommend flu vaccinations for Hajj pilgrimages before departure, most hajjis do not adhere to these recommendations. Influenza vaccine has been recommended since 2005 for all pilgrims, especially people at high risk, which includes those >65 years of age and people with preexisting medical or immune-compromising conditions. 14 Despite all these recommendations, the influenza vaccine acceptance rate among Hajj pilgrims varies by country and by year. Many pilgrims refuse vaccines; the main reasons identified were reliance on ""natural immunity,"" lack of awareness or knowledge, and lack of access to vaccines. Many people have misperceptions about vaccines, including that they contain toxic preservatives and are themselves the cause of disease. Moreover these vaccinations do not cover variants of different strains of influenza. According to Alborzi et al., 13 in their study on Iranian Hajj pilgrims, influenza vaccination could not prevent respiratory infections in pilgrims statistically (P ¼ . 19 ), but the consequences of this limitation remains less. Two hundred fifty-five pilgrims were examined for respiratory infections, and influenza virus was identified more in unvaccinated than in vaccinated pilgrims (16.5% vs 9.2%) in laboratory tests of their nasal swabs. 13 Another study among Malaysian pilgrims found that influenza vaccine was effective for 50%-60% in preventing hospitalization and pneumonia; it was 80% effective in preventing death in patients older than age 60. 15 Eventual vaccination failures might be explained by a new drift of variants or inappropriate vaccine handling and storage. * Therefore examining the circulating influenza strains is recommended in * For men, the special outfit for the hajj consists of 2 pieces of white, unsown cloth. One of the pieces is wrapped around the midriff to cover the lower body, and the other is draped around the shoulders to cover the upper body. Women must cover their bodies in a loose-fitting outfit and cover their hair. However, they are not permitted to cover their faces. addition to strict enforcement of vaccination policy. 13, 15 Among other precautionary measures, the use of a face mask is considered by some pilgrims religiously prohibited while wearing ihram. Some consider using alcohol-containing products (eg, hand sanitizers) as prohibited during Hajj ritual too, although recently religious scholars have allowed the use of alcohol for medical purposes. However, the challenge remains to endorse use of mandatory face masks in the crowd and organize screening at arrival. 9 The Saudi government faces extreme pressure during the time of Hajj because there are multiple routes of entry for pilgrims. 16 Therefore it is recommended to enforce the law on compulsory vaccinations and control for dishonesty and corruption by the home countries as prerequisites for visa processing. Also, a supply of adequate vaccines at limited or no cost is strongly suggested. 14 Skin infections among pilgrims are very common because of lengthy rituals of standing and walking, heat, and unfavorable conditions. 17 Primary pyoderma (including impetigo), carbuncles, furuncles, folliculitis, pyoderma complicated eczema, and cutaneous leishmaniosis are commonly found. Pilgrims are barefoot while walking in some holy places, or emotionally climb some rocky places. Accidental injury, severe burns, and soreness of the sole are common occurances, 18 and these kinds of injuries increase the chance of secondary skin infections, especially among elderly and comorbid people. Noncommunicable Diseases. Many studies and the Saudi Arabian government's health statistics have indicated that noncommunicable diseases, particularly cardiovascular diseases and diabetes, pose critical health challenges for pilgrims and the government's health system during Hajj. [19] [20] [21] Pre-hajj medical states, being of old age, rigorous physical activities, exhaustion, fatigue, heat, and trauma are related to and cause exacerbation of chronic noncommunicable diseases during Hajj. 19, 22, 23 Cardiovascular diseases were the main cause of hospitalization and intensive care. Almalki 24 reported in his Hajj study that out of 110 inpatients from 20 different countries, 34% had ischemic heart disease, 20% had elevated blood pressure, and prevalence of stroke was 17%. According to Madani et al, 19 more than 60% of the intensive care unit admissions in 7 hospitals in Mina and Arafat were due to cardiovascular origins. Among these cases, myocardial infarction and left ventricular failure were of the highest occurrence. 19 Furthermore, cardiovascular events were the major cause of death during pilgrimage. The review study of Al Shimemeri 25 confirmed that cardiovascular diseases during the Hajj period caused more death than other communicable and noncommunicable diseases. In 2008, 66% of deaths were due to cardiovascular diseases out of 446 deaths among Indonesian pilgrims. 26 Many factors been identified that lead to poor diabetes control during Hajj. 27 A study by Khan et al 21 revealed that 31.9% of 689 emergency patients had diabetes; meanwhile 34 patients were diabetic among 160 acutely hospitalized patients.

Mortazavi et al 28 observed that in the 2012 Hajj, 106 Iranian patients were referred back to Iran for health reasons. Among them were 13 cardiac, 28 psychiatric, 11 neurologic, 4 endocrine, and 4 nephrology patients. 28 Crowd Safety. Crowd management requires enormous skills, experience, and efficiency. Research on crowd psychology has been going on since at least Gustave Le Bon's work in the 1890s, and more advanced techniques and models have been used by the entire world during the last 20 years. 29 Hajj is one of the most crowded gatherings of all, and it is expected that the number of pilgrims will increase by 10% each year. Furthermore, most of the pilgrims are from underprivileged countries and are elderly, poor, and illiterate, and the Hajj is often their first international trip. 2, 16 The major concern of this mass gathering is that the entire Hajj event must be completed over a limited and fixed schedule of only 5 days. The rituals are performed in a specific unchangeable sequence following a fixed route within a geographic area that does not exceed 4 km 2 . 16 Such rigor and strictness leads to a series of disasters almost every year, keeping pressure on the authorities.

The Holy Mosque covers an area of 356,800 m 2 , including the outdoor and indoor prayer spaces. Its total capacity is about 2 million people. When fully occupied, the buildings could accommodate an average crowd density level of 4 people per square meter (4 ppm 2 ). However, at certain locations, and closer to the Kaaba, the level of density becomes 6-8 ppm 2 and 12 ppm 2 . A similar crowd density is observed near Jamarat, and incidences like stampedes occur commonly. 30 The Saudi government has involved religious scholars, administrators, immigration and security personnel, health officials, and other officials to Three stone pillars that are pelted as a compulsory ritual of hajj in imitation of the Prophet Abraham (Peace on Him). manage the Hajj. In spite of all these efforts, several disasters have taken place every year, such as stampedes, fires, bottlenecks, and more. Although during the past few years several scientific and technological efforts have been applied to assist in the management of crowds, in 2015 a devastating stampede occurred. The use of crowd simulation models, assessment of the efficient ways of grouping and scheduling pilgrims, luggage management, video monitoring, and changes in the transport system are the latest measures to improve the management of the pilgrimage event. 31 There are many methods for modeling and simulating pedestrian crowds followed worldwide, such as agent based, social force, cellular automata, fluid dynamic, fixed GPA monitoring and queuing models, and so on. Pedestrian crowd dynamics have different scales, such as the microscopic scale, dealing with individual pedestrians, and the macroscopic scale, dealing with the characteristics of a crowd. These methods have shown several self-organizing principles about the patterns of crowd phenomena; corresponding patterns have been noted in real crowds. These included macroscopic crowd patterns that result from local interactions of multitudes of pedestrians at the microscopic level. 29 Some studies reported that people moving in groups had positive psychological effects, including less feeling of being lost, and that proper guidance from a group leader had a positive effect as well. 30 In absence of these factors, real crowds are perceived to pose a threat. However, updates on the crowd flow must be announced and displayed by electronic boards at different parts of the roads to assist in crowd management. Heat-Related Sickness. The ambient temperature in the Kingdom of Saudi Arabia remains very high, particularly during summer. Even during winter, the temperature is higher than in many parts of the world. Weather during Hajj season shows extreme variations, with average temperatures >38 C during daytime and >25 C at night and a monthly rainfall averaging 3 mm. 4 The heat island effectdthat is, heat radiance from vehicles, highrise buildings, pitched roads, ongoing construction, dust particles, and noisedis a recent concern in both Mecca and Medina.

The pilgrims come from different countries worldwide, and their acclimatization to the local temperature is equally diverse. Therefore, exhaustion syndromes may or may not be accompanied by increased body temperature. With a body temperature of 40.0 C (105 F), one person may be able to walk to a clinic for help, whereas another person might die after heat stroke. Moreover, supplicating pilgrims remain unware of the extreme heat exposure until symptoms are noticeable. Addressing the diagnostic criteria of heat stroke thus remains challenging for Hajj pilgrims. 32 Although water mist sprayers are operated in the desert of Arafat, those places remain highly crowded. However, there are various measures that can be applied in performing rituals (eg, using an umbrella, applying sunblock creams, performing rituals at night). 17 

Objectives. This paper describes gaps between preexposure preparation to the mass gathering and postexposure experiences, as well as the disparities between expectations and the reality of the holy place.

We collected 4 case studies by using a semistructured questionnaire facilitating in-depth interviews among Hajjis who were purposively selected from Bangladesh, Myanmar, New Zealand, and Pakistan. Participants were selected following critical case sampling method. Questions covered pre-exposure preparations and postexposure experiences, as well as health and safety concerns. Interviews were audio-recorded in the interviewees' own languages and transcripts translated into English. Case Studies. Pakistan. A 60-year-old retired government officer of the Water and Power Development Authority of Pakistan and electrical engineer went to perform Hajj by government quota with his family and friends. He and his group were vaccinated against polio, influenza, and meningitis 1 week before departure, and vaccination cards were provided along with visas.

On arrival at Jeddah International Airport at Dhur prayer, they had to wait 3 hours for transportation to Mecca. Accommodation was arranged at Mecca 16 km from the Holy Mosque. However, rooms were overcrowded and lacked proper ventilation. Washing machines at a laundry area were provided, and although pilgrims were guided to dry their clothes on the top roof of the hotel building, many pilgrims dried their clothes inside the rooms. Medical facilities and food supply and delivery were inadequately available and not properly managed in the vicinity of the accommodation in Mecca, but in Medina, they were managed properly. The transport system was not properly arranged either. The respondent and his group had to change 2 buses with pilgrims from various other nationalities to reach haram 5 times a day. To avoid this hassle, they tried to spend most of time inside the haram. There was a lack of enforcement on the use of mataf. z Although there were different mataf areas for wheelchair users, people tried to conduct circumambulation with wheelchairs in the same mataf area, and as a result, many accidents happened. A couple from their group witnessed a crane crash accident in haram, but they survived. The pilgrimage route between Mina, Arafat, and Muzdalifah, presented in Figure 1 , posed multiple challenges for pilgrims. The respondent, with his family, had to stay inside a heat-resistant cloth tent for 5 days in Mina. However, tents in Mina had few cooling facilities and were overcrowded. The temperature in Arafat was around 55 C, and they had to stay in open tents without any cooling system. They spent the night in Muzdalifah in an open space; however, the place was overcrowded and untidy. Sanitation was available with sufficient facilities, but cleanliness was not well managed; there was garbage and litter all around. Transportations were overcrowded with poor safety arrangements; many got injured while boarding. Because all Hajj rituals had to be performed at specific times and places and all pilgrims wanted to complete the pilgrimage route as early as possible, there was obvious struggle among all. Many pilgrims traveled long routes on foot. In Mina, many pilgrims experienced heat stroke, and many were killed in a stampede at Jamarat during the stoning of Satan. The respondent thinks extreme hot weather, overcrowding, language barriers, and challenged pilgrimage management made the Hajj difficult.

Myanmar. A 30-year-old medical doctor performed his fourth Hajj from Myanmar in 2015. He went as a medical officer recruited by a hajj service company. He prepared medicines and medical requirements for more than 600 pilgrims. He gave a 15-minute health session to them during a dinner gathering hosted by the company. The session was too short to cover all health-related matters. Despite mandatory cholera and meningococcal vaccinations for visa application, nearly all Hajj agents including his company provided fake vaccination certificates. He led a 120-pilgrim group to Mecca. The hotels at Mecca were next to Kaaba and were well furnished, but rooms were overcrowded by sneaking in extra beds. In case of emergency such as a fire outbreak, people might get into trouble. Moreover, the condition contributed to increased risk of transmission of airborne diseases. For many pilgrims, the visit was their first international trip and they had never experienced such a huge gathering of people before.

In the Kaaba compound, many tower cranes were assembled at the time of the pilgrimage. Although the crane operations were temporarily shut down, pilgrims were worried about the hazard of cranes after a crane accident occurred earlier. Squeezed into a corner of the holy black stone, with vigorous pushes and wild rushes, was a dangerous place for the old, the sick, and children. Surprisingly, there were some hazardous places like collapsed billboards and unhygienic out-of-order public toilets in walking distance from the holiest mosque. The holy city's waste management was very good, yet sorrowfully, many pilgrims did not care about being at holy places and they littered carelessly in public places.

At Mina, the camps and toilets were very crowded. At Arafat, camps were also crowded and there was no electricity supply. The camp had a mud floor. The weather was too hot on the day of Arafat. Thus, people suffered from heat exhaustion, back pain, and exacerbation of hypertension. Leaving to Muzdalifah, routes were heavily crowded by pilgrim pedestrians. This posed a potential challenge causing delay for the emergency medical teams to reach patients. Although security squads were deployed, their number became significantly increased only after the devastated stampede at Jamarat.

In Medina, the conditions were relatively comfortable in all aspects, particularly good accommodation and fewer crowds. One very dangerous attempt of pilgrims during visiting Islamic historic sites was climbing steep sides of the Hira Cave § wall, eager to reach the cave, and once inside pushing to perform supererogatory prayers.

Bangladesh. A 36-year-old young government officer of Bangladesh Army's engineering core said that his group didn't receive any vaccinations because their visas were done with fake vaccination cards. Yet they arranged meningitis vaccines by themselves on the advice of experienced persons. The accommodations at Mecca and Medina were not up to their expectations and the deposit they provided to the agency. The rooms were tiny and overcrowded, with no or minimal ventilation. No arrangement for drying wet clothes made the room damp and unhygienic. The building had no elevator and no emergency exit. The electric cables and wirings had no secure fittings, posing an increased risk of short circuit. Littering on the road was found beyond limitations. For the reasons of convenience and avoiding various challenges, the respondent spent most of the time inside the Holy Mosque, and thus he witnessed the most tragic accident of a crane collapsing. Being an army officer, he was better equipped to cope with casualties and volunteered to assist the injured. He found many warning signs stating ""Construction Site, Do Not Sit"" inside the mosque, but enforcement of those warnings was lacking. Escalators were used by elderly in wheelchairs besides pedestrians. Because of overcrowding and neglect of regulations, he witnessed the fall of one old person rolling down with his wheelchair. There were different and tidy arrangements for circumambulation of wheelchair holders, but many pedestrians used the same circle in front of law enforcers. The front wheel of wheelchairs thus hurt the backs of the ankles (Achilles tendons) of many pilgrims.

From the respondent's perspective, Mina and Arafat were the most mismanaged places. Pilgrims from developing countries were assigned to stay in overcrowded tents with a low-capacity air cooler. In Arafat, the tents were open without any cooling facilities. One of their groupmates died from heat stroke.

New Zealand. A New Zealand pilgrim went to Hajj alone with an agency. According to the respondent, vaccination and doctor's certificate were mandatory for visa approval. The agency didn't provide any health training or Hajj-ritual education. On arrival in Mecca, accommodation and transportation were facilitated in a well-organized manner. They were provided a 5-star hotel in the vicinity of the Holy Mosque, both in Mecca and Medinah. Even in Mina, Arafat, and Muzdalifah they were provided air-conditioned tents with clean-hygiene sanitary facilities, although he was of the opinion that it could have been improved. A doctor visited their tents and provided medicines. In his language, the ""NZ-AUS Hajj camp was of superior quality. I went for Hajj for spiritual purpose. If someone remains calm, the spiritual side can be felt by overcoming worldly limitations.""

In short, our study clearly revealed dissatisfaction and discontentment regarding the management among Hajj pilgrims from developing countries. This sacred ritual was hijacked as a business opportunity by broker agencies of developing countries, infested with corruption, dishonesty, and greed to make profit from pilgrims. Implementation and enforcement of law from both the custodian country and sending countries were found to be weak in several aspects. Notwithstanding the custodian country's adoption of various recent research insights, techniques, and models to enhance security and safety, and arrangement of relevant annual conferences and capacity-building initiatives, a significant gap was found between planned and actual implementation strategies. A discriminatory and judgmental attitude of the custodian country officials was encountered toward pilgrims from developing countries.

Case studies adopt a retrospective approach; therefore, a potential recall bias among respondents should be considered. We also acknowledge the limitations of a purposive sampling that may enhance the risk for selection bias. Interanalyst reliability testing was employed to minimize potential bias in the analysis. In the absence of post-travel surveillance in respondents' home countries, follow-up on the post-travel period except about their own was not feasible. The value of our study § The place where Muhammad PBUH is believed to have received the first revelation of the Qura'n.

is that it gave a voice to Hajj pilgrims themselves and offers a benchmark in revealing practices that undermine health and safety of pilgrims from developing countries.

Although Saudi Arabia keeps pilgrims' health and safety as their top priority, inevitably causalities do happen. Many recent incidences and risks of morbidity and mortality suggest that it is not the sole responsibility of one country but should be attributed to all home countries of pilgrims. Especially to meet the challenge of infectious diseases, a well-coordinated global approach is priority. Epidemiological modeling at the microscopic level is suggested by many researchers and already monitored in other mass gatherings, such as Notting Hill Carnival. Dishonesty related to vaccination and treatment ultimately endangers the whole world as illustrated in the literature. Henceforth international collaboration, law enforcement, and research are vital to ensure accessibility of vaccines and prescribed medications for the pilgrims. Capacity building within host and sending countries focusing on prevention of infectious disease, heat-related sicknesses, and injuries before Hajj are important and should be adopted worldwide. Hajj causalities data should be shared accurately; and the custodian country should welcome global participation and deal with criticism in a constructive way. Furthermore, an international event of this magnitude such as the Hajj creates communication challenges. To minimize language barriers, law enforcers must be proficient in English, and written instructions or directions should make use of main global languages. 29 Hajj is a unique mass gathering provides excellent is annual opportunity to reflect on the public health risk posed by international and multicultural crowds and the value of mitigation strategies. New and challenging risks can be introduced during mass gatherings. From a global health and human security perspective, strengthening of core capacities in managing mass gatherings and researching risks posed by such gatherings are paramount to safeguarding the public's health.

",0.6761454502066123
Mass Gatherings Health 6 Research agenda for mass gatherings: a call to action,Public health research is essential for the development of eff ective policies and planning to address health security and risks associated with mass gatherings (MGs). Crucial research topics related to MGs and their eff ects on global health security are discussed in this review. The research agenda for MGs consists of a framework of fi ve major public health research directions that address issues related to reducing the risk of public health emergencies during MGs; restricting the occurrence of non-communicable and communicable diseases; minimisation of the eff ect of public health events associated with MGs; optimisation of the medical services and treatment of diseases during MGs; and development and application of modern public health measures. Implementation of the proposed research topics would be expected to provide benefi ts over the medium to long term in planning for MGs.,"A mass gathering (MG), as defi ned by WHO, is ""any occasion, either organized or spontaneous, that attracts suffi cient numbers of people to strain the planning and response resources of the community, city or nation hosting the event"". 1 MGs can be spontaneous or organised and include sports events, social or cultural functions, gatherings of displaced populations due to natural disasters or war, and political or religious congregations. [2] [3] [4] [5] Major MGs are likely to involve communities from diff erent parts of the world. Participants and host and home communities face health risks that are of international concern. Therefore, the control of infectious diseases and provision of medical services for non-communicable dis eases and other risks to health are increasingly important in the planning for MGs. Risks commonly associated with MGs include health systems having to accom modate a surge in capacity; challenges to existing health interventions; introduction and transmission of non-endemic diseases during and after MGs; diffi culties associated with risk communication to participants from diff erent cultures; and those arising because of the high profi le of some events, such as security risks.

International MGs can amplify the transmission of infectious diseases, and infections can spread from the home to the host community during travel to and from the event and from the host to the home community on return. 6 Such events can pose substantial risks to global health security and present challenges for surveillance of the spread of such diseases to new ecological settings and vulnerable populations. 7 In addition to risks of infectious diseases during MGs, non-communicable risks include cardiovascular diseases, environment-related heat injury, fi re-related injury, illnesses related to use of drugs and alcohol, occupational injuries, trauma or crush injuries associated with stampedes, exacerbation of respiratory diseases, and crowd safety. 8 Large MGs can also provide opportunities for terrorist activities.

In planning for MGs, international and local health authorities need to ensure that the strengthened public health systems and rapid responses to health risks are integrated with other important components of the overall event management. Eff ective public health policy should be based on evidence.

The organisation of international MGs generally requires provision of huge amounts of resources by the host country and the dividend of such high visibility is called legacy. This legacy should be measured not just in terms of absolute improvements (eg, new roads and ambulances) but also in terms of improvements to the public health system and society as a whole. For example, the legacies of strengthened integration of clinical or laboratory services, early warning or syndromic surveillance, and fi eld epidemiology or response adopted for the past summer Olympics (Sydney, Australia, Athens, Greece, and Beijing, China) [9] [10] [11] are used routinely in the health systems.

The development of eff ective methods for, and improvement of, planning and handling of the health risks associated with MGs will strengthen global health security, prevent excessive emergency health problems and associated economic loss, and mitigate potential societal disruption in host and home communities. Such development requires credible evidence to support activities that can reduce the global eff ect of infectious diseases and address local public health issues related to morbidity and mortality resulting from noncommunicable diseases during MGs. However, there are gaps in knowledge about many public health issues that contribute to eff ective planning. 6, 12, 13 Therefore, a robust knowledge about illnesses, from basic scientifi c understanding to societal eff ects of infections and noncommunicable diseases, is essential for modern public health practices and policy development related to the planning for MGs.

Several reports [11] [12] [13] [14] [15] and WHO planning and guidance documents 1,16-18 have drawn attention to the importance Series of research into public health issues associated with MGs and identifi ed those that need immediate attention. Despite these eff orts, an overarching research agenda based on public health to address the gaps in knowledge in MG health has not yet been developed. Moreover, international coordination to prioritise and enable the funding and implementation of such an agenda has been lacking. The recent recognition and rapid development of MG health can provide focus on such issues.

Previously identifi ed research priorities for MGs tended to focus on logistical issues relating to site security and emergency management, crowd control, and surge in the need for medical services. 12, 13 Other specialties tend to focus on each of the specifi c public health topics that are associated with MGs. Much research into existing and emerging infectious diseases is devoted to the development of rapid diagnostic methods, surveillance and response, and treatment and vaccines. Although a research agenda based on public health must be underpinned by basic science, applied science and operational research are areas of particular interest to the organisers of MGs and the decision makers for public health, especially those in poorly resourced countries. The modern day idea of MG health has developed from the notion that ""mass gatherings medicine is concerned with the provision of emergency medical care at organized events with >1000 people in attendance"" 18 to include several specialties (fi gure). The intricate interactions between the diff erent specialties is essential for the planning and success of MGs. New advances or interventions undertaken in other specialties should be integrated with research into MGs.

The proposed research agenda is a broad strategy for research into public health, with a focus on issues related to MGs. It is not intended to be an exhaustive compilation of all possible research questions about the strategic planning for and operation of MGs. Instead the agenda is an outline of key research into methods to control public health and policy. The results of this research can provide an evidence-based platform for policy decisions and practices to reduce the risks and eff ects of MG-associated health issues and global security risks to public health.

The principal objectives of this research agenda are to identify topics for research and underpin and prioritise their importance in achieving interventions for the control of public health; provide a research framework to gather evidence to address health issues associated with MGs and global security risks to public health; ensure focus on less well addressed issues such as operational and implementation research, particularly for under-resourced regions; provide a platform to enable co ordination, discussion, and interaction among organisers of MGs, public health professionals, and researchers; and encourage a multidisciplinary approach to address gaps in knowledge about health risks associated with MGs and their control.

The proposed research agenda is organised as a framework of fi ve major public health research directions.

Although many public health emergencies associated with MGs are not predictable, much can be done to prevent and minimise their eff ects. Continuous monitoring of participants' vulnerability to health risks at MGs and understanding trends in risks that are associated with specifi c events (religious, sports, or concerts) can be used to predict what might happen in the future. They are also essential for successful preparedness and management of risk reduction and strengthening the response capacity of host and home communities. Recognition and analysis of the changing risks and vulnerabilities during MGs are starting points for raising awareness and communication of pending risks. Building global capacity for health intelligence for noncommunicable and infectious diseases is important for the elucidation of the risks associated with MGs. Most of this information is available through networks such as the emerging infectious disease networks 19, 20 and the WHO Global Non communicable Disease Network. 21 Importantly, the information can be used to implement strategies for risk assessment and mitigation in planning for specifi c MGs (panels 1, 2). 

Morbidity and mortality at MGs can be mitigated through the assessment and management of risks associated with pre-existing non-communicable diseases. Mitigation methods such as the provision of essential drugs and information about their availability at the MG can be initiated during pretravel medical care and advice. 5, 26 Incidence of trauma and heat-related illness at a site can be reduced with the provision of advice and installations to combat the eff ects of weather, and eff ective crowd control. 5 Models for the prediction of the spread of infections and occurrence of other emergency health issues during MGs have yet to be validated. 7 Many of the diffi culties in restricting the spread of emerging communicable diseases [6] [7] [8] 23 are not new and have proven diffi cult to resolve. There is also uncertainty 

Global clinical and laboratory surveillance systems for communicable diseases, such as those for seasonal and pandemic infl uenza, 22 are well established. The establishment of country-level surveillance systems for infectious diseases that can be adapted to diff erent epidemiological settings for mass gatherings (MGs) could also provide alerts for the occurrence of non-communicable diseases, such as radiation-related or chemical-related illnesses or those caused by extreme environmental temperatures (eg, Program for Monitoring Emerging Diseases). 23 An important component of the alert and response strategy is an integrated event management system that provides a platform for rapid dissemination of devices and procedures required for the management of health risks. Research into their development and implementation is needed. Syndromic surveillance can potentially provide rapid initial information about the occurrence of both non-communicable and infectious diseases. However, its establishment and assessment of eff ectiveness during MGs might require further assessment. Further work is also needed to identify appropriate parameters for assessment of the eff ectiveness of such surveillance systems during MGs.

Surveillance of non-communicable diseases is a formidable but necessary step for the improvement of the health of the global community. An estimated 63% of global mortality in 2008 was attributable to non-communicable diseases and 80% of such deaths occurred in low-income and middle-income countries. 24 Age-specifi c and sex-specifi c profi les of non-communicable diseases by country 25 allow host countries of MGs to estimate possible risks of non-communicable diseases in participants from specifi c countries and plan for mitigation strategies.

Although the challenges for the organisers of MGs and the eff ects of infectious diseases at such events have been summarised, 6, 14 there are many gaps in our understanding of emerging communicable diseases. Integration of information from local and international surveillance of infectious diseases is important for strengthening the intelligence about the global threats before, during, and after MGs. 23

The risks of non-communicable and infectious diseases during MGs are proportional to the probability of occurrence of risk factors during the event. The identifi cation of these risks factors for the diff erent types of MGs will provide a scientifi c basis for planning eff ective prevention. Although major risk factors associated with non-communicable and communicable diseases are likely to be similar worldwide, factors specifi c to the type of MG might lead to health problems. Systematic risk assessment helps identify potential risks of outbreaks and guides the establishment of eff ective risk management solutions. Systematic assessment will also identify potential or deliberate health security risks that require assistance from other authorities and government agencies.

The leading causes of morbidity and mortality during the Hajj are heat-related illnesses and trauma-related injuries. Identifi cation of such risks allowed event planners to instigate preventive measures and rapid response strategies. Provision of shaded areas can reduce the incidence of heat-related illnesses and eff ective crowd control reduces the risk of a stampede. Drug and alcohol use were identifi ed as health risks for other types of MGs; 26 therefore, restriction of their use can mitigate the associated illnesses. The types and magnitude of health risks associated with spontaneous MGs due to natural disasters and confl icts are diff erent from those of organised MGs. Objectives for risk management at such events are focused on facility-based health-care provision in addition to prevention.

The potential for importation and subsequent global spread of infectious diseases during MGs are well understood. Many emerging human infections are recognised as zoonotic diseases (eg, severe acute respiratory syndrome [SARS], infl uenza A H5N1, Nipah virus infection). The emergence of novel or rare pathogens in home communities and their subsequent spread to the host community and beyond can be amplifi ed during MGs.

Planning for the potential risks and hazards that are associated with MGs is essential to ensure success. Many reports and manuals are available for planning MGs; 1,27-32 however, their use should be tempered by the results of the risk assessment. Importantly, planning should maximise the legacy of the MG. The conceptual model of a lasting public health legacy as a framework for the relation between planning inputs, implementation, and public health outcomes was put forward by WHO and the International Olympic Committee to ensure sustainable, positive health eff ects for the host communities after the Olympic Games. 11 Legacy planning should also include passing the knowledge gained to future hosts of similar MGs.

Series about how observations pertaining to particular pathogens, population groups, or settings can be used to develop public health policies for planning diff erent types of MGs. The development of evidence-based strategies for non-pharmaceutical inter ventions is urgently needed to address infection control and mitigate spread in the absence of available drugs and vaccines. Additionally, such strategies are of particular concern for countries that do not have adequate access to pharmaceutical interventions such as vaccines and antimicrobial drugs. In some instances, available data for planning MGs might not have been assimilated in the best way for policy. A balance between basic scientifi c research and operational research is essential to inform the implementation of prevention strategies, best practices, and public health decision making (panel 3). Research into how to contain the spread of infectious diseases should have the broadest possible applicability in diff erent settings and at diff erent resource levels. However, some results might not be generalisable to the planning for MGs, such as those from studies of pathogen transmission in health-care settings.

Eff ective management of health risks for noncommunicable and infectious diseases during MGs requires planning in advance. It is an integral part of planning that consists of risk identifi cation, communication, analysis, assessment, prevention, and monitoring. 23, 28, 29 Many of the processes for risk management of non-communicable and infectious diseases during MGs are common. However, each can be specifi c to the type of MG and needs to be addressed accordingly. 8, 26 High visibility of MGs complicates risk management and can lead to political and media pressure and thereby aff ect the decision-making process.

Prevention of the occurrence of non-communicable and infectious disease at MGs requires coordinated risk assessment and management before, during, and after the event as shown in the planning for the Hajj. [2] [3] [4] [5] [6] 26 Ideally, the primary prevention of human infections with emerging communicable diseases is the eff ective control of pathogens at their source. Since at-source elimination of all emerging pathogens is not possible, secondary interventions (eg, pharmaceutical or nonpharmaceutical) are needed to mitigate the spread of infection during MGs. However, the eff ectiveness of such interventions has not been established. 33, 34 In addition to reduction of the rates of morbidity and mortality associated with human infections during MGs, reduction of both the circulation of pathogens and human exposure might lessen the global health security risks.

The eff ect of emergencies and crises on health can be substantially reduced if home and host communities are well prepared and are able to reduce their risks. The main challenge during MGs is the existence of systematic operative capacities such as risk assessment plans, coordinating mechanisms and standard procedures, institutional capacities, legislation and budgets, skilled Vaccination is highly eff ective in the prevention of infectious diseases. However, many countries, particularly those with insuffi cient resources, have not developed strategies for vaccinating their populations at risk and people travelling to MGs. The reason is partly related to the lack of information about the transmission of infectious diseases (eg, infl uenza) and the social, economic, and health eff ects to the host and home communities. Public health authorities need to decide how to eff ectively prioritise vaccine use on the basis of available information about disease burden and severity, epidemiology, and vaccine eff ectiveness and safety for vaccine-preventable infections associated with MGs. 6 A failure to promote and implement the polio vaccination programme caused the re-emergence of poliovirus in Nigeria and subsequent international spread, 35,36 emphasising the risks associated with insuffi cient vaccine coverage for participants at MGs. An outbreak of Neisseria meningitides serogroup A (originating from Africa) during the Hajj in 1987 37 was later successfully controlled with the introduction of mandatory pretravel vaccination and use of fl uoroquinolones among African pilgrims. 38 However, there are infectious diseases, including some of the most important and most dangerous, for which there are no vaccines.

Risk assessment and management during the planning for MGs can enable the development of eff ective health policies. Strategic risk assessments are used to gather, coordinate, and analyse data that are necessary to identify existing risks, anticipate potential diffi culties, establish 

• Enhance applications of existing vaccines against possible infectious diseases that are associated with MGs • Assess the global vaccine supply and production to improve the processes of rapid response, surge in capacity, and rapid deployment and tracking of vaccine use for planning MGs • Develop innovative clinical trial methods to study the eff ectiveness and safety of novel vaccines before and after licensing • Develop new vaccines, platforms, and formulations that are safe with enhanced immunogenicity, especially in children and elderly people Series priorities, and provide the basis for enacting targeted policies and implementation of corrective interventions.

A system is needed for the measurement of the eff ect of public health policy and estimation of the probability of success. These interlinking processes are well described for regularly organised events such as the Hajj and Olympic Games. 5, 11, 14 An example of the eff ective development of health policy is the organisation of the Hajj-such as a smoke-free environment for the prevention of fi re, structural changes to prevent crowding, and recommendations for vaccination of pilgrims to prevent transmission of infectious diseases (eg, infl uenza, meningitis, poliomyelitis). 5 However, such eff ective policies are not possible for spontaneous MGs such as population displacement as a result of natural disasters or confl icts.

Development of vaccines for emerging infectious diseases presents substantial challenges and can take many years for diseases that are caused by novel pathogens such as severe acute respiratory syndrome (SARS; panel 4). Even if a vaccine exists, it might need to be regularly updated, clinically assessed for safety and effi cacy, and promptly produced for immediate use (eg, infl uenza vaccines). The effi cacy and eff ectiveness of a vaccine are dependent on the immune responses that are determined by the age of the recipient and composition of the vaccine (eg, conjugated or adjuvanted). Improvements to vaccines and formulations that can provide longer-lasting and broader activity aff ord better protection, increase the applicability of vaccines, and reduce the frequency of vaccination. During an outbreak, the important factors are the rapid production and equitable distribution of vaccines to countries in need.

Ensuring rapid and eff ective management of patients and prevention of diseases requires robust health services at MGs. Providers of emergency services play an important part in ensuring public safety during such events. Knowledge and monitoring of medical service provision during MGs has been rapidly increasing in the past decade. 12, 13, 39 However, a lack of consolidated data for diff erent types of MGs means that organisers are not able to plan accordingly for the emergency medical services that might be needed. Improved and targeted clinical management and infection control can substantially reduce the incidence and transmission of infectious diseases during MGs. Optimum clinical management must be based on an improved under standing of the pathogenesis of these infections, advances in laboratory diagnosis, development and application of eff ective antimicrobial drugs, and other treatment modalities (panel 5).

There are many gaps in our basic understanding of how many of the pathogens that are associated with MGs cause disease in people and what factors aff ect severity of illness. Host immune responses, underlying comorbidity, age, and the properties of the infecting pathogen can all contribute to severity.

The clinical presentation of many infections, such as infl uenza, is not specifi c, which makes diff erential diagnosis and early treatment to reduce further transmission and severe outcomes diffi cult. For example, antibiotics can help control severity and further spread of travellers' diarrhoea caused by bacteria. Rapid and reliable diagnostic testing can expedite the initiation of timely and appropriate treatment and infection control. Increase in and optimisation of the repertoire of antimicrobial drugs immunomodulator drugs, immunoglobulins, and natural products) that are applicable in low-resource areas and in fi eld conditions (such as availability, whether licensed or not, acceptance, and effi cacy in diff erent ethnic, sex, and age groups) and are easy to administer in paediatric-care and emergency-care settings • Optimise management of people who are at risk of severe disease and complications, including emergency-care practices that are applicable across a range of resource settings

Health-care capacity and response • Assess the eff ectiveness of global, national, and local responses to outbreaks of communicable diseases and develop new methods for assessment • Undertake operational studies to investigate the surge capacity needs, particularly in host countries for MGs, including development of triage schemes in diff erent health-care and resource settings, and surge planning to maintain adequate resources • Undertake studies to identify evidence-driven clinical-care pathways and principles that optimise health-care delivery in a range of resource settings • Undertake studies to develop principles and practices for rapid assessment and introduction of new interventions during health emergencies, including systems for collation, sharing, and assessment in real time of clinical data Series and development of clinical research to assess effi cacy of putative adjuvant treatments such as immunomodulator drugs, passive immuno therapy, and traditional medicine that are suitable for use in under-resourced areas would be most benefi cial in the preparation for MGs (panel 5).

The availability and quality of health services contributes to the eff ect of infectious diseases in the source and home countries (panel 5). The same pathogen that might have a small eff ect on the rates of morbidity and mortality in countries with well organised health-care systems can be devastating in countries where health-care systems are suboptimum.

New public health methods need to be harnessed to help reduce the eff ect of health problems during MGs. Use of innovative communication channels, such as the internet and mobile phone networks, have the potential to aid surveillance, rapid risk assessment, and dissemination of accurate information. 23, 40, 41 Mathematical modelling and risk communication have potential applicability in all aspects of research into health risks associated with MGs.

Some countries and MG organisers use state-of-the-art approaches for early detection and monitoring of diseases such as syndromic surveillance. 3, 29 In some countries computerised health-care and laboratory-based infor mation systems are used for planning MGs and these systems can be adapted for monitoring large-scale outbreaks. Other innovative technologies such as mobile phones can be used in remote areas or countries that lack the resources to gather and transmit health-related data in real time, provide rapid feedback, and train health-care workers. 7, 23 Applicability and use of these modern methods of monitoring in diff erent settings and contexts require further investigation, with special attention to issues related to integration and interoperability of initiatives for infection control during MGs (panel 6).

Evidence-based public health decision making in planning and mitigating health risks requires rapid access to information. However, such information is often incomplete, evolving, and derived from an increasingly complex array of sources such as basic science researchers, epidemiologists, social and political scientists, and economists. Modelling is useful in that it can incorporate diverse data to inform public health policy and decision making. 42, 43 Advances in mathematical modelling for public health are expected to include computational structural biology; integration of epidem iological and geographical data into phylogenetic models; within-host and population-level susceptibility models; behavioural modelling; and assessment of the eff ects of climate change on disease transmission and the use of novel datasets on contact patterns and population mobility. [44] [45] [46] [47] [48] 

Communication is a key strategy in risk management in planning for MGs. The SARS outbreak in 2003 reinforced the idea that a timely and transparent public information policy could help reduce excessive and inappropriate public health responses and minimise the social disruption and economic consequences of a fast-moving global epidemic. 49, 50 Increased investment in identifying eff ective approaches and developing and assessing new communication methods will benefi t risk prevention and control eff orts. The specifi c challenge is to provide clear, credible, and appropriate communication to meet the needs of diverse communities and retain public trust in a dynamic yet unknown process. 51, 52 Some of the main research topics in this specialty include the link between communication and behaviour change models; development and assessment of methods that can be quickly accessed and used in MGs; and assessment of best practices, challenges, and barriers in risk Research in early detection and monitoring of disease • Identify, develop, and adapt modern technologies for early detection of outbreaks of communicable diseases and their application in disease surveillance during MGs • Integrate and continuously assess innovative approaches and channels for disease surveillance and monitoring • Develop effi cient mechanisms to address the global challenges to sharing information, data, and details about pathogens identifi ed during outbreaks at MGs in terms of local, ethical, legal, and research perspectives • Defi ne the timeliness and quality of data required for early detection of disease from local to district, regional, national, and global levels

• Assess the application of modelling to understand and estimate key parameters for risk management • Investigate the role of modelling to assess eff ect of public health policies for diff erent MGs • Assess modelling in public health policy planning and strategic decision making in MG planning 

Research into health issues related to MGs is at an early stage. Research directions outlined here should contribute to the evidence that can be used to formulate risk management guidelines and assist event planning and health-care policy makers. The research agenda presented here is not intended to be restricted to specifi c aspects of health research but rather to encourage a multidisciplinary approach focused on MG health and to help gain more knowledge. In the future, the focus should be on strategies directed towards developing common research frameworks and defi nitions. Additionally, the knowledge generated by use of the multidisciplinary approach to research ought to be assessed for direct relevance to MGs in terms of their capacity for integration in legacy building and systemic sharing of information.

JST wrote the text. MB planned the outline for the review, and provided and consolidated WHO policies and guidelines about MGs. ZAM provided the concept for the review and planned the content with the team. NS provided the outline and wrote the section about risk communication. SB provided information about outbreak control and research agenda for infl uenza. M-PK contributed information about health research directions and vaccine development. All authors reviewed and provided advice on drafting the review.

We declare that we have no confl icts of interests.

We identifi ed references for this review by searching PubMed, Medline, and the internet for articles published in English from January, 1960, to June, 2011, by using the search terms ""mass gatherings"", ""research"", ""infectious diseases"", ""communicable diseases"", ""non-communicable diseases"", ""public health"", ""alert and response"", ""mass gatherings planning"", ""legacy"", ""outbreak"", ""surveillance"", ""prevention"", ""treatment"", ""Olympic games"", and ""Hajj"". We reviewed the articles and information found during these searches. Additional references cited in the articles were also reviewed.

",0.6757420558603354
Considerations for Use of Investigational Drugs in Public Health Emergencies,"The paradigm for the use of investigational drugs in public health emergencies has been recently tested to prevent and treat highly infectious and lethal diseases. Examples include the successful implementation of vaccine and therapeutic clinical trials during the recent Ebola outbreak in West Africa. On the other end of the spectrum was the Emergency Use Authorization (EUA) of peramivir in the treatment of H1N1 influenza virus that did not provide an opportunity to collect data or understand the effectiveness of the EUA program. Between the gold standard of a randomized controlled clinical trial and the problems associated with EUAs are the domain of expanded access protocols that may provide an avenue to make products available while awaiting licensure. This paper will examine the regulatory pathways in the United States (US) for the use of investigational drugs in a public health emergency as well as considerations when making these products available outside the US. Descriptions of the applications of the various approaches will be presented. Regardless of the pathway chosen, public health and clinical research planners need to work together to consider several factors associated with the respective options and maintain a goal of working toward the collection of data to support licensure before faced with future outbreaks. Finally, this paper will consider the lessons learned from public health response in the context of investigational drugs in other diseases where ""right to try laws"" may pose opportunities, as well as challenges.","Infectious diseases are estimated to have killed more people throughout the history of humans than any other single cause. 1 As air travel continues to grow exponentially, increased exchange of infectious diseases will no doubt accompany the increase in cultural exchange. 2 The recent examples from the Ebola Virus Disease (EVD) outbreak, Middle East Respiratory Syndrome, and Zika highlight the limited availability of vaccines and drugs to prevent and treat these threats. Even old threats such as malaria and certain forms of influenza lack effective vaccination solutions.

Further complicating the clinical trial landscape with the EVD outbreak was the lack of limited research infrastructure in West Africa, primarily because of a prolonged recovery from a decades-long civil war in Liberia and Sierra Leone. A logical framework for proceeding in this context proposed by Lane, Marston, and Fauci provided the core principles for conducting clinical research in an outbreak. 3, 4 Those principles are identified in Table 1 . In their paper, they note that definitive studies lead to more rapid licensure, and it is hoped, more rapid distribution of approved vaccines and treatments. During the EVD outbreak in West Africa in 2014 and 2015, the governments of Liberia, Sierra Leone, and Guinea invited the National Institute of Allergy and Infectious Diseases (NIAID) to collaborate with researchers in their countries in a program of research that aimed to benefit those countries. The NIAID team was able to collaborate with West African investigators to conduct natural history, preventive vaccine, and therapeutic studies. For example, through engagement with the Ebola response framework in Liberia, research proposals were vetted that included community and scientific input as well as the development of infrastructure to conduct that research. The initial vaccine study concept was vetted through a standing body at the World Health Organization (WHO) and was overseen by a Data Safety Monitoring Board of international and West African composition. Results from these studies either have been published or are in press. Moreover, the NIAID research program has maintained a presence in West Africa further developing both physical and human clinical research capacity. The NIAID response was focused on the collection of high-quality data in an ethical framework to support answering important questions about investigational vaccines and therapeutics. However, during the Ebola outbreak, there was considerable pressure to use any product available regardless of their unknown risks and benefits.

The 2014-2015 EVD outbreak in West Africa brought an uncomfortable reality to the forefront of clinical research. Is it ethical to conduct a randomized controlled trial when there is no known treatment for a disease with a high mortality, assuming there exists treatment drug candidates with promising preclinical (animal) data?

The WHO convened an advisory panel on August 11, 2014 , to reflect on the ethical considerations regarding the use of investigational drugs to treat EVD. 5 The panel strongly recommended that promising investigational drugs and vaccines for patients with EVD be urgently tested in humans by scientifically sound, rigorous methods. In fact, the panel concluded that researchers have a moral duty to conduct well-designed trials to answer questions regarding safety and efficacy. The panel also recommended that the compassionate use of an investigational product not preclude or delay properly designed clinical studies.

Compassionate use and expanded access protocols often are at odds with the study of the drugs themselves, as expanded access programs are primarily intended to provide treatment, not collect data on the investigational drug. 6 While some data may be collected during the course of treatment through compassionate use and expanded access protocols, the provision of drug through these avenues does not meet the generally accepted gold standard of a randomized controlled clinical trial designed to answer a question or questions asked a priori. The uncontrolled settings in which these products are used casts doubt on the integrity of any data collected. The use of a proper comparator arm is also not typically an option, as a provider seeking compassionate use of investigational drug for one patient is likely to request it for all similar patients, thereby eliminating any chance at a proper control group with which to assess the efficacy and safety of the drug.

Providing the most promising investigational EVD therapies during the 2014-2015 outbreak was also complicated by their limited availability. Even if the drugs were made universally available through compassionate use, their demand would quickly outstrip their supply. From this perspective, making the drugs available only for a randomized controlled trial does not deprive the population the opportunity to receive an investigational treatment, but instead randomly redistributes the drug among those who wish to receive it. In the case of a randomized controlled trial, in contrast to compassionate use alone, society benefits from the data obtained.

From an economic perspective, the widespread provision of investigational treatments for compassionate use, in lieu of clinical trials, may inadvertently send a dangerous signal to pharmaceutical companies developing drugs for rare but deadly diseases. While these companies are allowed by US law to charge for their investigational drugs used under compassionate use provisions, 21 CFR 312.8(d)(1) states that ""a sponsor may recover only the direct costs of making its investigational drug available."" 7 While the US Food and Drug Administration (FDA) does allow for the manufacturer to recoup direct costs, many other costs are not compensated, unlike products with marketing authorization, such as those to recoup research and development costs. For example, the overhead costs associated with maintaining the safety oversight system becomes increasingly complicated when an investigational product is made available outside the context of traditional well-defined clinical trials. The greatest cost to manufacturers for the provision of a drug under compassionate use may actually be the opportunity cost of collecting data on their product to gain marketing authorization. The provision of a product under expanded access in an outbreak rather than the conduct of a controlled clinical trial to collect data to support approval has the potential to deny the pharmaceutical company from valuable information to support a marketing application. This translates to a time cost for approval, which also has a societal cost, as fewer patients will have access to the product prior to approval.

NIAID's initial EVD study, Partnership for Research on Ebola Vaccines in Liberia (PREVAIL), was launched within four months of the initial research invitation. PREVAIL was designed as a randomized, double-blind, placebo-controlled Phase 2/3 clinical trial to evaluate the safety and efficacy of two EVD vaccines among volunteers in the midst of the EVD outbreak in Liberia. 8 The ChAd3-EBO-Z vaccine (GlaxoSmith Kline) consisted of a ChAd3 vector with a DNA fragment insert that encodes the EVD virus glycoprotein. The VSVΔG-ZEBOV-GP vaccine (NewLink/Merck) was comprised of a single recombinant vesicular stomatitis virus (VSV) isolate modified to replace the gene encoding the G envelope glycoprotein (GP) with the gene encoding the envelope glycoprotein from the EVD virus Zaire strain (ZEBOV). The first phase of the study was to include 1,500 EVD-free adult volunteers at one clinical site on the outskirts of Monrovia and expand to 27,000 volunteers at multiple sites around the country. Fortunately for the people of Liberia, the public health measures they implemented were successful in bringing the bulk of the epidemic to a close by May of 2015 at about the same time that the initial 1,500 participants were enrolled. The data and safety monitoring board recommended against expanding the trial as it would have been statistically futile to achieve the number of endpoints to determine a difference between the study vaccines and placebo. While an efficacy endpoint was not possible, substantial safety and immunogenicity data were obtained to support a future licensure application. 9

The 4-month implementation for the NIAID PREVAIL study was quite remarkable given the context and location of the outbreak. Liberia is still recovering from a 10-year civil war that has left inconsistent availability of basic services such as health, electricity, and water. The introduction of a clinical research program required the establishment of facilities, identification of reliable power for cold-chain requirements, hiring and training a sizable workforce, and establishment of a clinical research protocol in a collaborative manner.

Additional clinical studies were eventually launched in Guinea and Sierra Leone. In April 2015, a collaboration between the Sierra Leone College of Medicine and Allied Health Sciences and the United States Centers for Disease Control (CDC) launched the STRIVE trial (Sierra Leone Trial to Introduce a Vaccine against EVD). 10 STRIVE was designed as an immediate versus delayed immunization strategy using the VSVΔG-ZEBOV-GP vaccine in health and frontline workers. The STRIVE study faced a similar challenge as PREVAIL as a result of the waning incidence of EVD.

In Guinea, the Ministry of Health along with the World Health Organization and several non-governmental organizations collaborated to conduct a study employing a ring vaccination cluster-randomized design with the VSVΔGZEBOV-GP vaccine. 11 An interim report indicated the strategy might be efficacious. However, questions remain regarding the statistical methods and assumptions made regarding the incidence of Ebola in the immediate and delayed clusters. 12 Therefore, it is not known if that data will be sufficient for licensure. Hopefully with the results from the Guinean study together with data from other studies, as well as from animal efficacy studies, licensure of VSVΔG-ZEBOV-GP vaccine may be possible. Regardless, the process followed by the WHO highlights the importance of working with regulatory agencies in an a priori manner to understand the implications of study designs on the potential applicability of the chosen methodology as it relates to the later use of research results to support licensure.

Through an iterative planning process initiated over a year before the EVD outbreak, the US Government research community and several partners, aiming to include frequent consultation with the FDA, identified a study design known as the Medical Countermeasures Study. 13 The initial agent selected for this study was ZMapp. ZMapp is a cocktail of monoclonal antibodies that target the surface glycoprotein of EVD and is an investigational therapeutic for EVD. 14 Implementation of PREVAIL II in West Africa required flexibility and ingenuity on a number of fronts. First, as the study involved inclusion of multiple sites in both Sierra Leone and Guinea, establishing cold-chain and clinical trial operations processes in those countries involved different approaches. In Sierra Leone, NIAID collaborated with the CDC in the utilization of a storage facility co-located with the UNICEF (United Nations Children's Fund) Expanded Program for Immunization. To implement the study, NIAID rotated in teams consisting of physicians, pharmacists, nurses, and logisticians to work with the medical staffs at participating EVD Treatment Units (ETUs). Additional in-country staff were hired to help with data collection and maintaining contact with research participants after they were discharged. In Guinea, NIAID worked with an INSERM (French National Institute of Health and Medical Research) team of clinical research personnel who were embedded in an ETU under management by the French Red Cross. In Guinea as in Liberia, NIAID was able to utilize cold-chain storage available through the US Embassy.

PREVAIL II used a unique, adaptive clinical trial design with frequent interim monitoring in order to rapidly assess the effectiveness of ZMapp plus standard of care arm versus the standard care alone arm. 15 Predefined stopping criteria relied on diverging mortality rates between the two groups, with smaller differences requiring the recruitment of more participants. While designed to enroll up to 200 participants, the trial was prematurely stopped by the Data Safety Monitoring Board for lack of enrollment prospects, with the enrollment of only 72 participants. While the standard of care alone arm had an overall fatality rate of 37% versus 22% for the ZMapp treatment arm, the posterior probability of ZMapp superiority was 91.2%, short of the 97.5% statistical significance threshold. 16, 17 Obtaining the desired enrollment for the study would have required participation by many additional ETUs. Unfortunately, the operation of the ETUs was not centrally managed within the country, and a variety of non-governmental organizations provided on-the-ground staffing for ETUs. Not all of those organizations were interested in taking part in clinical trials or, if they were interested, had philosophical differences with the concept of a placebocontrolled study. 5 Without the use of a placebo or nontreatment arm, it is difficult to attribute safety and efficacy to the intervention being tested. 3 It is important to determine if the study intervention harms patients, especially if there is an opportunity cost associated with the use of one treatment in lieu of another.

Other therapeutic trials were conducted during the outbreak, but as some of these studies used historical control groups in a setting where the standard of care rapidly changed, evaluation of results from those studies remains problematic. Moreover, ascertaining the reason for the discontinuation of at least one of these uncontrolled trials will remain controversial. 18

In addition to controlled clinical trials, other regulatory solutions to introduce unapproved products include expanded access trials and emergency use authorizations.

As discussed earlier in this paper, as the EVD outbreak waned, the opportunity to complete controlled clinical trials diminished in a proportional manner. Regardless, the need to retain access to the promising vaccines and therapeutic candidates remained in order to respond to sporadic outbreaks. In the United States, guidance regarding Expanded Access of investigational drugs can be found at http://www.fda.gov/NewsEvents/PublicHealthFocus/ ExpandedAccessCompassionateUse/default.htm.

When working internationally, additional options for expanded access also exist in that 21 CFR 312.110(b)(ii) allows for the export of investigational products for national emergencies. From a regulatory perspective, MappBio, Zmapp's manufacturer, and the FDA were keen to have patients enrolled in the PREVAIL II study, rather than providing drug for compassionate use to new EVD cases. However, from an operational perspective, NIAID and the participating countries ended the ZMapp trial in February 2016, as it had been more than 45 days since the last known case of EVD. It did not mean that the subregion of West Africa was truly free of EVD. For sporadic events that did occur, NIAID, as the Investigational New Drug (IND) sponsor for the PREVAIL II trial, did not request compassionate use of ZMapp in West Africa, but did provide regulatory advice and technical expertise to groups that did seek to use ZMapp on a compassionate basis for patients who were not eligible for enrollment in PREVAIL II or who contracted EVD after PREVAIL II ceased enrollment. Because the treatment of EVD is a time-sensitive endeavor, the only feasible way to get ZMapp to a patient in West Africa was through NIAID's PREVAIL II clinical trial supply. In this regard, NIAID worked with FDA and MappBio to allow for an ""alternative disposition"" of the needed number of ZMapp vials, pursuant to 21 CFR 312.59 and the previously mentioned 21 CFR 312.110(b). 19 Moreover, NIAID worked with the affected countries to ensure that mechanisms to provide the unapproved drugs were consistent with the FDA emergency use regulations. Provisions were established for participants to provide informed consent, and for the collection of safety and outcome data.

Within the US, expanded access INDs come under three sizes in accordance with 21 CFR 312.300. 20 The provision applies to (1) a single patient, including for emergency use; (2) intermediate-size populations that occur usually after the FDA has received a number of requests for single patient use; and (3) treatment INDs that usually occur during the time period after completion of pivotal trials but before the marketing application is approved. Criteria for application of this portion of the regulation require that the patient or patients to be treated have a serious or immediately life-threatening disease or condition, that the potential benefit to the patient(s) justifies the potential risks of treatment, and that the expanded access protocol does not interfere with initiation, conduct, or completion of clinical investigations that may support a marketing application.

Outside the US, the WHO has attempted to identify potential strategies to evaluate investigational drugs for outbreaks such as EVD. 21 The WHO framework for evaluation is an attempt to bridge a regulatory gap between the American, European, and Japanese authorities and the regulatory bodies in developing countries. The goal is to provide thorough vetting of potential solutions via the experienced regulatory bodies and to guide developing countries on the relative merits of potential preventive and therapeutic candidates.

Plans are currently underway to establish expanded access protocols for VSVΔG-ZEBOV-GP vaccine and ZMapp in West Africa. Médecins Sans Frontières (MSF, also known as Doctors Without Borders) is collaborating with the WHO and Merck to establish a ringvaccination strategy with the VSVΔG-ZEBOV-GP to prevent further spread of EVD in response to future outbreaks. Additionally, MappBio is working with the Ministries of Health in West Africa to make ZMapp available under a protocol that will allow for a systematic collection of adverse events and outcome data. These expanded access protocols will be necessary until either marketing applications for these products are approved or further research demonstrates that other products may be better suited for the prevention and treatment of EVD.

In the BioShield legislation of 2004, Congress created the Emergency Use Authorization (EUA). 22 The EUA is different from an emergency use of an investigational product under 21 CFR 312.300 as the EUA is based upon a declared public health emergency. The FDA takes into account the availability of sufficient evidence of safety and effectiveness and weighs it against the communicability, morbidity, and mortality of the threat. As the EUA does not involve documentation of individual informed consent, one of the key reviews is that of the information to be provided to health care providers and individual patients.

The first use of an EUA in 2005 allowed the US Department of Defense to provide for anthrax vaccinations to service members subsequent to a court challenge to the legitimacy of anthrax vaccines for protection from inhalational exposure. The EUA allowed for continued vaccination of deploying service members to regions of the world subject to high threats for biological weapons. The anthrax vaccine EUA was eventually discontinued upon successful resolution of the court case. Subsequent applications of the EUA provisions have been provided for emerging infectious diseases, in the realm of diagnostics, preventive vaccines, and therapeutics. For example, with the current Zika outbreak, as of September 22, 2016, there are 10 Zika diagnostics available through the EUA mechanism.

The biggest regulatory drawback to the EUA is that the approach does not provide provisions to collect outcome data in a prospective manner. At best, retrospective, noncontrolled information is available that provides limited support to any attempt at answering questions of safety and effectiveness. For example, the EUA process was implemented in 2009 in response to the H1N1 outbreak to allow for distribution of intravenous peramivir. Evaluations of that program are not well documented. Moreover, in order to collect data to evaluate the effectiveness of the EUA program, some sort of study needs to be in place in order to collect minimal data, albeit likely retrospective data.

An important question, which needs further consideration by the international community, is what role investigational drugs should play in a communicable outbreak. Communicable diseases are likely to present a more urgent need for patients to receive investigational drugs, as many infectious diseases have relatively acute onset with the potential for high transmissibility, morbidity, and mortality. Although cancer is one of the more common diseases for which access to investigational treatments is regularly granted, cancer is a disease that often comes with at least a reasonable opportunity to consider the use by the treating physician, patient, and FDA authorities. Weighing the benefits and risks of an investigational product in a public health emergency likely comes under a certain amount of duress by all parties involved, and with very little time to decide on its use.

A unique aspect of communicable diseases may also be considered: the very fact that the disease is communicable, and the use of an investigational vaccine or treatment by one patient may have additional societal benefits. Unfortunately, the individuals using the investigational drug must usually bear the entire burden of risk, and the benefit-to-risk ratio may change to comparing the risk to an individual to the potential benefit of a population.

In the scenario of a communicable public health emergency, manufacturers of an investigational treatment candidate or vaccine with promising animal or early-phase clinical studies may find themselves in an unusual situation: federal authorities from various agencies, especially the Department of Health and Human Services, may seek out their assistance with the provision of widespread access to their products. Such was the case with ZMapp, although only following completion of the PREVAIL II trial. The FDA actively communicated with the company that they supported the use of ZMapp through a standing expanded access protocol until the time that the full submission package could be submitted for approval.

While several states have attempted to pass ""Right to Try"" laws with the intention of providing patients access to investigational medications, these laws do not appear intended to apply to public health outbreaks such as EVD. 23, 24 The decision-making calculus involved between cancer and EVD is substantially different in that the decision in cancer is between a patient and his or her physician. In an outbreak, governmental organizations are involved in establishing population-level programs. However, some similarities can be found in the early days of HIV research. Back in the 1980s, NIAID remained resolute in advocating for controlled clinical trials and working with patient advocacy groups, the FDA and industry partners to determine which interventions might work. 3 At that time no drug alternatives existed, and 30 years later, starting with the clear establishment of the safety and efficacy of AZT, continued therapeutic improvements have been made and HIV has moved from a sure death sentence to a mostly chronic infectious disease.

While the intentions of ""right to try"" laws are geared at providing patients who have exhausted all known alternatives with additional options, these laws may unintentionally hinder the use of investigational medications in the context of a public health emergency. 23 While it's entirely plausible that states can have a reasonable expectation of autonomy and subversion of federal investigational drug laws on a patient-by-patient level, it is unlikely that in a public health emergency the affected states would enjoy such anonymity and low acquiescence by the federal government. Public health emergencies often have a national security implication, and the news is usually ready to oblige spreading information and misinformation regarding an outbreak, and play-by-play information on the government's response. Moreover, the need for procurement and distribution of a large quantity of investigational drug in an outbreak makes the ""right to try"" approach all that much less practical.

While public health emergencies present a brief but important opportunity to study the efficacy and safety of investigational drugs for rare but deadly diseases and other threats, another pathway exists for the approval of these drugs: 21 CFR 314.600 through 314.650 for drugs and 21 CFR 601.90 through 601.95 for biologics, known as the Animal Rule. In October 2015, FDA released a guidance for industry on Product Development Under the Animal Rule. 25 As outlined in this guidance, the Animal Rule may apply to ""drugs developed to ameliorate or prevent serious or life-threating conditions caused by exposure to lethal or permanently disabling toxic substances, when human efficacy studies are not ethical and field trials are not feasible."" The development plan for these drugs may include animal efficacy studies in lieu of human efficacy studies to support marketing approval, provided that the animal studies are reasonably predictive of a clinical benefit in humans. The examples previously provided of clinical trials conducted during the 2014-2015 EVD epidemic in West Africa highlight the fact that the ethics and feasibility of field trials may change over time, potentially impeding the use of the Animal Rule. Animal efficacy studies may also provide evidence to support the emergency use of a drug under an IND or EUA.

With the benefit of hindsight, it would have been ideal for NIAID to have had in place approved clinical research protocols necessary to respond to the Ebola outbreak. In reality, several legitimate barriers existed. First, most of the vaccines and drugs considered for use in the response had not undergone phase I evaluations. Second, the quantities available of most of these products were not sufficient at the start of the epidemic to support clinical trials. Third, the infrastructure necessary to conduct trials needed to be put in place. Finally, as pointed out in the introduction, the important relationships to establish the necessary trust with the local countries needed to be established.

As the epidemic waned, NIAID research has continued in West Africa and a much stronger research capacity is in place. Studies on Ebola survivors are underway and we recently launched a study to examine the impact of an antiviral on residual Ebola in the semen of male survivors. 26, 27 Additionally, as discussed earlier in this paper, NIAID has assisted Guinea and Liberia with compassionate use of ZMapp and VSVΔG-ZEBOV-GP vaccine from its clinical trial supply inventory. However, the reliance on a clinical trials study supply may jeopardize enrollment in future clinical trials if a meaningful drain on study drug inventory is realized.

One final concern is that in resource-poor settings, where neglected tropical diseases and outbreaks are likely to manifest themselves, relying on investigational drugs and the presence of ongoing clinical studies to supply drug and operationalize campaigns, can create a dangerous shadow public health system, which shortcuts the development of a fully functional, safe, and effective public health infrastructure. Establishing realistic and reasonable pathways to licensure will benefit all concerned-the countries impacted, the global population at risk as these diseases spread, government and private research institutions and industry partners as well as donor organizations that frequently help respond to public health crises. The goal of a commonsense regulatory framework is to avoid shortterm solutions that have the potential to result in long-term problems such as continued requirements for global support. One part of the solution is to establish a framework to respond more rapidly with clinical trial protocols and supporting materials, including trained personnel if possible, with the anticipation of an outbreak. Governmental research organizations such as NIAID and others are well suited to work with industry in this regard, as the resources required to implement research in an outbreak are significant, and many of these drugs are unlikely to be commercially viable in the traditional drug market.

The recent West African Ebola outbreak provides the clinical research community with the impetus to be organized to respond to threats in settings that do not necessarily lend themselves to the conduct of controlled clinical trials in pristine environments. However, as presented in this paper, controlled trials can be implemented. The challenge is in taking the lessons learned from the Ebola experience and preparing for the next unknown future threats. A practical scenario for consideration in future public health epidemics without approved treatment options may be to initiate a randomized controlled trial to collect data to support licensure, with the concurrent establishment of an expanded access program for patients ineligible for trial enrollment due to inclusion/exclusion criteria or geographic location. Kirchoff 

Ethical conduct to avoid exploitation-including respect for volunteers, local community engagement, and carefully informed consent Partnership with affected country investigators and officials-including identification of interested local investigators, bolstering of trial infrastructure as needed, and shared best practices regarding regulatory oversight Scientific validity-including plausibility of benefit from candidate countermeasures and sound trial design Independent review and scientific oversight-careful oversight by an independent and skilled Data and Safety Monitoring Board

Transparency-prompt sharing of data with practitioners and affected communities a Adapted in part from Emanuel et al. 4

",0.6752715081695004
Mass Gatherings Health 1 Emergence of medicine for mass gatherings: lessons from the Hajj,"Although defi nitions of mass gatherings (MG) vary greatly, they consist of large numbers of people attending an event at a specifi c site for a fi nite time. Examples of MGs include World Youth Day, the summer and winter Olympics, rock concerts, and political rallies. Some of the largest MGs are spiritual in nature. Among all MGs, the public health issues, associated with the Hajj (an annual pilgrimage to Mecca, Saudi Arabia) is clearly the best reported-probably because of its international or even intercontinental implications in terms of the spread of infectious disease.","Defi nitions of mass gatherings (MGs) vary greatly, with some sources specifying any gathering to be an MG when more than 1000 individuals attend, whereas others require the attendance of as many as 25 000 people to qualify. 1, 2 Irrespective of the defi nition, MGs represent large numbers of people attending an event that is focused at specifi c sites for a fi nite time. These gatherings might be planned or unplanned and recurrent or sporadic. Examples of MGs include World Youth Day, the summer and winter Olympics, rock concerts, and political rallies. MGs pose many challenges, such as crowd management, security, and emergency preparedness. Stampedes and crush injuries are common, the result of inevitable crowding. Outdoor events are associated with complications of exposure, dehydration, sunburn, and heat exhaustion. Other health hazards arise from lack of food hygiene, inadequate waste management, and poor sanitation. Violence is unpredictable and diffi cult to mitigate whether the MG is a political rally or a sporting competition. With few exceptions, however, the rates of morbidity and mortality resulting from these hazards are rarely increased outside the event. Global MGs, however, can lead to global hazards. Mitigation of risks requires expertise outside the specialty of acute care medicine, event planning, and venue engineering.

For centuries, Muslim pilgrims have converged in Mecca, Saudi Arabia, for the Hajj (fi gure 1) to participate in a series of sacred rituals that defi ne Islam. With about 1·6 billion Muslims and the obligation on believers to attend Hajj at least once in their lifetimes, this event has become the largest annually recurring MG in the world, with attendance reaching more than 2·5 million in 2009 despite warnings about pandemic infl uenza. Pilgrims come from more than 183 countries, leading to enormous diversity in terms of ethnic origin and socioeconomic status. Men, women, and children of all ages attend Hajj together; however, a disproportionate number of people will be middle aged or older before they can aff ord the journey. Comorbidities are common. The public health implications of the Hajj are huge-nearly 200 000 pilgrims arrive from low-income countries, many will have had little, if any, pre-Hajj health care, added to which are the Saudi Arabia's safety and security policies for Hajj attendees are well developed after decades of planning the annual event. Lessons learned have led to comprehensive programmes that are continually revised and coordinated by government sectors. Public health has involved global partners for decades. Far from being the only MG that aff ects global health, the Hajj is a useful model to understand the nature of risk management and the benefi ts of international collaboration and cooperation.

Pilgrimage is central to many belief systems and also appeals to mankind's recurring desire to be homo viator-a universal fi gure common to many cultures and civilisations, who wanders in search of spiritual enlightenment. In Hellenic civilisation, Delphi-home to Pythia the Oracle-was long a focus for pilgrimage. 3 Ancient tribal populations such as the Huichol of western Mexico, the Lunda of central Africa, and the Shona people of southwest Africa all included pilgrimage in their cultures. 4 Institutionalised pilgrimage came to prominence with the advent of world religions. Buddhism invites pilgrimage to Nepal, the birthplace of Siddharta. Hindus journey to Benares in India, and followers of Judaism to Jerusalem. Christendom has a complex history of pilgrimages through the ages including the modern era. Until the advent of modern air travel, the journey was associated with the greatest risks. A review of the historical data for the Hajj shows these dangers: ""…the oscillatory movement of the camel produces miscarriages, followed frequently by haemorrhage and death of the infant and mother. The caravan however cannot stop, and it is impossible to nurse effi ciently while the (journey) continues. If any portion of the caravan stopped it would certainly be attacked…"" 5

Kumbh Mela is a huge Hindu pilgrimage held at various locations along the river Ganges according to the zodiac positions of the sun, moon, and Jupiter. Purifi cation rites involve bathing in the Ganges and are believed to interrupt the cycle of reincarnation. The highest holy days arise every 144 years, but the normal Kumbh Mela is celebrated every 3 years, and often attract thousands of non-Hindu enthusiasts. This is the largest human gathering, so large that in 2001 movements of the amassed individuals could be seen from space. 6, 7 The Ardh Kumbh Mela in 2007 attracted 70 million pilgrims over 45 days in Allahabad; on the most auspicious day of the festival, more than 5 million participated. 8 Celebrations are accompanied by singing, religious readings, and ritual feeding of holy men and the poor.

Managing rival sects is a recurring challenge. Administrators overseeing the event have to negotiate bathing schedules. Clashes have resulted in deaths-eg, in 2010, a vehicle carrying members of the Juna sect struck several people, setting off a stampede. 9 In 1954, a stampede killed 500 people. 10 The festival probably contributed to the 1817-24 Asiatic cholera pandemic. Pilgrims are believed to have carried the bacteria from an endemic area in the lower Ganges to populations in the upper Ganges, from there to Kolkata and Mumbai, and across the subcontinent. British soldiers and sailors took it home to Europe and then to the far east. 11 The epidemic ended abruptly in 1824 after a very cold winter. Although cholera returned to the Kumbh Mela in 1892, authorities of the Hardiwar Improvement Society reacted to contain the outbreak. 12 Diarrhoeal diseases, including cholera, continue to be a risk at the gathering despite rapid monitoring and prompt public health interventions. 13 Another pilgrimage with a focus on water and religious rites is to Lourdes, France. This village in the Pyrenees attracts more than 5 million Catholics and other enthusiasts every year. Their destination is a shrine and nearby spring where a young village girl witnessed apparitions of the Virgin Mary in the mid 1800s. Drinking and bathing in Lourdes' water is believed to ensure health and cure disease, and is featured at the Water Walk where religious stations are situated and water is available for drinking or bottling. Spring water is also routed to a series of bathing stalls used by more than 350 000 pilgrims every year. 14 Although health issues have not been associated with Lourdes' waters, the French writer Emile Zola visited the spring in 1891 and provided a graphic description of the baths at the time:

""And the water was not exactly inviting. The Grotto Fathers were afraid that the output of the spring would be insuffi cient, so in those days they had the water in the pools changed just twice a day. As some hundred patients passed through the same water, you can imagine what a horrible slop it was at the end. There was everything in it: threads of blood, sloughed-off skin, scabs, bits of cloth and bandage, an abominable soup of ills...the miracle was that anyone emerged alive from this human slime."" 15 Stampedes and fi res continue to be major causes of death and injury at MGs-eg, the Sabarimala in Kerala, India, and the Feast of the Black Nazarene in Manila, Philippines. Inaccessible for 300 years after their construction, Hindu temples of Sabarimala in Kerala's Western Ghat Mountains have become increasingly popular despite the location and winter openings. With the increasing crowd sizes, tragedies have occurred. In 1952, 66 pilgrims burned to death when sheds containing fi reworks caught fi re, and more than 52 perished in 1999 when a hillside collapsed under the weight of 200 000 assembled worshipers triggering a stampede. 16 More than 50 million attended the most recent rites in Series January, 2011, uneventful until the last day when a motor vehicle accident caused a panic that triggered a stampede, killing 104 people. 17, 18 Although authorities off ered compensation packages, they could not quell unprecedented public criticism of Kerala authorities and the national government. 17 Manila's Feast of the Black Nazarene has fared a little better after religious leaders and municipal authorities joined forces to change the route of the annual Jan 9 procession after two deaths in 2008, and many stampedes and injuries caused by fi reworks and trauma over the years. The authorities responsible for the MG also recruited thousands of volunteers to manage the crowds. These changes and the addition of an information campaign have helped calm crowds and reduce injuries. Despite an estimated attendance of 7-8 million in 2011, no deaths or serious injuries were reported. 19 

Protests during the Arab Spring in 2011 drew millions of largely peaceful protesters to central locations of Tunis, Tunisia, and then Cairo, Egypt. More than 5 million were present when the departure of Egypt's President Hosni Mubarak was announced in February, 2011. Other MGs include political protests of the antiwar movement during the Vietnam War. 1968 was marked by massive student marches in major European, Asian, and Latin American capitals. Chicago, IL, USA, had a particularly violent succession of MGs that became riots after the assassination of the civil rights leader Martin Luther King and again a few months later during antiwar protests at the Democratic National Convention. By contrast, European marches in protest of the US-led invasion of Iraq were larger and more peaceful. More than 3 million attended the largest march in Rome in 2003 (fi gure 2). In 1999, antiglobalisation protesters assembled in Seattle, WA, USA, ahead of a scheduled World Trade Organization meeting. Along with international anticorporate interests and assorted domestic supporters, they successfully occupied Seattle's downtown core and the convention centre. Violence increased during the 5 days, culminating in a full-scale riot after anarchists joined in and police responded with tear gas and rubber bullets. The Battle in Seattle as it came to be known, caused damages that were estimated at more than US$3 billion. Despite the violence and very large crowds, estimated to be hundreds of 23, 24 Violent sports fans are as old as history. In 532, the Nika riots in Constantinople pitted rival charioteer factions and athletes against each other and Emperor Justinian. During the 1 month insurrection that ensued, half the city was destroyed and more than 30 000 people died. 25 Although sports violence continues to be a risk during matches between rival teams, the massive crowds, crowds in motion, and immovable barriers cause the greatest loss of lives. The worst sports riot in history occurred in South America during a 1964 football playoff game between Peru and Argentina when fans responded in protest after a controversial decision to annul a goal by Peru. Police responded by throwing teargas canisters into the grandstand. More than 500 fans were injured and another 318 died. Most were crushed trying to escape the locked stadium, others died from teargas asphyxiation. The disaster in Hillsborough, UK, in 1989 was the worst stadium tragedy in British history. 96 fans died and another 766 were injured as crowds surged into the stadium crushing others in front who were pinned against fences. Many of the deaths resulted from compressive asphyxia while standing. Ineff ective crowd control and poorly designed venues have also resulted in deaths at music festivals, most recently in 2010 during the Love Parade in Duisburg, Germany, in which 21 people were crushed to death and 500 were injured as a result of a stampede in a narrow tunnel. Occasionally, MGs cause structural stresses that threaten safety and security. In 1987, the 50th anniversary of the Golden Gate Bridge, San Francisco, CA, USA, was celebrated by closing it to vehicular traffi c. Though not catastrophic, the suspension cables had the greatest load factor ever when 500 000 pedestrians crowded onto the deck, fl attening its centre span. 26

Although the Hajj was undertaken in the Middle East before the arrival of Islam, the movements and rituals of pilgrims today have not changed since the Prophet Mohammad inaugurated the Islamic Hajj in his lifetime. 27 It has been recorded in Arabic literature known as Adab Al Rihla. Persian literature records Hajj in the Safarnameh (travel letter). At the core of Islamic belief is trust and this trust has been best exemplifi ed by the risks Muslims take when travelling. The Muslim individual must trust in his Maker and, in ancient times, in the benevolence of strangers who would host him on his perilous journey to Mecca. Nowadays, as a result of the dissemination of Islam across the world, Hajj removes national, cultural, and social boundaries between diverse people like no other event.

Hajj has been the focus of public health initiatives for centuries, as shown in contemporary medical reports. [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] During the 19th century, the Hajj attracted the interest of European powers, particularly the maritime travel to the Hajj, which dominated until the arrival of air travel. Colonial powers at the time were suspicious of political Islam, which was referred to as wahabism. Direct engagement in Hajj-related aff airs was seen as too intrusive by politically savvy imperialists who recognised the sanctity of this little understood religious pilgrimage. Instead, supervision, albeit Series displaced, and management of Hajj were gradual processes, including surveillance, regulation, secure passage through the Red Sea and protection of British littoral interests, and eventually formal organisational processes, which would quickly become central to these hidden concerns. Imperial organisations linked cholera morbus, a non-epidemic diarrhoea, to Hajj, allowing a public health industry to develop that used health concerns to control immigration, pilgrim passports, proof of suffi cient funds to allow return travel, maritime regulation, and vessel quarantine procedures.

By the mid 19th century, most of the Muslim populations using maritime travel for Hajj were from the Malay Peninsula and Indian subcontinent. About 2000 pilgrims travelled from the Malay Peninsula and between 5000 and 7000 arrived from the Indian subcontinent. Although there are few reliable data, the total number of pilgrims was estimated to be 10 000. 38 ""According to the Turko-Egyptian Sanitary Commissioners at Mecca, the number of Mohammedan pilgrims collected in and about the Holy City…amounted to two hundred thousand persons; composed of natives of Turkey, India, Egypt, Morocco, Arabia, Syria, Persia, Java etc."" 29 Most travellers came in small vessels of 100-300 tons under diff erent international aegis. Departures were concentrated around Singapore, Calcutta and Madras in India, Aceh in Indonesia, and other regional cities. Most pilgrims then, like today, disembarked in Jeddah, though some would land on southern Arabian coastal ports and then make a land journey through Yemen to Hijaz. Well into the 20th century, the conditions of passage were often appallingly cramped and unsanitary. 34 Many people died along the route from infection and dehydration. 83 pilgrims died on board a maritime vessel, which had embarked from Jeddah with 520 pilgrims en route home to Singapore. 34 ""When she drew abreast of the watcher she proved to be a pilgrim ship; the afternoon being hot, the travellers had all crowded to the port side to catch what little wind was stirring. Their numbers were so great that they appeared to cover all the deck space, while the ship was unable to right herself from the list…"" 34 Eff orts to manage Hajj were initiated by Dutch-Indonesian authorities, not for wholly altruistic reasons. The Dutch had established an association between returning pilgrims and societal unrest, so they introduced heavily surcharged passports as a way of restricting the number of travellers to Mecca. The ruling empires focused on health issues and justifi ed inspections of Hajj sites for compliance with contemporary public health directives, often focusing on quarantine as a means of protection at a time when many international arrivals, including maritime travellers, were reaching Mecca. Their inspections were disappointing-the Annual Sanitary Commission visited the sites of Hajj and noted that the focus was not on prevention, but rather on the easy option of quarantine. 37 When cholera was reported at Hagar's Well within the holy mosque in Mecca, the British Consul at Jeddah requested a scientifi c assessment. Samples were analysed at the Royal College of Chemistry in the South Kensington Museum, London, UK, and compared with those of London sewage, which was a source of cholera at that time. Recommendations after their alarming fi ndings were sent to the Secretary of State for India who reported the well to be infected with the bacterium. 35, 36 Similarly, entrepôt cholérique (cholera reservoir) was noted when authorities visited pilgrims from India intending to do the Hajj. These pilgrims were routinely detained on the island of Camaran as a quarantine station in the Red Sea to restrict the ingress of cholera into the holy sites. 37, 38 Pilgrims were detained for 5-10 days without adequate provisions or clean water. The long exposure to sun, however, was thought to be benefi cial for elimination of infection. After quarantine, pilgrims were often permitted into the site. Results of later studies showed a link between the pilgrims quarantined on Camaran with a series of eight subsequent outbreaks. The conclusions drawn from a review of these events at an international public health meeting at the International Sanitary Conference of Paris, France, 1895, were that the ""Turkish possession of Camaran remains the greatest hindrance to the abolition of cholera at Mecca"". 37 Infection was a frequent feature of the Hajj in the 19th and 20th centuries, not unexpected since infectious disease medicine became better elucidated and the fascination with the developing specialty increased. Epidemics of smallpox occurred in Iraq and Sudan between October, 1928, and April, 1929. A small epidemic of plague occurred Pamela Das

www.thelancet.com/infection Vol 12 January 2012 Series in upper Egypt and a larger one in Morocco (161 cases). 34 653 cases of typhus were reported in Egypt and 32 in Palestine during the same period. 34 These fi ndings led to some strong recommendations that are still relevant: ""The yearly pilgrimage will remain a danger to all the countries from which pilgrims are drawn as long as the conditions of transport and accommodation remain…as at present. Effi cient reorganization of the pilgrimage in every direction is needed and should be facilitated by the governments of the large number of the countries involved."" 34 By the early 20th century, non-Muslim European powers were heavily engaged in the management of the Hajj and would remain so until modern Saudi Arabia came into existence and acquired fi nancial independence through petrochemical wealth. The comparison of Hajj in the imperial era with the modern Hajj shows the absence of Muslim public health experts or authorities in managing this pilgrimage. 39, 40 This absence would gradually change and with the arrival of Ibn Saud's modern kingdom and its investments in Hajj. From this point, Muslims would solely administer the modern Hajj in its entirety. 41, 42 

The Islamic calendar is a lunar calendar, so the date of the Hajj moves forward by 10-11 days every year, presenting planners with additional challenges of health risks that are associated with seasonal variation. Temperature fl uctuations in Mecca might be extreme depending on the time of year; daytime highs can be 40°C and higher, and night-time temperatures occasionally fall to 10°C. Hajj can coincide with the northern hemisphere's infl uenza season, as in 2009, increasing public health risks. [43] [44] [45] [46] [47] Attendance in 2009 was not blunted despite offi cial recommendations encouraging pregnant women, and elderly and very young people to stay at home. 48 More than 2·5 million people attended, including 1·6 million foreign citizens, 753 000 of whom did not have valid Hajj permits. 49 To put the event in its local context, the infl ux of pilgrims is so great that it trebles the resident population of Mecca, which is normally 1·4 million.

Access to the Hajj for pilgrims has changed greatly with air travel gradually replacing maritime and overland travel. In the past decade, the breakdown includes about 92% of pilgrims arriving by air, 1% making the maritime journey, and 7% travelling over land. 50 Although a few pilgrims will arrive at Medina's international airport, Jeddah remains the major port of entry for all travellers as it has been for centuries. Increasing numbers of people attending the modern Hajj led to a 1980 decision by Saudi aviation authorities to partition Jeddah's King Abdulaziz International Airport and create a separate south terminal to serve all pilgrims. Now two-thirds completed, the terminal's capacity is 80 000 travellers at any time. When completed, its fi nal capacity will be greater than 30 million passengers per year. Important new features include health-screening systems, customs, and immigrations security. Each of its 18 hubs receives pilgrim fl ights; all hubs have two examination rooms. The terminal also features large holding areas that allow effi cient reviews of selected arrivals in segregated parts of the terminal. This permits verifi cation of the immunisation status and administration of any prophylactic drugs and vaccines according to set protocols.

The overall design of the terminal permits visitors arriving without required visas and health records to be managed outside the main fl ow of pilgrims who continue through the facility to join assigned groups or agents who are responsible for coordinating details of travel and housing. These regulated services will also escort their charges through the Hajj site. In Islam, Umrah is a shorter pilgrimage to Mecca. Although not compulsory, Umrah draws an additional 5 million pilgrims per year to the country; Jeddah's airport plays a major part throughout the year, controlling access and enforcing health protocols. Groups exiting the country and returning home are also monitored, allowing comparative studies between the two populations. At various times of the year, but most intensely during the Hajj season, public health teams, both stationary and mobile, use mobile devices to monitor inbound and outbound populations. Protocols are based on regularly reviewed case defi nitions. Gathered data are sent to centralised databases for real-time analysis. Many diseases are monitored during a Hajj season. Those given specifi c attention every year include both mild and severe respiratory diseases, food poisoning and gastroenteritis syndromes, haemorrhagic fevers, and meningococcal diseases. Reports of all diseases, but particularly those with immediate eff ect worldwidesevere acute respiratory syndrome (SARS), infl uenza, cholera, yellow fever, polio, plague, meningitis, and viral haemorrhagic syndromes-are expedited to WHO epidemiologists who work closely with Saudi authorities Reuters/Jim Young Series to analyse information and coordinate a response. The airport is also equipped with clinics for management of medical problems.

Humility, faith, and unity are emphasised throughout the Hajj. The pilgrims wear simple clothing, women and men comingle, women are enjoined not to cover their faces, children and adults of all ages are included, and families journey together. On arrival in Mecca, Hajj pilgrims do a series of synchronised acts based on events in the lives of Ibrahim (Abraham), his wife Hajra (Hagar), and their son Ishmael. Each pilgrim does an initial circumambulation (tawaf ) around the central Ka'aba seven times. When completed, the pilgrim leaves for Arafat, about 22 km east of Mecca. Hajj culminates in Arafat on the Day of Standing, when all 2·5-3·0 million visitors stand and supplicate together on the mountain. Mount Arafat is believed to be the site of Mohammad's last sermon to his followers. Many people attempt to pray at the summit believing prayers there are the most blessed. On the way to Arafat, the pilgrims make overnight stops for prayers and contemplation in Mina. Leaving Arafat, the pilgrims return to Muzdaliff ah, where stones are gathered; on the way to Mina, they stop at Jamarat bridge to throw stones at the pillars that are effi gies of Satan. When the pilgrimage is complete, the new Hajjee (pilgrim who has completed the Hajj) makes an animal sacrifi ce thanking Allah for accepting his Hajj. This is often a proxy sacrifi ce because the Saudi Government has established modern abattoirs that are staff ed by professionals who will do this on behalf of the pilgrims. Meat is then distributed to the poor, family, and friends. The fi nal farewell is undertaken with another seven circuits around the Ka'aba. Muslim men on completion of a successful Hajj shave their heads. After completion of the Hajj, most pilgrims exit the country at Jeddah airport, which has congestion so great that the telecommunications infrastructure has to be constantly updated to allow suffi cient capacity. A smaller number of pilgrims will visit the holy mosque in Medina. Some will also visit tourist sites in the Hijaz and the old city of Jeddah.

Because all Hajj pilgrims travel as part of small informal groups, there is order in what could otherwise be chaos. Groups take their shepherding of individual pilgrims seriously, with easily identifi ed group leaders who carry placards and fl ags and lead the entire group through the rituals without losing stragglers, infi rm individuals, or temporarily distracted people. Further, this fl exibility safeguards Hajj at the most pressured points, which could otherwise become treacherous. Despite this fl exibility, Hajj stampedes have been recurring events, most notably at the Jamarat site. 42 According to Islam, only adults should undertake the Hajj. The age at which Hajj is undertaken varies according to culture. Some nationalities seem to undertake Hajj at a uniformly young age (eg, Indonesian and Malaysian), whereas other nationalities defer Hajj until the late phase of life as a precursor to preparing for death. There might also be diff erences in sex distribution. Malaysia for instance has had a female dominated Hajj attendance for more than three decades. 42 In keeping with the Islamic spirit of compassion, Muslims are enjoined to undertake Hajj only when adequately healthy. Despite this strong scriptural admonition, many Muslims insist on Hajj even when wheelchair bound. Special accommodations for wheelchairs are provided at the holy mosque despite the tremendous crowd densities. These channels are wide enough to admit wheelchairs and one person pushing the wheelchair and are divided into two lanes (one for each direction). Pilgrims who are not well are provided transport by the Ministry of Health ambulance to Hajj sites as needed so they can complete their pilgrimage.

Because of the Islamic belief that death during the Hajj has a benefi cial outcome in the afterlife, a few sick pilgrims attend, hoping for death during the Hajj. Public health and religious offi cials do much to dissuade this belief, which is often tenacious. This cultural belief system aff ects care providers at Hajj, all of whom are Muslims (non-Muslims are not permitted to enter the holy sites). Anecdotally, this belief aff ects resuscitation eff orts of those in cardiac arrest, which once initiated (if the patient reaches the emergency rescue services in time) are unlikely to be pursued if not immediately successful. A do-not-resuscitate status is often requested by pilgrims who can speak for themselves. 51 Hajj itself has several qualities that aid public health security. 52 Attendees must practise specifi c behaviours for their Hajj to be considered valid, and these requirements are strict and closely adhered to by both clerical and community leaders. Crime is strictly forbidden at Hajj and the risk of violent altercation is reduced because of the weapon-free, drug-free, and alcohol-free environment. 42 Tobacco intake is also banned, curtailing the risk of inadvertent fi re hazards. By contrast with some other MGs, sexual relations are not allowed during Hajj and male and female pilgrims are accommodated separately even when travelling as families, eliminating the risk of sexually transmitted disease.

This observant, penitent, and sober crowd engrossed in worship is thus likely to remain cooperative and coherent if sudden events demand rapid cooperation with authorities. Insurrection, rioting, disinhibited behaviour, or hooliganism of any kind does not arise even in these extraordinarily massive crowds. Pilgrims are urged to safeguard themselves or others at all times, aiding the infi rm and assisting the fallen, behaviours that symbolise peaceful Islamic societies that enhance the public health security. The spirit of cooperation is central to a successful Series acceptance of the Hajj by Allah in the Islamic belief system and reduces the potential risk of disastrous events in such massive crowds.

Saudi Arabia's responsibility for the Hajj has aff ected the country's advanced health-care infrastructure and its multinational approach to public health. Although other jurisdictions have administered the Hajj, Saudi Arabia has invested in it. Within the immediate vicinity of the Hajj, there are 141 primary health-care centres and 24 hospitals with a total capacity of 4964 beds including 547 beds for critical care. The latest emergency management medical systems were installed in 136 healthcare centres and staff ed with 17 609 specialised personnel. More than 15 000 doctors and nurses provide services, all at no charge. This event requires the planning and coordination of all government sectors; as one Hajj ends, planning for the next begins. Infection and prevention strategies are reviewed, assessed, and revised every year. Coordination and planning requires the eff orts of 24 supervising committees, all reporting to the Minister of Health. The preventive medicine committee oversees all key public health and preventive matters during the Hajj and supervises staff working at all ports of entry. Public health teams distributed throughout the Hajj site are the operational eyes and ears of the policy planners.

In hosting the modern Hajj, Saudi Arabia has weathered a 20th century world war, global outbreaks due to newly emerging disease (including SARS and meningococcal meningitis W135), and regional confl icts. In this time, the country has acquired a unique, resilient expertise concerning Hajj-related public health. Important observations that are relevant to public health planners everywhere are part of this experience. One of the best examples of such cross-cultural translation has been in the preparation for Barack Obama's Presidential Inauguration and crowd management informed by the Hajj experience.

Yet the process of exchanging expertise is possibly even more instructive. Collaborative work on this scale shows the increasingly important global health diplomacy in which the Muslim world has an enormous part to play. First articulated by the US Health and Human Services Secretary Tommy Thompson, global health diplomacy usually includes the provision of a service by one nation to another. 6 The USA's rebuilding of maternity hospitals in Afghanistan or the deployment of the ship USS Comfort to serve as a site for temporary clinics in Vietnamese coastal waters are two recent examples. 53 As they struggled with the best responses to the global threat of pandemic infl uenza A H1N1, which coincided with the Hajj in 2009, colleagues at the US Centers for Disease Control and Prevention and the Saudi Ministry of Health worked together to deploy one of the largest real-time mobile databasing systems, which was designed to detect disease in real time at any MG. Senator John Kerry discussed precisely this joint eff ort in a speech in Doha at the 2010 US-Islamic World Forum. 54, 55 This international collaboration was realised only through both intense personal dedication and the confi dence the agencies had in their people. Such collaboration strongly resonates with President Obama's renewed hopes for US engagement with the Muslim world, as articulated in his speech in Cairo, Egypt, in June, 2009. 54 People who collaborate, write, and disseminate information internationally have long been aware of the latent value of such informal, positive exchange. In the fl at world of medical academia, individuals have immediate and palpable eff ects. Fostering such professional dialogues are everyday (albeit unseen) acts of global health diplomacy. When investigators and physicians work in a shared space, unfettered by the global geopolitics, global health diplomacy becomes alive and vibrant. Hajj medicine, as part of the emerging specialty of MG medicine, provides an extraordinary platform.

Saudi Arabia's experience in international service through public health is substantial and is promoting the emergence of the formalised specialty of MG medicine. Hajj continues to provide insights into advanced and complex public health challenges, which are unlocked through collaborative exchange. 56 Disease and suff ering remain universal, even in the 21st century. Solving these challenges is relevant to humanity everywhere. Islamic scholars have long referred to Hajj as a metaphor for ideal societal behaviour. 42 At the centre of these ideals is a unifying theme: collaboration.

Saudi Arabia's experience of Hajj medicine contains rapidly developing public health solutions to several global challenges. Multiagency and multinational approaches to public health challenges are likely to become major factors in the specialty of global health diplomacy, engaging societies globally, and drawing the west a little closer to the east.

In view of the global public health threats that might originate from MGs, medicine relevant to MGs has become an essential specialised, interdisciplinary branch of public health, particularly hybridised with global health response, travel medicine, and emergency or disaster planning. 52 Agencies outside the realm of public health should be closely involved in MG medicine. In the operation and management of an MG, several sectorshealth care, security, and public communications-need to know how to interface with public health services and resources quickly and eff ectively. Involving public health experts with the broader civic planning for any MG helps with parallel transparency in needs and expectations, ensuring that public health considerations are factored into the entire planning process instead of intruding too late in development, relegating public health security Series concerns to little more than ineff ective afterthought. Delayed entry of these actors into the planning process can debilitate or completely disable adequate responses to potential diseases during MGs. Experts must educate civic planners about the values of early collaborative approaches to MGs for these reasons.

Conventional concepts of disease and crowd control do not adequately address the complexity of MGs. The need for MG health policies that are guided by sound evidence but anchored in experience shows the importance of calls for a new academic medical and science-based discipline. MGs have been associated with death and destructioncatastrophic stampedes, collapse of venues, crowd violence, and damage to political and commercial infrastructure, but little is known about the threats from MGs to the global health security. WHO has worked closely with international agencies to address such risks. [57] [58] [59] MGs pose complex challenges that require a broad expertise and Saudi Arabia has the experience and infrastructure to provide unique expertise with respect to MGs.

ZAM and GMS co-wrote the text. Imperial powers and 19th century Hajj, Hajj culture, and most of the global health diplomacy sections were contributed by QAA. RS compiled the table.

We declare that we have no confl icts of interests.

We identifi ed references for this Review by searching Medline and the National Health Service hospital search service for articles published in English from 1880 to August, 2011. Additional articles were identifi ed through searches of extensive fi les belonging to the authors. Search terms used were ""mass gathering"", ""disease"", ""pilgrimage"", ""Hajj"", ""outbreak"", ""public health"", ""prevention"", ""travel"", or ""modeling"". We reviewed the articles found during these searches and relevant references cited in the articles.

",0.6687263038641251
An integrative approach to enhancing small-scale poultry slaughterhouses by addressing regulations and food safety in northern -Thailand An integrative approach to enhancing small-scale poultry slaughterhouses by addressing regulations and food safety in northern -Thailand,"Background: In Asian countries, small-scale rural poultry meat production can face challenges due to food safety policies that limit economic growth and hinder improvement of sanitation and disease prevention. In this study, an integrative, participatory research approach was used to elucidate the sanitation and disease prevention practices in small-scale poultry slaughterhouses in rural northern Thailand.","Interest in an integrative approach has been increasing, especially in the area of ecosystem health, in response to growing evidence that many important ecosystem changes are the result of human activity [1] . The current environmental imbalance is considered to be a factor contributing to the outbreak of emerging diseases, as well as re-emerging diseases such as severe acute respiratory syndrome (SARS), hantavirus, highly pathogenic avian influenza, and other foodborne diseases. Although traditional approaches involving medical technology combined with active involvement of social, ecological, and political disciplines can be an effective tool in controlling diseases [2] , there have been numerous outbreaks of SARS and other diseases over the last three decades indicating a need for additional methods.

Food safety is an issue which is related to ecosystem health that concerns all human beings, making it an important focus of public health strategies around the world, including Thailand. Cases of foodborne diseases are still increasing, especially in developing countries, so food safety remains a serious challenge.

Salmonella is one of the most common organisms causing foodborne diseases worldwide. In the US, Salmonella is the second largest cause of food poisoning, where it spread widely between 2009 and 2010 [3] . In EU countries, approximately 100,000 patients suffered from food poisoning caused by Salmonella in 2010. In Thailand, Salmonella was found to be the second largest cause of food poisoning, following rotavirus in 2008 [4] . The main cause of human salmonellosis is the consumption of meat such as chicken contaminated with Salmonella [5] . Processing procedures in poultry slaughterhouses have been identified as an important source of Salmonella contamination of chicken meat, [6] especially in small-scale poultry slaughterhouses where traditional slaughtering processes are common [4] .

To enhance food safety and control foodborne diseases, efforts have been made to enforce regulations specifying standards for poultry slaughterhouses in Thailand, however, many small-scale slaughterhouses have yet not been able to meet these standards [7] . This study aimed to identify problems related to food safety in poultry production and to develop a conceptual framework for elucidating the administration situation and the potential for enhancement of hygienic management of small-scale poultry slaughterhouses in northern Thailand to help them achieve compliance with standard regulations.

Since food safety, especially in rural areas of Thailand, is a complex challenge and involves many sectors, an integrated approach was applied in this study. The main stakeholders were identified in the early stages of the project, using participatory methods, e.g., researchers meeting with key stakeholders including slaughterhouse owners, as well as Department of Livestock Development (DLD) officers at the national and regional levels, to identify problems. The information obtained from discussions with stakeholders was then reviewed with experts in veterinary science, socioeconomics, and public health before being used as the basis for developing a conceptual framework.

Policies, laws, and regulations which include the key phrases ""food safety"", ""slaughterhouse standards"", ""current situation of poultry slaughterhouses"", or ""foodborne diseases in Thailand"" were collected from published and unpublished sources including the Royal Thai Government Gazette, the Eleventh National Economic and Social Development Plan of Thailand (2012-3016), the DLD strategic plan, as well as domestic and international research reports on poultry slaughterhouses.

Perceptions regarding the implementation of existing regulations were obtained through focus group discussions (FGDs) with DLD regional officers. The principle investigator and co-principle investigator led the FGDs. Purposive sampling was used to identify participants using the criteria: (1) DLD provincial officers; (2) heads of DLD district offices; and (3) individuals having responsibly for slaughterhouse control. Two FGDs were conducted with a total of 22 participants between May and June 2012. Qualitative content analysis was used to analyze FDG data using the five-step process as described by Agus et al. [8] : (1) Following transcription of the interviews, summaries of the discussions were compiled; (2) All interviews were coded and categorized, outlined, then grouped under appropriate headings; (3) Similar headings were combined and categories were generated to reflect the study aims; (4) Analysis of the trustworthiness of the results was performed by asking a colleague to generate a theme list; and (5) Each transcript was coded by theme.

The Chiang Mai province is located in the northern part of Thailand and is characterized as having dense areas of poultry production. In 2010, more than three million chickens were produced in this province [9] . For that reason, Chiang Mai was selected to be the focal site of this study. A total of 41 small-scale poultry slaughterhouses (each processing fewer than 50 birds/day) were visited during the period from July 2011 to May 2012. Data on the current status of the slaughterhouses, especially data regarding productivity, economic status, hygienic management, and opportunities and challenges faced in improving the plants and following the DLD slaughterhouse regulations, were collected using a structured questionnaire and interviews (see Additional file 2). In addition, a checklist, which was developed based on the DLD regulations, was used for triangulation (see Additional file 3). The data were analyzed by way of descriptive statistics using the Microsoft Excel 2010 program (Microsoft Corp.).

Evidence of the prevalence of Salmonella spp. was used to elucidate the public health and environmental hazards of poultry meat production. A study by Padungtod and Kaneene found a 9% incidence of Salmonella contamination in meat processed by slaughterhouses in northern Thailand [10] . Based on that data, this study used a 10% expected prevalence. Sample size was computed using the Epi Info™ program with a 3% confidence limit and a 95% confidence level.

A preliminary survey found that, in 2010, Chiang Mai had 55 small-scale poultry slaughterhouses with approximately 25,000 birds being sent to these slaughterhouses each day. Samples were collected from slaughterhouses located within 100 kilometers of the laboratory at Chiang Mai University to insure that samples could arrive there within three hours. A total of 410 meat samples from 41 slaughterhouses were collected. Each carcass was placed in a large bag with 250 ml of sterile peptone water which was then shaken inside the bag for one minute, then the rinse water was poured into a sterile bottle and used for identification of Salmonella spp. In addition, environmental samples, including 500 grams of soil taken from around slaughterhouse buildings, were collected and stored in sterile plastic bags. In addition, one-liter samples of wastewater were collected using sterile bottles before the water was drained into the environment. Sample collection was conducted from July 2011 to May 2012.

All samples were collected in the morning immediately after completion of the slaughtering process, put into single use zip lock plastic bags, kept on ice in an ice chest, and sent within three hours of collection to the Diagnostic Center, Faculty of Veterinary Medicine, Chiang Mai University, for testing for the presence of Salmonella spp. The cold chain was not broken during sample collection and transport to the Diagnostic Center. Scientists used a standard Diagnostic Center form to record information on each ice chest including the number of the ice chest, the owner of the slaughterhouse, the sender of the sample, and individual sample identification information. Samples were then stored in a refrigerator at 4°C prior to individual sample testing which were conducted the following morning. After each use, each ice chest was washed with dishwashing liquid and water and then dried in a plate dryer. To further preclude possible contamination, each ice chest was withdrawn from use for between five and seven days after delivering the samples. The corresponding author (DVM, MS Health Science) supervised sample collection in the field. The Diagnostic Center is certified by the Bureau of Laboratory Quality Standard (BLQS), Department of Medical Sciences, Ministry of Public Health.

Salmonella identification was performed according to a modified version of the US Food and Drug Administration Salmonella culture method (Bacteriological Analytical Manual) [11] , and ISO 6579: 2002 and 2007 with Annex D [12] . The prevalence of Salmonella contamination in carcasses, soil, and wastewater was calculated by dividing the number of samples positive for Salmonella by the total number of samples processed. Concurrent with the sample collection, data on the characteristics of the slaughterhouses, including slaughterhouse management and perceptions of the owners regarding slaughterhouse standard regulations, were collected by the principle investigator at the slaughterhouses using structured questionnaires and interviews. The questionnaire focused on processing capacity, biosecurity, disease control management, veterinary services, health status of workers, environmental management procedures, socioeconomic situation, and perception of zoonotic aspects.

In addition, a checklist of DLD regulations regarding the location of the facility, characteristics of the structures, the slaughtering process, and waste management was applied as a cross-check of the data. Descriptive statistical analysis was accomplished using Microsoft Excel 2010 (Microsoft Corp.).

The main stakeholders in this study were the slaughterhouse owners, DLD officers at the national and regional levels, regional public health officers, and local administration officers. Brainstorming meetings and interviews confirmed the stakeholders' views on the importance of food safety and food policies in Thailand. They realized that poultry slaughterhouses are an important link in the poultry meat production chain, that the standard regulations should be followed, and that there are many factors affecting the improvement of slaughterhouses. They concurred that the main problems to be addressed are the inability of most small-scale slaughterhouses to comply with the current standard regulations and a lack of appropriate strategies to motivate and assist small-scale slaughterhouses to comply with these regulations.

The conceptual framework of this study (see Figure 1 ) reveals the complex interactions related to achieving slaughterhouse improvements. For example, there are three main government agencies responsible for the control of slaughterhouses: (1) the DLD, which is primarily responsible for animal health and disease control on livestock farms plus improvement and updating of regulations governing slaughterhouses; (2) The Ministry of Public Health, which is responsible for setting food safety standards for meat products; and (3) Local administrative organizations, which are responsible for giving permission to slaughter animals and to distribute meat, as well as appointing meat inspectors. To effectively assist slaughterhouse owners to improve their slaughterhouses and to follow regulations, government officers from these agencies must work together in an integrative mode.

The framework also includes socioeconomic factors affecting the improvement of slaughterhouses, for example, the association of education level and age with perceptions of food safety, as well as issues of income from slaughterhouses, living expenses, and family debt that could affect opportunities for investment in slaughterhouse improvements.

The main regulation regarding slaughterhouse control is the Ministerial Regulation on Determination of Criteria, Procedures and Conditions for Establishing Slaughterhouses, Lairage and Animal Slaughter B.E. 2555 (2012) [13] . This regulation consists of seven topics: (1) the location of the slaughterhouse, e.g., slaughterhouses must be situated far away from communities); (2) the area and structure of the slaughterhouse buildings, e.g., the slaughtering process must be conducted in a concrete building and there must be a fence around the slaughterhouse; (3) local infrastructure and the area inside of the slaughterhouse, e.g., the area inside the slaughterhouse building must be appropriate for operations, easy to clean, and include separate clean and dirty zones; (4) equipment and facilities management, e.g., facilities used in the slaughtering process must be easy to clean; (5) holding pens, e.g., pens where birds are maintained for 8-10 hours before slaughter must be constructed of concrete and must prevent pathogens contamination of the slaughtering process; (6) waste management systems; and (7) hygiene management, e.g., cleaning the slaughterhouse every day after operation. To be licensed by the DLD, all slaughterhouses in Thailand must comply with this regulation.

In addition, good manufacturing practices (GMPs) for poultry slaughterhouses were announced, and their adoption has been mandatory since 2006. However, in practice, the GMP guidelines, which were intended to further improve operations including hygiene standards, have been enforced only in slaughterhouses which have been issued a government license, most of which are larger operations.

In the FGDs with DLD regional officers (provincial and district) on the implementation of laws and regulations, the officers accepted that they could not strictly enforce the ministerial regulatory criteria intended to promote the improvement of small-scale slaughterhouses. They acknowledged that the criteria are intensive and require high levels of investment, making them suitable for large-and medium-scale operations which generate sufficient profit, but not for small-scale facilities with low productivity and small profits. They agreed that if they attempted to strictly enforce the regulations, they would meet resistance from the slaughterhouse owners. They also acknowledged that during their regular visits to slaughterhouses every three to four months, they should focus on establishing a spirit of collaboration and cooperation in order to promote hygienic management and disease control in slaughterhouses rather than strictly enforce the regulations. They also indicated that the current regulations should be more flexible and practical. As one officer said, ""It would be useful if there was a prototype or a blueprint of a good, hygienically managed slaughterhouse that owners could use as a model for investment"". The majority of the participants agreed with this comment.

Department of Livestock Development officers indicated an awareness of the need to work integratively with officers from other agencies, including public health officers and local administrative officers, in order to improve food safety. However, they mentioned that there were obstacles to such joint efforts. For example, working with local administrative organizations was problematical because those organizations still had no official role in that area or any personnel specifically responsible for slaughterhouse control. In the case of Public Health agencies' work with food safety control, their main focus is on meat products sold in the market rather than conditions at slaughterhouses. On a positive note, just over half the participants (54.5%) indicated that they were willing to work in an integrative manner with other agencies to address issues of food safety. Table 1 summarizes the reflections of the participants regarding the themes of the FGDs.

Of the participating slaughterhouse owners (see Table 2 ), 46.3% were male; 48.8% were 50-59 years old; 68.3% had completed primary school; 24.4% had been operating a slaughterhouse for 11-15 years; 82.9% slaughtered 1-50 birds/day; and 100.0% did not have a DLD license for slaughtering.

The process of slaughtering was carried out in openair buildings as follows: the birds were killed with a sharp knife, and the carcasses were scalded in a water tank at a temperature of 50-70°C for 2-3 minutes. Defeathering was done using semi-automatic de-feathering machines. The carcasses were cleaned by dipping them in a bucket of water. Evisceration was done by hand, using a knife to cut open the carcasses; this process was carried out on chopping blocks placed on the floor. The carcasses were then dipped in hot water (50-70°C) to firm up the skin, then stored in a small vessel containing ice. The wastewater from the slaughtering process was discharged directly onto the area around the slaughterhouses.

The majority of the small-scale slaughterhouses, which processed fewer than 50 birds/day, did not satisfy all of the seven criteria described in the slaughterhouse law and regulation. The owners constructed simple facilities with only necessary equipment and located within their community. Birds were sold only in the local community the same day they were slaughtered, but quantities were small, just enough to meet local demand. Incomes were limited and not sufficient to invest in improvements to the slaughterhouses to meet the Ministerial criteria. Results of the slaughterhouse assessment and the hygiene management evaluation are presented in Tables 3 and 4, respectively.

Regarding slaughterhouse owners' perceptions of relevant laws and regulations, 33 out of 41 owners (80.5%) stated that some of the criteria in the current standard regulation were impractical for small-scale slaughterhouses. For example, meat from most of the smaller slaughterhouses was not inspected because the limited slaughterhouse income was not sufficient to hire a meat inspector. Owners stated that they would have to stop operating their business if the DLD strictly enforced all the standard slaughterhouse regulation requirements. Although 25 out of 41 owners (61%) accepted that they did need to improve their slaughterhouses, they indicated a desire that the regulatory criteria be more practical.

Salmonella spp. were isolated from 30 out of 410 carcass samples (7.3%) taken during this study. As for contamination in the environment, 12 out of 41 soil samples (29.3%) and nine out of 41 wastewater samples (21.9%) tested positive for Salmonella spp.

Over the last 30 years, several successful attempts have been made to control various infectious diseases in countries all over the world, especially in developed nations. However, threats still exist such as antimicrobial resistant bacteria and unsafe farming and food production practices, as well as threats created by the impact of urbanization and agricultural intensification [2] . In addition, traditional methods of controlling infectious diseases using conventional biomedical strategies have often failed, resulting in the emergence and outbreak of diseases such as SARS, H5N1 and H7N1 avian influenza, malaria, tuberculosis, etc. [14] . In order to address these challenges and to achieve improvements in overall health-not just human healththe crucial roles of social, economic, and cultural factors must also be considered. Thus it is imperative that nonmedical sciences be involved in the process of developing disease control strategies. To more effectively identify avenues for enhancing safe processing in small-scale poultry slaughterhouses, practitioners of veterinary and human medicine, social scientists, and economists cooperatively followed an integrative approach in the development of the conceptual framework and in participatory problem identification from the outset. That framework demonstrates the complexity of the problem and the linkages between the different disciplines. This study follows the successful integrative approach which was used to gain an understanding of and develop a suitable research agenda in the case of the emergence of leptospirosis in Hawaii [15] . This study evidences the importance of a transdisciplinary approach, as well as methods of implementing that approach as described and demonstrated by Pokras and Kneeland in their development of educational and policy initiatives to control the lead poisoning problem in wildlife, humans, and domestic animals [16] .

This study confirms that good hygienic management is not widely practiced in small-scale slaughterhouses in northern Thailand. This finding is consistent with the report in Bangladesh by Rimi et al. in 2013, which reported commonly observed improper practices such as slaughtering sick poultry in rural communities [17] . This study shows that, in general, hygienic practices do not fully follow existing regulations and that the government provided guidelines are not implemented by many slaughterhouses, e.g., the slaughtering process being performed on the floor, and the lack of methods or processes to prevent bacterial contamination of carcasses. The majority of slaughterhouse owners pointed out that they could not improve their operations in accordance with the current standard regulation. For instance, the regulation specifies that slaughterhouses must not be located in communities, a challenge to the small-scale slaughterhouse owners in this study currently located within a community. In fact, some of the slaughterhouses were in operation prior to the regulation. An authorized meat inspector is required to inspect the meat, but owners pointed out that they could not afford to hire a meat inspector because of the limited profitability of their operations. Owners stated further that if the DLD decided to strictly enforce the regulation, they would have to cease operation of their slaughterhouse and seek a new occupation, an outcome that would certainly adversely affect the socioeconomic status of the owners and their families.

Department of Livestock Development regional officers agreed that the current laws and regulations are, in fact, more suitable for large-and medium-scale operations which can afford the necessary high investment. Nonetheless, the DLD is attempting to encourage even small-scale poultry slaughterhouses to meet the standard. The slaughterhouse blueprint developed by the DLD and distributed to officers and slaughterhouse owners, however, is designed for operations processing 200-300 birds/day which is four or more times the daily production of small-scale slaughterhouses. Thus, the DLD-proposed blueprint poses a considerable challenge to small-scale slaughterhouses.

The importance of Salmonella as a public health hazard was clearly demonstrated in this study, which found a prevalence of 7.3% in the final product (chicken carcasses). That figure is close to the 9% prevalence of Salmonella in poultry carcasses after slaughtering and final products in Thai slaughterhouses reported in a study by Padungtod and Kaneene in 2006 [16] , but much lower than in other studies, e.g., Kueylaw et al. in 2008 [7] found a prevalence of Salmonella of 43%. Reports from elsewhere in the world also indicate a higher prevalence of Salmonella. For example, Elgroud [21] .

Salmonella prevalence in these reports is significantly higher than that found in this study. One possible reason for the lower Salmonella prevalence found in the current study could be that the survey was conducted at smallscale facilities, the majority of which processed fewer than 50 birds/day. Processing fewer birds might result in a lower bacterial load in those facilities and thus a lower Salmonella prevalence in carcasses compared with other studies such as the one by Padungtod and Kaneene (2006) , which was done in medium-and large-scale slaughterhouses. Moreover, the traditional slaughtering process commonly found in smaller operations includes the final processing step of immersing the carcass in hot water for a short time to firm the skin. Immersion makes the skin more attractive, an important factor for small-scale operations which sell the final product (carcasses) in the local community. That process also has the effect of decreasing pathogen contamination. However, this method could also have the negative effect of increasing the temperature of the carcasses, making them more suitable for bacterial growth and thus more susceptible to rotting. For that reason, it is not appropriate for carcasses treated this way to be stored overnight.

The 29.3% prevalence of Salmonella contamination found in soil collected around slaughterhouse buildings and the 21.9% contamination rate in wastewater drained onto the area around the slaughterhouse without treatment, however, is evidence that improper hygienic practices can affect not only end consumers, but also members of the local community and the surrounding environment. These results mirror findings in previous studies in other regions of the world. For example, 100.0% of sludge samples collected from eight pig and five poultry slaughterhouses in Belgium and the Netherlands were found to be contaminated with Salmonella [22] , and 7.4% of treated effluent samples from seven pig and seven poultry slaughterhouses in Brazil taken in 2003-2004 were positive for Salmonella spp. [23] . Seven out of 22 samples (31.8%) obtained in 1993 from untreated wastewater from Nigerian slaughterhouses and river water collected at sites near those slaughterhouses tested positive for Salmonella [24] . Thus, it can be inferred that slaughterhouses are a potential source for dissemination of foodborne pathogens into the environment, especially where poorly treated or untreated wastewater is discharged directly into the environment.

Improving small-scale poultry slaughterhouses to meet food safety standards and addressing the standard certification requirement are challenges that need to be addressed. This study, which found that unsatisfactory and inadequate management of hygiene was common in small-scale poultry slaughterhouses in Thailand, demonstrates that the use of an integrative approach for exploring a complex problem and developing a research conceptual framework can be an effective approach. Even though the level of Salmonella contamination found in carcasses was low compared with some previous studies, hygiene management should be improved to enhance food

",0.6677896845925758
Could influenza transmission be reduced by restricting mass gatherings? Towards an evidence-based policy framework,Introduction: Mass gatherings (MG) may provide ideal conditions for influenza transmission. The evidence for an association between MG and influenza transmission is reviewed to assess whether restricting MG may reduce transmission.,"It is well established that influenza is transmitted from person to person through close contact with an infected, symptomatic individual. The exact mechanisms by which transmission occurs are still unclear, but is believed to involve multiple routes, including respiratory droplets and direct/indirect contact [1] with secretions or fomites. Aerosol transmission may also occur, but controversy surrounds this [2] and some further evidence of aerosol transmission has recently been put forward [3] . This is an important issue as the mechanism of transmission is always a key factor in infection control planning.

While uncertainty persists regarding aerosols, droplet and contact transmission remain largely regarded as the most important and likely routes. Transmission through these routes clearly requires physical nearness to infected persons, via either direct touch or the propulsion of large droplets across a relatively short distance. This requirement makes the consideration of mass gatherings a crucial issue.

Whilst there is not a ''standard'' definition of what constitutes a mass gathering, there is an understanding that these are events that involve large numbers of people (suggested minimum of 1000 people) [4] [5] [6] congregating in finite geographical areas to share an event or experience. Individuals can be in very close proximity for variable periods of time and, if the event is over a number of days, may even share over-crowded and/or temporary accommodation.

Therefore for those infectious diseases such as influenza, where close contact is the main determinant of transmission, it appears self-evident that mass gathering events could lead to the rapid transmission of a new causative agent.

In the event of an influenza pandemic, minimizing transmission of influenza has been a priority for public health action. A variety of non-pharmaceutical public health interventions to reduce close contact between infected and susceptible individuals, or the opportunities for the virus to be picked up by susceptible people such as self-isolation of patients, respiratory etiquette and hand washing have been advocated. Banning or restricting mass gatherings has been seen as a logical extension of this policy, however, it is a particular concern of policy makers that the scientific evidence upon which to base guidance for mass gatherings is lacking. This is particularly important given the need to weigh any potential benefits against the economic and social disruption that banning or restricting mass gatherings could have on society.

There have been other systematic reviews undertaken in recent years on the broader subject of non-pharmaceutical interventions aimed at reducing transmission of respiratory viral infections [7, 8] . These reviews considered a range of such interventions, and they explicitly recognized that there is a paucity of primary evidence regarding restriction of mass gatherings (and other ''social distancing'' measures); thus they did not primarily focus on this specific area.

The aim of this review, therefore, was to attempt to reduce this evidence gap by assessing the available evidence base relating to the utility of restrictions on mass gatherings during an influenza pandemic. This is an important policy area not only because of the impact such restrictions could have on public confidence and morale, but also because of the economic and liability issues that such action might generate. The findings of the review may be able to help inform policy statements on the effectiveness of mass gathering restriction interventions that may be deployed to help reduce influenza virus spread during a pandemic.

The study protocol was largely based on the University of YorkÕs Centre for Reviews and Dissemination guidance for undertaking reviews in healthcare [9] . Work commenced in mid-July 2010 and the main part of it was completed in mid-October 2010. Further work was carried out in June 2011 to bring the literature coverage up to date.

The following specific questions were developed to capture the objectives of the review:

• Is there an association between mass gatherings and influenza outbreaks or spread? • Are there any particular characteristics (such as size or duration) of mass gatherings that influence transmission of influenza? • Does the restriction of mass gatherings reduce the spread of influenza within the community (compared with no restriction or with other interventions)?

The following types of studies were included in the review:

• Randomized controlled trial • Quasi-experimental study -Non-randomized controlled study -Before-and-after study

• Observational study -Cohort study -Case-control study

• Outbreak reports -Outbreak/cluster reports -Historical archival outbreak analyses

• Surveillance reports -Major event infection surveillance reports

The list above is largely in hierarchical order of study design quality [9] , but with the addition of outbreak and surveillance reports, not traditionally included in such lists, important to the topic of this review. To cover the other ''non-standard'' study design encountered, the term ''quasi-experimental'' is employed to describe controlled but nonrandomized studies [10, 11] .

Only studies published in English were included. Relevant systematic and narrative reviews and operational description papers were utilized for useful background information. The reference lists of the systematic reviews were scanned to identify potentially relevant primary studies that could be considered for inclusion. Case reports, mathematical modelling and human/non-human experimental laboratory studies were excluded from the review.

Outcome measures included laboratory-confirmed influenza infection (i.e., documented by virus isolation, molecular testing such as polymerase chain reaction, and serological studies) and clinical influenza-like illness as defined by the investigators, whilst recognizing that this is less specific.

The search strategy focused on primary studies taking into account the issues detailed above. The term ÔinfluenzaÕ rather than Ôpandemic influenzaÕ was used to reflect the entire spectrum of influenza for which guidance would be relevant. The transmission of other respiratory viruses was also included because of the similarity of transmission and therefore the potential applicability of any results to influenza. An initial ''scoping'' search was followed by further discussions and consultation on the work plan and search strategy. The term mass gathering on its own proved inadequate as a search term as there is no clearly accepted definition of what constitutes a mass gathering. Therefore, a range of additional terms were used, such as public gatherings, social gatherings, large crowds, mass events, festivals, Olympics, Hajj, championship, and others listed in Table 1 . Transit settings such as long-distance flights and cruise ships were not included, as it was decided that these settings would fit better in a separate review of evidence for influenza transmission through transport. The final search with revised terms and refined using the ''advanced search'' tool on PubMed was carried out as described in Table 1 , with no time period restrictions. The initial searches included the literature up till July 2010. A further search was conducted on 31 May 2011 to bring the literature coverage up to date as part of an editorial process.

In addition, secondary searches were also initially carried out in the following databases: Scopus, Excerpta Medica Database (EMBASE), and the Cumulative Index to Nursing and Allied Health Literature (CINAHL). These are described in Table 2 .

In addition to the search results, the reference lists of papers identified from the search were scanned for other potentially relevant studies. Efforts were also made to identify studies other than those published in the peer-reviewed literature; the authorsÕ 

Study selection was conducted in stages. At all stages, scanning and reviews were done independently by the authors, who then harmonized their selections. Papers identified by the searches were scanned and some excluded on the basis of the Ôti-tleÕ for relevance to the review. The abstracts of the remaining papers were then reviewed to identify studies that appeared to meet the inclusion criteria. The full text versions of all these articles were then sought and read in full. For papers that had relevant titles but no abstracts, the full text articles were also obtained, scanned for relevance, and if relevant they were read in full. In addition, some papers that did not meet the inclusion criteria (e.g. operational descriptions of interventions, commentaries, or editorial reviews of influenza transmission or pandemic influenza) were selected to provide relevant background or supplemental information. Data from the selected full text papers were extracted using a pre-designed form. Data elements included publication information, study characteristics, participant characteristics, the intervention (if any) and setting, outcome and results. The data extracted were used to determine the eligibility of each paper for inclusion in the review. This process resulted in a final selection of eligible articles that were then included in the review.

The main body of this work was undertaken in 2010 by the UK Health Protection Agency as part of a comprehensive influenza scientific evidence review informing the UK National Pandemic Influenza Strategy. Within this context, the work was carried out within a very tight three-month timeframe and with significantly limited resources. As a result, some of the most comprehensive steps required of a full systematic review could not be realistically accommodated. Therefore, a modified process was adopted in conducting this ''rapid evidence assessment'' rather than a full, formal systematic review. Rapid evidence assessments are well recognized as a pragmatic approach to include all the major elements of a full systematic review, ensuring an outcome that is as comprehensive as possible while accepting limited compromises in order to meet stipulated deadlines for feeding into the policy process [9] .

Due to the time constraints, it was not practical to pursue time-intensive contacts with authors or to seek translations for non-English articles, which were therefore excluded. Articles lacking abstracts were largely excluded, and there was a limited search for unpublished studies. However, to try to mitigate these limitations, full draft versions of the review were submitted to national experts at the UK Health Protection Agency and the Scientific Pandemic Influenza committee of the UK Department of Health, seeking their comments and advice on any papers or documents that may have been omitted. Some further work was carried out in May to June 2011 to update the literature.

At the outset, a strategy was developed to assess the quality of eligible studies using the Critical Appraisal Skills Programme (CASP) tools as appropriate for each type of study [12] . However, after completion of the literature search, it was clear that the types of studies and the study design types identified were not suitable for assessment by the CASP tools. Similarly for the outbreak reports included in the review, no suitable existing framework could be readily identified for quality assessment. The ORION tool was not strictly applicable as it is specific to outbreaks of nosocomial infection [13, 14] . Therefore, a modified approach was adopted, whereby each paper was categorized as having low risk, some risk, or high risk for bias with respect to the relevant review question addressed by that study. The grading was assessed based on the presence of significant methodological limitations. A high risk of bias was attributed to papers with at least three significant methodological issues as identified by the review authors, while papers with at least two issues were classified as having some risk. This approach was adapted from a system previously used by Jefferson et al. [7] . The quality of the review itself was tested against checklists based on the MOOSE [15] and PRISMA [16, 17] standards. Within the restrictions of the time constraints already described, key gaps and issues identified by the checklists were addressed as appropriate and/or acknowledged in Sections 2.5 (explaining the review context including time constraints) and 4.4 (recognizing the study limitations).

The data synthesis was restricted to a narrative approach that included an analysis of the relationships within and between studies and an overall assessment of the robustness of the evidence and limitations of both the studies and the evidence review [9] . In addition, the synthesis considered the implications for policy and guidance development as well as future research.

The search identified a total of 1706 papers after accounting for overlaps between the different databases. These were reviewed on the basis of ÔtitleÕ for relevance to the review (Fig. 1) , and on this basis 1593 papers were excluded. Abstracts for the remaining 113 papers were reviewed and a further 54 were eliminated.

Full copies of the remaining 59 papers plus 6 papers identified from scanning the reference lists of review papers, and 3 papers retrieved from the authorsÕ hardcopy files, were reviewed and summarized. Table 3 provides a classification of the 68 papers that were read in full text.

After full text review, 24 papers were classified as meeting the inclusion criteria. These included a quasi-experimental study (a non-randomized trial) [18] , nine observational studies [19] [20] [21] [22] [23] [24] [25] [26] [27] , seven outbreak reports [28] [29] [30] [31] [32] [33] [34] , three historical outbreak archive analyses [35] [36] [37] , and four event surveillance reports [38] [39] [40] [41] . Details for each of these studies are summarized in Table 4 . The 44 papers that were read but not included in the review are listed in Table 5 .

One quasi-experimental study by Qureshi et al. [18] attempted to investigate the incidence of vaccine preventable influenza-like illness among Pakistani pilgrims to the Hajj religious gathering in 1999. The Hajj is an annual religious event that takes place over a number of days in a very small geographic area of Saudi Arabia usually involving 2 to 3 million pilgrims from all over the world. Accommodation is at a premium during this event and many pilgrims stay in tents specifically erected for the event and that are often over-crowded. Although primarily a vaccine efficacy study, the rates of influenza-like illness reported in vaccinated pilgrims were 36% compared with 62% in non-vaccinated pilgrims. However, these results were based on clinical endpoints without microbiologic confirmation; a non-randomized design was used, and the study was not designed to address the primary question of this review.

Nine observational studies estimated the risk of acute respiratory illness and/or influenza-like illness associated with the Hajj pilgrimage by attempting to measure its occurrence in pilgrims. Four [23] [24] [25] 27] of these studies confirmed the cause of illness by laboratory testing, while the other five relied on specified symptom complexes as surrogate indicators.

Among the nine observational studies were four cross-sectional studies. Balkhy and colleagues [24] tested 500 Hajj pilgrims in 2003 who presented with symptoms of upper respiratory tract infection in the second week of the event; 30 (6%) of the 500 pilgrims tested positive for influenza. Rashid et al. [23] assessed the burden of laboratory-confirmed influenza and respiratory syncytial virus (RSV) infections in symptomatic British Hajj pilgrims in the 2005 event. Of 202 symptomatic pilgrims who underwent nasal swab testing, 28 (about 14%) had confirmed influenza (mostly A type), while only 9 (4%) had RSV infection. In the 2006 Hajj, the same investigators [27] found comparable levels of laboratory confirmed influenza (10-11%) in both UK and Saudi pilgrims. These relatively low levels of infection contrasted with the findings in the cross-sectional study by Deris et al. [20] who based their assessment on syndromic influenza-like illness rather than laboratory-confirmed infection. They found an influenza-like illness prevalence of 40% in Malaysian pilgrims who had just completed the Hajj.

The other five observational studies were similarly designed, involving groups of intending Hajj pilgrims who were recruited in their home regions or countries prior to the event, and then re-assessed This was a well-organized systematic prospective influenza surveillance program, described by the authors as the first of its type at a large Games event Limitations include:

-No indication of total numbers of people at the event or in the city -No indication of the background ILI activity in the city or country; or whether this was during the local winter influenza season after the pilgrimage. Even though three reports were described by their respective authors as ''prospective cohort'' studies [19, 21, 22] , none of these five studies included an ''unexposed'' (non-Hajj attending) control group for comparison, indicating that they should be more accurately regarded as ''before and after'' studies. Choudhry et al. [19] assessed Saudi residents (attending the Hajj from a different part of the country) and found an incidence of influenza-like illness of about 40%. Three studies of French pilgrims by Gautret et al. [21, 22, 26] found rates of cough of between 48% and 61%; while in El-Bashir and colleaguesÕ serological study of UK pilgrims [25] , respiratory symptoms occurred in more than 80%, but the seroconversion rate was 38% of 115 participants.

These variable data underline the difficulties of comparing different studies that employ contrasting measures of influenza. Interpretation needs to take account of the fact that studies using clinical outcomes suggest much higher levels of influenza-like conditions than the ones that involve laboratory confirmation. Even among the laboratory-based studies, differences in laboratory techniques may also be significant.

There were four types of outbreaks in the reports that were reviewed. The first of those was a paper by Pang et al. [32] that described the experience of dealing with the SARS (severe acute respiratory The next group consists of four reports of influenza outbreaks occurring at a religious event in Australia and at three large, open-air music festivals in Europe. All of the events lasted several days and involved crowds ranging from 100,000 to 400,000 people. It is particularly relevant that the three music festivals occurred during the 2009 influenza pandemic-two at the beginning and one later during the pandemic.

The report by Blyth et al. [28] described an outbreak of influenza at a large, 5-day religious event in Australia during July 2008 attended by over 400,000 participants from 170 countries. Over 100,000 of the pilgrims were accommodated in a variety of make-shift, overcrowded venues such as sports halls, community centers and schools. One hundred laboratory-confirmed cases of influenza were identified among attendees. Seven different strains of influenza were identified (four influenza A and three influenza B), highlighting the potential for the introduction of novel influenza strains.

Loncarevic et al. [31] described an outbreak of influenza at a 4-day music festival in Serbia during July 2009 involving over 190,000 participants, with a number of them coming from other European countries. Many of the participants stayed at a large campsite where overcrowding was an issue. Sixtytwo laboratory-confirmed cases of H1N1 (2009) were identified; some of which were secondary cases. Although the virus was already present in Serbia at the time of the festival, the sudden increase in cases, in particular the secondary cases associated with the festival, suggests possible local spreading. The authors also reported on a small outbreak of influenza at an international sporting event held in Serbia in July over a period of 12 days and attended by over 500,000 spectators. Seven confirmed cases of H1N1 (2009) were identified in six athletes and a volunteer helping at the games. Although a much larger event, the numbers affected are considerably smaller and seem to be restricted to participants rather than those attending.

Gutierrez et al. [30] described a 4-day music festival in Belgium during early July 2009 attended by an estimated 120,000 people from all over Europe. Twelve laboratory-confirmed cases of H1N1 (2009) were identified. Although sporadic cases of H1N1 (2009) had been detected in Belgium prior to the festival, an increase in cases was observed after the event and the decision to shift to mitigation was taken almost a week after the first festivalassociated cases were identified. The authors suggest that this festival highlights the potential seeding role for these events in the early stages of a pandemic. Presentation and discussion paper on data from the 1918-1919 influenza pandemic. The substantial research report arising from the study was included in the study [37] Memish Botelho-Nevers et al. [29] depict an outbreak of influenza at a 7-day music festival in Hungary during August 2009 attended by 390,000 people from all over Europe. Many of the participants were located on a campsite set up for the festival. Eight laboratory-confirmed cases of H1N1 (2009) were identified. Cases of pandemic influenza had already been identified in Hungary and at the time the community influenza-like illness rate was 7.8 per 100,000 against an estimated 3.6 per 100,000 at the music festival.

The third type of outbreak report described by Saenz et al. [33] involved a large, international medical conference held in Iran during September 1968 at the early stages of the 1968/1969 pandemic. The 7-day conference was attended by over 1000 participants from all over the world. It was estimated that about a third of the participants developed an influenza-like illness with an overall attack rate of 36%. The virus was isolated from throat and nasal washings; this was found to be the pandemic A/Hong Kong/68 virus. There was evidence that close contacts of returning attendees were also affected, but none of the episodes led to a rapidly expanding focus of infection. The high attack rate raises the question of whether the indoor setting may represent a particularly high risk.

Lastly, there was an innovative study examining the 2009 A(H1N1) outbreak in Mexico [34] . The authors described the details of 202 cases and then used a series of epidemiological tools to map the spread of the virus throughout the country. They identified two major contributory factors to the rapid spread of influenza: one was the very high population density in parts of Mexico City; the other was the massive religious festival involving more than 2 million people, which took place in the cityÕs Iztapalapa neighbourhood at the very early stage of the outbreak, shortly before the new virus was identified. It is suggested by the authors that this temporal association between [39] Low Schenkel [40] Low the mass gathering event and the subsequent increase in numbers of cases may reflect participants returning to their own neighbourhoods across the city, and from there to other parts of the country.

Inevitably for a subject of research such as mass gathering restrictions, where prospective studies present serious practical challenges, researchers have sought to utilize historical data to try to draw out major lessons for current impact. This review included three historical analyses of the non-pharmaceutical responses to the 1918-1919 influenza pandemic.

Markel et al. [37] examined the variety of nonpharmaceutical interventions that were deployed in 43 American cities during the 1918-1919 influenza pandemic. The combination of school closures and concurrent public gathering bans was implemented in 34 (79%) of the 43 cities and was the commonest combination of measures deployed. Applied early in the pandemic, this combination was significantly associated with reductions in the weekly excess death rates. Hatchett et al. [36] undertook a similar analysis on a smaller number of American cities where the timing of 19 different types of non-pharmaceutical interventions was available. They found that the early application of multiple interventions showed a trend towards lower cumulative excess mortality, but that no single intervention showed an association with improved aggregate outcomes for the pandemic. Both studies suggested that for non-pharmaceutical interventions to be beneficial, they should be applied early and in a sustained manner.

A further review of the U.S. public health response to the 1918 pandemic by Aimone [35] gave conflicting results. This review examined the public health response in New York City. In New York during the 1918 pandemic, mass gatherings were not prohibited nor were schools closed; instead, the city opted for a policy of staggered business hours to avoid rush-hour crowding, enhanced surveillance so that cases were quickly identified and isolated, and an intensive program of health education. The reported outcome measures for New York City were comparable with those seen in other American cities, and New York City experienced one of the lowest excess death rates on the eastern seaboard of the United States.

Surveillance reports from four major sporting events within the last decade were considered suit-able for review [38] [39] [40] [41] . The study by Gundlapalli et al. [38] reported the experience of influenza surveillance during the Winter Olympics at Salt Lake City in 2002. No indication of the numbers attending was given, but it is assumed that the numbers were large. Twenty-eight cases of confirmed influenza from three clusters of influenza-like illness (ILI) were identified and these were restricted to either participants in the Games or support staff for the Games. The clusters consisted of 12 members of a national team who trained and lived together, 8 participants of a sport and 13 law enforcement officers who worked and lived in close proximity.

Lim et al. [39] reported on the experience of managing the Asian Youth Games at Singapore in June 2009. These games involved over 2000 athletes and officials from 43 countries. Although numbers of spectators are not given, it is assumed that the crowds were large. At the start of the Games, Singapore had already reported 600 confirmed cases of H1N1 (2009). Six laboratory-confirmed cases of H1N1 (2009) were identified during the 8 days of the event-four on one football team. No information on the numbers of confirmed H1N1 (2009) in the population after the Games was available.

There was no increase in ILI activity during the Winter Olympics in Torino in 2006 [41] . The final study by Schenkel et al. [40] reported on the experience of syndromic surveillance during the FIFA World Cup in Germany during June/July 2006. At the time, Germany was experiencing a very large outbreak of measles. Measles is essentially spread by the respiratory route and is highly infectious. However, despite enhanced daily surveillance, no outbreaks of respiratory disease or measles associated with the World Cup were detected.

As stated in Section 2.6, the approach to quality assessment of the reviewed papers was based on a pragmatic framework for categorizing studies into low-or high-risk for bias, depending on the presence or absence of significant methodological limitations. A high risk of bias was attributed to papers with at least two significant methodological problems as identified by the review authors (Table 6 ).

The evidence to help address important public health questions around mass gatherings and influenza transmission is sparse, especially in the context of an influenza pandemic. In addition, the topic does not lend itself to ease of scientific investigation and there are probably many who may feel that it is self-evident that mass gatherings facilitate the transmission of infectious diseases. This systematic search of the literature identified a limited number of studies that addressed the review questions regarding whether mass gatherings are associated with influenza transmission and whether restricting mass gatherings reduces the spread of influenza within the community.

In attempting to understand and describe a situation or intervention, a common understanding or definition is essential. There is currently no generally accepted definition of what constitutes a mass gathering. However, the literature on mass gathering medical care highlights an emerging consensus amongst those providing emergency medical care at organized events. In this setting, mass gatherings are considered to be organized events with more than 1000 people in attendance [4] [5] [6] . A recently published guidance document from the World Health Organisation (WHO) expanded the term to cover any organized or unplanned event involving enough people to ''strain the planning and response resources of the (host) community, State or nation'' [42] .

The major limitation in trying to define mass gatherings is that any single definition would inevitably be too simplistic as it would need to incorporate events as diverse as the Hajj (lasting about 1 month and involving between 2 and 3 million people), and a football match (involving several thousand spectators over a period of about 2 h).

A system for classifying mass gatherings on the basis of size and duration is lacking and may be required.

In recognition of the difficulties of conducting hypothesis-based studies that directly implement and assess the effects of restrictions of mass gatherings in real life, an indirect approach was taken to address the review questions as follows:

To address whether mass gatherings are associated with influenza transmission, evidence was derived from the following: -A quasi-experimental study that was primarily designed to quantify vaccine efficacy in the form of a non-randomized trial. -Observational studies that assessed participants before and after exposure to mass gathering events. -Reports of influenza outbreaks and other respiratory illnesses at mass gathering events. -Communicable disease surveillance reports from some major events.

A number of studies [18] [19] [20] [21] [22] have consistently demonstrated, over a number of years, that respiratory virus transmission occurs amongst pilgrims attending the annual Hajj in Saudi Arabia, and it is recognized as an issue of international public health significance [43] [44] [45] [46] that could be particularly important in a pandemic situation. A significant proportion of pilgrims are affected by symptoms of either an influenza-like illness or an acute respiratory illness with the proportion affected reaching about 40% in some studies [19, 20] . The Hajj is, however, a unique event with almost 3 million people converging on a relatively small geographic area for a period of at least 5 days, extending up to 4 weeks. Crowd density is very high and overcrowding in the living accommodation is common. Given the unusual nature of this event, the applicability of these findings to other mass gatherings is therefore limited. There is also a question regarding the fact that several of these studies are based on clinical criteria that could be due to other viral respiratory illnesses besides influenza.

In a small number of outbreak studies involving influenza-like illness and confirmed influenza at large music festivals, there is varying evidence about the extent to which influenza transmission occurs. Outbreaks were based on laboratory diagnosis, and transmission was confirmed in all, though they had varying infection rates ranging from roughly 3 to 25 per 100,000. Two recent studies undertaken during the 2009 pandemic suggest that at the beginning of a pandemic, these gatherings may act as seeding events [30] ; indeed it has been suggested at the early stages of the 1999 pandemic A(H1N1), a huge Easter season mass gathering involving 2 million people may have helped in propelling the first wave of the outbreak throughout Mexico [34] .

There is no compelling evidence from event surveillance reports, such as international athletic events, the World Cup and the Winter Olympics, to indicate that these major events significantly increase community transmission of influenza. Outbreaks did occur in some cases, but these always seemed restricted to the actual competitors and staff rather than the crowds attending or the wider community.

Together, all of these reports point to the potential for influenza outbreak and transmission in connection with large, multiple-day, open-air events. Thus there is some evidence to indicate that mass gatherings may be associated with an increased risk of influenza transmission, but it seems to be very variable.

The type of mass gathering event seems to be of considerable importance in terms of the risk of influenza transmission ( Table 7) . Most of the evidence supporting the role of mass gatherings in the transmission of influenza comes from events where there are crowds with high crowd densities (which may be theoretically estimated at >5 people per square metre), and where the participants are likely to live close together for prolonged periods, e.g. the Hajj pilgrimage [18] [19] [20] [21] [22] and large musical festivals [28] [29] [30] . In these events, accommodations which are already likely crowded are also likely to be relatively basic, such as communal camp-style living, with the probable risk of suboptimal hygiene facilities. It seems apparent that events where close contact among participants extends beyond event venues and into accommodation areas are most associated with influenza. Event size, per se, does not seem to be a critical factor.

In contrast, there is no convincing evidence that major organized sporting events are associated with significantly increased influenza transmission in those attending the event [38] [39] [40] . An important example of this contrast comes from Serbia, where two major events of different type and scale happened to coincide in 2009, providing a ''natural experimental'' opportunity for comparison [31] . In the larger event, only four event-linked confirmed influenza cases occurred at the World University Games held over a 12-day period and involving almost 25,000 athletes and staff with about 500,000 spectators. However, in a relatively smaller event held in the same month and within the same country, as many as 47 event-linked confirmed cases occurred at a 4-day music festival with around 190,000 participants.

Furthermore, in surveillance reports from recent major international sports competitions, cases or small clusters of influenza were reported, but these were mainly among the event participants rather than among the overall population of people exposed to the events [38, 39] . This was true even for one major event that took place within a pandemic context [39] . These events showed no clear evidence of influenza transmission, indicating that influenza may not be a significant cause for concern at modern world sports events. This view is supported by the surveillance report from the 2006 World Cup in Germany [40] , where instances of transmission were not reported, meaning that they were either not detected or very low.

This situation may in part be explained by the brief transitory nature of contact in the crowds in highly organized international sports festivals such as the Olympics and the World Cup, which are usually seated events with good spacing in-between seats and mostly in open-air settings with dilution of any infectious droplets that may be generated. The apparently low or absent influenza transmission at such events may also reflect the contemporary fact that many people who attend major sports championships tend to have planned their visit a long time ahead, as ticket sales usually start months or even years in advance. Spectators as well as participants tend to stay in more conventional accommodations such as hotels rather than tents or other forms of portable or camp-style quarters with highly crowded conditions. It is also important to note that contemporary major events are now deploying increasingly developed systems for infectious disease surveillance and control [47] , which are crucial for early detection and containment where possible.

The other situation of note relates to indoor events such as large conferences, typified by the international medical conference held over 7 days in Iran during the early stages of the 1968/1969 pandemic [33] . During this meeting it was estimated that about a third of the participants developed an influenza-like illness with an overall attack rate of 36%; the pandemic virus was isolated from those cases where testing had been undertaken. This potentially highlights the role that ventilation may play in the reduction of influenza transmission and is another factor worthy of consideration.

In summary, the type of mass gathering event seems to influence the risk of influenza transmission, key factors being the degree of crowdedness, the event duration and, possibly, whether the event is held indoors or outdoors. Multiple-day events with crowded communal accommodations may be the links to mass gatherings most associated with influenza.

To address whether mass gathering restrictions can reduce influenza transmission, the mainly relevant papers found were archival studies of the 1918-1919 influenza pandemic [35] [36] [37] and an analysis of the 2003 SARS outbreak [32] . No relevant randomized controlled trials were found, reflecting the practical difficulties that such studies would involve.

Mass gatherings of varying dimensions were restricted at a large number of American cities during the 1918-1919 period. The reports again highlighted the difficulty of interpreting what was meant by a mass gathering, e.g. including schools, cinemas, theatres and other public places. In general, evidence suggests that these measures had a beneficial effect, especially where implemented early in the course of the outbreak [36, 37] . However, these benefits were not universal across all the cities [35] .

Restrictions were typically implemented as part of a set of interventions, e.g. combining quarantine and isolation policies with banning mass gatherings. As a result, it is extremely difficult to tease out the individual effects of mass gathering restrictions alone. Using multivariate techniques, investigators attempted to isolate the differential effects of individual restriction measures and found indications that certain interventions (such as closures of entertainment venues) had measurable specific impact [36, 37] , but this evidence is limited.

An analysis of the 2003 SARS outbreak in China attempted to probe the impact of mass gathering restrictions that were applied in a contemporary setting [32] . However, as with the historical studies, it was not possible to distinguish the specific effects of mass gathering restrictions from amongst the broad range of other public health interventions that were applied.

In summary, there is some evidence that when applied early and in tandem with other public health measures, such as isolation and quarantine and closures of educational institutions, mass gathering restrictions may help in reducing transmission. In a pandemic like that experienced during 2009, it is unlikely that the measures described above could be justified; however, in a much more severe pandemic, the cost-benefit equation could easily shift the other way. The application of bans on mass gatherings and other related public health measures are therefore highly dependent on an early indication of the severity as measured by its impact on individuals and society.

There are two further, critical domains of uncertainty that need to be considered in the development of evidence-based guidance and policies regarding mass gatherings. The first domain relates to issues around the current understanding about how influenza is spread, and factors that can affect transmissibility (e.g. host factors, pathogen factors, environmental factors and particle size) [9] . Key questions remain in these areas, which may be important in making specific recommendations regarding particular types and scope of mass gathering restrictions.

The second domain impinging on the potential effectiveness of any public policy on mass gatherings includes the whole range of factors affecting adherence and compliance. For instance, the experience of the 2009 pandemic has raised significant questions around how willing people might be to comply with bans imposed on mass gathering restrictions [48] [49] [50] [51] . Other challenging issues include the problematic ethical and legal frameworks for implementing restrictions for public health purposes [52, 53] , as well as considerable logistical and economic implications. If longplanned events were to be cancelled, who would be liable for the huge personal, corporate and national costs that such cancellations might incur? In considering policy recommendations within a pandemic context, the most practical approach for all but the more severe pandemics may be a strategy of encouraging voluntary restrictions. This would involve giving the public the best available information and advising rather than legislating that organizations and individuals avoid non-essential events where there is at least some evidence of transmission risk. For such a strategy to have a chance of succeeding, it would be most important to have in place a carefully and sensitively prepared communication strategy, since recommendations to avoid public gatherings are likely to run against powerful social pressures to do otherwise [54] . The success of any public health strategy-and this must be particularly true in relation to a potential or imminent pandemic situation-requires the building of trust [55] and an intelligent and purposeful engagement of the public even prior to the event [56] .

There are other important issues to note. Although this review has focused on mass gatherings, limiting transmission of influenza clearly requires a multifaceted approach. Some studies in this review reflected such an approach; for example, in the historical outbreak investigations where restrictions on mass gatherings were combined with other non-pharmaceutical measures [32, 36, 37] . It would be prudent to apply the best evidence relating to other social distancing interventions in conjunction with any specific policies on mass gatherings.

An additional question that should be considered is whether non-pharmaceutical health interven-tions such as mass gathering restrictions, actually reduce the health burden of influenza, or only modify the epidemiology by temporarily delaying the eventual impact of an event such as a pandemic. It is tempting to speculate on the possibility of a reduction in disease burden; historically, some of the American cities that implemented mass gathering restrictions during the 1918-1919 pandemic seemed to achieve significant reductions in peak morbidity and mortality that may well have reduced the overall burden. However, pragmatically, countries like the United Kingdom aim to achieve a slower spread, to prolong the troughs and to flatten the peaks of the epidemic curves to buy time for countermeasures to take effect and to enable services to cope with a lower volume of activity.

In this context, the most important point to emphasize is that mass gathering restrictions must only be a part of a range of interventions. The synergistic effects of multiple interventions is what would allow the best chance to achieve significant reductions in overall burden.

More evidence is needed on this subject. There is a need for well-designed studies to more accurately quantify the nature of the infections causing flu-like symptoms at mass gatherings, confirm the key parameters that influence the transmission of influenza in these settings, and to directly assess the impact of mass gathering restrictions. But as ''gold standard'' randomized comparison designs are extremely difficult to apply to this kind of intervention, attention needs to focus on optimal observational study options. From a UK perspective, British people travelling abroad for mass gatherings such as the Hajj or other large events could be approached for inclusion in prospective studies for comparison with appropriately matched, nontravelling controls, with care being taken to avoid drawbacks observed in existing studies. Within the UK, mass gatherings ranging from indoor events in theatres and cinemas, to outdoor events such as football matches and major musical events like the Glastonbury festival or TravellersÕ horse fairs represent potential opportunities for carefully designed prospective cohort studies to be undertaken. The biggest issue is persuading funders that given the current paucity of good studies, such further research is still needed.

As previously noted, this review has examined an intervention area in which there are a limited num-ber of relevant studies and there is limited common understanding of what constitutes a mass gathering. Despite these challenges, it has been possible to extract limited strands of evidence that may be useful towards policy development. Also, within the boundaries of the inclusion criteria, and despite the limited work timescale, this review was able to capture all the relevant studies identified by recent systematic reviews investigating non-pharmaceutical interventions designed to limit transmission of influenza and/or other respiratory viral infections [7, 8, 57] .

However, due to time constraints in carrying out this work, as described in Section 2.5, while every effort has been made to be as comprehensive as possible, it is acknowledged that the rapid evidence assessment process necessarily involved some limitations. The literature search is not necessarily exhaustive and there is therefore a possibility of incomplete retrieval of all potentially relevant studies.

While a range of study designs were reviewed, there was not a single randomized controlled trial that was suitable for inclusion. This is of course not surprising, given the formidable logistical, cost and ethical hurdles that make large-scale experimental epidemiological studies of the restriction of mass gatherings impractical and probably impossible. Some of the included studies had significant design and quality issues as duly reflected in the individual paper summaries and the discussion, and highlighted in Tables 4 and 6. A common issue with a number of the studies included in the review is that they depended on clinical symptoms like cough or syndromes such as influenza-like illness, rather than laboratory diagnosis of influenza. In such studies it is not possible to isolate the impact of influenza as opposed to other respiratory viruses, and the risk of influenza could potentially be over-interpreted. In the studies where laboratory testing was undertaken, influenza was only confirmed in minority proportions of people with respiratory symptoms, ranging between 6% and 14% [23, 24, 27] , but possibly up to a roughly estimated 40% in one study [25] .

This review did not include specific ''specialized'' settings that may arguably be regarded as mass gatherings, such as groups of people travelling for periods of time on cruise ships. The authors took the view that any transit-related settings would be best considered under a separate evidence review of influenza transmission through transportation. Modelling studies were also excluded, and it is acknowledged that their inclusion may offer an additional dimension in order to build a fuller picture.

The application of quantitative techniques could have potentially enhanced the simple narrative approach that was adopted for the analysis. However, there are insufficient studies presenting quantitative data on this subject, and moreover, there is a high level of heterogeneity amongst the identified studies.

In conclusion, there are limited data indicating that mass gatherings are associated with influenza transmission. Some evidence suggests that restricting mass gatherings together with other social distancing measures may help to reduce transmission. However, the evidence is not strong enough to advocate legislation or proscription. Therefore, in a pandemic situation a cautious policy of voluntary avoidance of mass gatherings would be prudent.

None declared.

",0.6651036492108932
Effect of non-pharmaceutical interventions for containing the COVID-19 outbreak in China,The COVID-19 outbreak containment strategies in China based on non-pharmaceutical interventions (NPIs) appear to be effective. Quantitative research is still needed however to assess the efficacy of different candidate NPIs and their timings to guide ongoing and future responses to epidemics of this emerging disease across the World.,"could have shown a 3-fold, 7-fold, and 18-fold increase across China, respectively. Results also suggest that the social distancing intervention should be continued for the next few months in China to prevent case numbers increasing again after travel restrictions were lifted on February 17, 2020.

The NPIs deployed in China appear to be effectively containing the COVID-19

outbreak, but the efficacy of the different interventions varied, with the early case detection and contact reduction being the most effective. Moreover, deploying the NPIs early is also important to prevent further spread. Early and integrated NPI strategies should be prepared, adopted and adjusted to minimize health, social and economic impacts in affected regions around the World.

Bill & Melinda Gates Foundation; EU Horizon 2020; National Natural Science author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint

The COVID-19 outbreak has spread widely across China since December 2019, with many other countries affected. The containment strategy of integrated nonpharmaceutical interventions (NPIs) including travel bans and restrictions, contact reductions and social distancing, early case identification and isolation have been rapidly deloyed across China to contain the outbreak, and the combination of these interventions appears to be effective. We 

To our knowledge, this is the most comprehensive study to date on quantifying the relative effect of different NPIs and their timings for COVID-19 outbreak containment, based on human movement and disease data. Our findings show that NPIs, inter-city travel restrictions, social distancing and contact reductions, as well as early case detection and isolations, have substantially reduced COVID-19 transmission across China, with the effectiveness of different interventions varying. The early detection and isolation of cases was estimated to prevent more infections than travel restrictions and contact reductions, but integrated NPIs would achieve the strongest and most rapid effect. Our findings contribute to improved understanding of integrated NPI measures on COVID-19 containment and can help in tailoring control strategies across contexts.

Given that effective COVID-19-specific pharmaceutical interventions and vaccines are not expected to be available for months, NPIs are essential components of the public health response to the ongoing outbreaks.

. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint

As of February 28, 2020 the COVID-19 outbreak has caused 78,961 confirmed cases (2791 deaths) across China, with the majority seen in Wuhan City, and 4691 cases (67 deaths) reported in the other 51 countries. 1 Further spread has occurred to all populated continents of the World, with many anticipating that a pandemic is approaching. 2, 3 As an emerging disease, effective pharmaceutical interventions are not expected to be available for months, 4 and healthcare resources will be limited for treating all cases. Nonpharmaceutical interventions (NPIs) are therefore essential components of the public health response to outbreaks. 1, [5] [6] [7] These include isolating ill persons, contact tracing, quarantine of exposed persons, travel restrictions, school and workplace closures, and cancellation of mass gathering events. [5] [6] [7] These containment measures aim to reduce transmission, thereby delaying the timing and reducing the size of the epidemic peak, buying time for preparations in the healthcare system, and enabling the potential for vaccines and drugs to be used later on. 5 For example, social distancing measures have been effective in past influenza epidemics by curbing human-to-human transmission and reducing morbidity and mortality. [8] [9] [10] Three major NPIs have been taken to mitigate the spread and reduce the outbreak size of COVID-19 across China. 11, 12 First, inter-city travel bans or restrictions have been taken to prevent further seeding the virus during the

Chinese new year (CNY) holiday. People in China were estimated to make close to 3 billion trips over the 40-day CNY travel period from January 10 to February 18, 2020. 12,13 A cordon sanitaire of Wuhan and surrounding cities in Hubei Province was put in place on January 23, 2020, just two days before CNY's day on January 25. However, Wuhan's lockdown is likely to have occurred during the latter stages of peak population numbers leaving the city before CNY, with around 5 million people likely leaving before the start of the travel ban, departing into neighbouring cities and other megacities in China. 14 Since CNY's day, travel restrictions in other provinces were also put in place across the country.

The second group of containment measures involves improving the screening, contact tracing, identification, diagnosis, isolation and reporting of suspected ill persons and confirmed cases. 11 Since January 20, particularly in Wuhan, searches for cases, diagnosis and reporting have sped up across the country.

. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint Local governments across China encouraged and supported routine screening and quarantine of travellers from Hubei Province in an attempt to detect COVID-19 infections as early as possible. In Wuhan, where the largest number of infected people live, residents were required to measure and report ther temperature daily to confirm their onset, and those with mild and asymptomatic infections were also quarantined in ""Fang Cang"" hospitals, which are public spaces such as stadiums and conference centres that have been repurposed for medical care. 11 The average interval from symptom onset to laboratory confirmation has dropped from 12 days in the early stages of the outbreak to 3 days in early February, highlighting how the efficiency of disease detection and diagnosis has greatly improved. 15, 16 Third, inner-city travel and contact restrictions were implemented to reduce the risk of community transmission. This involved limiting individual social contact, using personal hygiene and protective measures when people needed to move in public, and increasing the physical distance between those who have COVID-19 and those who do not. 11 As part of these social distancing policies, Chinese government encouraged people to stay at home as much as possible, cancelled or postponed large public events and mass gatherings, and closed libraries, museums, and workplaces. 17, 18 Additionally, to fully cover the suspected incubation period of COVID-19 spread before Wuhan's lockdown, the CNY and school holidays were also extended, with the holiday end date changed from January 30 to March 10 for Hubei province, and Feb 9 for many other provinces. [19] [20] [21] The implementation of these NPIs has coincided with the rapid decline in the number of new cases across China, albeit at high economic and social costs. 15, 16 On February 17, the State Council required localities to formulate differentiated county-level measures for precise containment of the COVID-19 outbreak and the restoration of socioeconomy affected by the outbreak. 22 The timing of implementing and lifting interventions is likely to have been and continue to be important, to take advantage of the window of opportunity to save lives and minimize the economic and social impact. 23, 24 The increasing numbers of cases of COVID-19 outside China and establishment of secondary transmission in multiple places highlights its pandemic potential. The best available scientific evidence is therefore required to design effective NPI strategies and disseminate this knowledge urgently to . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint help policy makers assess the potential benefits and costs of NPIs to contain COVID-19 outbreaks. Some previous studies have preliminarily explored the lockdown of Wuhan, [25] [26] [27] travel restrictions, 28-30 airport screening, 31, 32 and the isolation of cases and contact tracing for containing virus transmission, respectively. 33, 34 The conclusions of these studies are persuasive, there are still key knowledge gaps on the effectiveness of different interventions. 15 To fully justify the preparation, implementation, or cancellation of various NPIs, policy makers across the World need evidence as to the combination and timings of each, which remains lacking.

Based on near-real time human movement and disease data, here we conducted an observational and modelling study to develop a travel network-based modelling framework. We aimed to reconstruct COVID-19 spread across China and assess the effect of the three major groups of NPIs mentioned above. Given the expanding landscape of epidemics across the World, our findings contribute to improved understanding of the effect of NPI measures on COVID-19 containment and can help in tailoring control strategies across contexts.

A travel network-based stochastic susceptible-exposed-infectious-removed 

Three population movement datasets, obtained from Baidu location-based services providing over 7 billion positioning requests per day, 35, 36 were used in this study to measure travel restrictions and social distancing across time and . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint space. The first is an aggregated and de-identified dataset on near-real time daily relative outbound and inbound flow of mobile phone users for each prefecture-level city in 2020 (340 cities in mainland China were included) to understand mobility patterns during the outbreak. The daily outflow from each city since Wuhan's lockdown and travel restrictions that were applied on Table S1 ), as the policies of travel restriction and social distancing measures were implemented and occurred at the same time across the country.

We also collated data of the first case reported by county across mainland China to measure the delay from illness to case report as a reference of the . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint .

1 0 improved timeliness of case identification, isolation and reporting during the outbreak (appendix Table S2 ). The daily reported number of COVID-19 cases in Wuhan City, Hubei Province and other provinces were also used to futher validate our results. These case data were collated from the websites of national and local health authorities, news media, and publications (appendix note). 14,39,40

We constructed a travel network-based SEIR modelling framework (the code of model is available online at https://github.com/wpgp/BEARmod) for before-and-after comparable analyses on NPI efficacy. First, we simulated the COVID-19 spread across a metapopulation, where each population represented a city across China. Within each population, numbers of susceptible, exposed, infectious, and recovered/removed people were tracked per day. 3 The epidemiological parameters estimated for the early stage of the outbreak in Wuhan were used to parameterise the epidemic before widely implementing the NPIs. 41 The number of new people that could become exposed was calculated based on the daily contact rate ܿ and the number of infectious people in the city ‫ܫ‬ , and this was turned into a number of newly exposed people after multiplying by the fraction of people in ݅ who were susceptible (accounting for potential encounters with already-infected people, which did not lead to a new infection). Table S1 ). Finally, infectious people moved between cities, where the probability of moving from city ݅ to city ݆ ‫(‬ ) was equal to the proportion . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . 

Ethical clearance for collecting and using secondary data in this study was granted by the institutional review board of the University of Southampton (No. 48002). All data were supplied and analysed in an anonymous format, without access to personal identifying information.

. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint

The funder of the study had no role in study design, data collection, data analysis, data interpretation, or writing of the report. The corresponding authors had full access to all the data in the study and had final responsibility for the decision to submit for publication.

As of February 29, 2020, a total of 79,824 COVID-19 cases were reported in mainland China, with most cases (61%) having occurred in Wuhan ( Table 1 ).

The outbreak increased exponentially prior to CNY (Figure 2) . However, the peak of epidemics across the country quickly appeared through implementing strong and comprehensive NPIs, including dramatic reductions in travel and contact, and significant improvements in the timeliness of case detection and reporting across the country (Figure 1 and appendix Tables S1 and S2). We (Table 1) . However, the apparent effectiveness of different interventions varied (Figure 3 and appendix Figure S1 ). The lockdown of Wuhan might not have prevented the seeding of the virus from the city, as the travel ban was put in place at the latter stages of outbound travel prior to CNY's day ( Figure 3 ).

Nevertheless, if inter-city travel restrictions were not implemented, cities and provinces outside of Wuhan would have received more cases from Wuhan, and the affected geographic range would have expanded to the remote . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint northern and western areas of China (Figure 4a and appendix Figure S2 ).

Generally, the early detection and isolation of cases was estimated to quickly and substantially prevent more infections than contact reduction and social distancing across the country (5-fold versus 2.6-fold), but without the intervention of contact reductions, in the longer term, the epidemics would increase exponentially. Therefore, integrated NPIs would achieve the strongest and most rapid effect on COVID-19 outbreak containment ( Table 1 ).

The timings of intervention implementation are also critical. The number of cases could be dramatically reduced by 66%, 86%, and 95%, respectively, if the NPIs could be conducted one week, two weeks, and three weeks earlier than the actual timing across the country ( Figure 5) CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint sensitivity analyses of multiple parameters, our results on the relative effects of NPIs are robust to the possibility of changes in parameters. Considering the delay in case reporting, our approach and findings can provide critical and early evidence for outbreak control decision-making.

However, our study has several limitations. First, as our simulations were based on the parameters estimated for the cases found in the early stage of the outbreak in Wuhan, , which might not account for the asymptomatic and mild infections, our study may underestimate the total number of infections.

However, public awareness and enchanced case searching remained high throughout the study period, and a high proportion of infections was likely to have been detected, with nearly all reported cases eventually subjected to laboratory testing. Second, our findings could be affected by bias and confounding because the modelling is based on observations over a short period. Although we have shown that the apparent fall in incidence of COVID-19 since CNY's day in China is likely to be attributed to the interventions taken, we cannot rule out the possibility that the decrease was caused by varying timings and intensities of various NPIs taken in different areas as well as some other unknown seasonal factors, e.g. temperature and absolute humidity. 43 

We declare no competing interests. 1 8

The data of COVID-19 cases reported by county, city, and province across China are availalable from data sources detailed in the Supplementary, and the average days from illness onset to report of the first case by each county used in the modelling are detailed in appendix Tabel S2. The mobile phone datasets analysed during the current study are not publicly available since this would compromise the agreement with the data provider, but the information on the process of requesting access to the data that support the findings of this study are available from Dr Shengjie Lai (Shengjie.Lai@soton.ac.uk), and the data of travel and contact reductions derived from the datasets and used in our model are detailed in appendix Tabel S1.

. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint 1

Vertical lines: orange -date of Wuhan's lockdown; purple -Chinese New Year's day.

The median and interquatile range (blue) of estimates of COVID-19 cases are presented with reported cases (red) by date of illness onset as of February 13, 2020.

The reported data of COVID-19 cases in the scatterplot were obtained from the Chinese National Health Commission, as of February 29, 2020.

. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.03.20029843 doi: medRxiv preprint It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the

The copyright holder for this preprint It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the

The copyright holder for this preprint 

",0.6644945936737017
Indications for healthcare surge capacity in European countries facing an exponential increase in COVID19 cases,"European healthcare systems face rapidly increasing pressure from COVID-19. We calculated pressures on EU healthcare systems by relating both country-specific accumulated COVID-19 deaths (intensityapproach) and active COVID-19 cases (magnitude-approach) to various estimates of hospital beds. On March 14 2020 -relative to Italy on March 11-we found Spain, Luxembourg, Switzerland and France to experience the highest pressure using the intensity-approach, versus Iceland, Denmark, Norway, Sweden, Spain, Switzerland and Slovenia using the magnitude approach.","In the past days and weeks, it has become clear the SARS-COV-2 poses an insurmountable health threat on a global scale. Europe, and especially Italy with 17 666 cases reported by March 13, 2020, faces an exponential increase in the number of new COVID19 cases. Several institutions have communicated the necessity to 'flatten the curve' in order to lower pressure on health care institutions and to buy time for antivirals and other medication to become available in the short term and vaccines in the longer term.

Remuzzi & Remuzzi report that in Italy ICU bed occupation will exceed capacity [1] . Since other countries are considering to implement large scale interventions such as school closure at the right time [2, 3] , an important urgent question is how close other countries are from reaching an Italy-like pressure on the health system, and which countries are closest to such a situation. Furthermore, policymakers require information on the magnitude of additional health care capacity that is needed for the fu ture course of the epidemic, when at a later stage, new exponential growth may occur.

We calculated the relative pressure on national healthcare systems by determining the number of deaths and cases per hospital bed for each country. Three categories of hospital beds were considered: (1) available hospital bed capacity and (2) curative hospital bed capacity, as reported by Eurostat [4] from 2017 or 2018 (most recent data extracted) and (3) critical care hospital bed capacity as reported by Rhodes et al. [5] . As a first measure, we compared the number of COVID-19 related deaths relative to the number of beds, which provides an approximation of the severity of cases up to the present, and therefore also more likely to approximate pressure experienced on the health care system. The accumulated deaths to beds ratio provides an assessment that is likely less susceptible to inter-country variation in testing practices, and associated variation of underreporting of cases. We refer to this measure as the ""intensity-approach"". In a second measure, we compared the number of active COVID-19 cases, relative to the number of beds. The latter is referred to as the ""magnitude-approach"". We used the number of active COVID-19 cases and cumulative number of COVID-19 related deaths in Italy on March 11, 2020 as a benchmark. These numbers, 10 590 active cases and 827 deaths were chosen as a benchmark, based on Remuzzi & Remuzzi [1] . The authors forecast the number of cases attending ICU to exceed the ICU bed capacity by March 14. As such, this benchmark acts as a reference for a ""soon to be overloaded"" healthcare system.

We retrieved the number of active cases and the cumulative number of COVID-19 related deaths on March 14, 2020 [6] and calculated the proportion per hospital bed for each of the hospital bed types for 30 countries. We calculated, for each country, six ratios by dividing the number of active cases and cumulative deaths by the number of beds for each bed type. Afterwards, we normalized ratios by the bedspecific benchmarks for Italy, which is defined as the cumulative number of COVID-19 related deaths (intensity-approach) or active cases (magnitude-approach) on March 11, divided by the number of all hospital beds, curative care beds and critical care beds, respectively.

In Italy, the concentration of critical care beds is at the higher end with 12.5 critical care beds per 100 000 population, compared to the European average of 11.5 [5] . In Figure 1 , using the intensity-approach, Italian hospitals are extremely impacted if we look at the numbers in terms of deaths per hospital bed (left axis). Spain is, next to Italy, the most severely affected country at about 45% (right axis), compared to the benchmark situation. Luxembourg, Switzerland, the Netherlands and France's health systems are also experiencing relatively high pressure, but at this stage still lower than 15%, in comparison to the benchmark situation. In Figure 2 , we show that the number of cases relative to the number of hospital beds is especially high in Iceland and the Scandinavian countries. Indeed, in these countries, the outbreak of COVID-19 has been very high, whereas hospital bed capacity is typically at the lower end for these countries. The pressure on the Icelandic healthcare system is even higher than in Italy. The Scandinavian countries are followed by Spain, Switzerland, Slovenia and Luxembourg, which all face severe pressure on ICU wards especially. But even if we look at all available hospital beds as a measure of health care capacity, most EU countries are relatively close to the Italian situation on March 11, 2020 -our benchmark situation. . CC-BY-NC 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

The copyright holder for this preprint . https://doi.org/10.1101/2020.03.14.20035980 doi: medRxiv preprint Figure 2 : Relative pressure on the healthcare systems in different countries by the ""magnitude-approach"". Ratios are normalized to the number of cases in Italy on March 11, 2020. Ratios were calculated for all hospital beds (blue), curative care beds (yellow) and critical care beds (red).

The measures proposed in this paper are an approximation to the current hospital capacity in Europe. Unfortunately, data on health care capacity is rel atively scarce and we have to rely on data from 2017 or 2018 for all available hospital beds and curative hospital beds. Data availability is even worse for critical care beds, on which we rely on multi-country data from 2010. Remuzzi & Remuzzi report ICU capacity of 5200 beds for Italy as a whole, which is lower than the 7550 critical care beds reported by Rhodes et al. It is likely that other EU countries have downsized their critical bed capacity too. In terms of curative beds per 100 000 population as reported by Eurostat, for most countries (all except Ireland, Bulgaria, Poland and Romania), we observed a downward trend in the past 10 years. Moreover, hospital capacity strain was recently found to be associated with increased mortality and decreased health outcomes [7] . Note that if we would assume the same extent of downsizing in Italy and other EU countries, our relative comparisons would not change. Our analysis shows that many European countries are soon to be confronted with a health care pressure that will exceed current hospital bed capacity. Based on the intensity-approach, we believe that for Spain, Luxembourg, Switzerland, The Netherlands and France the pressure on healthcare systems will soon reach Italy's levels. With respect to the magnitude-approach, Iceland, the Scandinavian countries, Spain and Switzerland were found to be more vulnerable. Where this is not yet done, policymakers should urgently expand their health care capacities to avoid pressure as experienced in Italy.

Funding statement: This work is funded by the Epipose project from the European Union's SC1-PHE-CORONAVIRUS-2020 programme, project number 101003688 . CC-BY-NC 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

(which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.14.20035980 doi: medRxiv preprint

",0.6628904760634577
Making difficult ethical decisions in patient care during natural disasters and other mass casualty events,"OBJECTIVE: Recent experiences in the United States with unprecedented terrorist attacks (9/11) and a devastating natural disaster (Hurricane Katrina) have demonstrated that the medical care of mass casualties during such disasters poses ethical problems not normally experienced in civilian health care. It is important to 1) identify the unique ethical challenges facing physicians who feel an obligation to care for victims of such disasters and 2) develop a national consensus on ethical guidelines as a resource for ethical decision making in medical disaster relief. STUDY DESIGN: A survey of pertinent literature was performed to assess experience and opinions on the condition of medical care in terrorist attacks and natural disasters, the ethical challenges of disaster medical care, and the professional responsibilities and responsiveness in disasters. CONCLUSIONS: It is necessary to develop a national consensus on the ethical guidelines for physicians who care for patients, victims, and casualties of disasters, and to formulate a virtuebased, yet practical, ethical approach to medical care under such extreme conditions. An educational curriculum for medical students, residents, and practicing physicians is required to best prepare all physicians who might be called upon, in the future, to triage patients, allocate resources, and make difficult decisions about treatment priorities and comfort care. It is not appropriate to address these questions at the time of the disaster, but rather in advance, as part of the ethics education of the medical profession. Important issues for resolution include inpatient and casualty triage and prioritization, medical liability, altered standards of care, justice and equity, informed consent and patient autonomy, expanding scope of practice in disaster medicine, and the moral and ethical responsibilities of physicians to care for disaster victims.","O n Tuesday, September 11, 2001 , the American Academy of Otolaryngology-Head and Neck Surgery Foundation was holding its scientific meeting in Denver, CO. The morning plenary sessions were underway when early reports of tragic events in New York City, Pennsylvania, and Washington, DC, began to be seen on the television screens in the common areas of the Denver Convention Center. As the events unfolded quickly, the scope of the tragedy soon became apparent. The leadership of the AAO-HNS/F made the decision to continue with the educational meeting and kept the attendees apprised of what was happening. Because the national airspace was closed to air travel, individuals and groups began making alternative travel plans, including purchasing cars, renting vans and trucks, and even chartering buses. For those physicians who lived in the eastern United States, returning to their homes and work was vitally important. Because there remained the possibility that terrorist attacks might occur in other parts of the country, everyone wished to travel home. Many physicians made the long trek to New York City to volunteer as medical care providers, if needed. There was a general sense of professional responsibility and willingness to help, even at personal risk. Fortunately, the terrorist events were limited in scope and location, but they remain the largest death toll from this type of event in the United States to date.

The worst natural disaster in US history occurred on August 29, 2005, when Hurricane Katrina devastated the Gulf cities of Biloxi, MS, and New Orleans, LA, and all areas between these cities. It is estimated that the forces of the hurricane were greater than that of the atomic bomb blasts on Hiroshima and Nagasaki, Japan, in World War II. The hurricane itself was a bad one, but the resultant loss of integrity of certain sections of the levees in New Orleans resulted in a tremendous flooding of much of the mid-city and isolation of many hospitals in the area. Dr Anna Pou, an AAO-HNS Fellow and academic colleague of many, was on duty at Memorial Medical Center in New Orleans. Although many other physicians in New Orleans were either fleeing the city's flooding with their families or unable to reach their medical facilities, Dr Pou made the brave decision to stay and provide medical care to the patients and staff who, like her, were isolated in the facility.

As a head and neck surgeon, Dr Pou was accustomed to caring for postoperative patients in the surgical intensive care unit, but during her ordeal at Memorial, she also took responsibility for caring for very sick and nearly terminally ill patients in an acute care long-term facility located within Memorial Hospital. As resources began to become scarce and the conditions in the hospital became more extreme, Dr Pou and other health care providers found it necessary to develop a simple priority system for patient evacuation when outside relief finally materialized. Under extremely harsh conditions, Dr Pou made medical decisions that cannot be second guessed by those who were not there. After her heroic efforts, Dr Pou found herself facing charges of homicide in the deaths of four elderly and critically ill patients at Memorial. Fortunately, after nearly 2 years of difficult personal and legal challenges, Dr Pou was exonerated by an Orleans Parish grand jury's refusal to indict her. During the ordeal, Dr Pou was supported by the AAO-HNS, the American Medical Association, the American College of Surgeons, and the Louisiana State Medical Society. [1] [2] [3] [4] [5] These societies acknowledged the difficult decisions that were required under conditions not previously experienced in the US civilian health care system.

There are multiple layers of ethical issues that have arisen in the aftermaths of the terrorism events of 2001 and the natural disaster of 2005. At the very foundation of the concerns raised is the question of how much personal risk and sacrifice are required or expected of physicians during such mass casualty events. At the higher level, it is the difficult issue of how to make ethical decisions in caring for patients, both existing patients and casualty patients, in the face of limited resources and expanding scope of the disaster. As has been clearly emphasized by the situation with Dr Anna Pou, an otolaryngologist-head and neck surgeon who voluntarily placed herself in a position to render a level of care to patients clearly beyond her training and expertise, most physicians in the United States are ill prepared to face the unique ethical decision making that is required in such situations. It is now apparent, with the constant threat of future terrorist attacks and natural disasters, that consensus national guidelines must be developed that will assist volunteer physicians to make ethical, yet appropriate, decisions in the midst of such uncommon and challenging situations.

There are both similarities and differences in the challenges posed by bioterrorism events and natural disasters. If taken in good part, bioterrorism events have the potential to place the physician at greater initial and temporally increasing personal risk for contamination and exposure to toxins or radiation from both the event and the affected patients. 6 With natural disasters, the initial event may pass, and the subsequent risks are lower with the passage of time.

Past and current disaster response plans have primarily been based on modern military models that have evolved over the years, as both the technology of war and the technology of medicine have advanced. 7 These models depend on a defined area of involvement, with initial treatment of casualties on site and subsequent evacuation by air or ground to a more sophisticated treatment facility in a nearby ""safe"" area. The terrorist events of 9/11 were sufficiently isolated so that this model was generally effective, although the Pentagon attack resulted in a rapid medical response on the grounds of the Pentagon itself. Using the military model and building upon the experiences of 9/11 and Hurricane Katrina, as well as the events of worldwide earthquakes and tsunamis, a new specialty of medicine is emerging-""disaster medicine."" A major issue that was found to be a major aspect of the aftermath of Katrina and is currently being addressed, is that of the ""surge capacity"" of US hospitals, in which acute casualties may overwhelm hospitals, requiring a prioritization of care of both the newly ill casualties and the chronically ill inpatients. 8, 9 There has been significant planning by agencies of the federal government since these two disasters, in good part because the United States was ill prepared to handle these disasters, both from a medical and a response preparedness perspective. The US Department of Health and Human Services issued a directive entitled Bioterrorism and Other Public Health Emergencies-Altered Standards of Care in Mass Casualty Events as a federal guide in response to the 9/11 terrorist events and the subsequent anthrax attacks. 10 One of the main issues addressed by the guide was ""how current standards of care might need to be altered in response to a mass casualty event in order to save as many lives as possible."" Additionally, the report proposed that ""the basis for allocating health and medical resources in a mass casualty event must be fair and clinically sound."" As part of the recommended use of altered standards of health and medical care, a model was proposed for emergency state health powers that recognized the difficulties of providers and institutions when providing care under stress with less than a full complement of resources. It was recommended that the plan would ""have to provide for 'hold harmless' agreements or grant immunity from civil or criminal liability under certain circumstances."" Rolfsen 11 addressed the ethical issues of liability or criminal prosecution in a disaster situation in the Journal of the Louisiana State Medical Society. He stated, ""During disasters such as Katrina, many deaths of both previously healthy and chronically ill patients can be expected. Despite society's tendency to attempt to place blame for these tragedies, attributing causation of the deaths to medical providers is a complex process."" Contributing to the complexity are the harsh and unusual conditions under which medical care must be provided in a disaster, and the overwhelming requirements for medical resources in the face of rapidly dwindling supply.

Medical care during such disasters should likely fall under a state's Good Samaritan act to provide freedom from undue legal liability for the providers.

A major issue for discussion and guidance is that of the willingness of physicians to participate in the care of patients injured or ill from terrorism attacks or extensive natural disasters. In the earlier example of the rallying of US physicians to be of assistance in the immediate aftermath of the 9/11 events, it can be contrasted to the isolation of physicians from New Orleans after Katrina who were unable to reach their hospitals because of flooding and criminal activities. Because most of the ongoing medical care of the Katrina victims took place at a distance from the actual flooding, many physicians volunteered to staff the shelters and make-shift medical treatment facilities where hundreds of thousands of individuals received medical care. Infectious diseases were rampant in the victims of the hurricane, and some analogy might be drawn from physician volunteerism in severe acute respiratory syndrome (SARS) pandemics. Ruderman and associates 12 reported on the issue of ""the duty to care"" by health care professionals. They noted that physicians and nurses who volunteered during the SARS pandemic in 2003 ""continue to struggle with the aftermath of the crisis."" They noted that there are no unique guidelines for the moral obligation of physicians to care for SARS victims and urged that ""organizations give clear indication of what standard of care is expected of their members in the event of a pandemic."" They further recommend that there is a pressing need to clarify the rights and responsibilities of health care providers during an infectious disease outbreak, and that ""these rights and responsibilities ought to be codified in professional codes of ethics.""

The American Medical Association has addressed the issue of physician obligation in disaster preparedness and response in its Code of Medical Ethics. 13 As part of the ""Opinions on Professional Rights and Responsibilities,"" the Code states: Because of their commitment to care for the sick and injured, individual physicians have an obligation to provide urgent medical care during disasters. This ethical obligation holds even in the face of greater than usual risks to their own safety, health, or life.

Yet there remains the question of whether physicians in the United States (and specifically otolaryngologist-head and neck surgeons) are properly prepared for, and willing to, participate in disaster relief of a significant magnitude, especially when there is grave personal risk. Alexander and associates 14 used a national, cross-sectional, random-sample survey in 2003 to address this issue. Seven hundred forty-four physicians responded to the survey. Only 43% of emergency physicians and 21% of primary care physicians agreed that they were generally well prepared to play a role in responding to a bioterrorism attack. The majority of the respondents believed that disaster preparedness should also include infectious pandemics.

Alexander and Wynia 15 expanded their examination of physicians' feelings about bioterrorism events and their responsibilities to their willingness to treat patients despite personal risk. Eighty percent of respondents reported that they would be willing to treat affected patients in the face of a hypothetical outbreak of an ""unknown but potentially deadly illness.""

Belief in a duty to treat was associated with their willingness to treat patients under conditions of personal risk. The authors concluded that ""the threat of new disease outbreaks, from bioterrorism or natural causes, has provided an opportunity for physicians to rearticulate and reaffirm longstanding ethical principles regarding the duty to treat.""

Making the decision to care for patients in the face of a nuclear, biological, or chemical terrorist attack; an infectious pandemic; or in an isolated hospital in a natural disaster caring for critically ill patients beyond one's area of expertise requires extraordinary commitment and courage. This level of obligation is similar to that emulated by combat medics and military surgeons who care for wounded soldiers on the battlefield under direct fire themselves. There is no higher achievement of moral commitment and professional dedication than this. However, it must be recognized that not all physicians possess the capabilities to administer effective care under such conditions, or at least may not know whether or not they can until the actual event occurs. The AMA Code of Medical Ethics notwithstanding, although it will not be possible to count on every physician to participate in the direct medical care of patients under such conditions, there certainly are other important ways that physicians can be of benefit to the medical disaster relief, such as resource allocation and personnel coordination.

Larkin and Arnold 16 have characterized the extraordinary virtues required of physicians who respond to terrorism events in the face of triage, system overload, and ethical decision making with every patient-""prudence, courage, justice, stewardship, vigilance, resilience, and charity."" These are admirable traits that few of us possess in their entirety. They also raise several issues that need to be discussed and guidelines to be provided before the situations might actually be encountered, including the above seven cardinal virtues: The authors further propose that ""virtue-based ethics are more adaptable to the multiplicity of rapidly changing disaster circumstances than mere principles, rules, and proto-cols, particularly since the scope, magnitude, and dynamics of a particular terrorist challenge cannot be determined in advance."" They applied the seven cardinal virtues to times of terror and how they might be helpful to physicians who find themselves overwhelmed by the tasks at hand in caring for disaster victims. Of these virtues, justice, stewardship, and charity imply a sense of obligation for physicians to attend to the sick and wounded, regardless of personal risk. However, ""virtue-based ethics"" reflects primarily the responsibilities and obligations of physicians to care for victims of such disasters, whereas ""utilitarian-based ethics"" requires physicians to make medical decisions based on the resources available and a triage system that favors applying those resources to victims who are predicted to have the best chance of survival. As with many ethical dilemmas, virtuebased ethics and utilitarian-based ethics may be in opposition, and the solution may lie in identifying the best applicable aspects of both to the situation at hand.

Weapons of mass destruction cause a particularly difficult challenge to physicians because of the dosage-related prognosis for exposed victims and the personal risk to physicians of non-decontaminated patients. Pesik and associates 17 recommend that triage of the victims be ethically based on the medical model of ""best prognosis."" Under this model, the patients are triaged according to their prognosis or survivability. The authors suggested that ""if something cannot be accomplished (i.e., saving all lives with the limited available resources), then there is no ethical obligation to do so."" However, whatever model of triage is used in disaster management, the physician's obligation is to care for patients/victims in such a manner as to provide the most benefit to the most patients. The physician assisting in disaster medical relief must move from doing the most he/she can do for each individual patient to doing what can be done for those who have the best chance for survival. This is a difficult transition, and one that may not be easily made without prior planning and acceptance of a new care model.

In a commentary on US health policy in the aftermath of Hurricane Katrina, Rosenbaum 18 proposed that the apparent need to improve the public health system in this country to better respond to disasters might also have a positive effect on the just provision of health care for the underserved and low-income population. As exposed by the hurricane effects, the majority of the victims were those who were unable to escape from the flooding, primarily because they lived in the low-income areas adjacent to the levees, as well as not having the transportation capabilities to escape before the hurricane struck. The author admonished that ""the notion that the world's most powerful nation would continue to lurch from disaster to disaster, jury-rigging inadequate and temporary solutions, is simply untenable."" Disasters such as Katrina have the ability to expose the frailties of the medical system, and thus, the potential inability to adequately support the efforts of conscientious physicians who put themselves in harm's way to care for the victims.

If physicians accept the tenet that it is a professional obligation to care for victims of terrorism attacks and extreme natural disasters, then how can the medical profession best prepare physicians for this role? Wynia and Gostin 19 reaffirmed that there is sufficient agreement in the profession for physician obligation to treat in these circumstances. Yet, it does not serve either the physician or the potential victims/ patients well for a physician to be ill prepared to deal with a wide range of nuclear, biological, chemical, infectious, and other medical problems without adequate ethical guidelines. An integral part of such preparation would be the development of ethical guidelines, both principled and practical, emphasized by case-based scenarios, to be a major part of undergraduate, graduate, and continuing medical education.

In a recent study of the self-assessment of public health workers' preparedness for bioterrorism or other public health disasters, the authors identified a wide range of perceived needs for additional training. 20 They suggested using competency-based goals, which would be assessed by drills, exercises, and tests. This approach is not unlike the American graduate medical education system of competencybased learning, with specific goals and objectives. It is possible to apply this system to ethical decision making in disaster medical response, as well.

However, before educational models are developed to teach ethical decision making in disaster medicine, it will be necessary for national discussions to occur, with the development of consensus guidelines across medical and surgical specialties that will encourage both virtue-based ethics as well as the cardinal principles of ethical behavior by physicians. Such guidelines would need to recognize the practical issues of need/prognosis-based triage, both of incoming casualties as well as patients already hospitalized and requiring large amounts of medical resources. Inpatients with do not resuscitate orders and those who are terminally ill would likely be placed in the ""expectant"" category of patients, with the resources currently applied to their care reallocated to incoming patients with a better chance of survival, save for comfort care and pain palliation. Once there is a national consensus within the medical and nursing professions, then the public needs to be educated about the changing requirements for ethical decision making under the dire conditions of disasters. Public education will not completely alleviate controversies and concerns about triage, but it would be quite helpful.

The educational curriculum for ethical decision making in disaster medicine should become a part of the medical school curriculum, as well as for resident physicians in all specialties. Until the students and residents have progressed into practicing physicians, there will be a need to provide continuing medical education courses on this subject for both community and academic physicians, who will bear the responsibility for caring for victims of terrorist attacks and extreme natural disasters. Table 1 is a proposed model curriculum for ethical decision making in disaster medicine.

It is necessary to develop a national consensus on the ethical guidelines for physicians who care for patients, victims, and casualties of disasters, and to formulate a virtue-based, yet practical, ethical approach to medical care under such extreme conditions. An educational curriculum for medical students, residents, and practicing physicians is required to best prepare all physicians who might be called upon, in the future, to triage patients, allocate resources, and make difficult decisions about treatment priorities and comfort care. It is not appropriate to address these questions at the time of the disaster, but rather in advance, as part of the ethics education of the medical profession. Important issues for resolution include inpatient and casualty triage and prioritization, medical liability, altered standards of care, justice and equity, informed consent and patient autonomy, expanding scope of practice in disaster medicine, and the moral and ethical responsibilities of physicians to care for disaster victims. Maintaining comfort care and pain palliation for patients The principle of ""double effect"" as applied to casualty care OE Good Samaritan laws by state and implications for disaster medicine OE Medical liability in disaster medicine OE Altered standards of care in mass casualty events OE Justice and equality of care in disaster medicine OE Informed consent and patient autonomy OE Advising hospitals in ethical considerations during the development of hospital-based medical disaster plans OE Expanding scope of practice in disaster medicine-when and how Capabilities, training, and knowledge of physician vs actual risk to patient OE The moral and professional obligation of physicians to alleviate pain and suffering in disaster relief vs selfinterest and care of one's family-an ethical dilemma OE Euthanasia and physician-assisted suicide under conditions of terrorist events and natural disasters-avoiding the pitfalls OE HIPAA regulations and patient confidentiality OE Case-and scenario-based studies in application of principles of disaster medicine

",0.6595940078055729
Intentions to Perform Non-Pharmaceutical Protective Behaviors during Influenza Outbreaks in Sweden: A Cross-Sectional Study following a Mass Vaccination Campaign,"Failure to incorporate the beliefs and attitudes of the public into theoretical models of preparedness has been identified as a weakness in strategies to mitigate infectious disease outbreaks. We administered a cross-sectional telephone survey to a representative sample (n = 443) of the Swedish adult population to examine whether self-reported intentions to improve personal hygiene and increase social distancing during influenza outbreaks could be explained by trust in official information, self-reported health (SF-8), sociodemographic factors, and determinants postulated in protection motivation theory, namely threat appraisal and coping appraisal. The interviewees were asked to make their appraisals for two scenarios: a) an influenza with low case fatality and mild lifestyle impact; b) severe influenza with high case fatality and serious disturbances of societal functions. Every second respondent (50.0%) reported high trust in official information about influenza. The proportion that reported intentions to take deliberate actions to improve personal hygiene during outbreaks ranged between 45-85%, while less than 25% said that they intended to increase social distancing. Multiple logistic regression models with coping appraisal as the explanatory factor most frequently contributing to the explanation of the variance in intentions showed strong discriminatory performance for staying home while not ill (mild outbreaks: Area under the curve [AUC] 0.85 (95% confidence interval 0.82;0.89), severe outbreaks AUC 0.82 (95% CI 0.77;0.85)) and acceptable performance with regard to avoiding public transportation (AUC 0.78 (0.74;0.82), AUC 0.77 (0.72;0.82)), using handwash products (AUC 0.70 (0.65;0.75), AUC 0.76 (0.71;0.80)), and frequently washing hands (AUC 0.71 (0.66;0.76), AUC 0.75 (0.71;0.80)). We conclude that coping appraisal was the explanatory factor most frequently included in statistical models explaining self-reported intentions to carry out non-pharmaceutical health actions in the Swedish outlined context, and that variations in threat appraisal played a smaller role in these models despite scientific uncertainties surrounding a recent mass vaccination campaign. Citation: Timpka T, Spreco A, Gursky E, Eriksson O, Dahlströ m Ö , et al. (2014) Intentions to Perform Non-Pharmaceutical Protective Behaviors during Influenza Outbreaks in Sweden: A Cross-Sectional Study following a Mass Vaccination Campaign. PLoS ONE 9(3): e91060.","Although encouraging the public to undertake specific protective behaviors has proved useful in containing outbreaks of infectious disease [1] , more research has been called for examining the social, demographic, and cultural factors that influence these efforts [2] . This is particularly important to understanding people's hesitations to heed official advice, particularly in the absence of clear scientific evidence regarding the disease outbreak [3] . The AS03-adjuvanted PandemrixH was the most commonly used vaccine in response to the Influenza A(H1N1)pdm09 outbreak in Europe [4] ; Finland and Sweden recommended this vaccine to their entire populations. In August 2010 reports of a possible association between exposure to the vaccine and occurrence of narcolepsy in children and adolescents emerged in both the latter countries, which led to a review of the vaccine by the European Medicines Agency (EMA). Subsequently, increased narcolepsy diagnoses associated with the start of the campaign have been confirmed [5] . In Sweden, scientific uncertainty regarding the safety of this mass vaccination was both publicly discussed [6] and questioned by researchers [7] .

Beliefs that the interventions suggested are effective and safe [8] , that the illness has severe consequences [9] , and that there is a high likelihood of exposure [10] have been associated with compliance with behavioral recommendations. It has also been pointed out that behavioral research in epidemics should not only identify determinants of individual and population behavioral responses, but also clarify the mechanisms underpinning these [11] . Protection Motivation Theory (PMT) [12, 13] posits that an intention to perform protective activities is determined by perceptions of threat and the ability to cope. In addition to intentions and preceptions, a recent review concluded that protective behavior needs to be investigated with regard to sociodemiograpic characteristics in order to identify the ''contagious'' effect and contextual nature of perceptions and mediating mechanisms [14] . For instance, coping appraisals are made in interaction with environmental resources, which vary in availability across population subgroups. Protective behavior associated with influenza outbreaks has also been investigated with regard to general estimates of health status [15] , but few studies have used validated measures of self-rated health as a means for the sub categorization. At present, several such measures are available for use in population-based research [16] .

To provide a snapshot of intended self-protective behaviors during a period when scientific uncertainty pervaded public discussions addressing infectious disease control, we carried out a cross-sectional telephone survey of a demographically representative sample of the Swedish population. The specific aim was to examine to what extent self-reported intentions to improve personal hygiene and increase social distancing during influenza outbreaks can be explained by perceptions of threat and the ability to cope as outlined in PMT, self-reported assessments of health, trust in official information, and sociodemiographic factors.

The study used a cross-sectional design to analyze associations between intended protective behaviors during influenza outbreaks and items in a theoretical model of explanatory factors [14, 17] . A random sample of 1,011 persons ranging between 20-90 years of age was drawn from the Swedish national population register. A combined telephone and questionnaire survey was carried out during the first quarter of 2012.

The study was conducted according to the World Medical Association's Declaration of Helsinki from 1964 regarding ethical principles for medical research involving human subjects, revised in 2008. Potential study sample participants were informed about the study by letter via postal mail and invited to participate in a telephone survey on protective behaviors during influenza outbreaks. Those agreeing to participate returned their consent in writing. All collected data were managed confidentially and analyzed anonymously. The study design was approved by the institutional (ethics) review board at Umeå University (Dnr 2011-314-31Ö ).

A hypothetical explanatory model was constructed to inform the analysis of the main research question; i.e. to what extent selfreported intentions to perform protective behaviors during influenza outbreaks can be explained by perceptions of threat and the ability to cope as outlined in the PMT, self-assessments of health status, trust in official information, and sociodemiographic factors. In this model, protective behaviors during outbreaks are restricted to two categories: increased personal hygiene (use of disinfectants and other handwash products; frequent washing of hands when having touched common objects, such as door knobs) and social distancing (staying home from work or school; avoiding use of public transportation). The intentions to carry out a protective behavior are assessed by asking whether the respondent would try to perform the behavior during a mild and severe influenza outbreak, respectively. Both outbreak scenarios described personal risk of infection as high (i.e., 1 in 3 people infected). The mild influenza description details moderate health consequences (less than 1 in 1000 infected people dying) and a minor lifestyle impact (services mainly operating normally). The severe scenario describes serious health consequences (1 in 50-100 infected people dying) and services no longer being able to operate normally.

The first set of explanatory factors concerned perceptions of threat and the ability to cope. Based on the notion of subjective expected utility [18] , which postulates that people's choices are a product of assessments of probability and utility of options, healthrelated methodologies such as the PMT and the Health Belief Model [19] have included formally quantified models of subjective health risk perceptions, i.e., as the likelihood of contracting a disease multiplied by disease severity. Together with different types of cost-benefit valuations and self-efficacy expectations, these perceptions of risk are presumed to determine health-protective behaviors. In the present study, the collection and analysis of data on protection motivation in relation to influenza outbreaks are structured according to the PMT. This theory suggests that threat appraisal will generate an intention to act, while coping appraisal determines the type of action. Threat appraisal is in this study characterized in its three dimensions [11, [20] [21] :

-perceived relative risk of catching influenza; measured by one item assessing personal likelihood of infection, if no preventative action was taken, -anxiety about catching mild and severe influenza; measured by one item for each influenza type, and -perceived severity of the consequences of catching mild and severe influenza; measured by one item for each influenza type.

Coping appraisal is also represented in its three dimensions:

-Response efficacy; assessed by one item asking about protecting oneself from influenza by employing enhanced personal hygiene and one item asking about social distancing, -Self-efficacy; measured by two items asking whether the respondent felt it is possible to carry out protective behaviors by social distancing and increased personal hygiene, respectively, and whether they were confident they could carry out these actions if they so desired [22] , and -Response costs; defined as the estimated efforts needed to overcome perceived barriers on carrying out protective actions. For social distancing, this dimension was assessed by asking for 'work concerns', i.e. guilt and anxiety about not completing work. Response costs for increased personal hygiene were assessed through items asking for concerns associated with acquiring adequate soaps and disinfectants (handwash products) and learning the correct techniques to use them.

Self-reported health assessments have in epidemiological studies been found to be valid indicators of health status as measured by prediction of future physician contacts and all-cause mortality [23] . In this study, self-reported health is measured by the SF-8 TM 24-hour recall questionnaire in order to examine associations with intentions to carry out protective behaviors. This general selfreported health instrument contains eight health-related questions that, in turn, can be summarized in two overall measures of physical and mental health: physical component summary (PCS) and mental component summary (MCS), respectively [24] . It is derived from the SF-36 for the purposes of yielding comparable scores for the 8 health dimensions and 2 summary measures of the SF-36 with minimal respondent burden.

Trust in government information during influenza outbreaks has in previous studies been found to be associated with greater self-efficacy and personal hygiene [25] . Trust in official information was therefore included in the explanatory model, asking for agreement with a single statement about trust in government information during outbreaks. The sociodemiographic factors included in the model were marriage status, number of children living at home, formal education, employment status, and ethnicity.

Prior to the telephone call, the subjects were asked to complete a paper-based survey, querying for sociodemiographic data and data elements from the SF-8 TM . The remaining data were collected in the telephone interview. To catalyze their considerations about the research topics, each subject was presented with brief scenarios of mild and severe influenza outbreaks. Interview data were derived from open statements, and the respondents were asked to score their agreement along a seven-point scale from 1 (strongly disagree) to 7 (strongly agree). The collection of data on perceptions associated with precautionary behaviors was structured in accordance with the PMT (Text S1). To assess trust in official information in this study, the single statement ''For information during influenza outbreaks I do rely on government sources'' was used.

We conducted a drop-out analysis based on the demographic variables available for the entire sample, i.e. gender, age and place of residence. All collected data were first subjected to descriptive statistics, i.e. mean, median and standard deviation for continuous data and frequency and proportions (%) for categorical data. The primary end points for the ensuing analyses were intentions to increase social distancing (staying home while not ill; avoid public transportation), and enhance personal hygiene (use of handwash; frequent washing of hands after touching common objects) during mild and severe influenza outbreaks, respectively. The theoretical model of potential explanatory factors was used as the basis for the analysis. For each endpoint, logistic regression analyses were applied using the items in the model as explanatory variables. These included trust in official information; variables corresponding to PMT items (the threat appraisal items of perceived personal risk, emotional response (worry), perceived severity; and the coping appraisal items of general response efficacy, self-efficacy, and response costs); variables representing the SF-8 summary items (PCS and MCS); and sociodemographic characteristics (age, gender, educational level, living with partner, living with child, and employment). When used as response variables, ordinal variables were dichotomized (agree/do not agree). To contrast expected perceptions against other perceptions, the variables were converted with the agreement scores in the expected extreme as one category. For threat appraisal, agreement scores in the low extreme were contrasted against other opinions, except for the estimates of the severity of the consequences of getting infected where the scores in the high extreme were contrasted against the other opinions. Regarding coping appraisal, the personal hygiene scores in the high extreme were contrasted against other opinions for response efficacy and self-efficacy and in the low extreme for response costs. For social distancing, agreement scores in the low extreme were contrasted against other opinions for response efficacy and in the high extreme for self-efficacy and response costs.

The area under the ROC curve (AUC) was used as model performance indicator and Nagelkerke R 2 to estimate the determination level for each model. The limits for interpreting the AUC (or c-statistic) were set to 0.90, 0.80, and 0.70, denoting very strong (outstanding), strong (excellent), and acceptable discriminatory performance, respectively [26] . All tests were twosided and P,0.05 was regarded as statistically significant. All calculations were done using SPSS version 18 or higher.

Two-hundred and fifty-four persons in the total population sample (n = 1,011) could not be reached by a telephone call. Of the 757 persons reached, 443 provided a complete response, leading to a 59% response rate to the telephone survey and a 44% participation rate with regard to the total sample. The age category 65-90 years was slightly over-represented (54% response rate) among the study participants when compared to the total population sample (p = 0.039). However, the effect size of this difference in participation was small (Cramer's V = 0.08). Thus, while elderly individuals were overrepresented in our data, the impact of this deviation from the reference population was of a small magnitude. In terms of place of residence, those living in small labor market regions (with a total population of less than 100,000 inhabitants) exhibited the highest participation rates: 51%, compared to 41% in large regions (with a population greater than 1 000,000 inhabitants). The basic sociodemiographic characteristics of the final study participants are displayed in Table 1 . The general level of health in the study population as measured by SF-8 scores was above the reference values for all items except for Physical functioning and Vitality (lower scores) and General Health (equal scores) ( Table 2 ). There was no statistically significant difference between men and women regarding the mean scores of any SF-8 item or summary component.

Every second respondent (50.0%) reported high or very high trust (scores 6-7) in information about influenza provided by official sources (mean score 5.3; median 5.5; Standard Deviation (SD) 1.7)). Neither age, education, employment nor any component of self-rated health was associated with trust in official information about influenza. However, the level of trust was associated with gender, with men reporting lower trust levels than women (p = 0.018; Odds Ratio (OR) 0.60 (95% Confidence Interval (CI) 0.40;0.91)).

Regarding social distancing measures, 9% of the respondents scored strong (strong or very strong) agreement with the stated intention to stay home when not ill during mild influenza outbreaks, and 11% of the respondents scored strong agreement with this intent during severe outbreaks. More than twice as many respondents (23%) scored strong agreement with avoiding use of public transportation during a mild outbreak, while 29% of the respondents scored strong agreement with this intention during a severe outbreak. Regarding measures related to personal hygiene, 77% of the respondents scored strong agreement with the stated intention to use handwash products during mild outbreaks, while 85% of the respondents scored strong agreement with this intention during severe outbreaks. Regarding the intention to frequently engage in handwashing, 46% reported strong agreement in association to mild influenza outbreaks and 60% in association to severe outbreaks.

A model describing the intention to stay home without being ill during a mild influenza outbreak included eight significant variables and displayed a strong discriminative performance (AUC 0.85 (95% CI 0.82;0.89)) ( Table 3 ). This self-reported intention was strongly associated with coping appraisal; low perceived response costs associated with staying home and selfefficacy with regard to social distancing; and, interestingly, to a disbelief in the general efficacy of social distancing as an infectious disease control measure. Planning to stay home was also strongly associated with male gender and, with a weaker association, to being unemployed and living with a partner. The intention was also associated with threat appraisal, although with a weaker strength; with worry about getting infected and high perceived severity of the influenza threat. In comparison, the intention to stay home without being ill during a severe outbreak was represented by a model including only four significant variables, but that also displayed a strong discriminative performance (AUC 0.82 ((95% CI 0.77;0.85)). As for the mild outbreak scenario, this intention was strongly associated with coping appraisal; to response costs and perceived self-efficacy with regard to social distancing. However, staying home during a severe outbreak was also strongly associated with threat appraisal related to concerns about getting infected. Regarding sociodemographic factors, this intention was only associated with not having employment.

The intention to avoid using public transportation during a mild influenza outbreak was represented by a model including six significant variables and an acceptable discriminative performance (AUC 0.78 (95% CI 0.74;0.82)) ( Table 4 ). This self-reported intention was, also, strongly associated with coping appraisal; to perceived response costs associated with avoiding public transportation and to self-efficacy with regard to social distancing. The intention was also strongly associated with threat appraisal in terms of worry about getting infected. In addition, avoiding use of public transportation was associated with a lower level of formal education, living with a partner, and high trust in official information. In contrast, the intention to avoid public transportation during a severe influenza outbreak was described by a model including four significant variables and an acceptable discriminative performance (AUC 0.77 (95% CI 0.72;0.82)). As for the mild outbreak scenario, avoiding public transportation during severe outbreaks was strongly associated with coping appraisal; to response costs; and, with weaker strength, to perceived self-efficacy with regard to personal social distancing. With regard to threat appraisal, avoiding public transportation during a severe outbreak was associated with worry about getting infected and a high perceived severity of the influenza threat.

Planning to use handwash products during a mild influenza outbreak was described by a model including three significant variables and an acceptable discriminative performance (AUC 0.70 (95% CI 0.65;0.75)) ( Table 5 ). Planning to use handwash was strongly associated with female gender. This intention was, for mild outbreaks, also explained by self-efficacy with regard to personal hygiene and trust in official information. For the severe outbreak scenario, planning to use handwash products was represented by a model including four significant variables and an acceptable discriminative performance (AUC 0.76 (95% CI 0.71;0.80)). This intention was, too, strongly associated with female gender. In addition, it was strongly associated with coping appraisal; to a belief in the general efficacy of increased personal hygiene; and low response costs associated with acquiring of suitable products. Contrary to any of the other intended behaviors studied, the intention to use handwash products during severe outbreaks was associated with low self-rated physical health. An intention to frequently engage in handwashing after having touched common objects during a mild influenza outbreak was represented by a model including four significant variables and an acceptable discriminative performance (AUC 0.71 (95% CI 0.66;0.76)) ( Table 6 ). The intention was strongly associated with coping appraisal in terms of self-efficacy with regard to personal hygiene. It was also associated with female gender, higher age, and lower education. In comparison, planning to frequently wash hands during a severe outbreak was represented by a model including three significant variables and an acceptable discriminative performance (AUC 0.75 (95% CI 0.71;0.80)). Similar to the mild influenza scenario, it was strongly associated with coping appraisal in terms of a high self-efficacy with regard to personal hygiene. The intention was also associated with female gender and being born in the country.

Despite the fact that the safety of the mass vaccination during the A(H1N1)pdm09 outbreak had been questioned by national mass media in a campaign-like manner, two years after the outbreak every second respondent in a representative sample of the Swedish adult population reported high trust in official information about influenza. While the proportion of persons reporting intentions to improve personal hygiene during influenza outbreaks ranged between 45-85%, the proportion reporting intentions to increase social distancing did not exceed 25%. This pattern can generally be explained by the notion that the initial behavioral changes during an influenza outbreak are more likely to resemble familiar reactions and well-known routines [27] , such as increasing personal hygiene, rather than changes that require deductive planning, such as increasing social distancing.

The explanatory models developed in this study showed statistical associations ranging from strong (staying home without being ill) to acceptable (avoiding public transportation and increasing personal hygiene). Among the explanatory factors considered, coping appraisal was the factor most frequently showing associations (as displayed by odds ratios) with the reported intentions. In a validation analysis (data not shown), we fitted each model fully (including all terms in the five explanatory factors categories trust of information, threat appraisal, coping appraisal, sociodemographic factors, and self-rated health) and calculated the proportion of correctly classified cases for these full models for all eight scenarios. Then we left the terms from one of the five categories out separately, and calculated the proportion of correctly classified cases for each of these subset models. We found that the proportion of correctly classified cases without coping appraisal was lower than the corresponding proportion for all full models and lower or equal to the corresponding proportion for 28 of the 32 models excluding one of the other four categories. We interpret these observations combined as indicative evidence that of the explanatory factors considered, coping appraisal was the factor strongest associated with the reported intentions. Analogous to our results, a recent British web-based survey of university employees found that coping appraisal was the principal predictor of variability in protective intentions during pandemics [21] , and response costs have been reported as the largest predictor for emergency nurses not reporting to work during an influenza pandemic [28] . A contributing influence to the lesser relative importance of threat appraisal suggested by our results may be a Scandinavian tendency to perceive risks lower than in other countries [29] [30] [31] . One of the explanations for this tendency is that the media in Scandinavia appear to report more about risks abroad with less attention to risk inside the country [29] . In contrast to our results, self-efficacy during the A(H1N1)pdm09 outbreak in Hong Kong was found to be only weakly associated with social distancing [25] . However, Hong Kong residents are limited in their ability to avoid crowds, and the relatively mild impact of the outbreak could have led to the notion that people saw no reason to jeopardize their economic well-being and curtail other social activities. A socio-geographic theory of protective behaviors during infectious disease outbreaks suggested that efficacy beliefs of Chinese living in the UK and the Netherlands were comparable to those of native UK and Dutch residents during the SARS outbreak in 2003 [32] , indicating that country of residence is more important than ethnicity or country and culture of origin in determination of protective behaviors. However, with coordinated regional disease control efforts and increasing influence from social media, this may change.

Gender was the sociodemiographic characteristic that showed the strongest association with the observed variation in reported intentions. As also found in a Norwegian study from the same time Table 4 . Simple and multiple logistic regression models of explanatory factors for the intention to avoid using public transport displayed by influenza outbreak scenario. period [33] , the Swedish women in this study were more disposed to enhance their protective behaviors related to personal hygiene than were men. One explanation of this finding could be an interaction with concerns about the consequences of getting infected. A recent study from the U.S. reported that women were more worried than men about getting seriously ill or even dying during a severe influenza outbreak [34] . However, no gender differences with regard to threat appraisal were reported from the Norwegian study [33] . Originally, we did not include interaction terms in our statistical analyses. A secondary analysis (data not shown) did not reveal any statistically significant interaction between gender and any threat or coping appraisal item such that omitting the interaction from the model would disturb the estimation of the main effects. Therefore, an alternative explanation of our findings is that the female respondents were more disposed to enhance their protective behaviors related to personal hygiene than the male respondents because Swedish women purchase and use hygiene products more often than men [35] , and, in consequence, were more confident about the practical handling of handwash and liquid soap. Conversely, men were more inclined to stay home without being ill during influenza outbreaks. This could be explained by the fact that fewer of the employed Swedish men (12%) than women (46%) were at the time of the study working in caring or educational occupations that require physical presence at the workplace, such as nursing, child care, and teaching [36] . In other words, a larger proportion of men could consider the possibility of staying home while continuing to work during an ongoing influenza outbreak, which was not an option for many women. These findings indicate that more research is needed to understand gender-related differences in protective behavior during influenza outbreaks.

The main strengths of this study are its foundation on a current theoretical model [14] and a relatively large representative sample of the Swedish population. However, the study has also important limitations that must be taken into consideration when interpreting the results. The demographic characteristics available may not be the most important factors biasing the results. For instance, it is possible that individuals with low trust in official information about influenza were under-represented, and anxious individuals worrying about disease risks were over-represented, among the participants. Moreover, interpreting cross-sectional data on protective behaviors is difficult because they confound the motivation and accuracy-associated aspects regarding the causaltemporal relationship between perception and behavior [37] . The motivational hypothesis assumes that high perceived risk leads people's intention to adopt protective behaviors, while the accuracy hypothesis suggests that people who act in a more risky way should also feel more at risk. As an example, individuals having physical contact with many people through their occupation may have been aware of that daily routines are associated with a higher risk for getting infected. Accordingly, a negative correlation may indicate accurate relative risk perceptions, i.e. that people are aware of their risk status [20, 37] . Further longitudinal studies of protective behaviors during influenza outbreaks are thereby warranted [38] .

Another limitation is that we assessed self-reported intentions rather than objectively measured behavior. Nevertheless, intentions are a well-validated proxy for behavior predicting a moderate amount (30-42%) of the variance in actual behavior across a wide range of contexts [39, 40] . Moreover, proponents of dual-process health behavior models have suggested that analytic central and emotional-heuristic processes work in concert to select decisions [14] , and under certain circumstances emotions may even be the dominant force [41] . While the PMT used in this study does include an emotional component, it still represents a cognitive appraisal model in assuming that cognitive risk assessment determines experience of fear. Such a model is naturally applicable for the study of behaviors aimed at fending off long-term disease, where fear is likely to be less imminent and therefore secondary to more rational reflections about gains and losses related to protective behavior. However, in an acute threat situation, like a severe influenza outbreak, emotional aspects might gain more immediate importance. This would even be more likely during periods of scientific uncertainty, when fewer facts are available. It is in this context interesting to note that coping appraisal in this study was found to be the motivation factor that contributed most to the discriminatory performance despite the fact that threataffect was included in the general model, although indirectly through cognitive assessment. However, what role affect-or emotion-based judgments play in interaction with threat and coping appraisals is still an issue in need of clarification. Finally, it should be noted that there were relatively small differences between the reported intended behaviors associated to the mild and severe scenarios, respectively. One explanation of this observation can be the fact that the A(H1N1)pdm09 outbreak was relatively mild in Sweden, and that the respondents, wrongfully, related the severe scenario to their recent personal experience rather than the scenario description. However, the lack Table 6 . Simple and multiple logistic regression models of explanatory factors for the intention to wash hands after touching common objects displayed by influenza outbreak scenario. of difference can also be seen as a sign of its own, i.e. that the Swedish population may not be fully aware of the seriousness of a full influenza pandemic. Failure to monitor the beliefs and attitudes of the public has recently been identified as a weakness in preparedness strategies against infectious disease outbreaks [42] . We examined how items in a general explanatory model of intended health behavior were associated with personal hygiene and social distancing practices following a questioned mass vaccination campaign against influenza in the Swedish population. We observed a relatively high trust in official recommendations and a higher proportion of intentions to improve personal hygiene than those used to increase social distancing. Among the explanatory factors considered, coping appraisal was the factor most frequently included in models explaining self-reported intentions. Variations in threat appraisal played a smaller role in these models despite the uncertainties surrounding the mass vaccination during the A(H1N1)pdm09 outbreak. The results also show that not just from a third world perspective [43] it is necessary to consider that not all population sub groups have the same predispositions to enact specific behaviors to protect their health. For instance, they suggest that further studies are needed of gender differences in protective behaviors during influenza outbreaks. We conclude that developing interventions that support the general population's efforts to perform self-protective behaviors during influenza outbreaks and longitudinal studies of such interventions across several influenza seasons are warranted also in European countries.

Text S1 Interview guideline for collection of data on perceptions associated with precautionary behaviors. (DOCX)

",0.6587029337405874
Mandating influenza vaccinations for health care workers: analysing opportunities for policy change using Kingdon's agenda setting framework,"Background: The consequences of annual influenza outbreaks are often underestimated by the general public. Influenza poses a serious public health threat around the world, particularly for the most vulnerable populations. Fortunately, vaccination can mitigate the negative effects of this common infectious disease. Although inoculating frontline health care workers (HCWs) helps minimize disease transmission, some HCWs continue to resist participating in voluntary immunization programs. A potential solution to this problem is government-mandated vaccination for HCWs; however, in practice, there are substantial barriers to the adoption of such policies. The purpose of this paper is to identify the likelihood of adopting a policy for mandatory immunization of HCWs in Ontario based on a historical review of barriers to the agenda setting process. Methods: Documents from secondary data sources were analysed using Kingdon's agenda setting framework of three converging streams leading to windows of opportunity for possible policy adoption. Results: The problems, politics, and policies streams of Kingdon's framework have converged and diverged repeatedly over an extended period (policy windows have opened and closed several times). In each instance, a technically feasible solution was available. However, despite the evidence supporting the value of HCW immunization, alignment of the three agenda setting streams occurred for very short periods of time, during which, opposition lobby groups reacted, making the proposed solution less politically acceptable. Conclusions: Prior to the adoption of any new policies, issues must reach a government's decision agenda. Based on Kingdon's agenda setting framework, this only occurs when there is alignment of the problems, politics, and policies streams. Understanding this process makes it easier to predict the likelihood of a policy being adopted, and ultimately implemented. Such learning may be applied to policy issues in other jurisdictions. In the case of mandatory influenza vaccinations for HCWs in Ontario, it seems highly unlikely that a new policy will be adopted until perception of the problem's importance is sufficient to overcome the political opposition to implementing a solution and thus, create a window of opportunity that is open long enough to support change.","The consequences of annual influenza outbreaks are often underestimated by the general public. This circumstance may be due in part to a large proportion of healthy young adults experiencing only minor symptoms [1] , which has contributed to a lack of public awareness and complacency regarding the severity of its effects on vulnerable populations such as the elderly and individuals that are immunocompromised. Despite this misconception, influenza poses a serious public health threat around the world. The World Health Organization (WHO) estimates that annual influenza epidemics cause 3 to 5 million cases of severe illness worldwide, along with 250,000 to 500,000 annual deaths [2] . These epidemics are also associated with overwhelmed clinics and hospitals, many of which need to account for increased staff absenteeism and productivity losses due to illness of health care workers (HCWs) [2] .

The impact of annual influenza epidemics on some of the most susceptible populations, such those living with chronic conditions, is even more striking. Global metaanalyses of risk factors for severe disease from pandemic influenza found that 31 % of patients hospitalized had at least one other chronic medical condition, as did 52 % of those admitted to intensive care units and 62 % of fatal cases [3] . In the United States, individuals who were 65 or older consistently accounted for approximately 90 % of all influenza related deaths between 1976 and 2007 [3] .

Fortunately, vaccination can mitigate the negative effects of this common infectious disease. During the influenza seasons of 2010-2012, the Centers for Disease Control and Prevention found that influenza vaccination helped reduce children's risk of influenza-related intensive care unit admissions by 74 % [4] . The use of vaccinations has also been associated with a 77 % reduction in influenzarelated hospitalization among adults aged 50 or over [5] . In addition, rates of influenza infections were found to be lower in vaccinated HCWs versus unvaccinated HCWs [6] and other healthy adults [7] . However, while vaccination may reduce the risk of influenza in HCWs, a recent systematic review reported the reduced risk to be less than 50 % as compared to HCWs who are not vaccinated; which suggests the need for additional solutions beyond HCW vaccinations [6] . Furthermore, recent reviews highlight the lack of clear evidence to support the benefit of vaccinating HCWs [8] to prevent the spread of influenza to elderly [1] .

Within high-income countries, influenza vaccination is readily accessible and many jurisdictions have implemented publicly funded programs to cover the cost [9] . However, public vaccination uptake has been variable [10, 11] , and perhaps more importantly, some HCWs continue to resist participation in vaccination programs.

For example, in Canada, voluntary uptake of the seasonal influenza vaccine by HCWs remains below the 90 % recommended level [12] and varies substantially across health care organizations [13] . Other studies have reported that in some locations more than 50 % of physicians also fail to take advantage of the annual influenza vaccination [13] . Inadequate levels of HCW immunization can place the vulnerable populations they care for at greater risk of health complications [14] .

A potential solution to this problem is governmentmandated inoculation for HCWs. However, in practice, there are substantial barriers to the adoption of such policies. For instance, the attitudes of HCWs in the United States toward whether to be vaccinated against influenza or not remains divided, prompting a backlash against suggestions of any mandatory policies [15] . Surveys have revealed multiple reasons why HCWs disapprove of mandated vaccination. These include beliefs that the decision to be vaccinated is personal, fears of side effects, and concern that influenza vaccines are ineffective [15] . There have also been legal obstacles to mandating vaccination as worker unions and professional associations have argued that such policies violate individual rights and freedoms [16] . Specifically, in Canada, it has been argued that mandated immunization violates one's right to refuse unwanted medical treatment under section 7 of the Canadian Charter of Rights and Freedoms [16] .

New public policies are rarely adopted based solely on supportive evidence, public opinion, or interest group lobbying; rather, they typically require a confluence of events that is difficult to predict or orchestrate. Through the use of relevant frameworks, theories, or models, policy analysis can provide a greater understanding of the various processes involved and, in turn, enhance the likelihood of recognizing and seizing opportunities for new policies to be adopted [17] . In this vein, Kingdon's [18] three process streams framework helps to explain how three elements or ""streams""-problems, politics, and policiesaffect policy agenda setting dynamics. Kingdon argues that, while there is some interplay among the streams, they are largely independent of one another [18] . This framework can be used to explain how issues may make their way onto the government's decision agenda, which is a key step in the policy adoption process. The purpose of this paper is to identify the likelihood of adopting a policy for mandatory immunization of HCWs in Ontario based on a historical review of barriers to the agenda setting process.

Background information regarding the effectiveness and uptake of influenza vaccinations was gathered via narrative review, which included academic and newspaper articles, as well as government reports. Subsequently, these materials were analysed using Kingdon's agenda setting framework of three converging streams leading to windows of opportunity for possible policy adoption. No permissions to analyse the data used in this study were necessary or granted.

The authors independently categorized historical events as falling primarily into the problems, politics, or policies stream (see Table 1 ). Categories are based on Kingdon's description as noted below. Any discrepancies in classification were resolved through discussion.

In Kingdon's framework, the problems stream involves the identification of a particular social problem that has gained public or decision-maker attention and cannot be easily ignored. These problems may come to light through awareness of a change in an indicator, such as an increased infection rate, or a focusing event/crisis, such as the Severe Acute Respiratory Syndrome (SARS) outbreak in 2003. In the politics stream, the governmental agenda is formulated and the list of issues or problems to be given attention is prioritized. This stream is impacted by political events (such as changes in public opinion/national mood), and organized forces (such as a change in government due to elections). In the policies stream, experts analyse the various problems, and suggest technically feasible and politically acceptable solutions to them.

Generally, the three streams flow independently of one another; however, a time-limited window of opportunity may open when these streams converge (as politically acceptable solutions to prioritized problems are identified). At this point of convergence, issues are most likely to reach the government's decision agenda-where policy problems and their proposed solutions are under active discussion by government decision-makers-and thus, most likely to result in the adoption of a new policy. Policy windows can also close rapidly. This tends to occur when: problems, politics, and policies are not adequately linked together; initiatives that are implemented lead to either failure or success; indicators of severity lessen; other critical items push the issue off the agenda; there has been movement through an ""issue attention cycle"" (i.e., a gradual decline of interest in the condition).

Using Kingdon's agenda setting framework (three process streams that lead to windows of opportunity when they converge) the objective of this paper is to analyse the likelihood of government adopting a mandatory vaccination policy for HCWs in Ontario.

Ontario endured through a difficult 1999-2000 influenza season. Hospital emergency departments experienced serious overcrowding [19] , which resulted in less access to health services for patients seeking critical care. This unfavourable set of circumstances was a focusing event/crisis (problems stream) that resulted in heightened media attention and highlighted the need for government action (politics stream) [19] . During this period, an effective influenza vaccine that had been used as part of a public program in other jurisdictions was available, suggesting that mandatory vaccination was a technically viable and politically acceptable solution (policies stream).

In July 2000 it was announced that Ontario would be the first province in the country to offer free influenza vaccinations to all citizens for the upcoming season to relieve the aforementioned pressure on emergency departments [19] . Ontario's Universal Influenza Immunization Program (UIIP) is offered annually through the Ministry of Health and Long-Term Care (MOHLTC), with the goal of reducing the number of influenza cases and the associated negative impact on the health care system. All individuals who live, work, or attend school in the province, and are 6 months of age or older, are covered by the program [20] . In recent years, UIIP has increased the accessibility of vaccinations by making them available at a variety of locations such as employer-sponsored clinics and pharmacies [20] .

By immunizing a critical mass of the general population, as well as HCWs, it was expected that there would be a dramatic reduction to the death toll as well as the social and economic costs associated with annual influenza outbreaks. Studies have shown that since introducing UIIP in Ontario, the number of reported cases of influenza has [22] . Despite broad public acceptance and substantial participation in the voluntary immunization program, pockets of HCW resistance persisted (politics stream), and outbreaks in long-term care facilities and hospitals continued to occur, resulting in preventable illness and death [23] . As a consequence, Ontario's MOHLTC attempted to promote voluntary vaccination of HCWs by requiring health care facilities to report vaccination rates [24] . By winter 2002, UIIP was in its second year and the issue of mandatory vaccination of HCWs was beginning to become controversial and disputed in the courts [25] . When a health care system in Hamilton instituted a mandatory vaccination policy during outbreaks, or work suspension for non-compliance, 15 staff members refused inoculation and were removed from duty without pay. This led to arbitration where the union presented its case as forced medical treatment. The arbitrator agreed with the union position citing that the policy was not supported by regulatory or statutory authority. In the decision, the arbitration board declared a violation of section 7 of the Charter of Rights and Freedoms, which assures security of the person. Mandatory influenza vaccination, the arbitrator decided, was a forced medical act and therefore, contravened the Charter (St. Peter's Health System v. CUPE local 778, 2002) [25] .

Around the same time, a paramedic working in Ontario was making a constitutional challenge against the addition of annual influenza immunization to the Ontario Ambulance Act (North Bay Hospital v. CUPE local 139, 2003) [25] . His challenge also argued that mandatory immunization violated section 7 of the Charter. Pressure from the unions resulted in an amendment to the Ambulance Act in 2002, replacing mandatory vaccination with a requirement for influenza education. These cases/ political events represented a coordinated force in opposition to the mandatory vaccination initiative (politics stream). As a result, the problems, politics and policies streams were no longer aligned, closing the window of opportunity for policy change.

After the SARS outbreak, significant changes came to the public health system in Canada, bolstering pandemic preparedness and the ability to monitor emerging disease [26] . When an arbitration board faced a union challenging mandatory influenza vaccination, this time in British Columbia, the arbitrator sided with the employer noting a requirement for general immunization in the union collective agreement. In this case the employee had choices: vaccination, anti-viral medication, or to be off work without pay. The arbitrator ruled that the choices, which were not dealt with in the St. Peter`s decision, meant there was no violation of the Charter under section 7. An important note in the decision summary was the rationale that preventing the spread of influenza to vulnerable populations is clear and was agreed upon by both the employer and the union, which allowed for a realignment of the policies stream with problems and politics streams.

May 2009 saw the first wave of the H1N1 pandemic influenza strain. By July 2009, the Public Health Agency of Canada reported it still did not have a plan in place to manage HCWs who refused the seasonal influenza vaccine [27] . The seasonal vaccine did not include protection from the novel H1N1 strain and thus, worldwide production of a vaccine was implemented. Stories of production delays, low vaccine supplies, and rationing caused long lines at vaccination clinics and resulted in public anxiety [28] . Even after this pandemic experience, uptake of the vaccine by HCWs the following season remained low [23] . The H1N1 pandemic could have resulted in the opening of a new policy window, but with the relatively mild nature of the H1N1 strain, the mandatory vaccination issue did not gain adequate or sustained public attention (politics stream), closing another window of opportunity.

A long-awaited economic appraisal of Ontario's UIIP was published in 2010 [21] . Based on a measure of qualityadjusted life years, the study concluded that UIIP was cost-effective and might be considered for adoption in other provinces and countries with similar populations, health care models, and influenza rates. This evidence bolstered Ontario's influenza vaccination program and may have served as a catalyst to support further expansion of the program to include mandatory HCW vaccination, however, it was not sufficient to make mandatory vaccination a politically acceptable option.

In other jurisdictions, most notably the US, consideration of mandatory vaccination for HCWs was gaining momentum. A large US health care employer had published their successful program requiring influenza immunization for employment [14] . In Canada, the province of British Columbia initiated a task force in 2011 to investigate and recommend plans for mandatory immunization programs [29] . The task force visited nine US sites with programs in place before making recommendations in 2012 to the provincial government.

Pressure on the Ontario government to act intensified as both the evidence and prominent organizations supported mandatory HCW vaccination. For instance, the Influenza Surveillance Protocol for Ontario Hospitals was revised and included the National Advisory Committee on Immunization statement that ""… refusal of health care workers who have direct patient contact to be immunized against influenza implies failure in their duty of care to their patients [30] ."" The Association of Medical Microbiology and Infectious Disease also presented a position paper supporting mandatory immunization acknowledging that all efforts at voluntary programs had failed [19] . Public Health Ontario, The Canadian Medical Association Journal, and The American Academy of Pediatrics joined in the call for mandatory immunization [31] .

In July 2012, the British Columbia government announced a new policy that would require either influenza immunization or the wearing of a surgical mask for all HCWs during influenza season when in direct patient contact. By October 2012 the Health Sciences Association of British Columbia union had filed a grievance citing privacy issues relating to the Freedom of Information and Privacy Act (FIPA) and violations of the Human Rights Code, the Charter of Rights and Freedoms, and their collective union agreement [29] . Facing an election the following spring, the incumbent government did not repeal the policy but announced that it would wait for the outcome of the arbitration hearing and that workers who did not comply in the 2012-13 season would not face disciplinary action [32] . The government revised and softened the policy in July before going to arbitration [29] . The decision was released in October and the arbitrator had sided with the employer. It was determined the policy aligned with FIPA and there were no violations to the union agreement, the Charter of Rights and Freedoms or the Human Rights Code.

This court ruling provided the Ontario government, and health care organizations, with greater assurances that movement in the direction of mandatory vaccination for HCWs could be a viable policy option subject to HCWs being given some limited alternative to vaccination. Despite this, the Ontario government did not step forward with a consistent policy for the province. In an effort to fill this vacuum, numerous high profile Ontario hospitals announced plans to implement a mandatory ""vaccinate or mask"" policy that would require HCWs who chose not to get vaccinated to wear a mask during the influenza season [33] . Even though the Canadian Nurses' Association supported mandatory influenza vaccinations for HCWs, the Ontario Nurses' Association filed a grievance in opposition to the ""vaccinate or mask"" policy adopted by several Ontario hospitals. Following several weeks of testimony, the arbitrator found that the policy was unreasonable (policies stream) [34] . Concerns identified included that wearing a mask revealed personal health information about the HCWs decision not to vaccinate, masks were not effective, and the policy was designed to coerce HCWs [34] . This ruling leaves Ontario hospitals with no clear policy direction for the foreseeable future.

By analysing the progress of mandatory HCW immunization through the lens of Kingdon's framework, the separate streams of problems, politics, and policies can be seen to converge and diverge repeatedly over an extended period (policy windows have opened and closed several times). Since the implementation of UIIP in Ontario, there have been several potential opportunities to pursue a mandatory HCW vaccination policy. In each instance a technically feasible solution was available (policies stream). However, despite the policy's importanceprotecting the public-the problem's prominence (problems stream) and the political environment (politics stream) only aligned with the policies stream for a very short period of time. During these periods there was inadequate support for the issue to remain on the government's decision agenda long enough to result in the adoption of a new policy. In part, this seems to have been due to other problems gaining prominence and displacing the issue of mandatory vaccination of HCWs from a priority position. In addition, each time this issue gained prominence, opposition lobby groups reacted, making the proposed solution less politically acceptable.

Although the call for mandatory HCW vaccination has withstood some legal challenges-including to the Charter of Rights and Freedoms, The Freedom of Information and Privacy Act, and the Human Rights Code-there remains small but committed groups who continue to oppose such a policy. As a result, it seems highly unlikely that a mandatory HCW vaccination policy will be adopted by the Ontario government until perception of the problem's importance is sufficient to overcome the political opposition to implementing a solution.

Abbreviations FIPA: Freedom of Information and Privacy Act; HCWs: Health care workers; SARS: Severe Acute Respiratory Syndrome; UIIP: Universal Influenza Immunization Program; WHO: World Health Organization Acknowledgments Not applicable.

Not applicable.

The datasets during and/or analysed during the current study available from the corresponding author on reasonable request.

Authors' contributions AJ was responsible for the conception and design, data collection, analysis of data, interpretation of data and drafting of manuscript. NB was responsible for revisions to design, analysis of data, interpretation of data and revisions to

",0.6575486034353258
ScienceDirect Use incentive approach to promote BAT/BEP for centralized incineration facilities of medical waste in China,"The GEF project of environmental sustainability management for medical waste in China has been developed and implemented. The overall objectives of the project are to incorporate the concept of overall-process management in life cycle of medical waste into medical waste management and disposal of China, facilitate BAT/BEP application and promotion, improve China's abilities of medical waste management and disposal, promote reduction of medical waste and realize the target of harmlessness, by introducing the advanced experiences in medical waste management and disposal from foreign countries. Currently, most of the activities in demonstration stage have already been completed, and it is expected to reach the dioxin concentration of 0.1 ngTEQ/m 3 in flue gas of 15 incineration (including pyrolysis) facilities. Hence, the project decides to implement the incentive plan among eligible medical waste incineration facilities in China. In this plan, a process with steps as application, review, examination, technical transformation, verification and award was designed. To standardize and externalize the process, an entire package of files including principles, guidelines, quota and criteria, table and checklist templates were developed, considering all the involved stakeholders. As a result, with active involvements and contribution of the MEP, local governments, enterprises, experts and monitoring instructions, the incentive plan vigorously guided and promoted BAT/BEP replication and application for medical waste disposal, avoided and reduced the generation and emission of dioxin POPs and other toxic substances. Currently, there are 8 incinerators which realized the aim of controlling their dioxins emission under limited concentration of 0.1ng TEQ/m 3 in the flue gas.","China produces approximate 1,780 tons medical waste (MW) per day. MW incineration was recognized as one of the key release sources of PCDD/Fs in China, and therefore was listed into the inventory of foremost controlled sources by China's National Implementation Plan (NIP). As required by Article 5 of the Stockholm Convention 1 on Persistent Organic Pollutants, China give priorities to take measures to reduce releases of PCDD/Fs and and other unintentionally produced POPs (UP-POPs), by means of the application of best available techniques and best environmental practices (BAT/BEP).

Following the outbreak of Severe Acute Respiratory Syndrome (SARS) in June 2003, the Government moved quickly to establish the National Plan for Construction of Facilities for Disposal of Hazardous Waste and Medical Waste (NPHMW), in which China is committed to construct 332 dedicated MW disposal facilities across the country. Similar with the practice in many other developing countries, the NPHMW envisaged adopting incineration as the technology of choice for most of these facilities at first. Under the influence of BAT/BEP concept, nearly half of the facilities which are almost minor ones turned to choose the non-incinerations technologies, such as autoclave, microwave and chemical disinfections, which are alternative and supplementary to incinerations and could avoid emissions of PCDD/Fs and other UP-POPs. However, there are still about half facilities using incineration like rotary kiln and pyrolysis (continuous or by-batch feeding).

In order to fulfill the obligations of Stockholm Convention, and implement NIP, Ministry of Environmental Protection of the People's Republic of China (MEP, designating its branch ""FECO""--Foreign Economic Cooperation Office) and the United Nations Industrial Development Organization (UNIDO) jointly developed and executed the full-size project of Environmental Sustainability Management for Medical Waste in China in support of Global Environment Facility (GEF) 2 . Integrating with the implementation of NPHMW, this project was to promote the adoption of BAT/BEP in MW disposal and treatment sector, ultimately reducing environment pollution and protecting human health. There have been three incineration facilities (each stands for a representative incineration type) selected for BAT/BEP demonstration, aiming to reduce PCDD/Fs release (in flue gas) down to 0.1ngTEQ/m 3 , by means of engineered and managing methods. To further promote BAT/BEP for MW incineration in China, the project designed and kicked off an incentive plan 3 for centralized incinerators to compensate for up to 15 facilities to decrease their PCDD/Fs release limitation in the flue gas from 0.5ngTEQ/m 3 to 0.1ngTEQ/m 3 . The successful implementation of the incentive plan will substantially promote the revision of Pollution Control Standard 4 for Hazardous Wastes Incineration (GB18484-2001) in which the PCDD/Fs release limitation in the flue gas will be decreased from 0.5ngTEQ/m 3 to 0.1ngTEQ/m 3 , and furthermore promote the replication of BAT/BEP nationwide. In this abstract, the mechanism and framework of the incentive plan will be laid out and discussed.

medical waste PCDD/Fs polychlorinated dibenzo-p-dioxins and dibenzofurans, also as dioxins NIP National Implementation Plan for the Stockholm Convention on POPs UP-POPs unintentionally produced POPs BAT/BEP best available techniques and best environmental practices SARS severe acute respiratory syndrome NPHMW national plan for construction of facilities for disposal of hazardous waste and medical waste MEP ministry of environmental protection UNIDO united nations industrial development organization

According to the requirements of current Pollution Control Standard 4 , the limit of PCDD/Fs emission concentration into atmosphere from incinerator is 0.5 ngTEQ/m 3 . The revision of the standard is on-going currently, and influenced by the project the limit will be decreased to 0.1 ngTEQ/m 3 . However, more monitoring data and technical practice information are required to verify the possibility of the application of BAT/BEP not only at demonstration level but also at a massive replication level. Hence, it is necessary to regulate the facilities reaching 0.1 ngTEQ/m 3 by economic incentives and based on the industrial market, to accelerate and support the revision process.

The three BAT/BEP technical demonstrations of rotary kiln, continuous pyrolysis and by-patch pyrolysis have completed and will enter the summary stage. The technical proposals and successful experiences advanced the revision of the corresponding technical engineering regulations and will provide practical references for the incentive plan. Additionally, Guidance of Best Available Techniques for Pollution Control of Medical Waste Disposal (trial) (HJ-BAT-8) (BAT guidance) 5 has already been issued, which will provide important technical guidance and basis for the incentive plan. In terms of supervision, several law and regulations such as revision of Laws of Environmental Protection, Judicial Interpretation on Several Issues of Applicable Laws to Deal with the Environmental Pollution Criminal Cases, and Notice on Further Strengthening the Medical Waste Management, were issued to compel and regulate enterprises to take active actions to meet the above pollution control standard and technical regulations.

Currently, there are more than 100 centralized medical waste incinerators (including those for joint disposal of medical waste and hazardous waste) running in China, most of which can reach a disposal scale of 5 ton/day. Beyond the demonstrations, the monitoring data of PCDD/Fs emission in flue gas of some MW incinerators shows that some facilities can or have good potential to reach 0.1 ngTEQ/m 3 . Therefore, the general situation of disposal facilities and the existing monitoring data show a solid foundation for the incentive plan.

The budget plan of incentive plan includes RMB 15 million Yuan awarded for the enterprises (15 incineration disposal enterprises reaching 0.1 ngTEQ/m 3 will be awarded with RMB 1 million Yuan respectively) and about RMB 1 million Yuan as evaluation and management fees of incentive plan, i.e. RMB 16 million Yuan in total as budget fund (about USD 2.4 million dollars, based on exchange rate of 6.2).

In the incentive plan, the main calculation basis is as below:

Investment cost of flue gas purification facility According to the engineering investment budget of the three BAT/BEP technical demonstrations, investment for equipment eligible for awarding can reach RMB 2 million to 4 million Yuan (that incorporating SCR technology. However, SCR catalyst for purification of flue gas generated in medical waste incineration accounts for a large proportion in the cost and is monopolized by foreign companies, and the delivery time is long, thus the newly upgraded facilities in the incentive plan is not available for use basically), and the total investment is RMB 3 million to 10 million Yuan. According to equipment estimate in BAT guidance, investment for flue gas purification facility is RMB 1.5 million to 2 million Yuan for the rotary kiln medical waste incinerator with disposal capacity larger than 10 t/d, and that for pyrolysis incineration facility with disposal capacity of 5 t/d-10 t/d is RMB 1 million to 1.5 million Yuan. Based on the data above, investment for flue gas purification facilities may be determined as RMB 1.5 million Yuan (equipment expenses only).

A rotary kiln incinerator consumes 45 kg~150 kg diesel fuel, 300 kW•h~400k kW•h electric energy and 10 t~14 t water annually. It runs 330 days per year, and 24 hours per day, and its operating cost is determined as RMB 3,000 Yuan/t (2,500-3,500 Yuan/t actually), thus its annual operating cost is RMB 9.9 million Yuan.

A pyrolysis incinerator consumes 15 kg~30 kg diesel fuel, 400 kW•h~500 kW•h electric energy and 3 t~6 t water annually. It runs 330 days per year, and 20 hours per day, with the operating cost determined as RMB 2,000 Yuan/t (RMB 1,500 -2,500 Yuan/t actually) and the disposal capacity of 8 t/d, thus its annual operating cost is RMB 5.28 million Yuan.

To sum up, in consideration of investment for upgrading of the flue gas purification equipment only, the incentive fund of RMB 1 million Yuan accounts for 67% in the investment, and the other 33% is undertaken by the enterprise itself. In further consideration of annual operating cost and other capital factors, the proportion undertaken by the enterprise itself may reach 1:5-1:10, which is compatible with the designed fund proportion of 1:4 for the whole medical waste project. In the context that a new emission standard will be issued, it becomes a general trend to promote the medical waste incinerators to reach 0.1 ngTEQ/m 3 . It is attractive for an enterprise with longterm perspective to obtain RMB 1 million Yuan incentive funds and to participate the incentive plan of the project.

The incentive plan includes 5 stages in all as Fig.1 shows.

1. Preparation stage. FECO will organize research forces to prepare the action program and application guidance of the incentive plan, invite the competent authorities, incineration enterprises, the headquarters of incineration enterprises, equipment suppliers and relevant authorities and organizations to attend the briefing meeting, introduce the incentive plan and listen to the opinions from all parties.

2. Application stage. FECO will release the approved application announcement and guidance for incentive plan on relevant media platform, organize the related parties to hold an explanation meeting, and introduce the contents and requirements of incentive plan to the entities willing to participate. Eligible entities (applicant) shall make preparations according to the requirements in the guidance for application and submit the application files to FECO. Approval and recommendation shall be obtained from the local competent environmental protection authority (provincial or municipal) before application. The deadline of application may be postponed as appropriate if there are less than 15 applicants.

3. Examination stage. FECO will perform formal examination in terms of integrity, normativity and effectiveness for the application files. Comments of formal examination will be provided to the applicant within five (5) work days after the application files have been received. The applicant shall supplement the application files as required within five (5) work days. FECO will make technical examination in terms of technical feasibility and operability for the application files passing formal examination (on-site re-examination will be performed when necessary) and provide comments of technical examination. The applicant shall submit the revised technical proposal within ten (10) work days. FECO will sign an agreement of incentive plan with the applicant passing technical examination (qualified applicant). FECO will negotiate with UNIDO if less than 15 applicants pass the examination finally.

4. Implementation Stage. The qualified applicant shall conduct relevant activities according to the technical proposal after signing the agreement of initiative campaign, and accept the dispatching and inspection of FECO. If the actual implementation process varies largely from the technical proposal, the qualified applicant shall inform FECO in a timely manner to actively consult for solution. After completing the activities listed in the technical proposal, the qualified applicant hall entrust an entity certified for dioxin monitoring to monitor the dioxin emission in flue gas of the target facility. The monitoring and sampling process shall be witnessed by a person of FECO or an authorized representative of FECO, and the representative of competent environmental protection authority at the local place where the enterprise is located. The applicant with monitoring results compatible with the judgment criteria may submit a summary report for the incentive plan to FECO, and submit the application for on-site verification.

5. Verification, Awarding and Summary Stage. FECO will organize an expert group to perform on-site verification at the place of applicant, and provide comments for the verification. A dioxin monitoring organization may be entrusted to re-examine and monitor dioxin emission in the flue gas of disposal facility based on the needs of on-site verification. FECO will award the qualified applicant eligible for awarding according to the agreement of incentive plan. 3.Openness and fairness. All the requirements specified for the incentive plan and the main implementation process are in the open, and the incentive criteria are set in the spirit of fairness.

4.Guidance and encouragement. Those incineration disposal enterprises, which have reached the incentive criteria specified in the Guidance by raising investment, upgrading equipment and strengthening management relying on their own efforts, will be provided with one-time incentive funds, as demonstration and guidance for technology and management improvement in the medical waste disposal industry.

The applicants shall meet the all following requirements:

1.An applicant must be an independent legal entity within the territory of the People's Republic of China, which mainly specializes in medical waste disposal via incineration. The applicant must have a valid business license for hazardous waste disposal.

2.The facility covered in the application shall be of centralized disposal type, approved by the provincial or municipal competent environmental protection authority and serving the corresponding region.

3.The designed disposal capacity (unit set) of facility covered in the application shall be above 5 ton/day (included), and the annual disposal capacity for medical wastes (unit set) shall be above 1000 ton (included) (excluding the facility with a license obtained less than one year ago or at the stage of pilot running).

4.The monitoring result of pollutant release of an applicant within one year before application shall meet the requirement of emission limit in Pollution Control Standard for Hazardous Wastes Incineration.

5.The applicant must be law-abiding, honest and faithful (having valid business license and pollutant release license).

An applicant with incinerator eligible for incentive within the incentive period will be provided with RMB 1 million Yuan in one time. The incentive object shall be in compliance with the following judgment criteria:

1.Have completed all contents listed in the technical proposal which has been submitted and approved; 2.In the monitoring results verified, the dioxin concentration in flue gas shall not be larger than 0.1ng-TEQ/Nm 3 , and the emission levels of other pollutants shall be within the limits defined in Pollution Control Standard.

3.The applicant shall provide records of operation parameters for the incinerator in normal operating conditions for a month within the incentive period (according to ""best available process parameters"" in BAT guidance).

4.Meet the main requirements in BAT guidance.

5.The facility covered in the application shall meet the requirements in Technical Specifications of Engineering Construction for Centralized Incineration Disposal of Medical Wastes.

The Notice of the incentive plan was published on June 4th, 2014 and an explanation meeting was held jointly by FECO and UNIDO in Beijing, June 17th 2014. By the end of 2014, there were 15 incinerators of 13 enterprises passed the examinations and won the qualification to pursue for the incentive fund (see Fig. 2 ). Until the end of May 2015, there are 8 incinerators passed the verification of the incentive plan. In consideration of possible failure and quit of some incinerators, it is necessary to prepare for the successive activities. The following plans are suggested: 1. Make a complementary recruit. If the qualified incinerators exceed the gap, the incentive fund will be awarded with the method of first come first served. 2. Encourage replacement of incinerators with non-incineration technologies.

",0.6571552123922191
ECONOMIC IMPACTS OF VISA RESTRICTIONS ON TOURISM: A CASE OF TWO EVENTS IN CHINA,"Increasing attention has been paid to evaluate the impacts of large scale events on tourism demand. For providing policy suggestions, it should consider the economic impact of both events themselves and other factors. This paper evaluates the economic effects of visa restrictions on tourism as a result of the 1989 Tian'an Men Square Incident and the 2008 Beijing Olympic Games by using an innovative combination of econometric and computable general equilibrium (CGE) models. The results show that both events generated economic losses. The unexpected negative economic impact of the Beijing Olympics seems attributable to visa restrictions. Suggestions for the alleviation of the negative impact of visa regulations are provided.","Researchers have begun to evaluate the impacts of large-scale social, economic, political and natural events on tourism (Ritchie, 1984) . The September 11, 2001 terrorist attack on America, the Indian Ocean earthquake and tsunami in 2004, the 2008 financial crisis, and the Olympic Games are examples of one-off events (Jennings, 2010; Lagadec, 2004) . Such events are normally highly visible with economic, social and political consequences (Decker et al., 2005) . In this paper, only crises and special events are examined. The Organisation for Economic Co-operation and Development (2003) identified five sources of crises: natural disasters, technological accidents, epidemics, terrorism, and food safety. Special cultural and sporting events are ShiNa Li is Senior Lecturer in Events Management at Leeds Metropolitan University (Headingley Campus, Leeds, UK. Email: <S.Li@leedsmet.ac.uk>). Her research interests include economic impact evaluation, impacts and legacies of events and tourism, and economic modelling.

Haiyan Song is Chair Professor of Tourism in the School of Hotel and Tourism Management at The Hong Kong Polytechnic University. His research interests include tourism demand modelling and forecasting, tourism impact assessment and tourism supply chain management.

''major one-time or recurring events of limited duration, developed primarily to enhance awareness, appeal and profitability of a tourism destination'' (Ritchie, 1984, p. 2) . Policy-making in tourism involves risk. Thus assessing the impact of one-off events on tourism activities is imperative for policy-makers to avoid unnecessary fears, waste of scarce resources, or missing other important factors that may influence their decisions (Hardaker, Fleming, & Lien, 2009) . Detailed evaluation of the impact of one-off events on tourism demand can lead to the formulation of effective and efficient management strategies at both the firm and industry levels (Eugenio-Martin, Sinclair, & Yeoman, 2005) .

Any evaluation should consider the economic impact of a one-off event and the effects caused by other factors. The economic consequences of inbound tourism during a one-off event may be attributed to factors such as exchange rate changes, bad weather, inflation, and social instability. Many studies have evaluated the economic influences of tourism due to such one-off events as foot and mouth disease (Irvine & Anderson, 2005) , terrorist attacks (Arana & Leon, 2008; Eugenio-Martin et al., 2005; Pizam & Smith, 2000) , economic crises (Okumus, Altinay, & Arasli, 2005; Smeral, 2010; Song & Lin, 2010) and sporting events (Giesecke & Madden, 2011; Li, Blake, & Cooper, 2011) . Researchers have also identified the factors that may influence the demand for tourism and have evaluated their economic consequences. For example, Li et al. (2011) studied the total effects of visa control and the torch relay incidents on tourism due to the Beijing Olympics; however, they did not capture the separate impact of each of these two factors. Song, Gartner and Tasci (2012) evaluated the economic consequences of visa restrictions as a result of the 1989 Tian'an Men Square Incident and the 2008 Beijing Olympics, but they did not evaluate the effect of the events themselves. Page, Song and Wu (2012) examined the simultaneous influences of the 2008 economic crisis and swine flu on inbound tourism to the UK using an innovative method that separated the effects of the two events that happened during the period of the assessment.

There are two levels of economic evaluation of one-off events. The first is to analyse their economic consequences in terms of a change in tourism demand, especially a decrease or increase in the number of tourists and in their spending. The second is to evaluate the economic impact of these consequences, which is the economic impact of a change in tourism demand. It can be shown in terms of a decrease or increase in Gross Domestic Product (GDP), prices, employment and household welfare. However, the differences between these two levels of assessment have not been clarified in the literature. This paper evaluates the economic influences of one-off events which are attributed to the event itself and visa restrictions. The two events include one crisis (the 1989 Tian'an Men Square Incident) and one special event (the 2008 Beijing Olympic Games). This study makes a dual contribution. It evaluates the economic consequences of inbound tourism demand as a result of the events themselves and of visa restrictions imposed during the events. This study also innovatively combines two approachesthe econometric and computable general equilibrium (CGE) models in the impact assessment. The estimation results of the econometric models, especially the estimates of the demand elasticities, are used as inputs of the CGE models. The published impact studies have only either used econometric models or CGE models to evaluate the economic consequences of tourism. The CGE studies have mainly used the crude tourism demand estimates and elasticities from other secondary sources, which might not be directly related to the destination under consideration. As a result, the calibrated CGE models were often criticized for their unreliable results. This study, therefore, attempts to overcome these challenges by combining the strengths of both econometric and CGE approaches, and to obtain more robust and reliable results in addition to a separate evaluation of the effects created by the events and visa restrictions.

A crisis discourages potential inbound tourists from visiting a destination by increasing their perceptions of psychological risk (Li, Blake, & Cooper, 2010) . A special event, however, is expected to attract more inbound tourists. Politicians perceive special events as an opportunity to showcase their country, and to over-estimate the positive consequences of the event while under-estimating its potential risks (Moran, 2001) . Policy makers tend to be risk averse and thus sensitive to possible losses due to security risks during an event (Jennings, 2010) . One way of reducing the potential security risk is to impose visa restrictions on potential tourists to the country during the event. The main purpose of the visa regulation is to ''control the movement of modern and masterless men'' (Song et al., 2012, p. 398) . Visas, which control the movement of a population, are an effective and straightforward means of preventing the entry of potential terrorists (Torpey, 1998) .

Visa regulations can negatively influence the inbound tourism to a destination, and therefore its economy. According to VisitBritain, as a result of visa restrictions imposed to the Chinese tourists, the UK received 60% fewer potential tourists from mainland China (Tourism Alliance, 2012). The visa restrictions imposed by the USA after the September 11, 2001 terrorist attacks have reduced tourist revenue by $859 billion with potential job losses of at least half a million (Silva, 2011) . The World Travel and Tourism Council (2012) estimated that relaxing visa regulations would bring 122 million international tourists to the G20 economies and a total of US$206 billion in additional tourist revenue. This would create over five million new jobs by 2015.

Tourism is vulnerable to external factors. Song et al. (2012) showed that although Chinese tourism growth was quite stable between 1978 and 2009, there were three obvious downturns associated with the Tian'an Men Square Incident in 1989, the severe acute respiratory syndrome epidemic in 2003, and the financial crisis in 2008. These downturns may have been caused by multiple factors. For example, Song et al. (2012) identified the negative impacts of visa restrictions during and after the Tian'an Men Square Incident on the tourist expenditure in China using the econometric approach. They stated that the downturn in 1989 could be attributed to not only the event itself but also to the visa restrictions. However, their assessment was mainly related to the direct economic impacts.

The situation in 2008 was even more complex; three factors affected tourism in China: the Beijing Olympics, visa restrictions during the Olympics, and the global financial crisis. There were more news and reports indicating the visa restrictions during the 2008 Beijing Olympics. The government prioritised security and thus tightened visa regulations when facing safety challenges including ''warnings about a potential terrorist attack during the Games, riots in Tibet, Olympic torch-relay protests and an alleged terrorist plot to kidnap journalists covering the Olympics, and the Sichuan earthquake'' (Barboza, 2008; Bennhold & Rosenthal, 2008; Song et al., 2012, p. 399) . Concerning of security and visa regulations, some foreign tourists cancelled or delayed their travel bookings from Chinese travel companies during the Beijing Olympics (Premo, 2008) . Figure 1 shows the monthly growth rates of inbound tourists and their expenditure in China in 2008. Both growth rates started to decrease in February and became negative from April onward. However, the growth started to reduce again in October. Before and during the Olympics, the government cancelled or refused to renew many visas, such as multiple visas and demanded that visa applicants provide detailed supporting documents such as hotel and airline booking confirmations (Li et al., 2011) . The government's concern for security inevitably damaged China's tourism industry. For example, the occupancy rate of large hotels was below 70% and lower than 50% for small hotels (Branigan, 2008) . The Olympics itself, however, seemed to have slowed down the decline in both tourist arrivals and receipts.

Crises increase tourists' negative perceptions about a destination as they may affect the safety and health of tourists when they visit the destination. Crises may also create a fear of being trapped, as happened in the 1999 Taiwan earthquake (Huan, Beaman, & Shelby, 2004 ), of contracting a disease, as happened in the UK in 2001 with foot and mouth disease (Ritchie, Dorrell, Miller, & Miller, 2003) , of being attacked by terrorists (Blake & Sinclair, 2003; Fall & Massey, 2006) , or being in an unsafe social or political environment (Beirman, 2002; Okumus, Altinary, & Arasli, 2005) . Special events can also generate tourists' perceptions about pollution, noise, high prices, and traffic congestion in the destination (Li & Blake, 2009; Preuss, 2004) . These perceptions towards one-off events may discourage tourists from-arriving at the host destination (Huan et al., 2004) .

The literature has evaluated the economic influences of crises on tourism at the firm and industry levels. Irvine and Anderson (2005) conducted surveys to analyse the impact of the foot and mouth disease on small and large firms; the results showed that larger firms suffered more from the negative effects because smaller firms were more adept in responding to the crisis. Okumus et al. (2005) revealed that the 2001 economic crisis in Turkey had both negative and positive effects on the tourism industry in Northern Cyprus. The impacts of crises on consumers have also been examined. Arana and Leon (2008) studied the impact of the September 11, 2001 terrorist attacks on tourists' preferences and found a significant decrease in tourists' utility and a change in various destinations' images.

Many studies have analysed the impact of crises on the demand for tourism. Pizam and Smith (2000) identified major terrorism attacks between 1985 and 1998 and found that more than three-quarters of the crises examined significantly reduced tourism demand. By comparing the responses of French, American and German tourists to two crises, it was found that the foot and mouth disease affected French tourists the most while the September 11 attack mostly affected Germans in terms of arrivals and receipts (Eugenio-Martin et al., 2005) . The findings of Kuo, Chen, Tseng, Ju and Huang (2008) reflected that tourism demand in Asian countries was more negatively affected by severe acute respiratory syndrome than by avian flu. Some researchers have also explored the impact of crises on the whole economy. Dwyer, Forsyth, Spurr and Vanho (2006) indicated that the world tourism crises, including the Iraq War and severe acute respiratory syndrome in 2003, caused a net decrease in real GDP by $62.418 million in Australia.

The economic impact of an event is attributed mainly to an increase in tourism spending from tourists, organizers, delegates, sponsors and others (Dwyer, Forsyth, & Spurr, 2005) . There is a rich literature on the evaluation of economic consequences and impacts of special events, especially sporting events (see Daniels, Norman, & Henry, 2004; Gelan, 2003; Giesecke & Madden, 2011; Hotchkiss, Moore, & Zobay, 2003; Kim, Gursoy, & Lee, 2006; Lee & Taylor, 2005; Li et al., 2011) . Although most of these studies have shown large positive economic consequences generated by special events, several have shown that special events have negative economic impacts. For example, Li et al. (2011) conducted both ex-ante and ex-post analyses of the economic impact of the 2008 Beijing Olympics. Their findings suggested that the ex-ante research forecasted a positive economic impact, while the expost research revealed a negative economic impact which was attributed largely to visa restrictions by the Chinese government.

This research combines two approaches-the econometric and CGE modeling methods in assessing the economic impacts of two distinctive one-off events. Blake, Durbarry, Eugenio-Martin, Gooroochurn, Hay, Lennon, Sinclair, Sugiyarto and Yeoman (2006) suggested that econometric models should not be considered as substitutes for CGE methods, but the former can complement the latter. The output of the econometric models (the change of tourism demand) can be used as the input of the CGE models. Furthermore, the accuracy of these parameters is important and may affect the results of the CGE models. Therefore, reliable estimates of these feeder parameters by the econometric models are the key to insure the reliability of the CGE models. However, since the demand elasticities are generally unavailable in many destinations, studies that used the CGE approach always adapt these parameters from other CGE studies, which may not always corresponding to the destinations under study. This study, therefore, improve the reliability of the CGE models by incorporating the demand elasticities obtained from the econometric models.

The most common approaches used to evaluate the economic influences of crises and special events on tourism include econometric, CGE and input and output (I-O) models. Although most studies claim to have evaluated the economic impact of crises or special events on tourism, most of these studies only captured the economic impacts by using one of the two categories of methods. The first category is econometric models which evaluate tourism effects of demand. Particularly, the econometric models were used to evaluate the changes in either tourist arrivals or their expenditure, caused by an event. The second category is CGE and I-O modelling which evaluates the economic impact of tourism demand. Specifically, the changes in economic indicators such as GDP, employment, imports and exports caused by a change in tourism expenditure as a result of an event are captured. Most studies confuse the direct tourism effects with the total economic impacts. The tourism effects are the changes in either tourist arrivals or expenditure caused by the event; the latter pertains to the economic changes brought about by the changes in tourist arrivals or receipts within the economy.

The tourism demand function is the basis of econometric models that are used to assess tourism impacts of special events. Econometric models can estimate the changes in tourism demand caused by one-off events, which the CGE and I-O models cannot do. Econometric models have been used to assess the economic gains and losses of tourism caused by one-off events such as financial crises (Lim & McAleer, 2005; Smeral, 2010; Song & Lin, 2010) , terrorist attacks and activities (Goodrich, 2001; Pizam & Fleischer, 2002) , diseases (Kuo et al., 2008) , and the Olympics (Athanasopoulos & Hyndman, 2008) . One of the advantages of the econometric models is that the causal relationships between the dependent variable (tourism demand) and the explanatory variables (its influencing factors) can be evaluated (Song & Li, 2008) . These influencing factors include but are not restricted to tourists' income, tourism prices, exchange rates, transportation costs, and the one-off events (Song & Li, 2008; Wang, 2009) . Another advantage of the econometric models is that they can measure the change in tourism demand as a result of one-off events based on long-run relations between tourism demand and its influencing factors (Blake, Gillham, & Sinclair, 2006) . The influences of one-off events on tourism demand are normally accounted for by the use of dummy variables in the demand model (Wang, 2009) . The results of the econometric models are very useful for tourism policy formulations especially when the influencing factors change or when a one-off event occurs . However, the policy implications may be restricted to tourism policy makers only since the wider economic impact of tourism at macroeconomic and industry levels cannot be evaluated by the econometric models.

This paper focuses on the assessment of the impacts of one-off events and the associated visa restrictions using the CGE modelling approach, which has been shown to provide more robust results (Dwyer, Forsyth, & Spurr, 2004) . Unlike I-O modelling, CGE modelling puts constraints on the factors of production allowing changes in wages and prices, which can more fully capture the negative impact (Dwyer et al., 2004) . CGE modelling simulates the economy and reflects changes in the economy when all markets clear simultaneously (Starr, 1997) . The CGE models consist of the primary, secondary, and tertiary sectors, so they can take into account the relationships among economic agents including businesses, households, and the government (Li et al., 2010) .

Unlike econometric models, the CGE models can capture the feedback effect, i.e., the effect of tourism demand on non-tourism sectors, which further affects the tourism sectors . For example, if the foot and mouth disease decreased the inbound tourism demand in the UK, there would be a flow of the value of labour and capital use from tourism to non-tourism sectors, such as the primary sector. The increase in the supply of labour and capital would decrease the costs in primary sectors and then reduce the prices of their products, such as agricultural products, which would decrease the retail price of food. As food is central to the catering sector, the decrease in food prices would decrease catering prices. This is just a simple example and the real economic feedback effects are more complex as changes in economic sectors affect each other. Most studies using CGE or I-O models to evaluate the impact of one-off events on tourism/economy are based on secondary data and simulations. None of the studies are based on more accurate estimates of the economic losses/gains from the econometric models. This paper fills in this gap by combining the two methods.

A framework is constructed for this research (see Figure 2 ). This framework consists of linked outer and inner parts. The outer part suggests that the 2008 Beijing Olympics and the 1989 Tian'an Men Square Incident affected tourism and the economy through two channelsthe event itself and visa control. An event can have either positive or negative influences on the demand for tourism. Organized special events normally bring positive effects by enhancing awareness, building new images and attracting additional tourists. Crises, however, bring negative effects to the destination. Both types of events can jointly affect the industry and the economy as a whole. Visa restrictions generally have negative economic influences on the destination economy. When considering these two factors (the event itself and associated visa restriction), the overall impact can either be negative or positive. This framework is applied to both types of events using the same modelling approach. The inner part depicts the connection of the two modelling methods in the assessment-the econometric and CGE models. The input of the econometric models includes economic variables such as tourists' income, tourism price, the substitute prices and the dummy variables, which capture the tourism influence of the one-off events, especially the 2008 Beijing Olympics and the 1989 Tian'an Men Square Incident. The outputs (results) of the econometric models are the tourism impacts of one-off events which are the inputs of the CGE models. The econometric models provide the key parameter-price Figure 2 . A Framework of this Research elasticity for the CGE models. The outputs (results) of the CGE models are the economic impacts. These refer to the welfare losses/gains at the macroeconomic level and to the economic indicators such as the values of labour and capital, imports and exports and prices at the industry level. This research presents the economic impact of the Beijing Olympics and the Tian'an Men Square Incident on tourism and its related sectors by considering the events themselves and the visa restrictions in force at the time. Song et al. (2012) identified the input parameters, which are the price elasticities of tourism demand by international tourists. The effects of the two events on the demand for tourism in China in terms of both arrivals and receipts form the basis of the CGE modelling exercise in the following section. The econometric models used by Song et al. (2012) are known as the autoregressive distributed lag model. The Tian'an Men Square Incident, the 2008 Olympics, visa restrictions, severe acute respiratory syndrome and the 2008 financial crisis were accounted for by dummy variables. Table 1 presents the price elasticities of inbound tourism demand to China and Beijing from various source markets.

The CGE approach captures the circular flow of income. It models and simulates the behaviour and activities of each economic agent such as the production, household, government and export-import sectors. It also captures the interactions and feedback among these agents. These economic activities include intermediate and final consumptions, supply and demand, and international trade. The theoretical bases of CGE models are utility maximization subject to resource constraints (demand functions) and profit maximization subject to resource constraints (supply functions). As the model includes hundreds of functions, a high-level modelling system known as the general algebraic modelling system and its subsystem, the mathematical Lofgren, Harris, and Robinson (2002) . The main functions used in the models are the Leontief, the Cobb-Douglas, the Constant Elasticity of Substitution and the Constant Elasticity of Transformation technology. The details of these functions can be found in Li et al. (2011) . One of the key elasticities is the price elasticity of tourism demand, taken from Song et al. (2012) . The averages of the price elasticities from the ten key source markets are À0.802 for China and À0.878 for Beijing (see Table 1 ).

The three models are applied to evaluate the economic impacts of the economic losses/gains of tourism demand estimated from the econometric models. Therefore, unlike the standard model structure by Lofgren et al. (2002) , the three CGE models are extended to include activities pertaining to the demand for tourism. In the extended models, a new sector (the tourism exported sector) and a new representative consumer (international tourists) are introduced. The tourism exported sector produces tourism products and services such as transportation, hotel rooms and catering services for international tourists. Details of modelling tourism using CGE models are discussed in Wattanakuljarus and Coxhead (2008) and Li et al. (2011) . This extension requires the introduction of two additional functions into the standard model.

Aggregate export of tourism-related goods is represented by a Cobb-Douglas function:

where p T is the aggregate price of international tourism; k is a shift parameter; p i is individual product price; and P i a i ¼ 1. The demand for the Cobb-Douglas aggregate product is a function of the aggregate tourism price:

where q T is the quantity demanded by inbound tourists; Q T is the benchmark quantity demanded by inbound tourists; e is the exchange rate; and l is the price elasticity of tourism demand.

The resulting economic impacts are shown at both the macroeconomic and industry levels. At the macroeconomic level, instead of evaluating the economic impact in terms of GDP, which has been used in most studies, this paper uses equivalent variation (EV) to measure the welfare impacts of the economic losses/gains due to the two one-off events in monetary terms. EV is defined here as ''the amount of extra income that, at unchanged prices, would allow consumers to reach the utility that they actually reach as a result of a policy change, or other exogenous shock'' (Fane & Ahammad, 2003, p. 176) . EV is ''the amount of income that would have to be given to (or taken away from) the economy before the policy change (or an external shock) to leave the economy as well off as the economy would be after the policy change'' (Andriamananjara et al., 2004, p. 17) . Besides, EV is the economic indicator that is more frequently used to measure welfare in CGE modelling in the literature (see Ahmed, 2008; Fane & Ahammad, 2003; Margaret & Mabugu, 2008; Ye, Lee, & Chen, 2006) . Maximising gross state product at the state level or GDP at the national level is second best to maximising the economic welfare. GDP includes increased income earned by non-resident owners of capital, non-resident labour and the government through taxes while welfare can measure the wellbeing of local residents (Abelson, 2011) . In this case, EV might have more policy implications than GDP as policy strategies on a one-off event should depend largely on the effect of welfare on local residents. Table 2 shows that both the events themselves and the associated visa restrictions related to the 1989 Tian'an Men Square Incident and the 2008 Beijing Olympics reduced China's inbound tourist arrivals and decreased China's tourism receipts from the ten origin countries under consideration. Visa restrictions were responsible for larger losses to tourism receipts, which were $-88.232 million as a result of the 1989 Incident and $-963.860 million as a result of the 2008 Olympics.

Economic impacts of China's inbound tourism caused by the 2008 Olympics and the 1989 Incident are depicted in Tables 3 and 4. The input of CGE models-the economic consequences of tourism receipts were generated from the econometric models and the key output of CGE models at macroeconomic level are the welfare losses as a result of the events themselves and visa restrictions, respectively. The rest of Table 3 gives the results of CGE models. The economic loss of tourism receipts caused by the 1989 Tian'an Men Square Incident brought relatively small welfare losses, i.e., $2.111 million as a result of the Incident and $2.847 million as a result of visa restrictions (see Table 3 ). The welfare losses caused by the 2008 Beijing Olympics were much greater. One would expect a crisis to cause a larger welfare loss than a special event, which supposedly should bring welfare gains by attracting more tourists. However, this is not what we have found, for several possible reasons. Notes: (a) Korea and Malaysia are omitted from the 1989 dummies because data before 1994 are not available for them. Therefore, loss of tourist arrivals, i.e. (1), is calculated by adding the differences in tourist arrivals from eight countries when the 1989 dummy (D89) equals to 1 and 0; (b) Gain/loss of tourist arrivals for the Beijing Olympics in 2008, i.e. (1), is calculated by adding the differences in tourist arrivals from ten countries ( (2) and (3) are adjusted to 2008 price levels. First, the scale of inbound tourism in China in 1989 was much smaller than that in 2008 and thus the decrease in inbound tourism demand in 1989 was likely to be small. Second, the negative welfare change due to the Olympics itself could be explained by the crowding out effect superseding positive tourism effects. Most policy makers have overlooked the crowding out effect. However, it can play a dominant role. The crowding out of inbound tourism means that inbound tourists either cancel their trips or change their plans of visiting an event host country to avoid traffic jams, long queues, noises, security concerns and increased prices (Li & Blake, 2009 ). Third, visa restrictions during the Olympics caused the largest welfare loss ($-963.86 ). This welfare loss occurred when inbound tourists could not come to China because they had not been able to obtain a visa. Fourth, every unit change in real tourism consumption would reduce welfare for the 2008 event more than it would for the 1989 event. Welfare loss per change in real tourism consumption (6) is around 0.20 for the 2008 Olympics, which is almost seven times higher than it was for the 1989 Incident (about 0.03). One reason might be that when one unit of tourism spending flows into tourism-related sectors, such as hotel, and further money then flows along the supply chain, for example food processing and agriculture, employees in these sectors earn extra income from tourists' spending. Since the income level in 2008 is higher than that in 1989, the extra income received per unit of tourism spending is higher in 2008 than that in 1989.

A decrease in real tourism consumption (4) is slightly less than the economic loss of tourism demand (1). This occurred because of a small decrease in the price of inbound tourism consumption (4), which slightly offset the decrease in real tourism spending. When tourism demand decreases because of a one-off event, the supply may remain unchanged, which then leads to a decrease in equilibrium price. The smallest percentage of price decrease in inbound tourism (À0.009) is due to the 2008 Olympics. This may be because between 2002, when Beijing was chosen to host the Games, and the Games themselves in 2008, there was sufficient time to adjust the supply to meet the demand while the 1989 Incident happened suddenly without leaving sufficient time for the supply to be adjusted.

In order to present the results of the industry-level impact, the sectors included in the I-O table (44 in the 2007 China and Beijing tables and 33 in the 1989 China table) are categorised into primary, secondary and tertiary industries. This division has already been made in the original I-O tables. Primary industries include the agricultural sector; secondary industries include manufacturing sectors such as food production, coal mining and chemical manufacturing; the tertiary industry consists of service providers such as transportation, accommodation, catering and entertainment. In general, the economic effects at the industry level caused by visa restrictions were larger than the impacts generated by the events themselves (see Table 4 ). The industry results show that both events have reduced changes in the value of labour and capital uses, percentage change in price index and output of tertiary industry in 1989 and 2008 since tourism-related sectors belong to the tertiary industry which suffered from the economic loss of tourism demand.

In contrast, primary and secondary industries enjoyed an increase in the value of labour and capital uses and percentage change of output, both of which are attributable to allocation effects. When inbound tourism demand decreases and tourism supply and production increase correspondingly, the result is a flow of production factors, for example labour and capital, from tourism to non-tourism related sectors. CGE modelling captures the allocation effects, which have been observed in the literature (Blake, 2005; Li et al., 2010 Li et al., , 2011 Madden, 2002) . It can also be observed that primary industry experienced a smaller increase in 2008 than it did in 1989, although the economic loss in 2008 was larger. This might be because of the industry structure of the two years. The proportion of primary industry in GDP composition decreased steadily from about 30% in the late 1980s to about 10% in 2010 according to the China Statistical Yearbook.

The 2008 Beijing Olympics attracted 77,910 thousand inbound tourists and an extra $84.766 million in tourism receipts (Table 5) . However, visa restrictions reduced the number of inbound tourists by 270.381 thousand and tourism receipts by $294.185 million. Due to lack of data, the economic consequences of Beijing's inbound tourism receipts are not calculated for the 1989 Tian'an Men Square Incident. Table 5 is calculated based on Table 5 . Table 6 shows that the event itself had an opposite effect from visa restrictions. Visa restrictions caused a welfare loss of $73.957 million brought by a $282.130 million decrease in real tourism consumption. This means that every $100 decrease in real tourism consumption caused a $26.2 million decrease in welfare (6). The percentage of price of inbound tourism consumption decreased by 0.328. However, unlike the negative impacts in China, the event itself generated a small welfare gain of only $21.421 million which was stimulated by an increase of $80.975 million in real tourism consumption. Although congestion, overpriced hotel rooms, traffic jams and security concerns may have crowded out some potential inbound tourists to Beijing, the Olympics At the industry level, visa restrictions reduced the value of labour and capital use and the percentage of output in tertiary industry (Table 7 ). Like the industry-level impact of China's inbound tourism, allocation effects explain the flow of labour and capital into the primary and secondary industries. A decrease in inbound tourism demand as a result of visa restrictions led to a decreased price index for all industries. As to the industry impacts contributed by the event itself, the Olympics brought a small stimulus to tourism sectors. The tertiary industry benefited from employment and capital use and output growth, while the primary and secondary industries were crowded out with a decrease in these indicators. The price index was bid up by a small increase in inbound tourism demand because of the Olympics.

This paper has evaluated the economic effects of the 1989 Tian'an Men Square Incident and the 2008 Beijing Olympics. It is the first attempt to evaluate one-off events in terms of the effects of the events themselves and of the visa restrictions that were in force at the time. This study uses econometric modelling and CGE modelling. Combining the strengths of both methods is rare in the tourism literature but could generate more reliable results and more useful policy recommendations. The autoregressive distributed lag model, which considers the time path of the tourist decision-making process, is used to evaluate changes in the number of tourists in the course of a one-off event. The tourism effects are taken as the model input for the CGE models which then evaluate the economic impact of inbound tourism in terms of the change in welfare at the macroeconomic level and changes in labour and capital use, percentage changes in price index and output at the industry level. CGE models take into account all economic agentsproduction sectors, factors, household, government, and international trade-and are able to capture their feedback effects.

The results show that in China the economic loss of $88.232 million caused by visa restrictions and $65.713 million by the event itself (as a result of the 1989 Tian'an Men Square Incident) reduced welfare only by about $2.847 million and $2.111 million, respectively. The 2008 Beijing Olympics caused large economic and welfare losses due to both the event itself and visa restrictions apart from a small economic gain to Beijing due to the event itself. Crises understandably have negative economic consequences, but the large economic welfare loss caused by these special events may be ''unusual'' and ''unexpected''. The ''unexpected'' outcome is largely affected by visa restrictions which reduced welfare by $193.563 million in China and by $73.957 million in Beijing. The event itself brought a welfare loss in China ($44.337 million) and a small welfare increase in Beijing ($21.421 million) as a result of the Olympics, which were attributed mainly to the deterrent effects created by pollution, high price and overcrowding.

In the two events examined here, the potential decrease in tourism demand caused by both the event itself and by visa restrictions did not receive sufficient attention nor was relevant information collected and reported. During a one-off event, priority is normally given to security risks, so the number of visas has to be limited. Although the findings of this paper show that the use of visa restrictions could cause a large welfare loss, this study does not suggest granting visas freely. In order to mitigate the economic risk of smaller number of inbound tourists and the accompanying loss of revenue, governments can use public spending to subsidise the tourism sectors if they suffer, and use marketing strategies to promote one-off events during and after the events by improving tourists' negative perceptions of a destination. If visa restrictions are relaxed, then governments must engage in direct actions, such as quickly moving armed forces, managing tourist flows, and extensively adopting technologies of social control, such as security cameras (Jennings & Lodge, 2011) .

",0.6566711002878965
Evaluation of knowledge and behavior of workers in Prince Mohammed International Airport in Western Saudi Arabia regarding public health emergency measures applied during Hajj season 2014 OPEN ACCESS,‫بن‬ ‫محمد‬ ‫األمير‬ ‫مطار‬ ‫في‬ ‫العاملني‬ ‫وسلوك‬ ‫معرفة‬ ‫تقييم‬ ‫األهداف:‬ ‫خالل‬ ‫باملطار‬ ‫املنفذة‬ ‫العامة‬ ‫الصحة‬ ‫وطوارئ‬ ‫آليات‬ ‫عن‬ ‫الدولي‬ ‫عبدالعزيز‬ ‫4102م.‬ ‫العام‬ ‫حج‬ ‫موسم‬ ‫بن‬ ‫محمد‬ ‫األمير‬ ‫مطار‬ ‫في‬ ‫االستطالعية‬ ‫الدراسة‬ ‫هذه‬ ‫جريت‬ ُ ‫أ‬ ‫الطريقة:‬ ‫أثناء‬ ‫وذلك‬ ‫السعودية‬ ‫العربية‬ ‫اململكة‬ ‫املنورة،‬ ‫املدينة‬ ‫الدولي،‬ ‫عبدالعزيز‬ ‫جمع‬ ‫مت‬ ‫لقد‬ ‫4102م.‬ ‫سبتمبر‬ ‫إلى‬ ‫أغسطس‬ ‫من‬ ‫الفترة‬ ‫خالل‬ ‫احلج‬ ‫موسم‬ ‫وقد‬ ‫الشخصية.‬ ‫باملقابالت‬ ‫القيام‬ ‫أثناء‬ ‫استبيان‬ ‫مأل‬ ‫طريق‬ ‫عن‬ ‫البيانات‬ ‫احتكاك‬ ‫لهم‬ ‫أشخاص‬ ‫مقابلة‬ ‫إلى‬ ‫عشوائي‬ ‫الغير‬ ‫العينات‬ ‫اختيار‬ ‫هدف‬ ‫أكثر‬ ‫املسافرين‬ ‫من‬ ‫العدوى‬ ‫انتقال‬ ‫خلطر‬ ‫املعرضني‬ ‫وأولئك‬ ‫باملسافرين،‬ ‫مباشر‬ ‫غيرهم.‬ ‫من‬ ‫الذكور‬ ‫املشاركني‬ ‫نسبة‬ ‫بلغت‬ ‫وقد‬ ،ً ‫شخصا‬ 186 ‫الدراسة‬ ‫شملت‬ ‫النتائج:‬ ‫مختلفة‬ ‫قطاعات‬ 8 ‫في‬ ‫يعملون‬ ‫البحث‬ ‫في‬ ‫املشاركني‬ ‫كان‬ ‫ولقد‬ .92.5% ‫القطاع‬ ‫إلى‬ ‫ينتمون‬ ‫البحث‬ ‫في‬ ‫املشاركني‬ ‫من‬ 26% ‫كان‬ ‫حيث‬ ‫باملطار،‬ ‫أكثر‬ ‫صحية‬ ‫الغير‬ ‫بالقطاعات‬ ‫العاملون‬ ‫كان‬ ‫ولقد‬ ‫باملطار.‬ ‫العامل‬ ‫الصحي‬ ‫باملقارنة‬ ‫وذلك‬ ‫باملطار‬ ‫عملهم‬ ‫أثناء‬ ‫بالعدوى‬ ‫اصابتهم‬ ‫احتمالية‬ ‫من‬ ً ‫قلقا‬ ‫املعدية‬ ‫األمراض‬ ‫أكثر‬ ‫وكانت‬ .)p=0.023( ‫الصحي‬ ‫بالقطاع‬ ‫بالعاملني‬ ‫بلغت‬ ‫حيث‬ ‫الفيروسي‬ ‫اإليبوال‬ ‫مرض‬ ‫احلج‬ ‫موسم‬ ‫أثناء‬ ‫للمخاوف‬ ‫املثيرة‬ ‫في‬ ‫العاملني‬ ‫من‬ 30% ‫بالعدوى‬ ‫إصابتهم‬ ‫من‬ ‫القلقني‬ ‫األشخاص‬ ‫نسبة‬ ‫أبدى‬ ‫ولقد‬ ‫األخرى.‬ ‫القطاعات‬ ‫في‬ ‫العاملني‬ ‫من‬ ‫و%74‬ ‫الصحي،‬ ‫القطاع‬ ‫معرفتهم‬ ‫عدم‬ ‫الصحي‬ ‫بالقطاع‬ ‫العاملني‬ ‫الغير‬ ‫األشخاص‬ ‫من‬ 47% ‫حوالي‬ ‫نسبة‬ ‫وكانت‬ ‫باملطار.‬ ‫العامة‬ ‫الصحة‬ ‫طوارئ‬ ‫حاالت‬ ‫في‬ ‫املتبعة‬ ‫باآلليات‬ ‫القطاعات‬ ‫في‬ ‫العامة‬ ‫الصحة‬ ‫بطوارئ‬ ً ‫متعلقا‬ ً ‫تدريبا‬ ‫تلقوا‬ ‫الذين‬ ‫األشخاص‬ ‫األشخاص‬ ‫نسبة‬ ‫عن‬ ‫اإلحصائية‬ ‫الناحية‬ ‫من‬ ‫واضحة‬ ‫بصورة‬ ‫أقل‬ ‫صحية‬ ‫الغير‬ .)p<0.00001( ‫الصحي‬ ‫القطاع‬ ‫في‬ ‫التدريب‬ ‫ذلك‬ ‫تلقوا‬ ‫الذين‬ ‫العاملني‬ ‫وتدريب‬ ‫تثقيف‬ ‫على‬ ‫التركيز‬ ‫ينبغي‬ ‫بأنه‬ ‫الدراسة‬ ‫أظهرت‬ ‫اخلامتة:‬ ‫كما‬ ‫باملطار.‬ ‫عملهم‬ ‫أثناء‬ ‫احملتملة‬ ‫الصحية‬ ‫املخاطر‬ ‫معرفة‬ ‫على‬ ‫باملطار‬ ‫الصحة‬ ‫بطوارئ‬ ‫العالقة‬ ‫ذات‬ ‫التعليمات‬ ‫لنشر‬ ‫آلية‬ ‫بتفعيل‬ ‫البدء‬ ‫وينبغي‬ ‫املطار.‬ ‫قطاعات‬ ‫كل‬ ‫في‬ ‫العاملني‬ ‫جميع‬ ‫على‬ ‫العامة‬ Objectives: To evaluate the knowledge and behavior of workers at a Saudi airport regarding public health emergency measures applied during Hajj season.,"A irports are frontier gates where proper public health measures are likely to reduce the possibility of allowing the entrance of communicable disease to a country. According to the World Health Organization )WHO( International Health Regulations mandate WHO member states to ensure that every designated point of entry is equipped with staff and instruments enabling smooth movements of the travellers while maintaining appropriate public health measures. 1 Maintaining such regulations is likely to enhance proper travellers' movement during public health emergencies by reducing possible interference between application of preventive public health measures, and the ability of travellers to access the designated points of entry. Additionally, ensuring the application of public health measures might aid in preventing the occurrence of any public health emergencies. A public health emergency is defined as any situation with health consequences that are likely to overwhelm the community's routine capability of addressing them. A health situation can be considered as an emergency if there is a risk due to timing, such as facing emerging diseases threats during Hajj season, scale, as with an overwhelming number of causalities, or due to the unpredictability of the situation. Defining the nature of potential public health emergencies is crucial to allow competent development of preparedness plans. 2 Having a large number of passengers arriving at a particular point of entry during a limited time is a burden on the available health services. A meticulous state of readiness is required to respond to any risk of spreading a communicable disease. The state of readiness is maintained by several steps including preparation of staff, equipment, and buildings. 3, 4 The burden of not maintaining effective public health event response measures is aggravated if an epidemic is announced in a particular region of the globe where travellers form these areas are scheduled to arrive in the country. During the Hajj season of 2014, the Ebola Virus Disease )EVD(, which is a viral hemorrhagic disease, was announced as an epidemic disease in Guinea, Liberia, and Sierra Leone in West Africa. Additionally, a localized spread of the virus was announced in certain areas of Nigeria. 5 The Saudi Arabian government, as a preventive measure, decided to prevent citizens of EVD-affected countries from entering the country. However, nationals of Nigeria were exempt as no extended transmission of EVD was announced. 6, 7 Nonetheless, several procedures were applied to prevent the transmission of Ebola virus among thousands of Nigerian pilgrims arriving in the country. These measures were mainly related to exit screening of travellers in Nigeria, 8 and entry screening at points of entry in Saudi Arabia. Additionally, the Ministry of Health in Saudi Arabia produced response plans for infectious diseases )Middle East Respiratory Syndrome [MERS] and EVD( to be implemented during Hajj. 9 There are many potential sources of infectious disease transmission from a single infected individual. The presence of infected travellers, such as an EVD infection, on an aircraft increases the risk of transmitting infection to neighboring passengers and flight crew. Airport workers, such as ground workers handling cleaning of aircrafts and lavatories, are at risk of the infection, especially with the presence of spilled infectious materials. Airport workers handling the flow of passengers during busy times are at risk of acquiring infection such as immigration, customs, security, and healthcare workers. Therefore, all of these individuals have to be aware of the potential health threats at the airport, should be aware of preventive methods, how to use preventive methods, and know what to do when facing a public health emergency event in the airport. Prince Mohammed Airport )Al-Madinah, Saudi Arabia( workers' level of knowledge of the potential health threat at the airport is currently not known. Additionally, the attitude and practice of the staff when a public health emergency is announced in the airport is not measured. The significance of this study stems from the ability to investigate the degree to which airport workers, at the time of dealing with travellers during Hajj season, were able to deal with potential urgent infectious cases, and to adhere to the relevant protective guidelines. 

The target population of this study was airport workers who were likely to be exposed to the risk of transmitting infection from airport passengers. The population included healthcare workers, customs workers, immigration workers, security officers, ground services workers, and Hajj services providers. Hajj services providers were responsible for the provision of logistic needs to ensure smooth movement of arriving pilgrims and directing the pilgrims to their designated transportation vehicles. Male and female workers were targeted during recruitment. However, as this study was mainly targeting workers who were in direct physical contact with the pilgrims, staff not in direct contact with the pilgrims was excluded. To answer the research questions of this study, purposive, non-random sampling was performed to recruit the targeted sample. Study participants were grouped into health workers and non-health workers to study any significant differences relating to knowledge, and behavior related to public health events. This grouping was mainly conducted due to the observed variation in the proportions of recruited workers from non-health workers.

Data collection. Data were collected using a semistructured questionnaire. The questionnaire was constructed by a focus group including 4 consultants from the community and family medicine. Closedended and open-ended questions were utilized, including questions measuring demographics, level of knowledge, practice and attitude relating to public health preventive measures at points of entry. The questionnaire was constructed in Arabic language. The questionnaire was piloted on 20 individuals )10 males and 10 females( and additional questions and choices were conducted according to the piloting findings. All questionnaires were filled out during personal interviews. All interviews were conducted by 3 trained interviewers. There was no audio recording of conducted interviews. However, notes of the open-ended responses were taken during the interviews and were transcribed by the interviewers.

Ethical approval to conduct the study was provided by the ethical committee of the College of Medicine at King Saud University, Riyadh, Saudi Arabia. Additionally, approval to conduct the study was granted by the directory of Public Health in Al-Madinah, and from the airport administration. This research was conducted in accordance with the Helsinki Declaration of medical research involving human subjects. All questionnaires were anonymous, and the right to refuse to participate in the study was explained to all the individuals approached. Data analysis. Data analysis was performed by IBM SPSS software version 22 )IBM Corp, Armonk, NY, USA(. Frequencies and proportions of the study variables were utilized to assess the level of knowledge of the potential health threats at the airport. Chi-square test was used to test for the presence in any statistically significant variation of the study variables between the 2 groups. A p-value of 0.05 or less was designated as statistically significant for applied statistical tests. As the study used open-ended questions as a tool for data collection, all completed questionnaires were transcribed and reviewed after data collection. The reviewing process produced several responses to each question. Similar responses to the open-ended questions were grouped after data collection and coded accordingly.

Results. One hundred eighty-six participants were recruited in this study. As illustrated in Table 1 , this sample is predominantly composed of male subjects )92.5%(. This is expected as females only represent a minority of the workers at Prince Mohammed airport. Most of the recruited workers had secondary or above degrees. Twenty-six percent of recruited subjects were health professionals, and the remaining were related to different sectors. Although security officers represent an important work force at the airport, they were very hesitant to participate in this study due to security reasons, despite availing relevant permission. Non-health workers were more likely to be concerned on acquiring infection compared with health workers. This notion was mostly apparent when asking the participants regarding concerns related to acquiring EVD infection. However, 95% confidence intervals for difference in proportions of airport workers who were concerned with acquiring infections were wide enough to suggest similar levels of concern among the 2 groups. Almost all airport workers were aware that risk of infection can be reduced by adhering to precautionary measures. The most frequently reported method of infection risk reduction is wearing a facemask followed by wearing gloves. A lower proportion of non-health workers reported vaccinations compared with health workers as a preventive measure. Similarly, the proportion of non-health workers who reported hand washing as a preventive measure was lower compared with health workers. The detected variation in knowledge regarding preventive measures among non-health workers compared with health workers might suggest that non-health workers are generally aware that infectious diseases transmission could be prevented, but were less able to report specific preventive measures compared with health workers.

When asking the study's participants about their reaction when investigating a traveller suspected of having an infectious disease, non-health workers were apparently not aware of any procedures that should be conducted during this scenario ) Table 3 (. Additionally, most of the participants reported that they will inform .0( 0 )0( *For the difference between health workers and non-health workers. P-value <0.05 is considered statistically significant the relevant authority when dealing with a traveller suspected with an infectious disease. However, only a minority of health workers were aware that there is a rapid response team or infection control at the airport. None of the non-health workers were aware of the specific notification method when facing a public health threatening event, and only mentioned reporting to 'the health services' without revealing specific individuals with a specific job title. Table 4 summarizes the sources of knowledge of the study participants of the potential health threats at the airport. Most non-health workers reported not receiving any forms of training )79%(. The proportion of non-health workers who reported not receiving any training was significantly lower compared with health workers. Only a minority of the recruited workers reported having training before the Hajj season. Most of the trained health workers received their training when they were present at the airport during the season )79.5%(. The most frequently reported method of training was attending lectures, which were mostly delivered at the airport.

Discussion. The findings of this study indicate that non-health workers were more likely to be concerned on acquiring infection when dealing with travellers during the Hajj season compared with health workers. This concern was noticed by reviewing responses given when the participants were asked regarding specific protection equipment, or specific guidelines to adhere to when dealing with a traveller suspected of having an infectious disease. This notion was further confirmed, as the lack of knowledge was mostly due to lack of training and education, either before attending the airport during Hajj season or during their presence at the airport.

Although the study respondents were able to indicate important preventive measures such as wearing facemask and gloves, a lower proportion reported hand washing and environmental hygiene as precautionary methods. Additionally, approximately 40% of the respondents suggested isolation of suspected travellers as a method of protecting airport workers and other travellers. However, at the time of conducting this study, there was no isolation facility at the airport. This is the first study conducted to measure the level of knowledge and behavior relating to the public health emergency measures of workers at Prince Mohammed International Airport at Al-Madinah. However, a study conducted by Al-Ghamdi and Kabbash 11 investigated the knowledge of health workers regarding preventive measures against communicable disease at King Abdulaziz Airport in Jeddah. As the population of the study by Al-Ghamdi and Kabbash 11 was limited to health workers, the scope of knowledge of preventive measures investigated was deeper compared with our study. Acknowledging the marked differences of questions used to measure level of knowledge of preventive public health measures in our study compared to the study by Al-Ghadmi and Kabbash, 11 it might be possible to conduct an overall comparison of level of knowledge detected in both studies. When comparing the level of knowledge among the health workers recruited in our study compared with the findings of the study by Al-Ghadmi and Kabbash, 11 it is possible to notice a similar range of variation in number of respondents who were knowledgeable of different preventive measures. The proportion of health workers who were aware of specific prevention measures Table 4 -Reported source of knowledge on the potential health threats at the airport and protection methods of workers at Al-Madinah airport, Al-Madinah, Saudi Arabia during Hajj season 2014.

Non-health workers (n=137) CI -confidence intervals in our study varied between 32-97%, which is similar to the findings of Al-Ghadmi and Kabbash 11 where the proportion of health workers who were able to give correct answers regarding preventive measures ranged between 32-95%. In addition to the similar variation in level of knowledge of preventive measures, there is a similar pattern pertaining to the training given for the health workers relating to preventive health measures. In the study by Al-Ghadmi and Kabbash, 11 the proportion of individuals who received training in infectious diseases or preventive measures at points of entry varied between 8-19% of the study sample. When the training was received was not reported, however, the low proportion of health workers who received training in this study is similar to the low proportion of individuals who received training before the season, which strengthens the evidence of a lack of proper organizational efforts to educate airport workers of the risk of infectious diseases and the methods of protection.

Apart from the study by Al-Ghamdi and Kabbash, 11 there are no similar studies conducted in other countries. Most of the studies that were conducted in airport settings were mainly targeting travellers rather than airport workers. These studies were able to measure knowledge level of passengers who are travelling to areas endemic with particular infectious diseases such as malaria, 12 hepatitis, 13 and H1N1 influenza. 14 This study was able to detect important and new aspects where several conclusions could be drawn. Our study was able to detect multiple systematic flaws indicating organizational difficulties rather than individual incompetence. These difficulties can be summarized in few points. Firstly, there was a significant lack of a competent system to deliver health-related knowledge in a timely manner, especially in important events such as Hajj seasons. Secondly, since a substantial proportion of airport workers hired during Hajj seasons are recruited based on temporary contracts, those workers are very likely to be unaware of the airport facilities and health services providers. This notion was apparent when 40% of the respondents suggested the isolation of suspected travellers as a preventive measure despite the unavailability of this facility at the Prince Mohammed Airport. Thirdly, improper preparedness for a large event such as the Hajj season is evident from the lack of training before the season, as only 21% of the respondents revealed receiving any form of training before the Hajj season. Having detected several systematic flaws mandates, the need for future research concerning identification of barriers hindering proper provision of preventive public health services at the airport. Additionally, this investigation should be replicated in other points of entry, such as Prince Abdul-Aziz International Airport, and the Islamic Seaport in Jeddah, and other land crossings where pilgrims enter the country during Hajj season. Further investigations should include all workers who are in direct exposure to arriving pilgrims, and not limited to health workers only.

Several practical implications could be suggested based on the findings of this study. A preparedness plan concerning public health preventive measures should be in place to address any public health concerns at the airport. This plan should be operational throughout the year and during Hajj seasons. This plan should include systematic and practical methods of updating the airport workers with any public health concerns at the airport. A specific channel of knowledge sharing between the airport authority and airport workers should be initiated to enhance knowledge of the airport workers of the current public health concerns, and to provide theoretical and practical training pertaining to health protection at airport settings. Additionally, airport workers who are hired based on temporary contracts during Hajj season should be educated before joining the airport work force, and informed of the current situation of the airport health facilities and equipment needed for health protection.

The first step towards ensuring a state of readiness to handle any public health event during Hajj seasons at the point of entry is to ensure the provision of an appropriate number of trained staff. Insufficient numbers of staff will lead to an inability to apply preventive measures to thousands of pilgrims arriving at a single point in a limited time. Secondly, members of staff have to be specifically trained according to the current potential health threats at the airport. Thirdly, there has to be provision of preventive equipment, such as gloves, masks, thermal cameras, protective gowns, hand sanitizers, and ambulances to ensure the ability to apply preventive measures. Finally, necessary space is needed for the storing of relevant equipment, provision of preventive services, and isolation of suspected travellers, along with the provision of urgent curative services. 15 This study has several strengths and limitation issues. The strength of this study was mostly related to the data collection method, as all questionnaires were completed during interviews. Additionally, using open-ended questions allowed for a proper test of knowledge and behavior. Conducting this study during the Hajj season is another strong point as information was collected when subjects were physically present at the point of entry. The limitations of this study were related to the small sample size and the refusal of security personnel to participate. Additionally, there might have been a possibility of measurement bias as respondents could have been hesitant to share critical views pertaining to provision of personal protective equipments )PPEs( and public health education. However, proper identification of the investigators, illustration of the study aims, ensuring anonymity, and taking notes as an alternative for audio recording might have reduced this measurement bias. Nevertheless, fear of being reported as complaining on the provided preventive health services was mostly apparent when approaching security personnel, which might have lead to a potential selection bias.

In conclusion, the level of knowledge of communicable disease and preventive methods among non-health workers is significantly low compared with health workers. The observed lower knowledge of non-health workers is mainly due to lack of appropriate knowledge sharing channels. Knowledge and training delivery channels should be established at the airport to ensure proper knowledge sharing and enhance preparedness levels. Enhancing the preparedness level during Hajj season is critical due to the large number of arrivals from all over the globe where several countries are endemic with known infectious diseases.

",0.654611979402032
Ethnoveterinary knowledge of farmers in bilingual regions of Switzerland - is there potential to extend veterinary options to reduce antimicrobial use?,"Ethnopharmacological relevance: In the pre-antibiotic era, a broad spectrum of medicinal plants was used to treat livestock. This knowledge was neglected in European veterinary medicine for decades but kept alive by farmers. Emergence of multidrug resistant bacterial strains requires a severely restricted use of antibiotics in veterinary medicine. We conducted a survey on the ethnoveterinary knowledge of farmers in the bilingual (French and German speaking) Western region of Switzerland, namely the cantons of Fribourg, Neuchâtel and Jura, and in the French speaking part of the canton of Bern. Aim of the study: To find out whether differences exist in plants used by farmers in French speaking and bilingual regions of Switzerland as compared to our earlier studies conducted in Switzerland. Additional focus was on plants that are used in diseases which commonly are treated with antimicrobials, on plants used in skin afflictions, and on plants used in animal species such as horses, for which the range of veterinary medicinal products is limited. Material and methods: We conducted in 2015 semistructured interviews with 62 dialog partners, mainly cattle keeping farmers but also 18 horse keeping farmers. Of these, 41 were native French (FNS) and 21 native German speakers (GNS). Detailed information about homemade herbal remedies (plant species, plant part, manufacturing process) and the corresponding use reports (target animal species, category of use, route of administration, dosage, source of knowledge, frequency of use, last time of use and farmers satisfaction) were collected. Results: A total of 345 homemade remedies were reported, of which 240 contained only one plant species (Homemade Single Species Herbal Remedy Reports; HSHR). A total of 289 use reports (UR) were mentioned for the 240 HSHR, and they comprised 77 plant species belonging to 41 botanical families. Of these, 35 plant species were solely reported from FNS, 20 from GNS, and 22 from both. Taking into account earlier ethnoveterinary studies conducted in Switzerland only 10 (FNS) and 6 (GNS) plant species connected with 7% of FNS and GNS UR respectively were ""unique"" to the respective language group.","Ethnoveterinary research is defined as ""systematic investigation and application of folk veterinary knowledge, theory and practice"" (McCorkle, 1986) . Therefore, it plays a major role in the conservation of traditional knowledge on medicinal plants, and in the exploration of alternative treatment options in veterinary medicine (Lans et al., 2007; Schäffer, 2010; Lans, 2016; Bullitta et al., 2018) . European ethnoveterinary research has been performed mainly in Mediterranean countries (Blanco et al., 1999; Uncini Manganelli et al., 2001; Viegi et al., 2003; Scherrer et al., 2005; Pieroni et al., 2004 Pieroni et al., , 2006 Bonet and Valles, 2007; Akerreta et al., 2010; Benitez et al., 2012; Mayer et al., 2014; Bullitta et al., 2018) , in the German speaking parts of the European Alps (Schmid et al., 2012; Disler et al., 2014; Bischoff et al., 2016; Vogl et al., 2016; Mayer et al., 2017; Stucki et al., 2019) , and some in eastern European countries (Mayer et al., 2014) , notably Romania (Bartha et al., 2015) . However, data from French speaking regions, such as western Switzerland, are lacking.

Uses of medicinal plants by farmers could differ not only by geographical territories but also between linguistic groups in a confined area. Such differences have been reported in human ethnomedicine (Menendez-Baceta et al., 2015) , but similar comparisons with regard to veterinary use do not exist. Even if (a) earlier studies, comparing ethnoveterinary data from different countries (Viegi and Ghedira, 2014; Bartha et al., 2015; Mayer et al., 2017) suggest that ""ethnoveterinary tradition may change with growing geographical distance"" (Mayer et al., 2017) and, (b) a within-country comparison of two geographically and linguistically separate regions in Switzerland (Mayer et al., 2017) , and a comparison of two populations within the same region of Romania (but without differentiating between different linguistic groups; Bartha et al., 2015) highlighted some differences in ethnoveterinary practises, a comparison of ethnoveterinary data of two linguistic populations within one region is still lacking.

In 2014 the WHO report on antimicrobial resistances (WHO, 2014) disclosed that resistance to antibiotics has meanwhile been detected in all parts of the world. There is evidence that farm animals act as a reservoir of resistance genes, and that transmission of such genes to humans could occur either via direct contact or via food consumption (WHO, 2004; Marshall and Levy, 2011) . In addition, pathogenic and non-pathogenic bacteria carrying antimicrobial resistance gene are spread into the environment via the excrements of farm animals (Woolhouse et al., 2015) .

Nonetheless, antimicrobials are still widely used to prevent or treat diseases in farm animals, but also for non-therapeutic purposes like growth promotion. On a global scale it is estimated that antimicrobials used in livestock production will increase by 67% from approximately 63′000 tons per year in 2010 to 106′000 tons by 2030 (Van Boeckel et al., 2015) . The major classes of antimicrobials used for humans are also used in livestock, including reserve antibiotics (Aarestrup et al., 2008) . In Europe the use of antimicrobials is highest with intensive livestock species such as pigs, poultry and young cattle (European Medicines Agency, 2014; Federal Food Savety and Veterinary Office, 2019), mainly for gastrointestinal and respiratory diseases (Bennett et al., 1999) . In adult cattle mastitis is one of the most prevalent diseases (Bradley, 2002) , and is oftentimes treated with antibiotics.

In Switzerland, the Federal Food Safety and Veterinary Office (FSVO) reported the increase of microbiological resistance in zoonotic pathogens and in indicator bacteria (FSVO, 2019) . A National Strategy on Antibiotic Resistance (StAR) has been developed to control and combat the development of antimicrobial resistance (StAR, 2015) . However, even though the use of medicinal plants in prevention or first line treatment of mild cases (Ayrle et al., 2016a) could possibly lower the need for antibiotic treatments of farm animals, it is not mentioned in StAR.

For indications such as topical treatment of skin afflictions, the number of available veterinary medicinal products in Switzerland is small. Only 18 veterinary drugs are registered for the treatment of dermatological disorders in livestock. The majority are either disinfecting agents or topical antibiotics (CPT, 2019). For some animal species the available veterinary medicinal products are even more limited. Over 100 antimicrobial drugs for systemic use in cattle are commercialized, but only 9 for use in horses (CPT, 2019).

There is a need to widen the spectrum of veterinary medicinal products and other therapeutic and preventive options for specific veterinary indications, and to reduce the use of antibiotics. However, even though the interest of veterinarians in herbal medicine has been increasing over the past decade (Kupper et al., 2018) , there is still a lack of clinical veterinary research with medicinal plants (Ayrle et al., 2016a) . In vitro and in vivo studies (Ayrle et al., 2016a) , historical veterinary literature (Stucki et al., 2019) and ethnoveterinary surveys can serve as knowledge base for the veterinary use of medicinal plants.

With a survey conducted in the bilingual regions of the Cantons Fribourg, Jura, Neuchâtel, and the French speaking part of Bern (administration of Bernese Jura) we wanted.

(a) to publish the first ethnoveterinary data from French speaking areas of Europe, (b) to compare the ethnoveterinary knowledge of native French and German speakers in the survey area, and (c) to evaluate whether medicinal plants discussed in this survey could be an option to enlarge the therapeutic spectrum with a particular emphasis on antimicrobial therapies, skin disorders, and horses.

The study was conducted according to previous ethnoveterinary studies from Switzerland (Schmid et al., 2012; Disler et al., 2014; Bischoff et al., 2016; Mayer et al., 2017; Stucki et al., 2019) .

The survey was conducted in 3 western Swiss cantons, namely Fribourg, Neuchâtel, Jura, and in the French-speaking part of the canton of Bern, namely the administration of Bernese Jura (Fig. 1) . The total study area is located between 6°3′ and 7°3′ E and 46°3′ and 47°3 N. The cantons Jura and Neuchâtel are bordered by France to the west, and the regions of previous Swiss ethnoveterinary studies (PSES, Fig. 1 ) to the east. The study covered an area of 3853.7 km 2 and a population of approx. 600′000 persons. The majority of the population in the study area are native French speaking and, depending on the canton, up to one quarter are native German speaking persons (Bundesamt für Statistik, 2019). The altitude is between 373 and 2386m above sea level. Annual precipitation of the area varies from 947 mm to 1441 mm, and the average temperature at 647 m above sea level is 4.6°C (Bundesamt für Meteorologie und Klimatologie, 2019). The area had a total of 5431 farms, of which 4365 kept cattle and 1361 kept horses (Bundesamt für Statistik, 2013).

Dialog partners (DP) were recruited according to a previously described approach (Disler et al., 2014) . In a first step, the Departments of Agriculture of the cantons Fribourg, Jura and Neuchâtel supported the project by informing all farmers via bulletin or newsletters. Second, all organic farmers in the study area were informed via personal letter or email. Additionally, farmers were informed through publications in the regional agricultural press. All farmers of the research area who were known to be member of complementary medicine working groups, or member of the farm research network of the Research Institute of Organic Agriculture (FiBL) were contacted by telephone. Furthermore, the project was presented at two general assemblies of regional associations of organic farmers (Bio Jura, Bio Fribourg, Fig. 2) .

A total of 62 dialog partners (DP) participated in the study: 26 DP (42%) in Fribourg, 15 (24%) in Jura, 11 (18%) in Bernese Jura, and 10 (16%) in Neuchâtel (Fig. 2 ). Thirty-one farmers spontaneously agreed (contacting the research team spontaneously based on the overall information via bulletin, newsletter, or individual letter or confirmed their participation during the telephone calls or personal meetings) to become a DP, while further 31 DP were recruited via snowball sampling (Bernhard, 2006; Disler et al., 2014, Fig. 2) . DP who had spontaneously agreed to participate were asked whether they knew of other farmers using medicinal plants for their livestock. Additional DP were recruited with the help of farmers or farm advisors who by themselves were not using ethnoveterinary knowledge. For these intermediary persons the term ""informant"" was used (e.g. informant 1 established the contact to DP23; Fig. 2 ).

Thirteen DP were recruited from the complementary medicine working groups, 12 from the organic farm research network of the FiBL, and further 37 farmers were recruited via regional information (Fig. 2) . The mother tongue of 41 DP was French (French Native Speakers, FNS), and German for the remaining 21 DP (German Native Speakers, GNS).

Each of the 62 DP was associated to a single farm, of which 25 (40%) were organic and 37 (60%) were non-organic. Forty-four farms kept cattle, 22 farms hen, 18 farms horses, 15 farms goats, 15 sheep, 6 rabbits, 5 pigs, 3 kept donkeys, 3 kept bees, and 1 kept deer. The farms were located between 430 and 1250 m (808m ± 194m) above sea level.

Thirty-two interviews were carried out with the DP only. In 27 cases, one or two family members, and in further 3 cases neighbours or farm staff were assisting the DP. All supplementary information from assisting persons were matched to the respective DP. In total, 93 persons aged between 28 and 87 years (54 ± 14) were interviewed. Of these 59 were men (63%) and 34 (37%) were women.

Open, semi-structured interviews with DP were conducted from mid-February to end of April 2015 by the first author (DM) who is a native French speaker with an excellent command of German. The interviews were conducted in the mother tongue of the DP by using questionnaires in either French or German. In some interviews, language was occasionally switched if it helped to clarify specific points between DP and the interviewer. All information from one interview was assigned to the mother tongue of the DP.

The interviews took between 0.5 and 4.0 h and were recorded (WS-818 Digital Voice Recorder, Olympus, Hamburg, Germany) after a written consent of the DP.

With the aid of their local vernacular names, plants were identified during the interviews together with the DP, utilizing the Flora Helvetica (Lauber and Wagner, 2007) , by cross-checking (a) illustrations and, (b) distribution areas of the plant species. If plant material was available, further cross-checking was done based on information and material collected in earlier studies (Disler et al., 2014; Mayer et al., 2017; Stucki et al., 2019) . Commercial products and herbal drugs or extracts from commercial sources were identified with the aid of the package leaflet, or were considered as correctly delivered by the pharmacy or drug store. In the summer of 2015 nine dialog partners were revisited. A total of 27 herbarium voucher specimen of 16 plant species harvested from the wild were collected together with the dialog partner, dried, labelled, and deposited at the herbarium of the ""Basler Botanische Gesellschaft"". All plants for which voucher specimens were available from this and from former Swiss studies (Disler et al., 2014; Mayer et al., 2017; Stucki et al., 2019) are listed in Additional File 1.

The same definitions as in earlier studies were used (Disler et al., 2014; Bischoff et al., 2016; Mayer et al., 2017; Stucki et al., 2019) .

Homemade remedy report ( The specification of use followed mainly the Anatomical Therapeutic Chemical classification system for veterinary medicinal products (ATCvet Code, WHO, 2018).

To estimate the dosing of plants, the weight of the amount of plant material used for a remedy was determined with a precision scale (EMB, 2000-2, Kern, Balingen, Germany). Whenever possible weight was determined using the original plant material of the farmer, or reference drugs of the interviewer on site. If this was not possible, the dosage was estimated by assessing the volume of plant material and measuring it subsequently by the interviewer. For orally administered remedies the oral daily dosage of medicinal plant (dry plant equivalent, dpe), and for externally administered remedies the concentration in g dpe per 100 g of final product was determined. Oral daily dosages were calculated in g dpe per kg metabolic bodyweight (kg 0.75 ) (Löscher et al., 2014) . The metabolic bodyweight was calculated with the average live weight of animal species and age classes described by Disler et al. (2014) , or based on additional information (Sambraus, 1996, 

The degree of satisfaction of the DP with the reported use was determined using a visual analog scale of 100 mm, ranging from ""no effect"" (0 mm) to ""very good effect"" (100 mm) (Zealley, and Aitken, 1969) .

A total of 345 homemade remedy reports were recorded during the 62 interviews. Between 1 and 13 remedies (6 ± 3) per DP were reported, and they were associated with one to four different UR (1.2 ± 0.5). A total of 121 plants species from 41 botanical families were reported.

A total of 240 (70%) homemade remedies contained a single plant species (homemade single species herbal remedy reports, HSHR), and 31 (9%) homemade remedy reports were for mixtures with two to twenty-seven plant species. The remaining 74 (21%) were homemade remedy reports without use of plants, but contained natural products such as vinegar, wine, clay, honey, propolis or ashes.

The 240 HSHR referred to 77 different plant species belonging to 41 botanical families. Species of the family Asteraceae were most frequently used (34 HSHR, 14%), followed by Linaceae (28 HSHR, 12%), Rubiaceae (20 HSHR, 8%), Apiaceae (19 HSHR, 8%), Theaceae (15 HSHR, 6%), and Berberidaceae (13 HSHR, 5%; Table 2 ).

The most often reported plant species was Linum usitatissimum L. (28 HSHR; 12%), followed by Coffea L. (20 HSHR; 8%), Matricaria chamomilla L. (16 HSHR; 7%), Camellia sinensis (L.) Kuntze. (15 HSHR; 6%), and Sanicula europaea L. (13 HSHR; 5%). Eighteen plant species were reported for the first time for an ethnoveterinary use in Switzerland. Of these, ten were mentioned solely by FNS (Citrus x aurantium L., Fagopyrum esculentum Moench, Lavendula x intermedia Emeric ex Loisel, Matricaria discoidea DC., Polygonum aviculare L., Raphanus raphanistrum subsp. sativus (L.) Domin, Rumex conglomeratus Murray, Sinapis alba L., Sorbus aucuparia L., Zingiber officinale Roscoe), and six solely by GNS (Atropa belladonna L., Cucumis sativus L., Hordeum vulgare L., Mentha longifolia (L.) L., Triticum aestivum L., Veratrum album L.). Additional two species (Oryza sativa L., Phyllitis scolopendrium (L.) Newman) were reported by both FNS and GNS.

The most frequently used plant parts were herbs (61 HSHR, 25%) and seeds (59 HSHR, 24%), followed by leaves (45 HSHR, 19%), flowers (35 HSHR, 15%), bark (12 HSHR, 5%), roots (12 HSHR, 5%), and fruits (7 HSHR, 3%). The remaining 9 HSHR (4%) included bulbs, resins, or unknown plant parts.

For 112 HSHR (47%) the plant material was purchased, for 110 HSHR (46%) gathered from the wild, and for 18 HSHR (8%) cultivated on farm. Dried plant material was used in 154 HSHR (64%), fresh plants were used in 71 HSHR (30%), while for 15 HSHR (6%) no information was available.

In more than half of the HSRH (142, 60%; Table 2 ), the remedies were prepared using an extraction process on farm. In 86 HSRH (35%) the plants were directly administered without prior extraction. This was mainly for oral or topical administration. Twelve HSHR (5%) comprised commercial products, out of which seven were essentials oils (Table 2 , Additional file 1).

For 125 HSHR an aqueous extraction was reported, either as infusion (92 HSRH), decoction (27 HSRH), or extraction at room temperature (6 HSRH). In 7 HSRH alcohol was used for extraction, and oil or fat in 10 HSRH. Sugar was used in one case to prepare a syrup from Picea abies (L.) H. Karst.

The weight of plant used to prepare the remedies could be determined for 173 HSHR (71%). In 78 cases the amount was directly measured on farm (in 26 cases with the original herbal drug, and in 52 cases with the reference drug of the interviewer). In 95 cases an approximate volume of plant was assessed, and the weight was determined afterwards by the interviewer. For 67 HSHR (29%) it was not possible to determine the amount of plant material used.

A total of 289 use reports (UR) were collected for the 240 HSHR. Most of them were for cattle (219 UR, 76%), followed by 39 UR for horses (13%), and 31 UR (11%) for other animals, such as sheep, goats, hens and rabbits (Table 3 , Additional file 1). The majority of the UR were for treatment (225; 78%), while 64 UR (22%) were for prophylactic use, mainly for prevention of cattle ringworm (Berberis vulgaris L.), or in the peripartum period (Linum usitatissimum L., Achillea millefolium L.).

Gastrointestinal disorders and metabolic dysfunctions (QA; 111 UR, 38%) were the most often mentioned indication, followed by skin alterations and sores (QD; 97 UR, 34%). Other indications included infertility and diseases of female genitals (QG; 20 UR, 7%), diseases of the respiratory tract (QR; 13 UR, 5%), mastitis (QG52; 6 UR, 2%), musculoskeletal system (QM; 6 UR, 2%), and others, such as general strengthening (36 UR, 12%).

The most frequently reported plants for the treatment of gastrointestinal disorders and metabolic dysfunctions were Linum usitatissimum L. (Table 3) .

Oral administration (168 UR, 58%) was most frequently used (Table 3) , mainly for the treatment of gastrointestinal disorders like diarrhoeas, colic, constipation or rumination problems. The plants were mainly administered as infusion or decoction. Topical administration was described for 90 UR (31%), mainly for the treatment of altered and sore skin (71 UR), for disinfection of open wounds, or for the promotion of wound healing. Farmers used these HSRH as washes, compresses, or by direct application onto the skin of ointments, oils, tinctures, or fresh plants. Administration on intact skin (19 UR) was mainly described for the treatment of swellings, inflammations of joints, or other subcutaneous afflictions. Treatment of housing Table 1 Metabolic bodyweight (according to Disler et al., 2014, and Sambraus, 1996 environment was mentioned in 28 UR (10%). To prevent or treat cattle ringworm twigs of Berberis vulgaris L, Ilex aquifolium L., or Prunus spinosa L., hanged up in the stable. Finally, two UR were for intravaginal/intrauterine administration, and one UR with essential oil of lavender (Lavendula angustifolia Mill.) was used as an inhalant for calming nervous horses.

FNS reported a total of 57 plant species connected to 180 UR, and GNS described 42 plant species and 109 UR. Thirty-five plant species were solely reported from FNS, and 20 solely from GNS, and these species were associated with one quarter of the respective UR each. Twenty-two plant species were reported by both FNS and GNS, and (15) 15 Herba (1) 1 Calendula officinalis L. (10) Flos (8) 1 g 1 2 4 Herba (2) 1 1 Achillea millefolium L. (3) Herba (3) 1 2 Other Asteraceae (5) (continued on next page) D. Mertenat, et al. Journal of Ethnopharmacology 246 (2020) 112184 were linked to three quarters of the respective UR. The degree of satisfaction of farmers with the treatments outcomes were evaluated for 247 UR, with an average value of 74 mm and no differences between the language groups (Fig. 3) .

With a survey covering bilingual regions of Switzerland we here published the first ethnoveterinary data from French speaking areas of Europe. A total of 62 DP were interviewed, and 289 UR based on 240 HSHR were documented, comprising 77 different plant species from 41 botanical families. Compared to previous ethnoveterinary surveys (PSES; Schmid et al., 2012; Disler et al., 2014; Bischoff et al., 2016; Mayer et al., 2017; Stucki et al., 2019) , 18 plant species were for the first time described for an ethnoveterinary use in Switzerland.

The methodology of the study was in accord with PSES. Most of the methodological recommendations for ethnopharmacological field studies were adhered to (Heinrich et al., 2018; Weckerle et al., 2018) . The short duration of the survey (3 months in early spring) was due to the heavy workload of DP during the rest of the year (Stucki et al., 2019) . Given that the survey had to be conducted outside of the vegetation period it was not possible to collect voucher specimen at the moment of the interviews. However, we collected some voucher specimen on some of the farms during the following summer (Additional file 1).

The cantons of Jura and Neuchâtel are officially French speaking, while Fribourg and Berne are officially bilingual (German and French). However, we considered only the French speaking Bernese Jura in our survey. Nevertheless, 41 DP were FNS, and 21 GNS. With one third of all DP, native German speakers appeared overrepresented if one considers the language distribution of the entire population of the study area (Bundesamt für Statistik, 2019). This may be due to the fact that German speaking Mennonites immigrating in the 18 th century to the Jura region (cantons of Berne and Jura) were predominantly farmers (Gerber, 1969) .

In general, farmers were satisfied with their treatment outcomes, and the mean value of 74 mm on the analog scale is comparable with values of PSES (70-80 mm).

We conducted the first comparative ethnoveterinary study with two populations of different mother tongues within the same geographic region of Europe. An earlier within-country comparison of two different linguistic regions of Switzerland (German and Italian) was characterized by topographical differences (lowland regions vs. southern mountain area; Mayer et al., 2017) , while a study in Romania compared ethnoveterinary data from a Hungarian speaking minority in a region of Transilvania with data from Transsilvania that had been gathered earlier, but without differentiation of the mother tongue of DP (Bartha et al., 2015) . To our knowledge, the only other study comparing traditional uses of two linguistic groups within the same geographic area was an ethnomedicinal survey conducted recently in north-eastern Spain (Menendec-Bacetta et al., 2015). (continued on next page) D. Mertenat, et al. Journal of Ethnopharmacology 246 (2020) 112184 When comparing the data from the present study with findings from PSES (Fig. 4, Additional file 2) , no difference between GNS (19% of the reported plant species are new for ethnoveterinary use in Switzerland) and FNS (22% of the reported plant species are new for ethnoveterinary use in Switzerland) was found. Furthermore, only 8% (FNS) and 10% (GNS) of the respective UR were connected to the 18 newly reported plant species, whereas three quarters of plant species connected to nine tenths of the UR were in accord with PSES, without a difference between FNS and GNS. Here were also no differences between FNS and GNS regarding the degree of satisfaction with their UR (Fig. 3) .

On first sight our findings are in contrast to Menendez-Bacetta et al. (2015) who found clear differences between Basque and Spanish speaking populations within the same geographic area. However, these differences were associated with political and cultural separatism. If separatist tendencies are missing as in our study area (Fig. 2) , different mother tongues of DP alone did not lead to significant differences in ethnoveterinary traditions.

There is increased interest of veterinarians in herbal medicine (Walkenhorst, 2017; Stucki et al., 2019) , and an undeniable need for possible therapeutic alternatives, in particular to reduce the use of antibiotics. Ethnoveterinary research may be an important source of knowledge on promising plant species and on dosage of administered remedies. As in earlier studies, we determined the quantity of administered plant material and compared with the dosage documented in PSES or in literature (Reichling et al., 2016; Wichtl, 2009; ESCOP, 2003; ESCOP, 2009, Tables 4 and 5) . We calculated oral daily doses for seven medicinal plants, including the first ethnoveterinary based dosage for Gentiana lutea L. (median 0.26g/kg 0.75 ). Our calculations were n.a. information not available. (1) (Melanthiaceae), Zingiber officinale Roscoe (1) (Zingiberaceae). g Parasites, general strengthening, behaviour, sensory organs, varia. h Sheep, goats, hens, rabbits. i Calendula ointment Sanicare (online pharmacy), Bombastus-Werke AG used in 1 remedy for 1 use report. j Arnica tincture (pharmacy) used in 1 remedy for 1 use report. k Comfrey gel (drugstore) used in 1 remedy for 1 use report. l Lavendula angustifolia oil (internet: aroma-zone.com) used in 2 remedies for 2 use report. m Lavendula x hybrid oil (internet: aroma-zone.com) with alcohol mixed, used in 1 remedy for 1 use report. n NJP Liniment (R) Swissgenetics (containing Mentha arvenses L. var. piperascens) used in 1 use report. o Plant part: twigs. p Plant Part: buds in two remedies, twigs in 3 remedies. q Plant part: buds.

based on information from at least three UR (Table 3 ). The medians of the determined dosages per kg metabolic body weight (kg 0.75 ) were similar to PSES, and to human and veterinary literature. However, as in PSES minimum and maximum values often differed by one to two orders of magnitude. Despite such big dosage ranges, placebo controlled studies in animals showed positive outcomes in the treatment groups, as was the case in studies with coneflower and garlic (Ayrle et al., 2016b (Ayrle et al., , 2017 . We calculated concentrations of five medicinal plants in finished products for external use, and the first dosage for Linum usitatissimum L. (median 6.25g/100g finished product). Again, the medians were similar to PSES, albeit slightly lower than recommended in human and veterinary literature (Table 5) .

Corresponding with the outcomes of PSES and former European ethnoveterinary data (Mayer et al., 2014) the categories QA and QD were the most often mentioned indications with 72% of all UR.

In Europe, antimicrobials are most widely used to treat gastrointestinal and respiratory diseases of pigs, poultry, and for fattening cattle, in particular calves (European Medicines Agency, 2014; Schnyder et al., 2019) . Mastitis and endometritis are major problems in dairy cattle (Cabaret, 2003; Hovi et al., 2003) , and the most important reasons for the use of antibiotics in these animals (Oliver et al., 2011) . Consequently, the UR related to the QA, QR and QG (and in particular QG52; WHO, 2018) categories are of particular interest for possibly lowering the use of antibiotics. However, only a few UR were related to QG (26 UR) and QR (13 UR), and no single plant species stood out in terms of importance (Table 3) . For the treatment of gastrointestinal disorders (QA) black tea, coffee, chamomile, linseed and oak bark were the most frequently used herbal drugs.

Farmers used infusions of black tea mainly to treat diarrhoea in calves. Black tea reportedly showed activity against bovine coronavirus and rotavirus (Friedman, 2007) which are among the main causes for diarrhoea and gastroenteritis in calves and cattle. Besides, antidiarrheal properties of black tea has been demonstrated in rodents (Besra et al., 2003; Hiller and Melzig, 2010) . The additional CNS stimulant properties of black tea (Einother and Martens, 2013) might be beneficial for lethargic calves. The antibacterial, antiadhesive, antidiarrheal and spasmolytic properties of black tea may be beneficial in gastrointestinal diseases. (Coffea L.,Coffeae semen) . As in PSES coffee was widely used, mainly to treat gastrointestinal disorders (QA) like constipation, colic, or rumination problems. Purine alkaloids reduce mental and physical fatigue, and chlorogenic acid stimulates gastric secretion (Hiller and Melzig, 2010) . There is some previous evidence that coffee may be helpful to treat and prevent polyfactorial infectious diseases in calves (Ponepal et al., 1996) .

Preparations with chamomile were used as an infusion mainly to treat diarrhoea in calves. Chamomile flavonoids showed spasmolytic effects on bovine intestinal smooth muscles (Mendel et al., 2016) . Chamomile also exhibited in vitro antibacterial properties against pathogens relevant in gastrointestinal disorders of youngstock (Ayrle et al., 2016a) . In vivo studies also show protective effects in gastric ulcera in mouse and rats (ESCOP, 2003; Cemek et al., 2010) , and antidiarrheal properties in rats (Sebai et al., 2014). 4.2.1.4. Linseeds (Linum usitatissimum L., Lini semen) . A decoction of whole linseeds was orally administered to treat diarrhoea in calves. Farmers also used linseed to treat constipation and ruminating troubles, and a poultice was administered in ruminitis. Anti-inflammatory, antidiarrheal, and spasmolytic properties of linseeds are well known (ESCOP, 2003; Ayrle et al., 2016a) , and linked to the content in mucilaginous polysaccharides. Linseeds are recommended to treat gastric inflammation and obstipation (ESCOP, 2003; Reichling et al., 2016). 4.2.1.5. Oak bark (Quercus robur L., Quercus cortex) . Farmers treated diarrhoea in calves with infusions or decoctions of oak bark, or fed oak bark powder directly. The high content in tannins is responsible for the antidiarrheal properties (Reichling et al., 2008) . Oak bark extracts furthermore possess antibacterial and anti-quorum sensing activities (Deryabin and Tolmacheva, 2015) .

Marigold (Calendula officinalis L.), St. John's Wort (Hypericum perforatum L.) and wood sanicle (Sanicula europaea L.) were the most frequently used plants for the treatment of skin disorders. While the therapeutic potential of marigold and St. John's Wort in veterinary medicine is recognized (Tresch et al., 2019) , wood sanicle is poorly studied and merits further investigation.

Wood sanicle (Sanicula europaea L., Saniculae folium et flos).

Wood sanicle was already reported in PSES, but only with few UR. However, in an earlier ethnobotanical study on human use of medicinal plants in the canton of Jura, wood sanicle was one of the most often reported species. Use of wood sanicle thus appears to be a distinct particularity of the study area (Broquet, 2006) . In the D. Mertenat, et al. Journal of Ethnopharmacology 246 (2020) 112184 present survey, farmers used infusions of leaves or flowers to treat wounds or scars. Washing the injured skin or a direct application on the skin with a bandage were mentioned. Wood sanicle contains saponins as major phytochemicals, and is has also been used in human folk medicine (Hiller and Melzig, 2010) . Anti-inflammatory activity of wood sanicle has been reported (Jacker and Hiller, 1976; Vogl et al., 2013) .

Compared to PSES, UR related to horses as target species were much higher in the present survey area (39 UR, 13%). This was likely due to the importance of horse breeding in this region (about 1400 farms kept horses). Apart from a study from the canton of Berne (Ryhner et al., 2018) referring to 27 plant species, recent European ethnoveterinary data in horses are scarce (Mayer et al., 2014) .

In category QA, Coffea L. and Linum usitatissimum L. were documented for the same indication as for cattle. In category QD, farmers used Sanicula europaea L. (7 UR), Calendula officinalis L., and Hypericum perforatum L.. In category QM, Arnica montana L., Harpagophytum procumbens (Burch.) DC ex Meisn., Equisetum arvense L., and Zingiber officinale Roscoe were used to treat sprains and degenerative joint diseases. In category QR, Phyllitis scolopéndrium (L.) Newman was used to treat cough, whereby the dried plant was directly administered orally. In vitro antimicrobial activity of Phyllitis scolopéndrium (L.) Newman against pathogenic bacteria has been reported (Ferrazzano et al., 2013) . Additional plants species mentioned in category QR were Abies alba Mill. and Citrus limon (L.) Osbeck.

Lavandula angustifolia Mill. was administered to calm nervous horses. Interestingly, this traditional use was recently corroborated in a clinical study with foals (Poutaraud et al., 2018) .

The plants most frequently reported by farmers in our current survey are inexpensive, easily available, and well known for human and veterinary medicinal purpose (Leonti and Verpoorte, 2017; Stucki et al., 2019) . The phytochemistry, pharmacological properties, and therapeutic effectiveness is documented with numerous research publications and reviews (Ayrle et al., 2016a; Tresch et al., 2019) . The use of complementary medicine and homemade remedies by farmers may help to lower the use of antibiotics . However, medicinal plants cannot replace antibiotics and other modern veterinary medicinal products, not the least for reasons of animal welfare. Nevertheless, a meaningful use of medicinal plants by veterinarians may limit the use of antibiotics and other modern veterinary medicinal products to particularly severe diseases. More clinical veterinary studies with medicinal plants are needed, and veterinary education as well as national strategies to control and combat the development of microbial resistance should take into account the use of medicinal plants as a means to reduce the use of antibiotics (StAR, 2015) .

Farmers of mainly French speaking regions of Switzerland reported a total of 77 medicinal plants to treat and prevent livestock diseases. Of (Schmid et al., 2012; Disler et al., 2014; Bischoff et al., 2016; Mayer et al., 2017; Stucki et al., 2019) . All plant species are given in Additional file 2. D. Mertenat, et al. Journal of Ethnopharmacology 246 (2020) 112184 D. Mertenat, et al. Journal of Ethnopharmacology 246 (2020) 112184 these, 18 were mentioned for the first time in a Swiss ethnoveterinary context. We found no obvious difference in ethnoveterinary knowledge between French and German native speakers. The most widely used plants are easily available and well known for human and veterinary purposes. Taking into account the current phytochemical knowledge, and in vitro and in vivo studies, several plants reported by farmers could be considered as promising treatment options for gastrointestinal and dermatological disorders in cattle and horses. The use of such plants and finished products could help to reduce, at least to a certain degree, the use of antibiotics. Thus, our findings may contribute to international strategies for limiting the use of antibiotics.

The authors declare that they have no competing interests. 

",0.6529088734679778
Simulating School Closure Strategies to Mitigate an Influenza Epidemic NIH Public Access,Background-There remains substantial debate over the impact of school closure as a mitigation strategy during an influenza pandemic. The ongoing 2009 H1N1 influenza pandemic has provided an unparalleled opportunity to test interventions with the most up-to-date simulations.,"The recent and ongoing 2009 H1N1 influenza pandemic has sharpened debate over the impact of school closure in the control of an influenza epidemic. Students interacting very closely in schools transmit influenza virus to one another and then subsequently to their families and others in the community. 1,2 Closing schools will curtail these interactions in the school and therefore eliminate an important potential transmission site.

However, closing schools is not a trivial decision. Closing schools can cause significant social and economic disruption to the local community by placing additional burdens on parents, causing loss of income to school staff, and interfering with children's education, nutritional status, and other activities. 1, 3 Parents who are forced to stay at home to care for their children when schools are closed may have to sacrifice needed income or even lose their jobs, which could be disproportionately more likely among low-income families. In addition, those parents may have essential skills (such as healthcare) that are consequently temporarily lost to the system. Beyond education, schools are the sources of many vital community services, such as lunches and after-school programs. Therefore, a thorough understanding of the potential benefits and effects of school closure should accompany school closure policy-making.

School closures rose in prominence as a possible epidemic mitigation strategy after studies suggested that schools play an important role in influenza transmission. Glezen wrote in 1996 that ""the fires of the epidemic are carried by healthy school-age children,"" a perspective supported by extensive historical data from the 1918-19 and 1957 pandemics and the 1978 influenza season in Houston. [4] [5] [6] School classes with children in kindergarten through 12th grade have ""high potential for virus transmission"" according to social network analyses conducted in the United States, 7 Germany, 8 and Belgium. 9 Public health and school officials face numerous decisions when preparing for a potential influenza epidemic. They need to decide whether and when to close schools, how many schools to close, and how long they should keep schools closed. An official may choose to keep schools open and simply isolate students who contract influenza, close schools in which influenza cases appear (individual school closure), shut down the entire school system as soon as enough cases appear in the community, or some combination of these various strategies. The optimal choice depends on the potential effectiveness of each strategy and any synergistic effects in mitigating an influenza epidemic. However, decision makers may not have enough available data to make effective choices between the different strategies, and prospective studies may be difficult or impossible to perform. Retrospective data may be limited and not generalizable to different situations and circumstances. Computer simulations can help decision makers explore different possible strategies and their probable consequences.

In May 2009, to assist the Allegheny County Health Department (ACHD) during the spring wave of the H1N1 influenza pandemic, the University of Pittsburgh Models of Infectious Disease Agents Study (MIDAS) group employed an agent-based computer simulation model (ABM) of Allegheny County, Pennsylvania. This model included explicit representations of the county's population, school system, work-places/commutes, households, and communities. The model also represented individual county residents, which allowed us to simulate different influenza epidemic scenarios and the ACHD to explore the effects of various school closure strategies. Our model incorporated many methods from other previously published MIDAS simulation models. 10, 11 The objectives of this study were to determine how school closures as an isolated strategy may minimize the course of an epidemic (as measured by the number of infected cases) and how these effects may vary with differing school closure durations, triggers, and types. Understanding the impact of school closure in isolation is important when other major mitigation strategies are not available or practical because the epidemic arrived unexpectedly (eg, the 2009 H1N1 pandemic when vaccines were not available and mass prophylaxis with antiviral medications was not attempted) or the infectious agent is completely novel. The model could be applied to other infectious diseases transmissible by casual contact, such as severe acute respiratory syndrome, by incorporating parameter values (eg, incubation period and transmissibility) specific to that infectious disease.

Our ABM represented each person in Allegheny County (a total of 1 242 755 agents living in households) with a synthetic computer agent assigned a set of characteristics and behaviors: age, sex, employment status, occupation, household location, household membership, school assignment of students and teachers, work location assignment of employed adults, work status as employed or unemployed, and disease status. 12 Construction of the agent population used a method developed by Beckman et al, US Census Bureau's Public Use Microdata files, and Census aggregated data to first generate a synthetic US population and then extract the Allegheny County population. 13, 14 Our model also represented individual households, schools, and workplaces in Allegheny County. The model included a total of 537 405 households, each containing a specific set of agents based on the US 2000 Census data. 15, 16 The model also consisted of 35 333 work-place locations. Assignment of Allegheny County's 563 874 employed adults to each workplace used the Census 2000 Special Tabulation: Census Tract of Work by Census Tract of Residence (STP 64) database. 16 A fraction of the workplaces consisted of 35 hospitals in Allegheny County, which employ an estimated 25 766 persons, of which 11 672 are HCWs who see patients on a daily basis.

Assignment of the 212 315 school-aged children to each school location was based on school capacity (used a similar method detailed by grade) and distance to schools as described by Ferguson et al 10 and Wheaton et al. 14 There are 43 school districts in Allegheny County including the city of Pittsburgh. These represent approximately 155 000 public school students in 289 schools. The additional schools and students attend in either private or parochial schools in the region. There is tremendous variability in socioeconomic status, race, and ethnicity by school district, with the city of Pittsburgh school system being the most diverse.

At any given time, each agent is in one of four mutually exclusive states: susceptible (S), exposed (E), infectious (I), or recovered (R). All people are initially susceptible (S) to disease until infectious individuals are introduced into the model. Contact with an infectious person has an assigned probability of disease transmission from the infectious person to the susceptible person, as listed in Table 1 , derived from studies by Longini et al, 17 Ferguson et al, 10 Germann et al, 18 and Halloran et al, 19 on the basis of the 1957-58 Asian influenza pandemic. For example, as the third row in Table 1 indicates, an infectious child coming into contact with a susceptible adult in a shared household has a .3 probability of transmitting the virus to the adult. By comparison, the fifth row of Table 1 indicates that an infected student who comes into contact with a susceptible student has a .0435 probability of transmitting the virus. The lower half of Table 1 delineates the frequency at which different individuals contact other individuals at different locations (eg, a student will contact 13.5 other students within his or her classroom and 15 students in the same school but other classrooms each school day). A newly infected person then moves to the exposed (E) state for the duration of the disease's incubation period and then to the infectious state (I) in which the person may infect others. Consistent with the results of the MIDAS combined-model study, only two-thirds of infectious patients exhibit symptoms. 11 After the duration of the infectious period, the person proceeds to the recovered state (R) and remains immune to subsequent infections for the remainder of the simulation.

These simulations incorporated a set of assumptions describing movements and contact patterns within the county population, with each individual represented by a computer agent, and is based on research done in other MIDAS network models. 10, 11, 18 Each weekday (in computer time), the computer agents moved back and forth from their households to their designated workplaces (if they were employed adults) or schools (if they were school-aged children), where they interacted with other agents in close proximity based on the contact rates shown on Table 1 . Each agent interacted daily with other family members who shared the same household. In schools and workplaces, each student or worker agent contacts a fixed mean number of agents per day (representing that agent's classroom or office) and has an additional random probability of interacting with other agents within the same school or firm but in different classrooms or offices. Worker agents in firms that have only one office repeatedly contact the same people each day. Every day, all agents, including students, potentially interact with each other in the community, although with a fairly low probability of transmitting the virus. On weekends, student agents do not go to school but have community interactions with an increase of 50 percent in frequency. Our base model also assumed that 20 percent of working adult agents work on weekends. 20 Our base case scenario assumed that 50 percent of sick students and workers stay home with no community contacts unless they see a doctor, and 40 percent of patients with influenza symptoms visit a clinic or emergency department, values that have been used in previous studies. 10, 19, [21] [22] [23] Our model also assumed that students would continue to maintain their usual community contacts during school closure, which included increasing community contacts by 50 percent during weekends. 24 

Initial model calibration utilized the Ferguson et al approach with data from historical (1957-58 and 1968-69) influenza pandemics. 10 Calibration targeted an epidemic with an attack rate (AR) of 33 percent as measured in the 1957-58 pandemic and established daily contact rates to satisfy the 30-70 rule developed by Ferguson et al, which posits that 70 percent of all transmission occurred outside the household (which includes 33 percent that occurred in the general community and 37 percent in schools and workplaces). 10 Once this is accomplished, straight multipliers of the transmission probabilities are used to strengthen or weaken the overall infection to produce desired ARs.

To account for the spectrum of potential influenza-transmission characteristics and dynamics, our simulation runs explored the effects of varying the basic reproductive rate (R 0 ), which is the expected number of secondary cases that a typical infected individual will produce in a completely susceptible population. Each simulation started with a single case (adult or child) and introduced the case into Allegheny County in a random location. The ABM was programmed in C++ and is naturally parallel with regards to statistical realizations. Simulations were performed at the Pittsburgh Supercomputing Center on Axon, an Intel Xeon-based Infiniband cluster. Each simulation run took on average 2 minutes over 100 computational cores.

Different simulation runs depicted various combinations of the following school-focused strategies:

• Isolating sick students: This strategy focuses on identifying and confining symptomatic sick students to their households while leaving schools open. As a request from the previously mentioned work with the ACHD, the base case scenario supposes that parents are able to identify and keep 60 percent of symptomatic children at home. Schools then are able to identify and send home 25 percent of symptomatic students within the first 2 hours of their arrival at schools. Schools can then identify and send home 10 percent of symptomatic students on the second day of symptoms. Schools and parents thus allow five percent of all symptomatic students to remain in schools throughout the duration of their illness. Sensitivity analyses varied the percentage of influenza cases that are symptomatic and the probabilities of parents and schools detecting symptomatic cases.

• School system closure: This strategy involved shutting down the entire school system once a certain trigger, that is, a threshold number or percentage of cases, was reached. For our base scenario, school system closure occurred when the total number of influenza cases of all ages in the county exceeded 1 percent of the Allegheny County population. Sensitivity analyses ranged this threshold and the duration of school system closure.

• Individual school closure: This strategy entailed closing only those schools in which influenza cases were identified. Our base case scenario assumed school closure 1 day after a case appeared in that school. Each school remained open until at least one case was identified among its students. Sensitivity analyses varied the number of cases that can occur in a school before closing the schools and the duration of the school closure.

All of the presented epidemic curves are the averages of 100 simulation runs that resulted in epidemics after seeding the population with one randomly infected individual. The continuous lines represent 4-day moving average trend lines, which smoothed out irregular patterns produced by the weekend effect (ie, students and workers having different weekend contact patterns). Figure  1 (a) shows incidence infections curves with no mitigations implemented for the various R 0 values calibrated for the research in the article, and Figure 1 (b) displays incidence infection curves for the same values with systemwide school closure of a duration of 8 weeks.

Isolation (identification of ill students and keeping them from attending schools) had minimal effect on the epidemic ( Table 2 ). In our sensitivity analyses, the maximal effect showed a decrease in the overall AR to 29 percent and a delay in the peak of the epidemic by 1 week (when parents were able to identify and prevent 60 percent of symptomatic children from going to schools). When parents were able only to catch 20 percent of symptomatic children before they went to schools, this strategy had no significant effect on the epidemic. Isolation as a strategy did not prevent asymptomatic infectious students from entering schools and spreading disease. In fact, the effectiveness of isolation as a strategy was highly sensitive to the percentage of infectious children who were symptomatic: decreasing this percentage from 75 to 25 increased the overall attack mean rate for an R 0 = 1.4 epidemic from 27.4 percent to 35.4 percent. Table 3 compares the overall AR, peak incidence day of epidemic, and peak incidence of infected individuals in an unmitigated epidemic (no school closures) with school closures of entire systems or individual schools and for varying durations (1, 2, 4, 8, and 16 weeks). Whether the school closure strategy was systemwide or individual made no consistent or significant difference on these epidemic parameters. For both school closure strategies, durations of less than 8 weeks had little impact on the overall AR and durations of 2 weeks or less actually slightly increased the overall AR.

For an R 0 of 1.4, 1-week systemwide school closure peak timing was similar to the unmitigated epidemic (day 68), but slightly higher levels of peak incidence compared with no school closure (12 841 vs 12 113 cases) and overall AR (35.3% vs 35.1%). A 2-week system closure also resulted in a slightly higher overall AR (35.0%) with a slightly delayed peak (between days 68 and 76). Only with a 4-week closure did the overall AR (33.2%) dip below the unmitigated level. Systemwide closures of 8 weeks or longer resulted in substantially lower ARs (25.1% for 8 weeks and 21.9% for 16 weeks), as well as a peak that was delayed by 8 days (day 76). Although the effects on overall ARs were modest, the peak incidence was substantially decreased for closures of over 4 weeks. The simulation runs showed similar results at the higher R 0 values for all systemwide closures, ARs were higher, and epidemic peaks were same as earlier. For individual school closure, variations of duration and R 0 value generated findings very similar to those observed for school system closures. Figures 2a and 2b present epidemic curves for varying lengths of school system closure and reactive school closures, respectively, for an R 0 = 1.4 influenza epidemic. Table 3 summarizes the simulations for 8-week school closures using different thresholds for implementation. For individual school closures, at the threshold the number of symptomatic cases was 1, 5, and 10. For systemwide school closures, the rate of symptomatic cases within the population was 0.10, 0.50, 1.00, or 1.50 percent. Interestingly, the overall ARs fell slightly when the thresholds were higher for both school closure strategies and all R 0 values.

Additional scenarios explored the effects of changing the number and timing of introducing new influenza cases to Allegheny County. Changing the initial seed from 1 to 10, 100, and 1000 cases made school closures even less effective. We also examined the effects of adding daily and weekly seeds; that is, 10 or 100 new cases are introduced every day or week. These changes did not affect the following general findings also listed above: school closure duration of less than 8 weeks had little impact on the overall AR and durations of 2 weeks or less actually slightly increased the overall AR, isolation (identification of ill students and keeping them from attending schools) had minimal effect on the epidemic, and closing schools earlier was not as important as ensuring that schools were closed until the majority of the epidemic was completed (ie, adequate herd immunity was achieved).

The results of his modeling exercise emphasize the importance of keeping schools closed until the epidemic curve is far past its peak. Closing schools quickly was less important than keeping them closed continually throughout the epidemic once they were closed. Very early school closure at the start of the epidemic actually worsened the AR when schools were reopened in the middle of the epidemic, releasing a whole population of susceptible students to mix and be infected and extend the length of the epidemic.

While other mitigation strategies (eg, vaccination and antivirals) may accompany school closures, there are many situations in which school closures may be the only major intervention option. Epidemics or pandemics, such as the 2009 H1N1 influenza pandemic, may arrive unexpectedly, leaving little time to design, produce, and distribute an effective vaccine (eg, egg-based cultures require 4-6 months to produce influenza vaccine). Even if the infectious agent is influenza and susceptible to antiviral medications, mass continuous prophylaxis with antivirals throughout the course of an epidemic may not be practical. Moreover, our model is not limited to influenza and could apply to any infectious disease transmitted by casual contact (eg, other respiratory diseases such as severe acute respiratory syndrome or measles) as long as the relevant parameter values for the disease are available. Adequate pharmaceutical interventions may not be readily available for such diseases, especially if the infectious agent is novel.

In February 2007, the Centers for Disease Control and Prevention (CDC) published a detailed ""community strategy"" as the first line of defense against a novel pandemic virus. 26 This strategy focused on ""nonpharmaceutical interventions"" since vaccine would be initially unavailable and medications would likely be unavailable or in very short supply. These guidelines focused on six strategies intended to be implemented as ""targeted and layered defense,"" including • closing schools,

• canceling public gatherings,

• planning for liberal work leave policies,

• tele-working strategies,

• voluntary isolation of cases, and

• voluntary quarantine of household contacts. 26 These strategies were developed in part on the basis of the established public health principles and supported by Markel et al 2 in their detailed study of the use of nonpharmaceutical interventions in 43 major US cities during the 1918-19 pandemic. 2 Markel et al noted that a strong association between reduced total mortality and early, sustained, and layered application of these interventions during this time period, with school closure appearing to play an important role. 2 A previous survey by Aledort et al 27 had found disagreement about using school closures during a pandemic in either its early localized phase or its advanced phase.

However, as the 2009 H1N1 influenza pandemic emerged and continues, considerable controversy persists over whether and how to close schools. In the spring of 2009, the World Health Organization recommended utilization of school closure as a potential strategy, although it noted that ""class suspensions"" instead of full school closures might be considered. 28 The CDC said that school closures should be implemented ""at the discretion of local authorities based on local considerations."" 28 The CDC-affiliated scientists have emphasized the need to close schools early in the course of a pandemic and to do so in combination with other mitigation measures. 29 Our analysis implied that school closures as an isolated mitigation strategy (under the epidemic conditions explored) are not effective unless implemented and sustained for at least 8 weeks.

At best, shorter school closures delayed the epidemic peak only briefly and, at worst, actually resulted in a slightly higher peak incidence. Increasing the peak incidence could tax the healthcare system especially if its daily capacity is severely limited (eg, clinics and emergency departments cannot handle an additional surge of cases on a given day). School closures were effective in reducing the overall AR only when schools remain closed throughout the bulk of the epidemic and reopened after the epidemic has subsided (or an additional effective mitigation strategy such as vaccination is implemented in the interim). Reopening schools during the epidemic simply released a population of susceptible children to be subsequently infected and may continue to fuel the epidemic. However, school closures for 4 weeks or more did have a significant impact in reducing the peak incidence, which would be of value in mitigating medical service needs during the epidemic.

In addition, our results showed no significant differences between individual reactive school closures and the more radical systemwide entire school system closure. This has several potential policy implications. First, systemwide school closures will result in more lost school days and, consequently, be much more costly than individual school closures. Second, an entire school system closure may be more difficult to coordinate and implement, depending on the location and jurisdiction, although it would certainly reduce confusion. It raises the questions of who has ultimate decision-making power and which schools in a county fall within the scope of those decisions. For example, in a given county, there may be different officials who hold authority to close public schools in various school districts and private schools. Individual schools with symptomatic students may be more willing and able to close quickly; ironically, schools may close in response to expressed parental concern or to reduce potential litigation.

There is also debate over the ""illness"" thresholds at which to close schools. Should schools be closed after a certain number of cases appear in schools, the community, or both? Will it make a difference if this threshold is based on the number of deaths or the severity of the observed cases? Our results suggested that the impact of school closures on overall ARs is not highly sensitive to changes in this threshold level, that is, when exactly school closure is initiated. Delays in closing schools did not have substantial effects on the epidemic. Instead, the most important factor seemed to be keeping schools closed for a long duration, that is, throughout most of the epidemic, once they are closed.

Our study also found that simply identifying and confining symptomatic cases will do little to mitigate an epidemic. Such a strategy will be effective very early in the epidemic only if it rapidly identifies and confines nearly all infectious cases before they can transmit the virus to others. This appears to be unlikely, since many infectious students will be asymptomatic and compliance among parents and schools will fall far short of 100 percent. So while this strategy may be locally beneficial, it will do little to stem the overall epidemic and essentially ""chases"" the epidemic without preventing asymptomatic infectious students from staying in schools and continuing to transmit the virus.

All computer models are simplifications of reality and can never account for every possible factor or interaction. Rather than make decisions, computer models provide information to decision makers about possible scenarios and relationships. An influenza pandemic and the resulting circumstances may not necessarily conform to the data and assumptions that our model drew from referenced sources or previously published models. School closures may still be effective under epidemic conditions not explored in our study (eg, R 0 <1.4 or >2.4).

This simulation did not include mortality as an epidemic parameter, which could vary with influenza viruses. Higher rates of mortality would probably influence decision makers to close schools at lower thresholds of incidence and for longer durations. We did not model the effects of hand hygiene measures because their quantitative effects on influenza transmission have not been established, although preliminary results from the CDC-funded Pittsburgh Influenza Prevention Project suggest that hand hygiene measures may have little effect on the overall epidemic curve. Our future studies may model the effects of layering additional interventions such as vaccination and antiviral medications on top of school closure.

Our study suggested that short-term school closures as an isolated mitigation strategy (under the conditions we explored) will do little to mitigate overall ARs during an influenza epidemic, but closures of 4 weeks or longer did have the effect of decreasing the peak incidence of illness. Schools must remain closed for at least 8 weeks, that is, throughout most of the epidemic, to have a significant impact on overall ARs. Simply, screening for ill students and keeping them from attending schools had minimal effect on the overall extent of the epidemic. Individual and systemwide school closures appeared to be equally effective so if the latter are more difficult to execute, the former can suffice. In addition, our results did not support any specific ""illness"" threshold level at which to close schools. In other words, short delays in school closure may not have a significant effect on the AR, timing of epidemic peak, or peak incidence. Finally, school closures may delay the epidemic peak for up to a week, which may provide more time to implement a second more effective intervention such as vaccination. The observed effect of reducing the peak incidence during an epidemic could place more importance on school closures when medical care capacity is anticipated to become strained or overloaded because of the number of patients needing care during the epidemic. This might be especially important in the setting where flu vaccine was becoming available just at the beginning of the school year. It is possible that delay of school opening for several weeks, in order to ensure that students get vaccination prior to beginning the school year, could be highly effective. As flu vaccine production becomes more efficient, this scenario becomes increasingly possible. Abbreviations: AR, serologic attack rate; PD, epidemic peak day; PI, epidemic peak incidence; and ΔPI, change in peak incidence compare to unmitigated scenario.

a Some students will be detected the first day that they appear sick in schools and some will be caught the second day.

Effects of varying the length of school closure 

",0.652600103299882
Public health implications of using various case definitions in The Netherlands during the worldwide SARS outbreak,"This study analysed the consequences of deviation from the WHO case definition for the assessment of patients with suspected severe acute respiratory syndrome (SARS) in The Netherlands during 2003. Between 17 March and 7 July 2003, as a result of dilemmas in balancing sensitivity and specificity, five different case definitions were used. The patients referred for SARS assessment were analysed from a public health perspective. None of the patients referred had SARS, based on serological and virological criteria. Nevertheless, all 72 patients required thorough assessment and, depending on the results of the assessment, institution of appropriate prevention and control measures. Changing case definitions caused confusion in classifying cases. A centralised assessment of the reported cases by a team with clinical and public health expertise (epidemiological and geographical risk assessment) is a practical solution for addressing differences in applying case definitions. The burden of managing non-cases is an important issue when allocating public health resources, and should be taken into account during the preparation phase, rather than during an outbreak. This applies not only to SARS, but also to other public health threats, such as pandemic influenza or a bioterrorist episode.","Severe acute respiratory syndrome (SARS) confronted communicable disease professionals with new dilemmas concerning the assessment and management of suspect cases, because of an initial lack of knowledge of the pathogen, its transmission route, its incubation period and its clinical presentations. The sense of urgency increased with evidence that the disease could spread easily and that the clinical course could be severe [1] [2] [3] [4] . Following the WHO global alert on 12 March 2003 and the WHO travel advisory notice on 15 March 2003, countries worldwide started reporting cases [5, 6] . In May 2003, the WHO revised the case definitions of SARS [7] , based on knowledge of the disease available at that time. Early studies in 2003 showed a low sensitivity (26%) when using the WHO criteria for clinical assessment of patients before admission to a SARS clinic in a region with extensive local transmission [8] . A comparable low sensitivity (27%) and a much lower positive predictive value were found when screening patients before admission to a general hospital in Singapore [9] . In the designated screening centre for evaluation of possible SARS cases in Singapore, the WHO case definitions were found to be helpful at initial assessment, despite an under-triage of 0.3% [10] . To address the low sensitivity of the WHO criteria, researchers from Hong Kong developed a clinical prediction rule for the emergency department [11] .

Case definitions are used for different purposes, i.e., surveillance, clinical care, research and service provision [12] . Their benefits and limita-tions in practice depend on their purpose, the context in which they are used, and the skills of the user. When formulating a case definition, balancing sensitivity and specificity is crucial [13] . The initial emergence and spread of SARS in the absence of specific diagnostic tests required that physicians and public health authorities worldwide formulate a set of criteria for separating suspected or probable cases from non-cases. This report describes the background, and the choices made with respect to classification and management, of patients referred for SARS assessment in The Netherlands. The consequences of the use of different case definitions for the reporting behaviour, patient management and evaluation of public health interventions are highlighted. None of the cases met the criteria for reporting to the WHO according to the case definitions in force at the time in The Netherlands. All cases were reevaluated on the basis of the latest case definition from the WHO (1 May 2003) [7] , and differences with important public health consequences were revealed. (Table 1) , for surveillance purposes and comprehensive clinical assessment, required obligatory radiographical evidence of lung infiltrates consistent with pneumonia, and well-defined epidemiological criteria for exposure (i.e., close contact with an individual with severe respiratory disease in the affected areas during the previous 10 days). This more specific case definition was issued to avoid considering SARS in the differential diagnosis of the expected large number of patients with acute respiratory infections at a time when the incidence of mild (common) respiratory disease in travellers was high and the influenza season was ongoing. General practitioners and clinicians were asked to report suspect cases on a voluntary basis to the Dutch Public Health Services. On 1 April 2003, mandatory SARS notification was introduced. The Public Health Services were asked to consult the National Co-ordination Centre for Outbreak Management (LCI) for an assessment of SARS probability. In the following weeks, four subsequent case definitions followed in which, gradually, sensitivity increased (Table 1) at the expense of specificity. The criteria for case definitions and notification were evaluated on each occasion by members of the Outbreak Management Team, who were responsible for issuing scientific advice. Major factors in balancing sensitivity and specificity were the severity of the clinical course, the frequency of local transmission in affected areas, and the level of contact with potentially infected individuals. On 10 June 2003, the WHO case definition for a suspect case (revised on 1 May 2003) was also adopted in The Netherlands for reporting patients for SARS assessment.

A retrospective descriptive analysis of the cases that were reported for SARS assessment in The Netherlands during the period 17 March to 7 July 2003 was conducted. Cases from two datasets were used. One dataset included the cases referred by regional public health physicians to the LCI. The second source was the dataset from the SARS reference laboratory in The Netherlands (Department of Virology, Erasmus MC Rotterdam), to which clinical specimens were sent. These cases were referred for SARS assessment on the basis of their clinical status, travel-associated risk and ⁄ or contact with suspect ⁄ probable SARS cases. The referral by the clinician ⁄ public physician was compared retrospectively with the assessment on the basis of the Dutch case definition at the time of submission, and with a reassessment on the basis of the WHO case definition issued on 1 May 2003. The risk ratio (RR) for being a suspect case was calculated by comparing the referral of the case by the clinician with the first assessment (RR1), the referral with the reassessment based on the WHO definition (1 May 2003) (RR2), and the first assessment with the reassessment (RR3). The data sources were compared using a capture-recapture method [14] . The same classification of cases was used for both data sources. Cases common in both data sources were identified using date of birth, date of onset, gender and city of residence.

Bacteriological cultures were performed on respiratory specimens in the referring hospitals. In the reference laboratory, serological testing for antibodies against SARS-coronavirus (SARS-CoV) was carried out, and virus culture was performed in a category 3 biological safety laboratory facility, where the samples were also prepared for PCR testing.

Serology Acute sera and, where possible, convalescent sera were tested for the presence of antibodies against SARS-CoV. The method used was an indirect immunofluorescence assay in which SARS-CoV-infected Vero 118 cells that had developed a cytopathic effect were used to coat microscope slides. Serum was diluted in serial two-fold steps from 1:10 to 1:40 in phosphate-buffered saline. After incubation with dilutions of the serum for 30 min at 37°C, the slides were washed with phosphate-buffered saline and incubated with rabbit antihuman IgG, IgA and IgM conjugated with fluorescein thiocyanate (Dako, Heverlee, Belgium). After washing and drying, the slides were examined with a fluorescence microscope [15] . The titre was defined as the highest dilution giving a 1+ or 2+ reaction.

Nose, throat and sputum specimens were tested specifically for respiratory viruses, including influenza A and B viruses, human respiratory syncytial virus, human parainfluenza virus types 1-4, adenovirus, human metapneumovirus and rhinovirus. Routine virological tests for respiratory pathogens were performed by a combination of virus isolation in cell cultures and immunofluorescence [16] . Virus isolation procedures for SARS-CoV were performed following inoculation of specimens on Vero 118 cells, a subclone from Vero 6 cells, human embryonic lung fibroblasts and tertiary monkey kidney cells. The presence of virus was confirmed by immunofluorescence, RT-PCR, or both.

Virus nucleic acid was purified using the Magna Pure LC automated nucleic acid isolation system (Roche Diagnostics, Mannheim, Germany). Swabs, bronchoalveolar lavages and sputum samples were processed with the Magna Pure LC Total nucleic acid serum plasma blood isolation kit. The presence of SARS-CoV RNA was assessed on an ABI Prism 7700 using the EZ rTtH RNA amplification kit (Applied Biosystems, Nieuwerkerk a ⁄ d IJssel, The Netherlands) [15] . RT-PCR with primers and a probe specific for the nucleoprotein gene of SARS-CoV was used, as such an assay may be more sensitive than RT-PCRs based on the polymerase gene [15, 17] . Specimens from the respiratory tract were also monitored for influenza A and B viruses, human respiratory syncytial viruses A and B, rhinoviruses, coronaviruses (OC43 and 229E) and human metapneumovirus, using essentially the same methods with specific primers [18, 19] . The date of onset of illness was known for 55 patients. The time between onset and referral varied from 0 to 40 days, with a mean of 4.5 (median 3) days. Same or next-day referral occurred for one-third of the patients. Three patients were referred ‡2 weeks after the onset of illness, with a maximum of 40 days (one case). The distributions of the clinical and epidemiological criteria for SARS among the referred patients are listed in Table 2 . The patients with no history of travel to a SARS-affected area had travelled to other countries in south-east Asia. Six patients had no history of travel, but fulfilled, according to the physician, at least one clinical criterion for SARS, and had a risk of exposure in The Netherlands through close contact with travellers from affected areas.

As presented, there were 52 (72.2%; 95% CI 60.4-82.1%) suspect cases and 20 (27.8%; 95% CI 17.9-39.6%) cases that were not suspect, but which had an indication for further assessment. The category 'not suspect with an indication for further assessment' included cases in which not all the criteria for classification as a SARS suspect case were met, but in which the reporting physician could not rule out SARS without further assessment. Suspect cases were divided equally between males and females. The assessment of the cases on the basis of the Dutch case definition in force at the time was compared with a retrospective reassessment on the basis of the WHO case definition of 1 May 2003 (Table 3 ). There was no significant difference between the risk of being a suspect ⁄ probable case at presentation vs. the risk following reassessment based on the WHO case definition (p 0.14), but there was a significant difference between the risk of being a suspect case at presentation and the risk in the first assessment based on the more precise case definition (RR1 1.46; p <0.05 by Fisher's exact test) used at that time in The Netherlands. The risk of being a suspect case was significantly higher in the reassessment than in the first assessment when the more specific case definition was used (RR3 = 4.5; p <0.001 by Fisher's exact test).

Virological investigations were performed on 37 patients, who presented as 33 suspect and four not-suspect cases. The same cases were categorised at the first assessment as seven suspect and 30 not-suspect. According to the WHO case definition used in the retrospective reassessment, the cases would have been divided into 12 suspect, 24 not-suspect and one probable. Antibodies to SARS-CoV were not detected in any of the acute sera. Convalescent sera from six patients were also tested for antibodies to SARS-CoV, but all were negative. Virus culture and RT-PCR testing for SARS on respiratory samples (16 patients) were also negative. Other viruses were detected in nine patients: influenza A in four, influenza B in one, rhinovirus in two, adenovirus in one and cytomegalovirus in one.

Six patients were put in strict hospital isolation [20] and seven were isolated at home until the laboratory results or the clinical course of the patient ruled out suspicion of SARS. Droplet and contact isolation precautions were advised for patients in home isolation. Unprotected carers and contacts of these patients were traced, and the intensity of their exposure was assessed. When necessary, body temperatures were monitored. For eight patients, restriction of contacts outside the household was advised in order to increase their social distance (e.g., by exclusion from work and social events), but isolation was not considered to be necessary. All suspect cases were given advice on respiratory hygiene and contact precautions.

The Netherlands a few weeks after the worldwide alert, and proved to be a valuable instrument, since most (78.2%) of the suspect patients were reported within 5 days of the onset of respiratory disease. Nevertheless, no date of onset was recorded for 17 of the patients, suggesting that data collection could be improved, especially as one of these patients was later reclassified as suspect. During the worldwide SARS epidemic in 2003, five case definitions were subsequently used in The Netherlands. The latest, used from 10 June onwards, was identical to that issued by the WHO on 1 May 2003. The three major criteria for SARS (i.e., respiratory disease, fever and travel-associated risk) were met in 25 (34%) cases. Of these, two patients had spent just a few hours in an airport in an affected area, and another two patients became ill outside the incubation period. Using the most sensitive case definition (WHO, 1 May 2003), 21 cases would have been considered as suspect, and two (2.8%) patients would have qualified as probable cases. These cases were not reported to the WHO because, at the time of their assessment, they did not meet the criteria for probable cases according to the prevailing Dutch case definition.

With strict application of the specific case definitions issued in The Netherlands (before adopting the WHO version on 10 June 2003), only nine cases would have been defined as suspect cases requiring SARS assessment. In reality, despite the case definitions used, 52 cases were considered to be suspect by the clinician or the local public health officer. In the UK, over 400 cases were referred for assessment, among which four probable cases were identified [21] . In France, 437 cases were notified, with seven being classified as probable cases, among which four were later confirmed as being infected with SARS-CoV [22] . The Netherlands was not a country affected by SARS, and there was no local transmission. SARS was ruled out on the basis of the criteria in force at that time. As soon as SARS-specific diagnostic tests became available, they were used on samples from 37 patients. Acute sera taken early in the course of the infection can result in false-negative serological results unless convalescent sera are also analysed at a later date. Negative serology was not the only criterion for ruling out SARS; discharge from observation took place only when the clinical course improved and ⁄ or an alternative diagnosis could fully explain the illness.

Although inevitable following the gradual accumulation of knowledge during the SARS outbreak, the changes in case definitions during the epidemic caused confusion in the management of individual cases. Subsequently, greater knowledge of the disease has revealed that transmission occurred mainly within hospitals caring for seriously-ill patients. The severity of the disease (pneumonia or respiratory distress syndrome) is an important criterion for including patients in the suspect group, and for referring them for further assessment in countries considered to be at low risk according to the WHO classification [23] . To increase the assessment specificity, the inclusion of significant travel exposure (i.e., contact with a person suffering from respiratory disease in areas with local transmission) or hospital exposure (i.e., the patient has been hospitalised or has been working in a risk area for SARS) should be obligatory when considering SARS in patients returning from abroad. Issartel et al. [24] concluded that contact exposure seemed to be one of the best criteria for identifying cases of suspected SARS.

A limitation of the present study is that, in the absence of confirmed SARS cases in The Netherlands, it is not possible to identify with certainty which components of the criteria for SARS assessment would be crucial in identifying genuine cases. In the early stages of SARS (or during a new incident), the vital diagnostic features will emerge only with time, and the criteria for separating cases from non-cases will initially be imprecise. Continuous vigilance by healthcare professionals and public services will provide the basis for identifying possible cases. From a public health point of view, in order to prevent the medical services from becoming overwhelmed, a first-line filter is needed to separate suspect cases from notsuspect cases before referring patients for further assessment. The first-line filter will be based on broad clinical and epidemiological criteria (e.g., history of contact and travel exposure) and will be refined gradually as knowledge concerning this disease becomes available.

Education of health professionals in the use of case definitions is needed before the event. Risk assessment and case classification should be done by individuals with experience in working with SARS case definitions. This could be achieved within a centralised surveillance and assessment system by an expert team with clinical and public health expertise (i.e., knowledge of the epidemiological features of the disease and the availability of necessary additional resources) [25] . This expert team, with both the necessary experience and the authority to confirm the suspicion, should be available at all times. Centralised assessment is appropriate as long as there are only imported cases, or local transmission remains at a low level. This approach might have limitations for countries in which a delay in isolating patients could occur because of large distances between hospitals. This is undesirable because shortening the period between the onset of symptoms and isolation, and proper management of contacts, have proven to be effective in containing SARS. The risk of secondary transmission has been shown to decrease significantly when cases are isolated within 3 days of the onset of symptoms [26, 27] . Telephone or online consultation with the expert team is a useful option.

In the coming years, and particularly during the seasonal increase in respiratory disease in autumn, the burden of managing non-SARS cases will be high, and public anxiety should not be increased unnecessarily by the use of a case definition that is overly non-specific for a country at low risk. A possible disadvantage of using too narrow a case definition is that missed cases could prove to be super-spreaders. This risk will come from unrecognised cases; however, sub-clinical infections seldom occur, and significant virus shedding with secondary transmission has been documented mainly in the second week after the onset of disease [28, 29] .

Individual situations vary, and no single set of assessment criteria can be applied to all settings. Some of the dilemmas encountered with SARS will reappear when worldwide threats provided by pandemic influenza, (re)emerging pathogens or bioterrorism occur. Compared with other infectious diseases, SARS has an estimated low R 0 (basic reproduction number) [26] , and transmission is confined mostly to spread from overt clinical cases. The extent of transmission of influenza by sub-clinical cases during a pandemic is difficult to predict, and in such a situation, timely preparedness and proper allocation of resources to manage both suspect cases and non-cases, and their contacts, are important. The danger of over-reporting cases should be taken into account when estimating and planning epidemic control resources before an outbreak occurs. The costs of managing non-cases should not be underestimated, as they require thorough assessment and the instigation of public health measures, thereby increasing existing workloads.

Both within the process of scientific advice and the preparation of outbreak plans, the consequences of the various case definitions should be considered, in addition to the training of professionals in applying case definitions to their patients. A first-line filter for separating non-cases from suspect cases should be followed by consultation with a centre of expertise, bearing in mind that the balance between the need to classify and manage cases correctly and the need to allay public anxiety quickly could be delicate.

We thank all internal medicine specialists, public health physicians, general practitioners and microbiologists who reported cases or consulted us for further assessment. The work described in this article was presented, in part, at the 14th European Congress of Clinical Microbiology and Infectious Diseases (Prague, 2004).

",0.6516845073136616
The need to include animal protection in public health policies,"Many critical public health issues require non-traditional approaches. Although many novel strategies are used, one approach not widely applied involves improving the treatment of animals. Emerging infectious diseases are pressing public health challenges that could benefit from improving the treatment of animals. Other human health issues, that overlap with animal treatment issues, and that warrant further exploration, are medical research and domestic violence. The diverse nature of these health issues and their connection with animal treatment suggest that there may be other similar intersections. Public health would benefit by including the treatment of animals as a topic of study and policy development.","Environmental hazards, stagnation of drug development, climate change, human population growth, emerging infectious diseases (EIDs), world hunger, and violence are among our most urgent public health concerns. Their multifactorial roots necessitate strategies that go well beyond the traditional health sector. One strategy that has been largely neglected and that can help address several of these issues involves improving the treatment of non-human animals (hereafter 'animals'). I briefly describe how EIDs are connected with the treatment of animals and discuss why this connection needs to be addressed in public health strategies.

I describe other areas of exploration, including issues concerning domestic violence and medical research.

Since 1980, more than 35 infectious diseases have emerged and 87 human pathogens have been discovered, an average rate of about three new pathogens each year. 1, 2 Of the 175 human pathogens that have been classified as emerging or re-emerging, three-fourths come from animals. 3 Several factors contribute to the increase in EIDs, including environmental changes and human population growth. Changes in animal agriculture and growth of the wildlife trade also contribute to the increase in emerging infections.

Each year, more than 64 billion animals are raised and killed for food globally. 4 Largely because of increased demand for animal products, intensive animal operations have mostly replaced traditional farming practices worldwide, particularly for pigs and poultry. 5 The stress and distress associated with these new farming conditions heightens animals' vulnerability to disease. A report published by the Pew Commission highlights how close contact of animals facilitates the exchange and evolution of pathogens and how stress induced by dense confinement increases the likelihood of infection and illness in animals. 6 Pathogens circulating in these animal populations may lead to human illnesses.

For centuries, the evolution of the influenza virus had remained relatively stable. [7] [8] [9] In recent years, however, the virus has undergone an evolutionary surge, with new variants emerging rapidly. The intensive confinement of animals is shown to be a major contributor to this surge. 9,10 Highly pathogenic avian influenza (HPAI) H5N1, first isolated in the Guangdong Province of China in 1996, is one of the most notable pathogens to appear recently. 11, 12 Most avian influenzas begin as mildly pathogenic viruses. 13 Once they enter domestic bird populations, they may rapidly mutate into highly pathogenic viruses. Research demonstrates that after circulation in domestic birds for very short periods, low-pathogenic avian influenza (LPAI) viruses can mutate into HPAIs. 14 Since 1990, outbreaks of HPAI virus subtypes have become more common in farmed birds compared with before 1990. 8, 9, 14 Intensive confinement of birds facilitates both the increasing frequency and scale of these outbreaks. 9, 10 According to the World Organization for Animal Health of the Food and Agriculture Organization, two lessons should be learned from these prior outbreaks. 15 First, that if LPAI viruses are allowed to spread among farmed birds, they can eventually mutate into HPAI viruses; and second, that densely confining birds considerably increases their vulnerability to infectious diseases.

Thus far, H5N1 has shown limited human-to-human spread. Pigs, however, may help H5N1 gain the ability to widely infect humans. 16 Pigs are highly susceptible to both avian and human influenza A viruses. 17, 18 Until recently, only sporadic reports described pigs infected with H5N1, but a study published in 2010 reveals that between 2005 and 2007, 7.4 per cent of 700 pigs tested in Indonesia carried H5N1, even though they lacked influenza-like symptoms. 8, 19 One viral isolate had acquired the ability to recognize a cell receptor present in the noses of both pigs and humans. This change might allow it to spread easily from pigs to humans and then among humans. The pigs may have been initially infected from nearby chicken farms, but there was also evidence of pig-to-pig transmission.

The transporting of animals across the globe also influences the emergence and spread of infectious diseases. Millions of animals annually are caught in the wild or bred in captivity for the wildlife trade. They are traded as exotic pets, entertainment, food, skins, medicinal objects, and for biomedical research. Trade, both international and within countries, has increased rapidly worldwide. 20, 21 Few laws protect animals from harm during any phase of trade. 22, 23 Where regulations exist, they are rarely enforced or the penalties are so minor that they provide almost no deterrent. 22 As a result, overcrowding, exposure to extreme temperatures, unsanitary conditions, and animal diseases are common.

The trade creates ideal conditions for pathogens to multiply. Of all contributors to the emergence of zoonotic pathogens (such as ecological factors and behavior), 'species-jumping' events may be among the most important. 24 They expand the range of viable hosts. Holding different populations of animals together, particularly sick and stressed animals, may result in the spread of new pathogens. A 2003 outbreak of monkeypox in the United States occurred after a shipment of infected African Gambian rats was sold to pet dealers, one of whom housed the rats with prairie dogs. 25, 26 The prairie dogs contracted monkeypox, then were sold as pets, and transmitted the pathogen to 71 people.

Severe Acute Respiratory Syndrome (SARS) probably emerged from the wildlife trade in Guangdong Province, China. 27, 28 Chinese authorities slaughtered thousands of civets, small arboreal mammals traded for their musk-producing glands, because they were suspected to be the source of the pathogen. SARS is now believed to have originated from large fruit bats. 29, 30 Bats had been captured in the wild and traded live in Chinese markets. At some point in the wildlife supply chain, infected bats were likely brought into contact with susceptible hosts, such as civets, in whom the virus amplified. The intermingling of species established a cycle in which susceptible animals and humans could become infected. After the SARS epidemics subsided, an editorial in the American Journal of Public Health made this observation: 'The concentration of animals, their overlapping sojourns in the markets (allowing disease to spread through vast numbers of animals), and their interactions with humans (facilitating human infection) make these markets ripe for zoonoses. Once an epidemic starts among these animals, it can spread to animals reared in less cruel conditions'. 31 Butchering and eating of non-human primates through the bushmeat trade probably led to the emergence of HIV. 32, 33 Similarly, Ebola virus has been traced back to the trade in non-human primates, bats, forest antelopes, rodents, and shrews. [34] [35] [36] The Nipah virus, first identified in the Sungai Nipah New Village in Malaysia, spread throughout the region through the trucking of infected pigs. 37 

Two other connections between human health and the treatment of animals warrant further exploration: medical research and interpersonal violence. More than 115 million animals are used each year in experiments or to supply the biomedical industry. 38 Although the topic of animal research has not traditionally been viewed as a public health issue, its use is inextricably tied to the safety and efficacy of medical therapeutics, that play an integral role in many public health applications, including those intended to combat EIDs. Successful anti-infectious disease campaigns, for example, often rely on the availability of safe and effective vaccines and therapeutics, that in turn, depend on reliable and predictive pre-clinical research. A new study found that the mouse models used extensively to study human inflammatory diseases, including infectious diseases, only poorly mimic the human condition. 39 This calls into question the reliability of mouse models to identify, develop, and test drug and vaccine candidates. 39 Indeed, because of apparent stagnation in the invention of useful drugs, leaders in the biotechnology and pharmaceutical industries have argued that poor predictability of animal testing is among the chief challenges facing the drug discovery community. 40 The stresses induced in the laboratory can affect animals' physiology and perturb study results. 41, 42 This begs the question: How relevant is animal testing for predicting human health outcomes? [43] [44] [45] [46] [47] [48] Researchers need to develop human-based testing methods to better identify and predict the safety and efficacy of potential pharmaceutics.

The establishment of non-animal testing methods to replace animal use remains underdeveloped, with too few resources devoted to the endeavor. [49] [50] [51] Investment in alternative testing methods fall far short of investments in animal testing. 51 Making predictive human-based testing models a higher priority would improve medical research and reduce the numbers of animals used in harmful experiments.

Domestic violence has become a public health urgency worldwide. As perpetrators of violence usually harm all those physically weaker than themselves, abuse of animals occurs frequently in conjunction with human abuse. 52-54 Domestic service and child protection organizations, such as the UK's National Society for the Prevention of Cruelty to Children, have called animal abuse a 'red flag' for other forms of violence. 54, 55 Merging anti-abuse strategieshuman and animalcan improve detection of all forms of violence. 56 Unfortunately, health agencies do not give the human-animal violence link sufficient weight in public health policy. Neither the World Health Organization nor the US Centers for Disease Control and Prevention, for example, emphasize the connection.

The examples above demonstrate a connection between human health and the treatment of animals. Awareness of this link grows. The One

Health Initiative now fosters collaboration between veterinary and human health professionals, and works, for example, to improve detection of EIDS. 57 Although a positive step, this initiative misses a vital point that many EIDS are due at least partly to the treatment of animals. Yet, animal treatment is largely absent from public health dialogue. Current strategic policies, which do not fundamentally address animal treatment, are off the mark.

Interventions against EIDs target surveillance and inspection activities, animal vaccination campaigns, and culling, but do not confront an underlying cause, the poor treatment of animals. Surveillance is constrained by our infrastructure plus scientific limitations. Relying on inspection of shipments remains impractical. Gerson and colleagues estimated that only 2 per cent of the more than 12 million commercial shipments into Canada annually are physically inspected. 58 Pathogens that cause little or no overt signs of illnesses in animals will likely go undetected. The H7N9 avian influenza virus, for example, does not cause apparent illness among farmed birds, thus silent spread puts humans at risk. 59 Trying to predict which virus subtypes circulating among animals that will pose dangers to humans is extremely difficult. 60 Culling and vaccination campaigns may in the short term quell an epidemic, but they have frequently failed to provide lasting solutions. More than 100 million birds were killed throughout Asia in an effort to thwart the spread of H5N1. 61 However, the next wave of H5N1 re-established itself in the same countries and spread to new ones. Poultry vaccination programs in Asia and Egypt did not prevent the re-emergence and spread of H5N1. 62, 63 Indeed, vaccination may spur the evolution of the virus. 64, 65 Whether vaccination programs in China lead to greater genetic diversity of the H5N1 virus and contribute to current strains is one of many questions to consider. 66 Moreover, was a low-pathogenic precursor to H5N1 from wild birds introduced to poultry populations through household or 'backyard' flocks? Or was a precursor introduced through commercial confinement operations? Although backyard flocks may serve as conduits that transfer avian influenza viruses to domestic bird populations, their contribution to the emergence and spread of H5N1 may have been overestimated. In an analysis of a 2004 poultry outbreak of H5N1 in Thailand, Graham and colleagues found the likelihood of outbreaks to be far greater in large-scale commercial operations than in backyard flocks. 67 Additionally, there are numerous routes by which pathogens can be introduced to commercial operations. Studies in Canada, the Netherlands, and Denmark reached similar conclusions. 68 The discovery of H7N9 suggests that current mitigation efforts are failing to prevent the emergence of new, dangerous strains. Ultimately, strategies will have the greatest chance of success in preventing further evolution of avian influenza and other pathogens if they confront the underlying causes of pathogen emergence, some of which are rooted in the poor treatment of animals.

Direct benefits to human health are possible if efforts to improve the treatment of animals are routinely incorporated into public health policy and strategies. If, to benefit animals, we focus on reducing the number of animals confined in industrial farms and traded worldwide, opportunities for the emergence and spread of new pathogens would decrease and benefit people. Plant-based diets can reduce the number of animals confined in industrial farms and further decrease emergence of zoonotic pathogens. Such diets also confer environmental and personal health benefits. 5, 22 Might animal protection organizations join public health colleagues to forge a public campaign against the keeping of exotic animals as pets and for entertainment? Public health agencies and animal protection organizations should work with policymakers to enact restrictions on wildlife importation. Reduced use of animals in experiments deserves emphasis by government institutions that fund research. 22 Finally, if there were greater coordination on animal protection between public health, veterinary and social services, together we might increase detection of all forms of violence and thwart future acts of violence. 56 

The emergence of many recent pathogens can be attributed, directly or indirectly, to the intensive confinement of animals raised for food and the poor treatment of animals appropriated for the wildlife trade. The strategies currently used to address EIDs would be much improved if efforts to improve the treatment of animals were integrated into public health policies.

Studying the connection between domestic violence and animal mistreatment can surely help control both problems. Strategies combating animal cruelty may increase detection and prevention of violence against humans. Similarly, critically assessing the value of the use of animals in The need to include animal protection in public health policies research is likely to benefit animals and improve research. We should strive to replace animal experiments with more predictive human-based testing methods. With further study, other connections between human health and animal treatment are likely to be discovered.

By discussing poor animal treatment, I do not intend to lay blame, but rather to offer suggestions for improved public health strategies. By ignoring the overlaps between human health and the treatment of animals, we may fail to see solutions to critical public health problems. If we do not recognize this connection, where it exists, opportunities to tackle important health issues may be lost.

Aysha Akhtar, MD, MPH is a Fellow of the Oxford Centre for Animal Ethics and a neurologist and public health specialist with the Food and Drug Administration. She is the author of the book: Animals and Public Health: Why treating animals better is critical to human welfare. The opinions expressed here are those of the author and do not represent the official position of the U.S. Food and Drug Administration or the U.S. government.

",0.6505924287307975
BMC Public Health Designing and conducting tabletop exercises to assess public health preparedness for manmade and naturally occurring biological threats,"Background: Since 2001, state and local health departments in the United States (US) have accelerated efforts to prepare for high-impact public health emergencies. One component of these activities has been the development and conduct of exercise programs to assess capabilities, train staff and build relationships. This paper summarizes lessons learned from tabletop exercises about public health emergency preparedness and about the process of developing, conducting, and evaluating them.","Since 2001, state and local health departments in the US have accelerated efforts to prepare for bioterrorism and other high-impact public health emergencies. These activities have been spurred by federal funding and guidance from the US Centers for Disease Control and Prevention (CDC) and the Health Resources and Services Administration (HRSA) [1] [2] [3] . Over time, the emphasis of this guidance has expanded from bioterrorism to include ""terrorism and non-terrorism events, including infectious disease, environmental and occupational related emergencies"" [4] as well as pandemic influenza [5] .

For any locality, the rarity of major public health emergencies necessitates the use of practice-based exercises to simulate real life experiences in order to develop and improve skills and to assess response capabilities over time. The US Federal Emergency Management Agency (FEMA) describes six levels of exercises, increasing in complexity from informational seminars that minimally exercise response capacities to simulations that mimic reality and exercise participants' capacity to implement emergency response functions [6] .

Intermediate in this progression is the tabletop exercise, which FEMA describes as a ""facilitated group analysis of an emergency situation."" As practiced in public health, there is considerable variability in how tabletop exercises are designed and conducted. Tabletop exercises may be structured discussions of evolving events or unstructured reactions to short scenarios; participants may be limited to public health staff or involve representatives from partner agencies or organizations; scenarios may range from simple to complex; and facilitation may range from being minimally directive, allowing participants to assume responsibility for managing the discussion through ""role play,"" to highly directive, enabling the facilitator to assure that specific questions are addressed.

Recognizing the need to exercise public health emergency response, and enabled by funding and directives from CDC and HRSA, health departments throughout the US have implemented exercise programs. These exercise programs have had varying goals, including building relationships among stakeholders [7, 8] , training staff [9] [10] [11] , and evaluating preparedness levels [12, 13] , and they have been used for a variety of purposes, including to identify gaps in preparedness [14] , make recommendations for improving preparedness [15] , and identifying variations in preparedness across health departments [16] . These exercises have involved diverse groups of stakeholders involved in public health preparedness, such as representatives from public health [17] , health care [18] , agriculture [19] , and emergency medical services [20] . Despite the commonality of preparedness and response compo-nents across a variety of biological threats, most exercises have been designed for single use and focus on single disease, such as smallpox [21] , pandemic influenza [22] , or a novel virus [23] .

These exercises have focused attention on the interaction between preparedness goals and exercise strategies, and have illuminated strengths and vulnerabilities in public health emergency decision making and response capacities. The increased utilization of tabletop exercises by health departments has not been accompanied by a parallel increase in knowledge sharing about lessons learned from them, either with regard to identifying common challenges that confront health departments or strategies for effective exercise design and management. Further, the literature dealing with tabletop exercises to date consists almost entirely of case studies and descriptions of a single exercise or a single disease. This paper describes lessons learned by public health researchers at RAND, and their collaborators, about the process of developing and conducting tabletop exercises in collaboration with state and local health departments in the US and their implications for public health emergency preparedness.

Data for this paper come from four related projects conducted from 2003-2006. Taken together, these projects involved developing, conducting, and evaluating 31 tabletop exercises with state and local health departments of different sizes and structures in 13 different states across the northeast, south, mid-west, and west regions of the country (Table 1) . Participating health departments did not incur any expenses through their involvement in these exercises other than the staff time required to participate.

Two of these projects, one in California and the other in Georgia, involved the conduct of exercises in multiple jurisdictions in the same state. In California, the Little Hoover Commission, a bipartisan, independent state body, asked RAND to assess California's public health infrastructure. A key component of the project, described in greater detail elsewhere [16] , was the development of a tabletop exercise that simulated a smallpox outbreak. This exercise was conducted in seven local health departments across California. In Georgia, RAND collaborated with the Georgia Division of Public Health and the Rollins School of Public Health at Emory University to develop, conduct, and evaluate a series of tabletop exercises focusing on different biologic agents in seven local health departments across Georgia, as well as one exercise focused at the state level.

The two remaining projects were funded by US Department of Health and Human Services (HHS) and involved the participation of multiple local health departments.

The first project involved developing ten different tabletop exercise templates and formats focusing on the local public health response to bioterrorist agents. These were tested in 13 local health departments in 12 different states. The second project involved developing a tabletop exercise to examine the interface between local health departments and health care systems in a hypothetical influenza pandemic. This exercise was tested in three local health departments in different states. Greater detail on the structure for these tabletop exercises as well as the tabletop exercise templates themselves can be found elsewhere [7, 17] .

All exercises focused on at least one of three related objectives: training, relationship-building, and evaluation. The structure and design of the tabletop exercises varied from project to project because their objectives were somewhat different. The key domains covered are outlined in Table   2 . The level of facilitator involvement varied with the exercise objectives. At one extreme, the facilitator's role was limited to introducing the exercise scenario and periodically interjecting updates. During these exercises, the participants were encouraged to lead the discussion themselves, based on their respective roles in their agency or organization. At the other extreme, the facilitator took a very active role by leading the discussion and interjecting questions or prompts. In between were exercises in which the facilitator turned the discussion over to participants but occasionally joined the discussion to request clarifications from the participants or assure that issues critical to the exercise objectives were discussed.

Despite these differences, all of the exercises shared common elements, including: evolving hypothetical scenarios, facilitated group discussions, and some level of collective decision making by participants emphasizing *< 100,000 = small, 100,000-1,000,000 = medium, > 1,000,000 = large; **Mild involvement-most of exercises was role played by participants, with very little intervention or direction from facilitators; Moderate involvement-most of exercises was role played or issue discussion, with the facilitator inserting additional probes and ensuring the discussion stayed on track; Active-most of the exercise was more discussion based, with facilitator asking questions or identifying issues that were subsequently discussed.

the role of local health departments in recognizing and initiating a response to an emergency. The scenarios typically began with a single case report or series of case reports that heralded a nascent disease outbreak and required a public health assessment. These situations exercised the internal communication and coordination across disciplines within health departments as well as the communication and coordination with partner agencies and organizations such as health care facilities and emergency medical service agencies. Several exercises extended beyond this initial response and included scenarios that progressed days or weeks into an outbreak, requiring greater interactions between local-and state-level authorities and attention to health care surge capacity.

Every exercise concluded with a ""hot wash"" in which participants discussed their collective performance, identified strengths and weaknesses, and when relevant, related their performance to experience with actual outbreaks or crises. In the latter exercises, participants were prompted to develop an initial 'action plan' that addressed key vulnerabilities identified in the exercise. The facilitators subsequently generated a written ""After Action Report"" (AAR) that summarized the exercise experience and highlighted the observed strengths and areas for improvement. In addition, participants completed exercise evaluation forms. These consisted of a series of structured and semistructured questions that asked participants to discuss what they learned during the exercise and to evaluate aspects of the exercise structure and conduct. For example, participants were asked to identify key gaps in preparedness that occurred during the exercise and to identify the most useful thing they learned during the tabletop exercise. The observations reported here are based on reviewing the after action reports, participant evaluations, as well as internal team discussions and consensus following the exercise debriefings.

The performance of health departments that participated in our tabletop exercises varied from agency to agency. However, there were consistent themes that emerged across the agencies, regardless of the structure or the biologic agent/disease discussed; nearly all agencies struggled with a common set of challenges. These challenges, summarized in Table 3 and described below, represent critical dimensions of an outbreak response.

Many local health departments did not have a structured process for notifying or soliciting case reports from health care providers in the community other than those in hospitals, largely because they did not have reliable contact information for private providers or a sure means to reach them rapidly. In most instances local health departments had good relations with staff in local hospitals (e.g., emergency department staff and infection control practitioners) but did not appear to have similar working relationships with non-hospital based practitioners.

Local public health officials were sometimes unsure about their direct role in following up with suspected ill patients and collecting and shipping clinical samples for laboratory testing. For example, there was frequently confusion around whether it was the responsibility of the local health department, the state health department, or the medical personnel at the hospital to collect laboratory samples. Once the samples were collected there was often confusion around whose responsibility it was to transport the samples, and in a few sites, local law enforcement were surprised to find out that they were the responsible party. A related issue was the ability of health departments to realistically generate enough surge capacity in their public health workforce to investigate or respond to a large event, especially one that encompassed multiple jurisdictions in the same state, thereby limiting the state health department's ability to shift manpower and resources from one jurisdiction to the next.

Few of the health departments in which we conducted exercises were proactive in their contacts with the media, and most waited until they were contacted by the media to begin communicating with the public. One consequence of this passive approach was that public health officials often responded defensively to early and sometimes unexpected media requests and in turn, had trouble quickly formulating an initial message to the public that was clear, informative, and alleviated anxiety.

Health departments consistently expressed uncertainty about how to effectively communicate with vulnerable or underrepresented population groups in their jurisdictions, and few had well established relationships with community leaders or organizations that could serve as messengers or communications channels to these groups.

In several sites, law enforcement and EMS personnel present in exercises had greater familiarity with these groups and could help identify trusted community messengers. Further, in some communities, health departments had limited language capacities or were not sufficiently familiar with community leaders to communicate effectively with these groups.

Communicating fully and effectively with response partners (e.g., law enforcement and EMS) about their occupational health risks and personal protection was also a challenge for local public health. In particular, while public health officials were usually quick to notify response partners soon after determining an event to be significant, response partners in many cases felt that public health officials were slow to provide them with critical information about the disease in question, what their risks might be, or what actions they should take to protect themselves. As a result, response partners frequently reported feeling either left out of the process or expressed concerns about continuing to work unless the risks to them were clarified and more was done to ensure their safety on the job.

The use of the National Incident Management System (NIMS) and its associated Incident Command Structure (ICS) structure is relatively new to public health. This was evident in the exercises, in that nearly all health departments had difficulties deciding if and when to implement the ICS process and in identifying the party who would serve as incident commander. Similar challenges were seen in the decision and processes related to opening an Emergency Operations Center (EOC). As a result, in many exercises, local public health officials delayed taking these steps and preferred maintaining a more informal management process. This approach was preferred even as the outbreak became progressively larger, thereby stressing these informal networks.

As outbreaks evolved, there was often a lack of clarity about whether and when local health departments should hand off control to the state health department, how responsibilities should be jointly shared between local and state authorities, and whether or when federal agencies, such as CDC, should become involved. In many of the exercises, state health departments were surprised by the level of assistance requested by their local health departments especially in the early stages of the outbreak; in other more rare examples, state health departments surprised local health departments by assuming roles and responsibilities local health departments regarded as their own. Regardless, the general consensus among local public health participants in most exercises was that CDC staff would be on the ground to help them fairly quickly, particularly in situations where bioterrorism was considered likely.

Most local health departments articulated some type of plan for increasing medical surge capacity by developing alternative care sites. In most instances however, these plans were unable to hold up to even a modest amount of scrutiny during the exercise because they were superficial and lacked sufficient detail necessary for rapid implementation. Related to this issue, local health departments frequently reported that there were not enough local health care workers to manage these sites even if they could create them. For example one participant noted, ""We have pop-up tents and beds to increase capacity, we just don't have pop-up people to staff them.""

Even obtaining a census of available staff members turned out to be challenging as many health care participants noted that some staff would likely be double-counted, particularly nurses and security officers who might work in several institutions. Increasing staff capacity through the use of community volunteers, including retired medical personnel, while often recognized as one potential solution to staffing shortages, proved to be extremely difficult to actually implement. Public health participants universally recognized the importance of volunteers, but learned that their plans to recruit, train, and mobilize large numbers of volunteers were too vague and lacked concrete actionable steps for realistic application during a real emergency.

In nearly all exercises, we also identified a number of strengths within participating health departments. However, there was far less commonality in these strengths than we observed with the areas for improvement. Universally, we observed public health leaders and staff who were committed and struggling to 'do the right thing.' The most commonly observed strengths were strong relationships between epidemiologists and hospital infection control practitioners and between public health workers and other emergency coordinators. In some instances, prior experience with emergency planning or response, such as involvement of health departments and emergency service agencies in coastal areas in preparing for or responding to hurricanes, was associated with stronger and more facile interactions between health department officials and partner agencies.

Our exercises were conducted over a period of several years. While we did not conduct exercises with any health department more than once and did not employ an experimental design to assess changes over time, we were struck by how the performance of health departments overall improved over time. First, compared to earlier exercises, local health departments appeared far more sophisticated about their early internal processes related to notification, enhanced surveillance and large outbreak investigations. In addition, by the end of the exercise period, health departments had considered plans for surge capacity, and participating hospitals had explicit plans for cancelling emergency surgery and discharging less severely ill patients. They also appeared more acutely aware of the challenges in assuring adequate numbers of staff to provide care.

The large number of tabletop exercises we conducted allowed us to test and compare different strategies for designing and conducting tabletop exercises. These comparisons enabled us to modify our exercises over time to build upon lessons learned from previous exercises. Below we briefly highlight five lessons we learned from this experience.

Exercises should be designed to achieve a specific objective When first developing the tabletop exercises, our assumption was that a single exercise could achieve multiple objectives, such as training, relationship building, and evaluation. While these objectives are interrelated and opportunities often exist to achieve them concomitantly in the same exercise, it is critical to define the priority objective for the exercise because different objectives have different implications for exercise design. For example, if exercise participants outlined a response that was flawed or problematic, in an exercise primarily focused around the objective of training it would be appropriate for the facilitator to pause and help the participants re-think their approach. On the other hand, if the objective of the exercise is evaluation, this type of facilitator involvement can lead the participants to choose a different course of action and therefore bias the overall outcome being evaluated. Taken further, in an exercise designed to build relationships and links across disciplines or agencies, a facilitator intervention implying that a participant had made a mistake could be embarrassing or diminish that person's credibility, depending on the level of trust among participants.

Exercises should be as realistic as possible while remaining logistically feasible Taken together, the optimal mix of design elements represents a balance between exercise objectives and logistic feasibility. The ideal balance is one that assures sufficient realism to provide a meaningful experience while minimizing distractions associated with the necessary artifice of exercise scenarios. Some departures from reality may be inadvertent if scenarios are developed with insufficient attention to local routines, forcing participants to sidestep usual procedures. Even seemingly minor design errors, such as using an outdated name for a hospital, or a time course for a disease that is inconsistent with its known epidemiology, can undermine the credibility of the exercise and can distract participants enough to take them out of their roles, thus disrupting the flow of the exercise.

The desire for a realistic exercise scenario can lead to the development of tabletop exercises around scenarios rather than issue areas based on local preparedness needs and priorities. This does not ensure that the participants will address the important issue areas. A broad mix of challenges related to a given scenario must be addressed, often simultaneously, ranging from conducting epidemiologic or environmental investigations, implementing and modifying interventions as information becomes available, communicating within and across agencies, and communicating with political leaders and the public. Introducing this full set of tasks into an exercise scenario in a way that meaningfully exercises relevant capacities is unlikely to align with the exercise's objective. Moreover, different stakeholders may want to address different issue areas and may become frustrated if their expectations are not met. It is therefore important for stakeholders to agree on a limited number of priority issue areas for the exercise and then to focus the design of the scenario around these areas.

For example, one set of exercises we designed focused on pandemic influenza preparedness in local health departments. Because it was infeasible to exercise the entire pandemic plan around a single scenario and in a single exercise, we developed the scenario and then the exercise by first meeting with local stakeholders to decide on the issue areas that would be covered in the exercise. These issue areas included disease surveillance, medical surge capacity, non-pharmacological disease control, and the use of antiviral medications. The scenario was then customized to unfold to deal with each of these issue areas.

Key decisions for each discussion point were then developed, as well as facilitator probes and instructions based on the specific objective of the exercise.

If not designed or facilitated properly, an exercise can lack focus and resolution, leaving participants to wonder what exactly was accomplished during the exercise. Therefore, it is paramount that exercises are designed to focus on issue areas that require concrete decisions over a limited period of time. For example, an exercise dealing with a simulated smallpox outbreak that unfolds over time might involve a discussion period dealing with movement restrictions in which participants are asked one or more questions such as, ""Should schools be closed at this point?"" Participants should then be given a limited amount of time to discuss this issue and make a decision. It is the facilitator's job to keep the discussion focused on the issue area and the specific question(s) at hand and to ensure that at the end of discussion, participants have collectively made the decision(s) they were tasked to make. Exercises can be designed to have multiple such issue area discussions as the scenario unfolds.

Depending on the goals and objectives of an exercise, an exercise can involve a narrow or wide range of potential participants. While broader inclusion would likely be more realistic, such inclusiveness needs to be weighed against the logistics of effectively managing a larger number of participants and the potential adverse effects of inclusion. For example, participants may be less comfortable discussing ideas, taking risks, or making mistakes, depending on who is in the room. Such constraints may impede the exercise process and undermine achievement of exercise objectives.

One solution to this problem would be to sequentially stage the involvement of different participants or to physically separate different groups in a way that more closely mimics actual situations. For example, some conversations that involve airing uncertainties or weighing difficult alternatives may normally involve a limited group of people, and members of that group may be more comfortable exercising such a conversation apart from colleagues from other agencies or organizations.

The disadvantage of this approach is that it is substantially more difficult logistically and it diminishes the opportunity for people from different groups to gain an understanding of one another's role and approach to problems. In those exercises where certain participants, notably law enforcement, joined the scenarios at different stages, the feedback was generally critical, and participants felt that staging participation diminished learning and teambuilding opportunities. Another solution is to split exercise participants into two or more groups that allow everyone to participate, often placing people at similar levels of responsibility in the same group, and to conclude the exercise with a session that brings everyone together to share what they learned.

The exercises described in this report represented collaborations between people familiar with local circumstances and people from outside the participating jurisdictions who had expertise in exercise design and facilitation. Because we did not test an alternative approach that exclusively involved local personnel, it is difficult to generalize from this experience about the value of engaging people from outside the participating agencies. Nonetheless, it was our impression that at certain points in the development, facilitation, and feedback steps, there was value in involving people who were not personally invested in local relationships or situations and who could offer seemingly independent advice or perspectives.

Tabletop exercises can provide useful insights into both strengths and vulnerabilities in public health preparedness. It is important to recognize, however, that exercise outcomes are influenced by the way they are designed and conducted. The exercises described in this report emphasized varying dimensions of public health preparedness, reflecting differences in state or local priorities for prioritizing exercise objectives. For example, some emphasized the early response to initial reports of suspect illness while others emphasized management of surges in demand for health care services that are likely to occur later emergency scenarios. Given the intellectual and emotional demands of participation in an exercise, participants (and facilitators) may be less energetic during later rather than earlier stages of an exercise scenario, affecting perceived capacity to execute different elements of a response. Potential gaps between observed and actual preparedness should be considered in interpreting after-action reports and evaluating exercises themselves.

The utility of tabletop exercises as tools to identify areas for improvement and make improvements on these problems is still evolving. Our ability to evaluate exercise performance is hampered by the lack of an evidence base about what constitutes optimal performance and by the lack of standards for assessing public health preparedness. There is a need to move beyond qualitative performance measures to ones that are quantifiable and can be measured over time. These quantifiable measures can range from simple checklists to Likert rating scales to scorecards. For example, in a series of our exercises we used checklists to assess the performance of health departments related to surveillance, risk communication and other functions.

One fairly consistent observation was that health departments identified gaps that had been identified in prior exercises or actual experience, but had not yet been addressed. Reasons for this included lack of time, and lack of knowledge about how to make change. We now conclude exercises by having health departments prioritize the challenges observed during the exercise and then have them develop initial action plans related to up to three priority items.

There are important limitations to our work and its interpretation that must be recognized. First, the nature of our exercises changed over time on a number of important dimensions, including the scenario, priority objectives, facilitation, exercise designers and facilitators, and attention to beginning an action plan after the hot wash. As a result of this variation, we are unable to provide a numerical tabulation of the numbers of health departments that struggled with each gap or displayed given strengths. Second, because we did not employ a methodology that could conclusively assess change over time, we cannot be certain that the improvements we identified were truly reflective of improvement, and not due to the inclusion of more sophisticated health departments in the latter part of our exercise period. We doubt this is the case, however, given the national emphasis on preparedness and planning and the ways in which health departments participating in later years qualitatively described their improvement. Furthermore, similar observations regarding improvements in public health preparedness during the same time period have recently been reported by others [24, 25] . We also cannot asses the potential influences that external events (e.g., hurricanes, outbreaks) may have had on health departments during the time period of our work, but it is noteworthy that all exercises were concluded before Hurricane Katrina struck. In addition, our exercises were not conducted in a random sample of health departments, and the findings may not be generalizable to all health departments. Finally, as discussed above, the evidence base for determining best practices in the design and conduct of exercises is extremely thin. We share our experience in the hope that it will help others, but do not propose that our recommendations constitute best, proven practices.

Developing, conducting, and evaluating tabletop exercises requires considerable planning and the perspectives of a variety of stakeholders. While these tabletop exercises identified both strengths and vulnerabilities in emergency preparedness, additional work is needed to develop reliable metrics to gauge exercise performance, inform followup action steps, and to develop re-evaluation exercise designs that assess the impact of post-exercise interventions.

",0.6493853538791636
Excess drug prescriptions during influenza and RSV seasons in the Netherlands: Potential implications for extended influenza vaccination,"Influenza and respiratory syncytial virus (RSV) infections are responsible for considerable morbidity, mortality and health-care resource use. For the Netherlands, we estimated age and risk-group specific numbers of antibiotics, otologicals and cardiovascular prescriptions per 10,000 person-years during periods with elevated activity of influenza or RSV, and compared these with peri-season rates. Data were taken from the University of Groningen in-house prescription database (www.iadb.nl) and virological surveillance for the period 1998-2006. During influenza and RSV periods excess antibiotic prescriptions were estimated for all age groups. In the age groups 0-1 and 2-4 years, excess antibiotic prescriptions during periods with elevated RSV activity (65% and 59% of peri-seasonal rates) exceeded the surpluses estimated during the influenza-activity periods (24% and 34% of peri-seasonal rates) while for otologicals excess prescriptions were higher for influenza (22% and 27%) than for RSV (14% and 17%). Among persons of 50 years and older, notably those without medical high-risk conditions, excess prescriptions for cardiovascular medications were estimated during the influenza periods at approximately 10% (this was also already seen in persons aged 45-49). Our results may have implications for influenza vaccination policies. In particular, extension of influenza vaccination to groups of non-elderly adults and young children may lower excess prescriptions during these influenza periods for all three types of drug prescriptions investigated.","In many countries annual influenza vaccination has been recommended for the elderly and persons with high-risk medical conditions. In the Netherlands until 2007, the age-threshold for such vaccination was 65 years for non-high-risk groups. In 2008, this threshold was lowered to include all persons aged 60 years and older [1] . In addition in the Netherlands, all high-risk groups are vaccinated, including individuals suffering from chronic conditions, respiratory diseases, cardiac diseases, diabetes mellitus, renal failure, those being immunocompromised and individuals aged less than 18 years of age on chronic salicylates use [1] . Vaccination rates among these high-risk groups under 65 years of age ranged from 66% to 83% in the Netherlands in 2005 [2] . Persons of 65 years and older with a medical indication showed higher vaccination coverages compared to those of the same age without such an indication, at 90% and 76% in 2005, respectively [2] . Influenza may cause acute bronchitis and pneumonia in high-risk groups and elderly and may lead to exacerbations of underlying chronic medical conditions such as cardiovascular diseases, asthma and diabetes, potentially leading to hospitalisations and death. Prevention of influenza infection by vaccination is of high importance for these groups with an increased risk for complications from influenza infection [3] [4] [5] .

Although some countries recommend routine influenza vaccination among children aged 6 months to 2 years, clinical data about the impact of vaccination are limited [1, 6] . Yet, vaccination of all healthy children in this age group could be cost-effective or even cost-saving for some societal settings [6] . In the Netherlands, universal influenza vaccination of such children is in debate, however yet the vaccine efficacy and effectiveness for this specific age group is not considered to be sufficiently demonstrated [7] [8] [9] . Therefore, the Dutch Health Council concluded in 2007 , not yet to start with influenza vaccination of these groups [1] . Comparable conclusions were made for healthy children aged 2 years and older. Although vaccination was shown to be effective for these children, influenza was considered not to cause serious morbidity or mortality in the Netherlands in those groups [1] .

Next to influenza virus A and B, respiratory syncytial virus (RSV) has been shown to cause similar types of complications, including complications in the respiratory tract. RSV is often recognized as a cause of morbidity and mortality among both children and adults, contributing to a major burden of illness [10] [11] [12] [13] [14] [15] [16] [17] . No preventive vaccine is yet marketed for RSV.

In general, it is difficult to estimate the individual contributions of influenza and RSV to the aforementioned disease burdens accurately as influenza and RSV co-circulate during winter seasons. For burden of disease often the label Influenza Like Illnesses (ILI) is considered to primarily comprise disease due to influenza infections, however known to potentially comprise various other agents inclusive RSV, with the identification of the relative contributions of both viruses being strongly hampered [18] [19] [20] [21] . In particular, even further viral agents, such as adeno-, parainfluenza-, and corona-viruses may contribute to the burden of ILI [21, 22] . For exactly analysing the burdens of disease of influenza and RSV separately, which is the goal of our current paper, ILI is therefore an inappropriate concept. Therefore, we chose to analyse isolates exactly related to the respective causal agents.

Given the potential complications of influenza and RSV, one may expect excess health-care resource utilization during epidemics [23] . Studies estimating such excess resource utilization have previously been directed to hospital admissions and GP-visits, but there is as yet hardly any information on the association between the occurrence of influenza and RSV and the use of medications among the general population [24] . Drug-use for specific complications of influenza and RSV could certainly temporarily be elevated. In particular, prescriptions for antibiotics, otologicals and cardiovascular medication may be elevated, dispensed for otitis media, cardiac complications, respiratory illness and other pulmonary complications. To address this topic, we investigated the association between weekly reported RSV and influenza isolates in the Netherlands and the weekly number of prescription drugs dispensed by Dutch pharmacies, in particular those for antibiotics, otologicals and cardiovascular drugs.

From the preventive point of view, high influenza-associated drug-use may justify more extended use of preventive measures, such as influenza vaccination. In particular, it may enhance the health-economic profile of extended influenza vaccination for currently yet unvaccinated groups. As such, our research may contribute to discussions as to whether, for example, young children should be vaccinated or whether the age limit of the vaccination program should be lowered further.

During 1998-2006, the Dutch Working Group on Clinical Virology gathered data from 20 laboratories throughout the Netherlands, testing clinical specimens for respiratory viruses, including RSV and influenza A and B. The number of patients who tested positive for RSV or influenza was reported on a weekly basis. In the absence of any drastic changes in recruiting and testing of specimens the weekly time series can validly be conceived as representing the actual time trend, without requiring any corrections to be applied on the data. To exclude weekly random fluctuations a 3-week moving average was used for presentation ( Fig. 1) and for defining the specific weeks exhibiting elevated activity for influenza and RSV.

Two methods for defining weeks with elevated activity of influenza and RSV were applied. The first method defined a week with elevated activity as one with a moving average of more than two times the gross overall average weekly number from week 40 in 1998 onwards to week 39 of 2006 (week 1 obviously being the 1st week in January; week 40 generally being considered as the start of the influenza season). Following Jansen et al., the second method defined the weeks with elevated activity as those weeks from any period of at least 2 consecutive weeks, with each individual week accounting for over 5% of the season's total number of influenzaor RSV-positive specimens [18] . From both methods, it appeared that periods consisting of a number of subsequent weeks resulted, rather than individual weeks or short periods of, for example, 2 weeks only. Such periods would be expected -and were indeed found -to be between week 40 and week 20 in the next year (the non-summer season) [18, 25] .

The weeks that contained both influenza and RSV activity were excluded for defining the periods with elevated activity, given the difficulty to separate out the individual influences in these weeks with combined activity. As a result, only those weeks with either elevated influenza or RSV were analyzed. Furthermore, a periseason was defined, containing the weeks from week 40 up to and including week 20 of the next year, which did not belong to the influenza or the RSV periods (seasons). The weeks from week 21 up to and including week 39 were labelled ""summer"".

Information on drug-use in the population was provided by a University of Groningen in-house prescription database (www.iadb.nl). The database iadb.nl contains prescription, demographic and population data of 500,000 persons adherent to 50 pharmacies in the North and East of the Netherlands. For analytical purposes, the prescriptions were divided over 5-year age categories. The age category 0-4 years was further divided in the ages 0-1 and 2-4, to enable analysis of infants separately. For presentation the following categorization was applied: 0-1, 2-4, 5-19, 20-49, 50-54, 55-59, 60-64 and 65+. The exact age of any person was determined every year on the 1st of October, close to the period in which in the Netherlands the invitations for influenza vaccination for risk groups are sent out by the GPs, supposedly just prior to the season with increased risk for influenza epidemics from October onwards to May. The annual total population sizes were based on estimates for the 1st of January by the local authorities in the places where the pharmacies are located.

Persons who received one of the studied drugs were divided in high-risk and low-risk groups for influenza. The respective populations belonging to both groups were estimated using prescriptions as a proxy, for those medications that are consistent with highrisk indications as specified by the Dutch Health Council [1] and the Dutch GPs (http://nhg.artsennet.nl) and that are uniquely prescribed for these indications [26] . For example, dornase alfa, a drug prescribed for cystic fibrosis, was included to define persons with respiratory diseases. Other medications for cystic fibrosis, such as acetylcysteine, are also prescribed for cough and were therefore not included. Table 1 lists these drugs specifically. For heart medication, diuretic sulfonamides were included. Other diuretics used for lowering blood pressure were excluded, as high-blood pressure is not a high-risk identified condition. Furthermore, only calcium antagonists with ATC-code C08D were included as only these are assumed to have cardiac effects. Beta blockers and RAAS system agents were excluded as these are not exclusively prescribed for cardiac diseases, but also, for example, for high-blood pressure only. For renal diseases, medication for the treatment of hyperkalemia and hyperphosphatemia, and antianemic preparations and sulfonamides were included as these are often prescribed for dialysis or renal insufficiency. To identify immunocompromised patients, immunosuppressives were included, which can be prescribed, for example, for patients after organ transplantation. Also HIV medications were included as also HIV-patients are listed as high-risk group.

As mentioned, in the Netherlands, influenza vaccination is recommended to those at the increased risk of complications based on specific medical conditions. To determine which specific patients should be labelled as belonging to the high-risk group, the recommendations of the Dutch Health Council [1] and the guidelines of the Dutch GPs (http://nhg.artsennet.nl, accessed 24th September 2007) for influenza and high-risk indications were used [26] . In particular, persons were labelled belonging to the high-risk group if they had two or more prescriptions from the same group of medications included on the list, on two different dates in the year before the first of October. The latter was supposed to guarantee that the condition would be chronic in that specific year (note that a person's risk status may change from year to year). The population size of the high-risk group was subsequently determined on the 1st of October by counting the number of high-risk persons in iadb.nl; the rest of the population was assumed at low risk.

We specifically directed our analysis to the drug groups of antibiotics, otologicals and cardiovascular drugs, as these drugs may be considered for those complications of both viral infections that have yet been published in the scientific literature [23, 24, [27] [28] [29] . From iadb.nl, prescriptions of antibiotics (ATC-code J01), otologicals (ATC-code S02) and medications for the cardiovascular system (ATC-code C) were selected.

Antibiotics are commonly prescribed for the treatment of acute otitis media (AOM), in particular for young children in which AOM accounts for approximately half of all antibiotics courses delivered [30] [31] [32] . Otologicals may also be prescribed for AOM, despite that they are not recommended by the Dutch College of General Practitioners (NHG) [29] . In addition, more than average numbers of antibiotics may be prescribed for elderly persons with ILI during periods with elevated activity, as particularly this group may develop acute respiratory illnesses (for example, pneumonia) as a complication of the viral infection (trimethoprim and nitrofurantoin were excluded from the analysis as they are prescribed primarily for urinary tract infections) [33] . For antibiotics and otologicals both initial and next prescriptions within the same year were considered. It is well known that influenza-related complications are more prevalent among persons with cardiovascular and other chronic diseases than in persons without such underlying conditions [4, 34] . In persons of 65 years and older with high-risk conditions an increased rate of hospitalisation for cardiac problems has been reported [17] . Some further recent studies suggest that there might also be an association between cardiovascular problems and influenza epidemics among groups without any cardiovascular history yet, such as the elderly or even among those aged below 65 years of age [23, 35] . For analysing cardiovascular medications, only first prescriptions were considered as our current interest was to investigate whether the viral infections were related to new cardiovascular disease and/or exacerbations of existing -yet untreatedbackground cardiovascular conditions, rather than identify chronic medications for cardiovascular diseases (note that chronic cardiovascular medication use was used as a criterion for assigning persons to the high-risk group). A prescription was defined as a first prescription when a person had not had a prescription for the same drug or a drug from the same subgroup in the year before that specific prescription.

After the various periods of elevated activity were determined, the number of prescriptions in the weeks belonging to the influenza and RSV periods/seasons were compared to the number of prescriptions in the peri-season. For comparative purposes also results for comparing with the summer season are presented. This was done in every age group for the total population, the high-risk group and the low-risk group. Incidence rates were calculated and corresponding 95%-confidence intervals were estimated [36] . The incidence rate (I) was calculated as the weekly number of prescriptions per 10,000 person-years:

with P i being the number of prescriptions in week i of a season (influenza, RSV, peri or summer), n the number of weeks of that specific period of interest and N the population size (per age group, for high risk or for low risk). The division by 52 is to transfer personweek estimates into person-years (for 2 years 53 instead of 52 had to be used). Microsoft Office Excel 2003 was used for processing the data, calculations and graphics.

During the study period, the average weekly number of influenza-positive and RSV-positive specimens were 11.77 and 36.90, respectively. The first method for defining the periods of elevated activity using two times the average number of isolates per week as a lower limit (23.54 for influenza and 73.80 for RSV) resulted in Table 2 , showing the weeks which were labelled as belonging to the influenza, RSV, peri-and summer periods.

The second method, defining the periods with elevated activity as at least two consecutive weeks in which each week accounted for over 5% of the season's total number of RSV-or influenza-positive specimens yielded comparable results (data not shown). Hereafter, only for those results where both methods differed, the results of both methods are presented otherwise the results of only the first method are shown.

Obviously, periods with elevated activity and the peri-season changed from year to year, both in length (the summer season of course always ranged from week 21 up to and including week 39 of the next year). In some years overlap in influenza and RSV activity weeks were seen, in which case weeks were excluded. Table 3 Person-years during periods of elevated activity per age group for the total population, high-risk group and low-risk group. Table 4 Excess drug prescriptions for the periods with elevated activity compared to the peri-season, shown as numbers of prescriptions per 10,000 person-years (as % of peri-seasonal levels). The number of persons was insufficient for valid estimation. a Result not statistically significant. b Although borderline significant not shown here for the whole age-group as further 5-year age-group specific analyses revealed that significance was related only to a significant and clinically relevant surplus for the age category 45-49: 113.12 (12.95%) and 122.78 (17.06%) for the total population and low-risk group, respectively. Table 3 presents the number of person-years for the influenza, RSV, peri-season and summer periods per age and risk group. For infants and children aged 2-4, the high-risk group was very small with 393 person-years in the influenza period and 454 personyears for the RSV period, and therefore only the figures for both low-and high-risk groups taken together were used for these ages. Furthermore, in all age groups the number of person-years in the high-risk group was lower than the numbers in the low-risk group. We also note, as expected, that the older age groups contained relatively more person-years in the high-risk group than the younger age groups. Fig. 1 shows the number of positive findings for RSV and influenza per week. The annual influenza and RSV epidemics are clearly seen, as is the overlap in some years. Also the number of (first) prescriptions per 10,000 persons for antibiotics, otologicals and cardiovascular medication is plotted for the total population. In particular, for antibiotics a clear pattern is visible in which during influenza and RSV activity periods a peak in antibiotic use occurs. Additionally, we notice that in the younger age groups amoxicillin was the mostly prescribed antibiotic, whereas from the age group 5-19 onwards other antibiotics were mostly prescribed (data not shown).

The excess prescriptions per 10,000 person-years during the influenza and RSV periods are presented in Table 4 , as compared to the peri-season. We noted that both methods used for estimating the activity periods showed similar results.

The number of prescriptions for antibiotics was significantly elevated during the activity periods in each age group. For the age groups 0-1 and 2-4, excess prescriptions were highest in the RSV periods, in the older age groups surpluses were higher during the influenza periods. For influenza, the low-risk group showed higher excess prescriptions compared to the high-risk group. Prescription of otologicals was significantly elevated during the periods of elevated activity, however only in the youngest age groups of infants and children aged 2-4 years. In contrast to antibiotics, excess prescriptions were higher during influenza periods than during RSV periods.

For first cardiovascular medications, the prescription rate in the RSV periods was lower than in the peri-season, although not significant. During the influenza period, higher prescription rates were found for those aged 50 years and older. When the age group of 20-49 was analyzed in 5-year age categories separately, also a significant difference for the age category 45-49 was seen. Fig. 2 shows the prescription rates per 10,000 person-years and confidence intervals for selected aggregated age groups for the different periods per year separately, as well as for the aggregated years. The incidence rates of antibiotics were increased during the influenza and the RSV periods in comparison with the peri-season (and the summer season). This increase was noticeable for every year. For otologicals, the incidence rate was increased but the confidence intervals were wide and therefore not every year rendered a statistically significant difference during the influenza and RSV periods. For first cardiovascular prescriptions also an increase was noticeable during influenza periods as compared to the peri-season, but again not for every year a significant difference was found. Finally, we note from Fig. 1 that elevations in the influenza and RSV periods are not necessarily followed by relatively lower levels in the peri-and/or summer seasons, suggesting that surpluses detected are actual extra prescriptions that are not ""neutralized"" by subsequent dips in prescriptions.

Statistically significant excess antibiotic prescriptions during periods with elevated activity of influenza and RSV were found for both viruses in all age groups, each year investigated and irrespective of the method used for exactly defining the influenza or RSV activity periods. For otologicals during both influenza and RSV-active periods, statistically significant surpluses were found in young children only. Oppositely, excess cardiovascular drug prescriptions were identified in adults and elderly in periods with elevated influenza activity.

We generally found a tendency for higher percentages of surpluses in low-risk groups than in high-risk groups. For antibiotics and cardiovascular drugs, this can probably be explained by the fact that individuals belonging to a high-risk group have a higher likelihood of being vaccinated against influenza, lowering the chance of infection and secondary bacterial or cardiovascular complications. This tendency also applied to low-risk elderly as compared to highrisk elderly, despite the fact that this whole group is recommended for vaccination. Some choices in our research should be noted. Specifically, the lower limit for the weekly moving-averaged number of isolates for weeks to be labelled as active was chosen by two different methods. The first used a cut-off of two times the average number of influenza or RSV isolates per week. This limit was chosen to achieve continuous periods per year of weeks subsequently labelled as active, i.e. to guarantee epidemic periods rather than fluctuations. The second method was used to be in concordance with a previously performed Dutch study using the weeks which accounted for 5% or more of the season's total number of influenza or RSV isolates [18] . In general, both methods resulted in similar results.

By using the influenza and RSV activity periods that excluded weeks of combined activity, possible major influence by the presence of the respective other virus was reduced. However, the exclusion of weeks in which both influenza and RS viruses were active was only necessary in 4 out of the years included in this analysis. Also, in methodology excluded weeks were not included in counting person-years, so the effect of excluding those weeks maybe limited. Yet, possible influence of any other respiratory virus obviously remains present. However, the impact of these viruses is probably limited, as they may have long periods of marginally increased activity rather than a clear seasonal pattern [18] . Furthermore, complications are expected to be milder compared to influenza and RSV infection [18] .

We compared our data for the activity periods with the periseason, which provided a more conservative estimation of the surpluses than if we would have compared with the summer period. Yet, some underestimation of the surpluses may be introduced in this way. We do feel, however, that comparison with the peri-season is most appropriate, as other potential influences concerning the climate and possible other viruses that circulate in non-summer periods are probably comparable between the peri-season and the activity periods.

Every year, RSV-positive specimens reached a relatively small and intensive peak around week 52. Earlier studies have shown similar tendencies, with RSV isolates peaking every year around the same time [14, 15, 37] . This suggests the presence of potential common strict seasonal factors which might increase both the number of isolates and prescriptions; however probably not invalidating the associations and surpluses found in our analyses, which are truly seen and are in line with other studies [14, 15, 37] .

Persons were labelled as belonging to the high-risk group based on specific medication profiles. In particular, persons belonging to the high-risk group were selected based on prescriptions that corresponded rather uniquely to the high-risk indications. Prescriptions potentially meant for other indications, not labelled as high risk, were consistently excluded. Still, it is possible that these selection criteria unjustly labelled individuals as belonging to the high-risk groups. Also, some individuals may have been incorrectly excluded and labelled as non-high-risk.

Finally, one may hypothesize that excess prescriptions are merely shifts in time of extra prescriptions later to be outweighed by dips in prescriptions (sometimes referred to as ""the harvesting effect""). Visual inspection of our data however did not give any reason to support this hypothesis in our study. Additionally, a formal statistical comparison of the number of prescriptions during the peri-seasons and during the first 5 weeks after the active seasons did not show any peak-dip pattern (data not shown).

Previously, various investigations have been performed on the association between influenza and RSV epidemics, on the one hand, and hospitalisations, mortality and outpatient visits, on the other [14, 17, 18, 24, 33, [37] [38] [39] [40] [41] [42] [43] [44] . Below, we briefly compare the outcomes, knowing that the validity of making such comparisons between studies is limited due to differences in outcome measurement, statistical models, study period, and health-care system concerned.

Previously, only one study investigated excess antibiotic use during influenza-activity periods in the general population [24] . This study did show that otherwise healthy children get more prescriptions for antibiotics during these influenza periods. However, the surplus reported was relatively low compared to our findings [24] . Two other studies estimated the excess antibiotic prescriptions during both RSV and influenza-active seasons, focussing on specific target groups [33, 43] . The first study focussed on patients suffering from chronic lung disease, showing the highest surpluses for the youngest age groups [43] . The latter study showed higher surpluses due to influenza as compared to RSV for those living in nursing homes [33] . Both results are in line with our findings for antibiotics [33, 43] .

Several other studies indicate that, in general, both during RSV and influenza-activity periods, infants and elderly show the highest morbidity and mortality rates [14, 18, 24, 37, [39] [40] [41] [42] [43] [44] . This is certainly in line with our findings on excess antibiotic and otological prescriptions among the youngest age groups and cardiovascular medication surpluses among the older age groups. In contrast to other studies which showed higher morbidity and mortality among the oldest age groups compared with non-elderly adults, our study shows that the elevation in the prescriptions of antibiotics in the oldest age groups (65 years of age and older) is not higher than in the two younger age groups (55-59 and 60-64). Yet, if compared with other adult age groups, a small increase could be seen. For influenza, this slightly deviating finding compared to non-Dutch settings, might be explained by the high vaccination coverage among elderly in the Netherlands.

A recent Dutch study showed that RSV-related excess hospitalisations were considerably higher as compared to those due to influenza [18] . Comparably, a study performed for England and Wales showed greater excess rates for complications during RSVactive periods among the youngest age groups as compared to influenza; similar rates were found for all other age groups [37] . Five other studies focussing on excess morbidity, mortality and hospitalisation among children confirmed these results, showing that RSV was responsible for higher hospitalisation rates than influenza [14, [39] [40] [41] [42] .

For the Netherlands, Jansen et al. recently showed excess hospitalisation for cardiovascular complications among the 50-64 years old, during influenza-active periods but not during RSV-active periods [18] . In line with their findings, we showed a significant surplus in first cardiovascular medication prescriptions during influenzaactive periods, but not during RSV-active periods for those aged 45 years and over. Elevated hospitalisation and prescription rates during influenza periods in persons aged around 50 years and beyond suggests that influenza may cause cardiovascular diseases or that it may aggravate existing non-diagnosed cardiovascular diseases in older adults. Another hypothesis explaining this increase might be that increased cardiovascular problems during influenza periods are related to the increased use of analgesics during those periods to alleviate influenza symptoms [45] . Further research is definitely needed into this topic.

Thus, in general, our results seem to be comparable with most other studies relating elevated viral activity to the use of health-care resources, morbidity and mortality. All studies consistently show that the highest excess rates for the youngest age groups are mostly due to RSV, whereas those for influenza are seen in elderly.

Vaccination may prevent part of the excess prescriptions we have found. For example, healthy children are not recommended to be vaccinated against influenza in the Netherlands, while their vaccination might prevent part of the surplus prescriptions found for this group. In particular, vaccination may prevent influenza infection and potential subsequent bacterial super infection(s) and thus avert antibiotics prescribed for the prevention and treatment of such bacterial super infections.

Additionally, reducing the prescription of antibiotics may also be important from the perspective of limiting the development of antibiotic resistance. Such reasoning could be an additional motivation for vaccinating yet uncovered groups against influenza [23] . An effective vaccine against RSV may potentially even prevent more antibiotic prescriptions, especially in young children [37] . Yet the introduction of a vaccine again RSV is not expected in the very near future [18] .

During influenza-and RSV-active periods, elevations in antibiotic prescriptions were identified in all age groups. For otologicals, such an elevation was shown in the age groups of 0-1 and 2-4 years, both during influenza-and RSV-active seasons. By vaccinating young children against influenza, a part of these prescriptions for antibiotics and otologicals may be prevented.

In persons of 50 years and older an elevation of prescriptions for cardiovascular medication was shown during the period of elevated influenza activity only, in particular for the low-risk population. Also for antibiotic prescriptions, the excess found was higher in the low-risk population than in the high-risk population, possibly indicating the effectiveness of the vaccination program in the highrisk group, in which a relatively high coverage rate is reached in the Netherlands.

",0.6490757620234886
A loophole in international quarantine procedures disclosed during the SARS crisis,"This study describes a loophole in the international quarantine system during the recent Asian severe acute respiratory syndrome (SARS) outbreak. Specifically, that of travelers disguising symptoms of respiratory tract infection at international airports, in order to board aircraft to return to their home countriesnotwithstanding the infection risks this involves to others. High medical fees for treatment to non-residents in epidemic areas were found to be the main cause for this behaviour. This phenomenon revealed a loophole in the control mechanisms of international quarantine procedures, letting travelers carrying a highly contagious virus slip by undetected and causing possible multi-country outbreaks of communicable diseases. Clinical evidence collected from medical records at medical centers can highlight this oversight. q","From November 2002 onwards, the severe acute respiratory syndrome (SARS) had spread rapidly via international air travel to at least 30 countries. As of December 31, 2003, reported cases had numbered 8096 with 774 deaths. Reasons for its rapid global spread were the highly contagious nature of the virus with its air-borne route of infection, the busy links between affected countries, and probably inadequacies in international quarantine procedures. The increasing volume of international tourism and trade has raised the risks for translocation of exotic diseases. 1 In other words, the increased mobility, mixing and congregation of civilian populations from different nations increase the rate of transmissible diseases. 2, 3 Countries need to cooperate more closely in the future, not only on finding the causes and the management of epidemic outbreaks, but also on preventing the further spread of them. For example, SARS had affected people in many areas: in Canada (Toronto); in China (Guangdong, Hong Kong, Shanxi, Beijing); in Taiwan; in Singapore; and in Vietnam (Hanoi). On March 12, 2003 , the World Health Organization (WHO) issued a global alert, 10.002 recommending that national authorities implement heightened surveillance for cases of SARS. Recommendations were aimed at limiting the spread of SARS and protecting international air passengers. The screening measures for potential SARS symptoms instituted by national health officials and port authorities, included interviews with passengers, as well as the taking of tympanic core body temperatures from boarding and disembarking passengers by way of electronic thermometers and infra-red cameras. National authorities also advised travelers with fever to postpone international travel from SARS documented areas. International travelers were educated through the public media on the symptoms of SARS and were advised to seek immediate medical attention should such symptoms occur. According to the WHO report, evidence had indicated that since the start of SARS global surveillance at the end of February 2003, a number of suspected and probable cases of SARS had departed from affected countries on flights to other countries (http://www.who.int/csr/sars/ archive/2003_04_11/en/). Local transmission could conceivably have occurred inside the cabin of an aircraft to persons seated close to a SARS infected person, by way of droplets discharged through coughs or sneezes. In total, 26 nations (27 administrative independent regions, including Hong Kong) were reported up to April 2003, to be infected by the SARS epidemic, in part a consequence of international travel. In fact, the international traveler is an efficient vector for SARS as well as new respiratory pathogens yet to emerge. 4 The aim of this study is to describe reasons for a loophole in international quarantine procedures.

This study was divided into two parts: information was collected at the international airport of Kaohsiung (Taiwan), on the physical conditions of passengers who flew from Hong Kong and landed at Kaohsiung from April 10 to 22, 2003. This information was compiled from questionnaires and brief interviews of arriving passengers. In addition, information was gathered from body temperature measurements performed on each arriving passenger who entered Taiwan. Records from at the Emergency Department of E-Da Hospital, in Southern Taiwan, of patients suffering from symptoms of respiratory tract infection who had departed recently from SARS areas from March 19, 2003 onwards were analyzed.

The investigation revealed that during this period a total of 46 passengers were symptomatic during transit on board aircraft, meaning that at least 2 and at most 9 passengers per day were found to be symptomatic during their flight. Since the flying time from Hong Kong to Kaohsiung is only 45 to 60 minutes, it is reasonable to suspect that symptomatic passengers were actually aware of their symptoms before boarding the aircraft. Thus they fully understood that they were possibly infected with the SARS virus when departing from an endemic area and before heading for Taiwan, despite the aggressive screening procedures put in place by the Hong Kong Customs and Department of Health at Hong Kong's International Airport. In order to explain the circumstances for this observed phenomenon and the underlying reasons for such behavior, we proceeded to the second part of our study.

Here, six patients visiting the emergency department of E-Da Hospital, from March 19 to 22, 2003, were found to show symptoms from respiratory tract infections, after they had departed from a SARS endemic area and had entered the territory of Taiwan by air. As shown in Tables 1 and 2, although none of the six patients were eventually diagnosed wild SARS, this observed phenomenon disclosed a very important loophole in the control aspect of international quarantine procedures: the inability to prevent persons with a highly contagious virus from slipping past undetected and thus preventing the further spread of epidemics like SARS on international travel routes. All of these patients admitted that extraordinarily high medical fees for non-residents in Hong Kong, was the major reason for them to hurry back to Taiwan, where the cost medical care is significantly lower.

In this study, we identified that there were loopholes in the international quarantine system for controlling the international spread of contagious disease like SARS, especially when travelers lack a strong motivation to cooperate with national health authorities. This arises particularly when the high medical fees are imposed on non-local residents in endemic areas, were a significant financial burden. Furthermore, the emergency room's medical records showed that patients were already aware of their symptoms such as cough or indications of high fever (though not necessarily SARS) before they boarded their respective fights. Nevertheless, they denied being sick before departure when questioned by health authorities, in full awareness of the infection risks, in order to reach Taiwan. Since Taiwanese residents benefit from very low medical fees in their health care system, in contrast to Hong Kong's high hospital fees for non-residents. Taiwan's medical fees are only V3.585 (NT$150) per attendance in the emergency room and just 10% of the total medical expense during the course of admission for in-patient care at a district hospital, with the remaining cost being subsidized by the national health care plan (See Table 3 ). Tables 4  and 5 show a strong correlation in different medical fees for residents and non-residents both in Hong Kong and Singapore. For example, the admission fee for in-patient care (general acute beds) is V10.4 (HK$100) per day in Hong Kong for residents or Hong Kong identity card holders, while a non-Hong Kong resident has to pay V343.2 (HK$3,300) per day for the same treatment-33 times higher. In comparison, all Taiwan nationals and residents are covered under the policy of the national health care insurance plan, and thus pay less than V11.95 (NT$500) per day for in-patient care. This cost differentiation for residents and non-residents in Hong Kong, is a phenomenon observed almost in every country in the world, with similar examples existing among different member countries in the Patients admitted that they were symptomatic before their departure from epidemic areas of SARS.

European Union. 5 This is only a natural human response for a symptomatic traveler to disguise his/ her illness at their point of departure, to flee back to his/her home country for medical care at a significantly lower cost. However, this apparent trivial aspect of human behavior turns out to be a very serious problem in terms of epidemics and quarantine control measurements, where communicable diseases could be introduced into a population by the arrival of outside foreign infectives.

Citing the example discussed here, the possibility exists that travelers may return home from a foreign trip with an infection acquired abroad. While an experimental model indicates that screening and quarantining of infectives can considerably reduce the infective equilibrium. 6 The egocentric human behavior of certain travelers who break quarantine rules, could be modified by better cooperation between governments. In today's highly mobile society, it is crucial to deter international travelers from spreading contagious diseases during an epidemic and lessons may be learned from the worldwide spread of SARS so that precautions can be taken in the policy-making process for the future since a similar tragedy may repeat itself anytime, anywhere in the world. In response to the main issue identified in this report, governments need to set sensible medical fees for the temporary hospitalization of 'aliens' staying in their territories during periods of epidemic outbreaks. This cost could be shared by governments across the globe, with coordination by the WHO, so as to enforce quarantine measurements more efficiently. The existing cross-border care and international payment coverage policy within the European Union can be seen as a good reference base for constructing such a cross-linking system to tackle this emergent problem of international quarantine. 7 This measure may stop people from becoming disease-vectors within their home countries and also to other passengers on the same plane who may carry infection to many different destinations all over the world. Thus, it is of importance, that governments in endemic areas publicize such policies, targeting foreigners staying within their boundaries during an outbreak. It is estimated that the sum of inter-governmental medical expenses incurred by infective or potential infective patients hospitalized at sensible cost in the 'host countries', would be substantially lower than the total social costs caused by the spread of communicable diseases, if these infectives were allowed to return to their mother countries. Thus, the aim of future research should be focused on the health and safety investment as well as risk control methods. 8 Since global surveillance of SARS began at the end of February 2003, some evidence has suggested that a number of suspect and probable cases of SARS were caused by persons being infected during travel on board aircraft (http://www.wpro.who. int/sars/docs/interimguidelines/part6.asp). They were probably seated in close proximity to persons releasing droplets in the air on coughing or sneezing. Such cases, gives impetus to more rigorous measures to prevent travel-related spread of SARS or other communicable diseases. Thus stricter travel enforcement must also be evaluated and added to existing quarantine measurements. This is in addition to the main focus of cross-border care and international payment coverage policies for aliens or temporary 'visitors' in afflicted areas.

This SARS outbreak may be regarded as a test of whether rigorous contact tracing and other stringent public health measures contained further spread, even though a large numbers of persons may have inadvertently been exposed to the virus. One intervention procedure to control the spread of infectious diseases is to isolate some infectives in order to reduce possible transmissions of the infection to susceptibles. Total isolation may have been the first historical known infection control method, since biblical passages refer to the ostracism of lepers, then later in time plague victims were often isolated. The word 'quarantine' meant historically a period of 40 days, the length of time that arriving ships suspected of plague were required to lay in anchor off the harbour before being allowed to dock. This practice started in the 14th Century at Mediterranean ports during the outbreak of the Bubonic plague. The word quarantine has evolved to signify a forced isolation or a stoppage of interactions with others. Over the centuries, quarantine has been used to reduce the transmission of human communicable diseases such as leprosy, plague, cholera, typhus, yellow fever, smallpox, diphtheria, tuberculosis, measles, mumps, ebola and lassa fever. 9 The 'Influenza Pandemic Preparedness Plan' developed in the United States of America is an excellent model from which every other country in the world should learn and extrapolate its underlying spirit. This plan has as objectives: to limit the burden of communicable diseases, to minimize social disruption and to reduce economic loss in the future when similar outbreaks of pandemic does occur. 10 Early in 2000, epidemiologists had warned the world that should the next pandemic be caused by a virus as deadly as that of 1918 influenza pandemic, the potential for disaster would be greater than ever. 11 As the world's population is now more than three times greater than it was in 1918, with nearly half that population residing in urban areas, including hundreds of millions crowded into slums and shanty towns in the developing world. Faced with today's highly mobile transportation links, a virulent virus could easily spread around the world in a matter of days. Another pandemic would challenge the world's public-health resources as never before. Therefore, an effective response to future pandemics of viral infection proportions, such as the 1918 Influenza Pandemic, will demand the full support and complete cooperation of the public. 11 Yet the global health community is not prepared for the next viral pandemic, according to Klaus Stöhr (WHO, Geneva, Switzerland), speaking at the International Congress on Infectious Diseases (Singapore, March 11-14, 2002) . 12 Furthermore, mankind's history and this case study provides evidence that travelers have contributed significantly to the rapid spread of AIDS 13, 14 , influenza 13, 15 and SARS, therefore strict international quarantine enforcement, must be considered for future epidemics. Only then, would we be ready to confront similar or even tougher challenges of pandemic outbreaks. In fact, bilateral, as well as regional agreements among different governments on visitors' health care are becoming more common. For example, an extensive list of countries has reciprocal health care agreements with the United Kingdom. 16 Other non-economic reasons for return home of febrile passengers during the SARS epidemic include fear for being infected in the epidemic region, reluctance to be isolated in a foreign country, unfamiliarity with foreign culture, planned travel schedule, etcetera. Nevertheless, according to the record of the medical history taken in our emergency department, all six patients admitted that the big gap between medical costs in a foreign country and their mother country was the main reason for disguising their fever on departure. In contrast to sea-voyage, air travel journey is relatively short in duration. Infected crew or passengers who travel on board ships would have their diseased status shown clearly during the long sea-voyage, and would have died or being quarantined when they arrived at their destined port. However, due to the relatively short duration of air-travel, the clinical condition of infected passengers on board airplane would not have sufficient time to progress to a serious stage which is obvious enough to detected by the custom at the destined airport. Here lies the loophole of international quarantine which would be easily overlooked by airport custom, but not sea-port custom. Thus, emergency departments or walk-inclinics are playing the important role in safeguarding the community from imported infectious diseases. Emergency physicians should maintain a high level of awareness regarding potential outbreaks of infectious diseases of any kind and play a role in alerting public health authorities to any loopholes in quarantine procedures.

",0.6487009303459921
Screening of FDA-approved drugs using a MERS-CoV clinical isolate from South Korea identifies potential therapeutic options for COVID-19,"In 2015, the Middle East respiratory syndrome coronavirus (MERS-CoV) reached the Republic of Korea, resulting from nosocomial transmission, and was the largest epidemic outside of the Arabian Peninsula. To date, despite various strategies to identify CoV interventions, there are only limited therapeutic options available. To address these unmet medical needs, we used a South Korean MERS-CoV clinical isolate and screened 5,406 compounds, including US Food and Drug Administration (FDA)-approved drugs and bioactive molecules, confirmed 221 hits by dose-response curve analysis in the primary assay, and selected 54 hits with a therapeutic index (TI) greater than 6. Time-ofaddition studies with 12 FDA-approved drugs demonstrated that eight and four therapeutics act on the early-and late stages of the viral life cycle, respectively. Among the early acting drugs, three therapeutics with a TI greater than 100 were cardiotonic agents. Together, our results identify potential therapeutic options for treatment of MERS-CoV infections and could provide a basis for a wider range of coronaviruses, including the currently emerging coronavirus disease 2019 (COVID-19) outbreak.","patients in the late stage of infection (Omrani et al., 2014; Shalhoub et al., 2014; Spanakis et al., 2014) . These results suggest that the administration of broad-spectrum antivirals to MERS patients is effective at some points during infection, although a specific treatment for MERS-CoV may be required full antiviral activity.

With the current severe outbreak of coronavirus disease emerging from Wuhan in China in late 2019, several countries worldwide battle to control the spread of this devastating virus.

The ongoing outbreak is accompanied by many human casualties and significant socio-economic losses globally and has now reached the proportions of a pandemic, with more than 81,000 COVID-19 confirmed cases and more than 2,760 related fatalities worldwide (February 26, 2020) .

Unfortunately, CoV-specific FDA-approved drugs are still not available in the clinics.

In this study, to address the urgent unmet medical needs and to facilitate the development or identification of more appropriate and effective medical care for patients infected with CoV, we implemented a high-content screening (HCS) strategy with the goal of repurposing newly identified inhibitors for MERS-CoV and a wider range of CoV, including COVID-19, therapy. Utilizing a Korean MERS-CoV patient isolate, we screened 5,406 compounds containing United States Food and Drug Administration (FDA)-approved drugs, bioactives, kinase inhibitors, and natural products; our library included 60% of all FDA-approved drugs (1,247 out of 2,069 total) (Fig. 1A) . Compounds were tested for their anti-MERS-CoV activity by determining the levels of the viral spike (S) protein expression of infected Vero cells by immunofluorescence analysis (IFA). Screening was conducted in two-times independently (screen 1 and screen 2) using chloroquine as a reference inhibitor at 100 μM for the maximum inhibitory concentration (IC90 = 93 μM) (De Wilde et al., 2014) . The calculated Z'-factor of >0.78 indicated a good separation of infected cells treated with the dimethyl sulfoxide (DMSO) control and chloroquine (Fig. 1B) . Two independent HCS analyses were further conducted (R 2 = 0.91) to select for hits that demonstrated a high degree of correlation between the two replicates. (Fig. 1C) . Primary hits were selected by choosing those that demonstrated >70% MERS-CoV the author/funder. All rights reserved. No reuse allowed without permission.

The copyright holder for this preprint (which was not peer-reviewed) is . https://doi. org/10.1101 org/10. /2020 inhibition and >70% cell viability, resulting in the identification of 256 compounds (Fig. 1D ). These hits were then confirmed using 10-point DRC analysis; from these assays, the 50% inhibitory concentrations (IC50) and 50% cytotoxicity concentrations (CC50) were determined for each compound (Fig. 1D . A representative example of a 10-point DRC analysis is shown in Supplementary   Fig. 1 . Based on this analysis, we regarded 35 compounds with a therapeutic index (TI ratio of CC50/IC50) values <1 as inactive and eliminated these from the confirmed hits list, resulting in 221 confirmed hits and selected 54 final hits with an in vitro TI >6 for further testing (Fig. 1D ).

Our approach aimed to identify FDA-approved drugs and bioactives that could be promptly repositioned or developed, respectively, to treat MERS-CoV and potentially COVID-19-infected patients. In previously reported studies, small molecule libraries, including approximately 300 drugs with FDA-approval or in clinical development, were screened against MERS-CoV (De Wilde et al., 2014; Dyall et al., 2014) . Here, our screening included 1,247 FDA-approved drugs, which covers approximately 60% of all FDA-approved compounds. As a result, we identified drugs that were not found in previous studies, indicating that there are still opportunities for identifying novel FDAapproved drugs and bioactives by screening larger compound libraries. Moreover, we corroborated four previously identified hits, including emetine dihydrochloride, ouabain, cycloheximide, and nelfinavir mesylate. This strongly suggests that despite the use of different viral isolates, both our HCS assays and previously published screens reproducibly identified drugs that could be repurposed as potential therapeutic options for patients suffering CoV infections (Dyall et al., 2014) .

Next, using publicly available drug databases, we classified the entire compound library, as well as our final hit compounds, into 43 categories of distinct pharmacological actions. The results are shown in Fig. 2 , with the distribution of the entire library shown as gray bars, and the final hits indicated in black. Important to note, the cardiovascular agents' group contained 14 compounds with TI >6 (26% of final hits). From this analysis, we found that the majority of final hits are classified as cardiovascular agents (14 out of 54), belonging to a class of cardiac glycosides. Cardiac glycosides the author/funder. All rights reserved. No reuse allowed without permission.

The copyright holder for this preprint (which was not peer-reviewed) is . https://doi.org/10.1101/2020.02.25.965582 doi: bioRxiv preprint are naturally derived agents that are used for the treatment of cardiac abnormalities and function to modulate the regulation of the sodium-potassium pump (Prassas and Diamandis, 2008) . Notably, however, glycosides have also been reported to possess antiviral activity against herpes simplex virus and human cytomegalovirus (Bertol et al., 2011; Kapoor et al., 2012) . Consistent with these previous studies, our data indicate that the cardiac glycosides, ouabain, digitoxin, and digoxin, efficiently inhibit MERS-CoV infection. Ouabain, in particular, has been found to block the entry stage of coronaviruses, such as MERS-CoV, through Src kinase signaling (Burkard et al., 2015) . Based on these data, we speculate that cardiac glycosides may generally act as anti-MERS-CoV agents through blockage of viral entry. However, more experimental work will be required to elucidate the exact mechanism by which this occurs.

FDA-approved drugs and inhibitors with known biological functions are particularly advantageous for repurposing due to their known pharmacological activities and safety profiles and potentially facilitate faster development of drugs, respectively. Therefore, we prioritized 12 FDAapproved drugs and six bioactives, which have not yet been reported to have anti-CoV activities and summarized the information in Table 1 and Table 2 To investigate whether the FDA-approved drugs act on the early-or late stages (pre-or post-entry) of the viral life cycle are inhibited, we conducted time-of-addition studies. Therefore, Vero cells were treated with each drug at a concentration higher than its IC90 value, at 1 h prior to, or at 0, 1, 2, 3, and 4 h post-infection (hpi). Viral infection was then quantified by IFA, as previously described (infected cells treated with 0.5% DMSO were normalized to 100% infection). Chloroquine, which was used as an early stage inhibitor control inhibited MERS-CoV infection up to 30% until 3 hpi but had no significant effect when administered at 4 hpi (Fig. 3) . A similar effect was observed for treatment the author/funder. All rights reserved. No reuse allowed without permission.

The copyright holder for this preprint (which was not peer-reviewed) is . https://doi.org/10.1101/2020.02.25.965582 doi: bioRxiv preprint with ouabain, digitoxin, digoxin, niclosamide, regorafenib, nelfinavir mesylate, ciclesonide, and benidipine hydrochloride, which all inhibited MERS-CoV infection only when administered before 4 hpi, which is consistent with previous reports indicating that ouabain and other cardiotonic steroids effectively block clathrin-mediated endocytosis of coronaviruses (Burkard et al., 2015; Zumla et al., 2016) (Fig. 3, Supplementary Fig. 2 ). In contrast, atovaquone, lercanidipine hydrochloride, permethrin, and octocrylene had only minor inhibitory effects throughout the time-course assay, indicating that these drugs likely act on later stages of the viral life cycle (Supplementary Fig. 2) .

Notably, our results indicate that lercanidipine hydrochloride and benidipine hydrochloride, both of which are dihydropyridine calcium channel blockers, display different patterns of viral inhibition (Epstein, 2001; Yao et al., 2006) . This observation could be explained by the types of calcium channels that each drug blocks; benidipine hydrochloride blocks triple voltage-gated calcium channel inhibitor, whereas lercanidipine hydrochloride blocks single voltage-gated channel (Klein and Köppel H, 1999; Ozawa et al., 2006; Wirtz and Herzig, 2004) .

Together, we identified 12 FDA-approved drugs that could be considered for repositioning to MERS-CoV or COVID-19 therapy. Especially the cardiotonic drugs ouabain, digitoxin, or digoxin with a TI greater than 100 in monotherapy or combined with remdesivir, a drug that is currently in clinical trials for the treatment of COVID-19, or with other therapeutics, might lead to drug synergism.

However, further in vitro and in vivo studies are required to investigate the exact antiviral mechanisms, determine potential synergistic effects, and to confirm their antiviral efficacy in an animal model to prioritize and select drugs for potential use in patients affected by the ongoing COVID-19 outbreak. Seven FDA-approved drugs that are not shown in Fig. 3 were analyzed by time-of-addition assay experiments, as described in Fig. 3 . the author/funder. All rights reserved. No reuse allowed without permission.

The copyright holder for this preprint (which was not peer-reviewed) is . https://doi. org/10.1101 org/10. /2020 Supplementary 

",0.6484744453910657
New regulatory strategies to manage medicines shortages in Europe,"Medicine shortages have been spreading in European countries. In many cases, the unavailability of medicinal products has a substantial impact on the capability of National Healthcare Systems in ensuring the continuity of care. Shortages originate from multifactorial causes. In particular, they can be due to supply-related factors (e.g., manufacturing issues, regulatory issues, logistics, distribution) and demand-related ones (e.g., fluctuating drug demand, parallel market, tendering, price and reimbursement policies). However, some extraordinary geopolitical events (e.g., Brexit) may also affect medicines' availability. The capability of European Regulatory Authorities and other stakeholders, which are involved in the pharmaceutical distribution chain and the healthcare assistance services, to define suitable problem-solving strategies has been limited for years by the fragmentation of the European regulatory framework, starting from the lack of a univocal definition of a medicine shortage. Only in 2019, the EMA and HMA joint task force released the first harmonized ""shortage"" definition in the European Economic Area (EEA) and guidance on public communication. This manuscript aims to review the current European regulatory framework on medicine shortages. To support the activities of regulators, manufacturers and other healthcare professionals, an algorithm was also proposed to be used as a harmonized procedure to determine the shortage/unavailability impact on public health and to rationalize the problem-solving strategies adopted in all different settings.","Medicine shortages have risen worldwide, straining the capability of National Healthcare Systems in ensuring patients' access to pharmacological therapies (Beck et al., 2019; Jia and Zhao, 2017, Schwartzberg et al., 2017) . The resilience of the pharmaceutical distribution chain and other stakeholders varies based on different factors, such as product characteristics and the involved countries (Birhli, 2013) . The unavailability of medicinal products to patients can be due to supply-related factors (e.g., manufacturing issues, regulatory issues, logistics, distribution) and demand-related ones (e.g., fluctuating drug demand, parallel market, tendering, price and reimbursement policies). Moreover, extraordinary geopolitical events [e.g., Brexit, Coronavirus disease (COVID-19) outbreak in China] can also have a strong influence on the resilience of the pharmaceutical chain and the patients' access to therapies. Even if globalized players control the research and production of medicinal products by now, the activities of Regulatory Authorities are still based on national or supranational [i.e. European Union (EU)] frameworks. In Europe, the capability of the Regulatory Authorities and other subjects involved in the pharmaceutical distribution chain and the healthcare assistance services in defining suitable problem-solving strategies has been limited by the fragmentation of the regulatory framework (Bochenek et al., 2018; De Weerdt et al., 2015a) . Only in 2019, the EMA and HMA released the first harmonized ""shortage"" definition in the European Economic Area (EEA) (EMA and HMA, 2019b) . Although the proposed definition was relevant only for Marketing Authorization Holders (MAHs) and Regulatory Authorities, it is the first step to promote the communication and the coordination among European pharmaceutical stakeholders, regulators and professionals working in the different National Healthcare Systems for improving their resilience to shortages. However, the kaleidoscopic regulatory framework on shortage is not the only critical factor, but the heterogenicity of the National legislation, reimbursement and price policies, and medicines' market around Europe limited the establishment of a solid European emergency plan to solve the shortage problem.

Considering that numerous factors cause shortages, a univocal problem-solving strategy cannot be established at the European level. The needs of big-pharmaceutical markets, such as Germany (Said et al., 2018) or Italy (Di Giorgio et al., 2019) , are far from those of small ones like as Finland (Heiskanen et al., 2017) . Moreover, excessive fragmentation of the National regulatory frameworks may be itself a cause of drug shortage. The relationship between the price policies and availability of medicinal products is known (Dave et al., 2018; De Weerdt et al., 2015b) . Considering the different reimbursement policies of Member States (WHO, 2018) , the price of the same medicinal product may vary significantly around Europe. In this context, the exportation of drug products to more profitable markets (e.g., parallel trade) is frequently cited as one of the demand-related causes of drug shortages (De Weerdt et al., 2015b) . Noteworthy, such a cause-effect relationship is still debated (Aguiar and Ernest, 2020) . The importation of medicinal products from other EU or extra-EU countries is also the most common strategy to mitigate the adverse effects of severe shortages (Said et al., 2018) . Notably, small European countries might not be profitable enough for MAHs to justify building new manufacturing sites and marketing medicinal products. In this context, for justified public health reasons, the National Regulatory Authorities can adopt regulatory pathways to speed up the release of the marketing authorization (e.g., Art. 126a of Directive 2001/83/EC) and, therefore, to make medicines available on the National market. Otherwise, parallel trade can be a possible solution to ensure the patients' access to treatments. For example, besides the granting of National marketing authorizations, a small English-speaking country like Malta ensures its medicines' supply mainly by Art. 126a authorizations, while parallel import licences contribute only to the residual needs (less than 10%) (Farrugia, 2018) . Indeed, the application of Art. 126a permits the MAHs of products already authorized in other English-speaking EU countries (e.g., UK, Ireland) to have easy access to the Maltese market. However, Brexit (EMA, 2019a) and some concurrent updates of the European regulatory framework might worsen such a delicate balance. For example, the new medicine traceability system introduced in the EU by the Regulation (EU) 2016/161 increased the costs of both the production and the distribution of medicinal products, affecting the economic sustainability of the all actors involved in the pharmaceutical supply chain (e.g., MAHs, parallel distributors). The potential repercussions on patients' access to treatments in the more fragile EU countries are foreseeable, especially in the case of low-price medicinal products (e.g., generics).

In the current situation, it is evident that the same strategy may have different efficacy in shortage mitigation according to the features of European countries involved. As a consequence, rational and practical shortage emergency plans should originate from stronger cooperation among European countries and based on the assessment of pros/cons balance at a supranational level.

This manuscript aims to review the current upgrade of the European regulatory framework on medicine shortages and to propose a theoretical approach for 1) determining the shortage/unavailability impact on the capability of the National Healthcare Systems and 2) rationalizing the problem-solving strategies to be adopted in function of the leading cause of shortage (e.g., production, price and importation/exportation). Such an approach would be useful to the European Regulatory Authorities in the harmonization of the metrics around the Union. Moreover, it would also be helpful to other subjects involved in the National pharmaceutical distribution chains and healthcare systems (e.g., manufacturers, wholesalers, hospitals, pharmacies, insurance companies, regional healthcare Authorities). In this light, it should be flexible enough to rationalize the risk-assessment and risk-management strategies for the mitigation of the shortage effects in different industrial and healthcare settings.

2. Current regulatory framework and problems till on the ground According to Article 81 of the Directive 2001/83/EC, MAHs and their distributors should ensure appropriate and continued supplies to pharmacies and authorized persons to meet the needs of patients in the different European countries. If a temporary or permanent disruption of the supply occurs, the MAH has to notify the competent Authorities, also providing information about the causes (Art. 23a of the Directive 2001/83/EC). The notification should be made no less than two months before the interruption of the supply is in place. In some cases, such deadline was extended, e.g. to 4 months in Italy (Italian Republic, 2019) . Moreover, Art. 123 of the Directive 2001/83/EC leaves plenty of room to the Member States in establishing appropriate measures to ensure the prompt communication among stakeholders about authorizing marketing, supplying and withdrawing a medicinal product, together with the reasons on which such decisions are based.

In the absence of specific, harmonized and detailed guidance on the detection and notification of supply interruption at the European level, the regulation fragmentation occurred at the National level. The term ""shortage"" has taken on different meanings around Europe (De Weerdt et al., 2015a , 2018 . The French legislation defined shortages the inability for a community or hospital pharmacy to deliver a drug to a patient within 72 hours (Bocquet et al., 2017) . In Germany, different words were used to identify the drug shortages at a delivery-level (i.e. lie-ferengpass) and those that compromise the patient therapies in the absence of available therapeutic alternatives (i.e. versorgungsengpass) (Said et al., 2018) . In Italy, the Italian Medicines Agency (AIFA) distinguishes between carenze, which identifies all manufacturing-related shortages including also those caused by MAH voluntary withdrawal (temporary or permanent), and indisponibilità, which identifies the unavailability of a medicinal product in a specific geographical area due to inefficiencies of the pharmaceutical distribution chain (Di Giorgio et al., 2019) . Such fragmentation has also been evident in the communication strategies adopted by the different EU Member States (Bochenek et al., 2018) .

According to a survey carried out by the EMA and HMA in 2019, about 87% of the EU competent Authorities published shortages alerts to healthcare professionals and patients, whereas industries and other regulators were the alerts' recipients only in the 60% of the cases (EMA and HMA, 2019b) . In general, the information on ongoing/expected shortages have been publicly available in web-accessible catalogues periodically upgraded, but other communication tools have also been used (e.g., relevant organizations' channel, press releases, professional/ medical journal, TV, newsletter, social media). In some cases, shortage alerts have been shared in the National electronic patient health systems or the electronic prescribing systems used by physicians and pharmacists (7%). However, in the majority of cases (69%), no specific criteria have been set by the National Authorities for the publication. Moreover, the published data vary significantly case-by-case and country-by-country, improving the confusion for professionals and patients at National and European levels.

In this light, significant efforts to implement the regulatory framework were undertaken by the European Authorities. Single National points of contact (SPOC) have been created to facilitate sharing of the information about nationwide medicine shortages and the coordination of emergency plans among the competent National Authorities and the EMA. For multi-country shortages, the EMA's scientific committees should also be involved to write harmonized recommendations to healthcare professionals.

In 2019, the EMA and HMA released two joint guidelines on shortages, which provided recommendations to improve the collaboration among Regulatory Authorities and stakeholders (EMA and HMA, 2019a,b) . The ""Guidance on detection and notification of shortage of medicinal products for Marketing Authorization Holders (MAHs) in the Union (EEA)"" contained the first harmonized definition of shortage valid for all the EU (EMA and HMA, 2019a). ""A shortage of a medicinal product for human and veterinary use occurs when supply does not meet demand at a national level"". According to such definition, unavailability of a medicinal product can be classified as a shortage only if the MAH supply is insufficient to fulfil the demand of a specific country overall, at least. Consequently, delivery-related issues due to regional supply disruption or inefficient National redistribution of stock are explicitly out of the definition, leaving to National Authorities plenty of room in establishing their approaches to prevent the unavailability issues. According to the new regulatory provisions, the MAH should notify any shortages, including impending/anticipated supply interruptions, to the Regulatory Authorities of the affected European countries (EMA for medicinal products authorised by a centralised procedure). Moreover, MAHs are asked to provide detailed information about the product, the nature and time of the shortage, the availability of alternative medicinal products, the population affected by shortage and the risk for patient safety or a reduction in treatment access (Annex to the guideline). Such information is then used by the competent Authority to triage and assess the shortage of risks.

The second HMA/EMA guideline, ""Good practice guidance for communication to the public on medicines' availability issues"", aimed to enhance and align the European communication on a shortage to improve the awareness of healthcare professionals and patients and the cooperation among European stakeholders (EMA and HMA, 2019b).

Several key recommendations are provided to National Authorities about what should be publicly available on shortages or unavailability due to revocation/cessations of the marketing authorization. The guideline states, for example, that all nationwide shortages should be published in systematic lists, which should be easily accessible and searchable by the public in all the EU countries. The shortage information should also be publicly communicated as quickly as possible to ensure the continuity of care, and it should remain accessible, at least, after six months from shortage resolution. For each ongoing/expected shortage, the systematic list should report details about the affected medicinal product, the shortage duration and cause, the adopted mitigation strategies, the advice for healthcare professionals, including potential alternative medicinal products and recommendation to change in clinical practice/in use of medicine. The best communication tool should be selected based on the shortage gravity; the high-profile tools (e.g., press release) should be preferred for severe shortages with a high impact on public health.

Following the publication of HMA/EMA guidelines, several European stakeholder associations have released position papers confirming their significant concerns on the shortages in the EU and proposing solutions to overcome criticalities still on the ground regardless of the HMA/EMA task force interventions. In May 2019 more than 30 organisations representing patients, consumers, healthcare professionals and public health advocates stimulated the European Commission to do more in identifying the factors leading to medicines shortages and providing clear and transparent information on the root causes (Various Associations, 2019a). In December 2019, the European associations representing manufacturers, parallel distributors, pharmaceutical full-line wholesalers and industrial pharmacists issued a position paper in which they stated their view on the root causes' classification and provided suggestions to improve the European cooperation in preventing shortage impact on public health (Various Associations, 2019b). In particular, it has been highlighted the need to find collaborative solutions for medicine shortages to coordinate EU and National policies and communication campaigns. The cooperation should start from the adoption of a harmonized shortage definition and the rational definition of a ""risk of shortage"" for essential products in the EU. Moreover, they also underlined the importance of improving the monitoring of shortages and cooperation in the EU by the establishment of a priority ranking of shortages to address ""high-risk"" medicinal products efficiently without introducing disproportionate initiatives that could have opposite effects to the ones intended.

Although the HMA/EMA guidelines released in 2019 represent an essential first step to provide a harmonized solution for the shortages in Europe, the proposed approach seems perfectible. For example, the cooperation process among the concerned National Authorities is not detailed, since no systematic procedure on the information sharing among National SPOC has been provided yet (EMA and HMA, 2019a,b) . At the moment, only EMA's decision tree specific for shortages due to GMP non-compliance/quality defects is publicly available (EMA, 2013). Moreover, no procedures to triage the shortage impact on patient's health were detailed in the guidelines, leaving plenty of room to the Member States in defining the measures to assess the information notified by the MAH and mitigate the risk of shortages. However, there is a strong need for proper and effective risk-assessment and problemsolving strategies to reduce nationwide shortages without wasting money and time. In the absence of a harmonized triage procedure, there is the risk of increasing the workload for MAHs and Regulatory Authorities to manage shortages with a low impact on public health, even though they are already mitigated and managed effectively at dispensing level. As reported by the Royal Dutch Pharmacists Association Farmanco® platform, the continuity of treatments is ensured in about 90% of the medicine shortage cases by the medicine substitution with therapeutic alternatives (PGEU, 2019). When more than one country is involved in a shortage, the situation is even more critical. For example, the same medicinal product may be triaged in different matters by competent National Authorities and different mitigation plan may be required to the MAHs. This hypothesis is particularly relevant for comarketing products produced by the same contractor or medicinal products authorized by decentralized or mutual recognition procedures. The involved SPOC may not be able to assess the shortage crisis quickly and to coordinate the mitigation strategies in a useful matter. Consequently, the efforts of MAH/contractor in restoring the National supply may be slowed down by different National obligations that contrast each other. For example, in the presence of reduced manufacturing capacity, the obligation for MAH to maintain a preventive stockpiling in some countries (e.g., Switzerland) may worsen the product availability in the other involved markets. Another critical issue, which has not been adequately tackled by the guidelines yet, is the management of product shortages induced by raw materials unavailability. Indeed, the ongoing/expected shortages of raw material may cause, for a ripple effect, shortages of medicinal products in several countries without MAHs can predict or notify them in advance. Economic globalization promotes the delocalization and gathering of manufacturing sites of pharmaceutical raw materials, impacting on the resilience of the pharmaceutical supply chain of many medicinal products to the shortage. For example, between 2008 and 2009, the so-called ""Great Acetonitrile Shortage"" caused shortages of several drug substances, for which synthesis the solvent was used, and drug products (PDA, 2014). Sartans and immunoglobulins give other examples. In the former case, after the discovery of cancerogenic impurities in sartans (i.e. valsartan, losartan irbesartan) produced by some manufacturers, the blockage of the drug substances' production and multiple medicines' recalls caused severe shortages in most of the European countries (EMA, 2019b). In the latter case, shortages are due to a low-manufacturing capacity in comparison to the growing demand of immunoglobulins. Although the immunoglobulins use has been tripled worldwide in the past 20 years, their production is limited by the availability of human plasma, from which the immunoglobulins are extracted and purified (Carrock Sewell et al., 2014) . In this context, several National Authorities (e.g., France, Germany, UK) have published priority rankings and guidelines for the use of immunoglobulins in clinical practice to reduce the shortage risks and rationalize the patients' access to therapies. In all these cases, the notification process proposed by the EMA/HMA guideline seems not sufficient to detect the risk of shortage since the MAHs are not aware of the resilience of its raw material suppliers to shortage and, therefore, they are not able to monitor effectively raw materials suppliers and predict failures in their manufacturing chain. For these products, more centralized coordination of the risk control strategies by the EMA and, consequently, a more efficient sharing of information among National Regulatory Authorities are needed to better mitigation of shortage impact at a national level and to fulfil the needs of all the patients.

Considering such critical issues and the latest position papers of stakeholder associations (Various Associations, 2019a,b), the establishment of harmonized metrics to classify the impact of medicine shortage on the National Healthcare Systems is a mandatory step to enhance the cooperation among competent National Authorities of the European countries and to rationalize the efforts to mitigate them. Regulators and professional associations have proposed several systematic measures to assess shortage problems (Beck et al., 2019; Bochenek et al., 2018; EMA, 2019c; Jia and Zhao, 2017; Panzitta et al., 2017; PDA, 2014) . However, the proposed strategies are generally designed to face a specific shortage's root cause (e.g., manufacturing failures, low price, distribution-problem). Such strategies cannot be applied to solve all shortage types, and the communications/cooperation among stakeholders may be limited. However, in the end, the existing problem-solving strategies aimed to face the same issue: the medicine unavailability for the patients and the increased risks for their health (Phuong et al., 2019) . Consequently, in the estimation of the shortage impact on the patient needs, this aspect should be primary for harmonizing the existing metrics and rationalizing the problem-solving strategies to be adopted. Based on this assumption, it is proposed a procedure to determine the impact of shortage/unavailability on the healthcare assistance and, therefore, the patient accessibility to therapies, other than the communication strategies to adopt (Fig. 1) .

A shortage can be imagined as a disease for a National healthcare system. Consequently, the procedure can ideally be considered as a shortage-against antibody. Two parts form an antibody: one specific to the antigen (variable region) it has to bind, the other for the recognition by the immune system (constant part). Similarly, an ideal regulatory procedure should be composed by a ""constant part"", which should be recognized by all EU countries and stakeholders, and a ""variable part"", which should be more focused on the shortage cause to tackle or the peculiarities of the National regulatory framework. In this way, the resulting metrics allow promoting, on the one hand, the harmonization of the risk assessment of the shortages at EU level and, on the other hand, the rationalization of the preventing management strategies in a cause-specific matter.

Such a procedure may be fully integrable with other existing and adopted strategies and, therefore, it may be an added value for both European Regulatory Authorities and other subjects involved in the National pharmaceutical distribution chains and healthcare systems (e.g., manufacturers, wholesalers, hospitals, pharmacies, insurance companies, regional healthcare Authorities). Indeed, a unique triage procedure for shortage impacts can improve the stakeholders' cooperation in managing the emergencies. At the same time, it can be useful for European Regulatory Authorities to assess the potential risks of shortage for a medicinal product since the obtaining of the marketing authorization, allowing them to establish preventive actions to improve MAH resilience and minimize risks for patients' health. Moreover, the procedure can also be adapted and applied in a hospital setting to allow pharmacists to identify those medicinal products in their stocks which are more likely to be exposed to the risk of shortages, and to set up most appropriate strategies to prevent inefficiencies in healthcare assistance caused by the shortage.

The constant part of the algorithm may be applied by the EMA in Fig. 1 . Scheme of the harmonized procedure to tackle drug shortage in the EU. U.M. Musazzi, et al. International Journal of Pharmaceutics 579 (2020) 119171 consensus with HMA members as part of the activities of the HMA/EMA task force (HMA, 2019). The constant part of the algorithm aims to provide a harmonized tool for evaluating the impact of drug shortage on clinical demand in each European country. Considering that the negative effects of a medicine shortage strongly varies according to the type of treated disease and the real-world patient assess to therapies, a procedure to assign the overall score for shortage impact (i.e. high, medium, low) may be designed on three levels: I) type of disease to be treated, II) the availability of therapeutic alternatives and III) the market shares of the product in a specific country in comparison to the available alternatives. Table 1 reportes the score for each level. The data needed for each level are either available to competent National Authorities or included in the notification the MAHs have to send them for any expected/ongoing shortages according to the guideline for MAHs (EMA and HMA, 2019a).

3.1.1. Level I -Type of disease to be treated As shown in Fig. 2 , the first step is to classify the medicinal products for which a shortage is expected/ongoing based on the therapeutic indications (Level I), namely: A) products for life-supporting, life-sustaining or rare diseases, B) products for severe or debilitating diseases (acute or chronic), C) products for other diseases. A shortage of a lifesupporting medicinal product resulted in a higher impact on patient health than one used to treat a nonserious illness, such as allergic rhinitis. The score assignment for a medicinal product can be carried out adopting the principles of the VEN (Vital-Essential-Non-essential) analysis (WHO, 2003) or other approaches. Alternatively, an indication of how a competent National Authority considers much essential medicine may be indirectly obtained from the official documentation provided to support its inclusion/exclusion from the list of reimbursed medicines. Indeed, in many European countries (e.g., Italy), the seriousness of the disease is one of the most relevant criteria on which the competent National Authority performs the Health Technology Assessment (HTA) a. Not more than two medicinal products containing drug substances in the same ATC level III (same therapeutic/ pharmacological subgroup) or IV (same chemical/therapeutic/pharmacological subgroup); b. More than two medicinal products for the same ATC level III, but not for the same ATC level IV; c. More than two medicinal products containing drug substances in the same ATC level IV, but no generic products are available for the same ATC level V (same chemical substance or therapeutic moiety); d. More than two generic products for the same ATC level V. III (product market shares in a specific country)

1. Market shares higher than 50% (annual product volumes); 2. Market shares between 25 and 50% (annual product volumes); 3. Market shares lower than 25% (annual product volumes). Fig. 2 . Determination of the shortage impact on patient health. U.M. Musazzi, et al. International Journal of Pharmaceutics 579 (2020) 119171 to determine the eligibility of the medicinal product to be reimbursed. For example, only medicinal products considered ""essential"" and those for chronic medicinal products get access to the reimbursement after the AIFA assessment (Italian Republic, 1993) . If the same medicinal product is indicated for the treatment of several diseases, the most serious and with the low prevalence should be considered to guarantee the continuity of care to the most fragile patient population.

The seriousness of the shortage impact on public health is also influenced by the existence of therapeutic alternatives that can ensure the continuity of the care (PGEU, 2019) . Therefore, each class should be analysed assessing the availability of other therapeutic options (Level II) (Fig. 2, Table 1) . A different score is assigned, in decreasing order of importance, based on the negative impact on public health ( Table 1) . The lower the number of therapeutic alternatives on the market, the higher the negative effect of the shortage on public health. The scores for the availability of treatments are: a) not more than two medicinal products containing drug substances in the same ATC level III (same therapeutic/pharmacological subgroup) or IV (same chemical/therapeutic/pharmacological subgroup); b) more than two medicinal products for the same ATC level III, but not for the same ATC level IV; c) more than two medicinal products containing drug substances in the same ATC level IV, but no generic products are available for the same ATC level V (same chemical substance or therapeutic moiety); d) more than two generic products for the same ATC level V.

It is noteworthy that available treatments are those that are marketed in a European country. Medicinal products should not be considered in the assessment if they have obtained the marketing authorization but have not been placed in the market by the MAH yet. Moreover, medicines with negligible market share (lower than 2% of the total demand of reference for the product in shortage) should not be taken into account as well, since their negligible impact in preserving patient access to therapies. The differences among scores are based on the criticality associated with the medicine interchangeability. Indeed, it is expectable that a shortage can be easily mitigated by the availability of therapeutically equivalent products, which can be substituted in a pharmacy setting in most of the European countries (Wouters et al., 2017) . Two medicinal products are therapeutically equivalent if they contain the same active substance or therapeutic moiety (ATC Level V) and, clinically, show the same efficacy and safety (EMEA, 2000) . Generic products fall in this class.

On the contrary, the unavailability of generic products implies that, in the best scenario, the benefit/risk balance of the therapeutic interchangeability should be evaluated by a physician. It is the case of medicinal products for which there are alternatives with the same ATClevel III (e.g., penicillins vs tetracyclines) or IV alternatives (e.g., amoxicillin vs ampicillin). In both cases, the efficacy/safety patterns of two alternatives may not be the same and, therefore, the substitution cannot be automatically carried out by the community pharmacists. Consequently, the shortage impact is potentially higher because the patient is exposed to a higher risk of cure discontinuity if the physician is not promptly accessible.

The existence of therapeutic alternatives may not be enough to mitigate the negative effect of a medicine shortage. Indeed, if the therapeutic alternative has low market shares, the MAHs supply cannot be enough to fulfil the patients' needs during the crisis. The risks of shortage for the low-market-share alternatives is also increased for a ripple effect. The more monopolistic is the market of a medicinal product, the higher the probability that competitors are not able to sustain the patient demands during a shortage. Indeed, Parsons and colleagues suggested the existence of an inverse correlation between the number of suppliers for antineoplastic drugs in the US and shortages' occurrence (Parsons et al., 2016) . Regardless of the presence of therapeutic alternatives on the market, the lower the supplier number, the greater the number of drug shortages. Therefore, for algorithm level III, the higher the market shares of a medicine (expressed as annual volumes) in comparison to the existing therapeutic alternatives, the higher the potential risks for the public health in case of shortage.

Consequently, the following scores were empirically defined (Table 1) : 1) a market share higher than 50% of the entire National market; 2) a market share between 25 and 50%; 3) a market share lower than 25%. For scores b) and c) of the Level II, the product market shares should be estimated concerning the ATC level III or IV in which the product is included. For score d) of Level II, the Level-III score should be determined in comparison to all the products included in the same ATC level V.

Based on the scores obtained in the constant part of the algorithm, decision trees for risk-management strategies (i.e. variable part of algorithm) can be built up according to the features of different settings (e.g., manufacturers, wholesalers, hospitals, pharmacies) or National regulatory frameworks. Some examples of the possible applications to the variable part of the algorithm are reported below.

A medicine shortage may be caused by a blockage of the production for different causes (e.g., the unavailability of raw materials, qualityfailures, GMP no-compliance, quality defects). In this context, considerable efforts have been made from Regulatory Authorities and professional associations to improve the quality in the pharmaceutical processes to minimize the risk for the patients and improve the effectiveness of the manufacturing processes (ISPE, 2014; PDA, 2014; EMA, 2019c; Various Associations, 2019b) . For example, a nine-fold increase in the EMA quality guidelines was observed between 2005 and 2015.

To mitigate the risk of quality-failures, the EMA promoted the application of the quality-by-design and quality risk assessment principles to the manufacturing of medicinal products (EMA, 2015a (EMA, ,b, 2017 . The rationalization of pharmaceutical manufacturing and the control strategies can be derived from a better knowledge of the product and process. The traditional quality control process has been generally based on the testing of most critical parameters and manufacturing steps can impact on the quality of the intermediates or the final medicinal product. The control setpoints and operating ranges are, therefore, fixed based on the experiments carried out in the pharmaceutical development, and their change should be notified/authorized by a Regulatory Authority as post-approval change. The application of a modern pharmaceutical quality system promotes a systematic and continuous evaluation, understanding and refining of the formulation and the manufacturing process by the identification of the critical material attributes and process parameters and by the determination of the existing functional relationship between such factors and the responses (i.e. quality profile of the products). However, effective pharmaceutical quality systems require close collaboration between the MAH and their suppliers of raw materials to promote a more rational definition of the critical quality attributes of the drug product and to improve the production resilience. The implementation of the quality risk management plan, including shortage-specific prevention and management plan, was proposed to enhance the resilience of manufacturers to shortages, (ISPE, 2014; PDA, 2014; Panzitta et al., 2017) . For example, Panzitta and co-workers proposed a qualitative risk assessment based on a classification of different risk factors, causing a shortage in the manufacturing process (Panzitta et al., 2017) . Based on the resulting risk ranking, the manufacturer can define proper strategies to prevent shortage (e.g., increase of the raw material suppliers, expansion of the material stocks). However, some measures may not apply to all the situation since their high impact on economic sustainability. For example, the maintenance of large material stockpiles or the expansion of the suppliers' number, with the relative increase of inspections and audits, might be not sustainable for manufacturers of the low-price drug products. For these considerations, the adoption of the preventing tools proposed by the stakeholders' associations (PDA, 2014; ISPE, 2014) could be carried out gradually by the MAHs based on both the shortage impact of a product on the treated patients and the economic sustainability for the MAHs production.

In this context, the proposed algorithm can be applied to enforce the gradual implementation of the pharmaceutical quality systems for MAHs most vulnerable to shortage risks. As shown in Fig. 3 , the obligations for MAHs are determined based on the shortage impact score. For products with a high shortage impact, the establishment of a shortage prevention plan seemed essential to ensure an appropriate and continued supply to fulfil patients' needs. For such products, manufacturer's resilience should be improved by more onerous provisions, such as the obligation for MAHs to qualify at least two suppliers of drug substance, the stockpiles' expansion and the implementation of the manufacturing technologies and risk assessment tools. In this context, the establishment of ad hoc agreements between European countries and MAH to conduct exceptional manufacturing activities in specific National sites can also be pursued. As an example, in Italy, National military manufacturing site has been involved in such activities to mitigate the impact of severe shortage, especially in the case of old and low-cost medicinal products (Di Giorgio et al., 2019) . On the other side, if the shortage impact of medicinal products on the public health is low, it is not reasonable to charge the MAHs with additional costs and, therefore, affect the economic sustainability for the implementation of quality assurance systems that may have a limited positive impact on the continuity of care.

Medicine prices are cited among the causes of shortages worldwide (Jia and Zhao, 2017; Bochenek et al., 2018; Heiskenen et al., 2017; De Weerdt et al., 2015b; Dave et al., 2018) . Indeed, especially for low-price medicinal products, the market competition may affect the economic sustainability of the MAHs significantly, decreasing their manufacturing capacity and resilience to demand fluctuation. In this context, MAHs may not be able to fulfil appropriately the guidance reported in the ISPE drug shortage prevention plan as suggested by the last HMA/ EMA guideline (EMA and HMA, 2019a) . Indeed, especially for old medicinal products, the costs of the upgrade to the required high-quality standards and risk assessment plan may be too high to maintain the MAH economic sustainability. In this context, European associations representing manufacturers, parallel distributors, pharmaceutical full-line wholesalers and industrial pharmacists recently stressed the need of establishing proper reward actions by the competent Authorities for supporting their activities to comply with regulation upgrades (e.g., Falsified Medicines Directive) (Various Associations, 2019b).

The application of economic incentives to MAHs has been proposed as a solution to prevent shortages. For example, Jia and Zhao demonstrated that a price increase of 30% results in a significant reduction of risk of shortage in the US market (Jia and Zhao, 2017) . However, a similar approach cannot be fully applied in the EU, considering the fragmentation of the European price and reimbursement policies (WHO, 2018) . Economic incentives to MAHs producing medicinal products can be provided as cut-off of the regulatory costs/times to implement the quality in their manufacturing process and to improve their resilience to production blockage. It is not feasible to introduce economic incentives for all medicinal products since it can affect the budgets of the Regulatory Authorities or, more in general, those of the National Healthcare Systems. Therefore, proper harmonized decision trees should be designed to apply regulatory incentives only to the most vulnerable medicinal products to shortages. Since fees and times for dossier variations vary significantly among the European Regulatory Authorities (EGA, 2015) , the extent of the regulatory incentives cannot be defined a priori at EU-level, but they should be calculated as percentages of current fee and regulatory time to be more adaptable to the different National settings (Fig. 4) . Fig. 4 showes a hypothetical decision tree for regulatory incentives that merge the shortage impact score obtained by the constant part of the algorithm (Fig. 2) and the product price. The latter was normalized by the real-world market share. Indeed, at the same market price, the incentive impact is more relevant for low-volume medicinal products than high-volume ones.

However, as Jia and Zhao demonstrated (Jia and Zhao, 2017) , the efficacy of the regulatory incentives in preventing shortages is also linked to the creation or the strengthened of failures-to-supply clauses for MAHs in the reimbursement agreements. These strategies permit to improve MAH sustainability and to ensure a proper supply for the patient demand. Noteworthy, it is not expectable that such provisions can solve the economic sustainability of a product in a long-term perspective, but there is no doubt that they can mitigate the MAH situations in short and medium-term, allowing the adoption of proper strategies to rationalize the production and to improve MAH sustainability.

If a medicinal product is affected by an expected/ongoing shortage in a specific country, the competent National Authorities can authorize its importation from other countries (EU or extra-EU) to ensure the patients' needs. A recent survey on German Pharmacists demonstrated that 38% of community and about 90% of hospital pharmacies have imported medicinal products in the last three months (Said et al., 2018) . German pharmacists are allowed to import medicinal products only in presence of an order for a specific individual patient and in a small quantity when no products, which are identical in terms of drug substances and comparable in strength, are available in Germany for the same therapeutic indications [Section 73 (3) of the Medicinal Products Act (Arzneimittelgesetz -AMG)]. In other European countries, the importation of medicinal products is also permitted if the importer obtained a formal authorization of the competent National Authorities (e.g., Italy). For example, the AIFA periodically releases on its portal a list of medicinal products in shortage for which hospitals are authorized to import them (234 products are included in the list at 9th August 2019) (AIFA, 2019a). Unlike the AIFA list of medicinal products in shortage (more than 2000 items) (AIFA, 2019a), the import-authorization list includes only medicinal products for with no therapeutic alternatives are available on the Italian market. Regardless of the procedure required at the National level, the medicine importation is, in most of the cases, the unique possibility for healthcare professionals to ensure the continuity of care to patients if a National therapeutic alternative is not available. However, an excessive recourse to parallel imports from other European countries may harm medicine unavailability in other ones for a ripple effect. This effect may be worsened by uncontrolled medicine movements for economic reasons by parallel trade, under the Art. 34 of the Treaty on the Functioning of the European Union (TFEU) (Di Giorgio et al., 2019) . However, Art. 36 of TFEU enabled competent National Authorities to establish prohibitions or restrictions on parallel imports, exports on the grounds of the protection of health and life of humans (European Commission, 2018). Consequently, several European countries included specific restrictions of the product exportation to preserve the National stockpiles during shortages in their regulatory frameworks (Bochenek et al., 2018) . However, restriction criteria for medicines affected by shortage are not harmonized at the EU level. In France and Poland, the exportation ban applies to all medicinal products of high therapeutic value that are in shortages. In Greece, the ban was first applied to vaccines and then extended to all medicinal products in shortages. In Spain, the competent Authority can restrict exportation only to medicinal products with not therapeutically equivalents. In Italy, the competent Authorities adopted a combined approach to prevent that exportations could worsen the products shortages/unavailabilities. On the one side, the AIFA has started a preventive verification program on the exporting wholesalers and distributors to detect potential violations of good distribution practice and traceability rules, causing abnormal exportation of medicinal products. As reported by Di Giorgio et al., a pilot verification project conducted showed effectiveness in reducing the distribution-related unavailability on specific tracing products (Di Giorgio et al., 2019) . In particular, the adopted counteracting measures seemed more efficient in preventing unavailabilities that affected community pharmacies than hospital ones. On the other side, the Italian regulatory framework has been updated to permit AIFA to impose temporary blockage of exportations to prevent or mitigate drug shortages or unavailability of a medicinal product authorized in Italy (Art. 13, Decree-Law n. 35 of 30th April 2019, as converted by Law n. 60 of 25 June 2019). At the moment, seven medicinal products [i.e. four Sinemet® strengths (levodopa/carbidopa), Questran® (cholestyramine), Famotidina EG® (famotidine), Ongentys® (opicapone)] are included in the AIFA list for exportation ban (AIFA, 2019b) . In most of the cases, the list includes products for which the shortage has recently ended and for which there are distribution-related unavailabilities in Italy (i.e. products containing levodopa/carbidopa and cholestyramine). Indeed, the provision aims to prevent that parallel exportation may delay the wholesalers' capability in restoring the supply to the Italian community and hospital pharmacies during the post-emergency period. The medicinal products containing opicapone (Ongentys® 50 mg, 30 capsules) and famotidine (Famotidina EG® 40 mg, 10 tablets) have been included for preventive purpose. Ongentys® is the only medicinal product containing opicapone, which is used to treat adults with Parkinson's disease. Since the MAH had communicated the medicine shortage from 6th November 2019 to 15th January 2020 for manufacturing issues, the exportation ban was activated by the AIFA to prevent any medicine depletion of the National stock that could affect the patient's access to therapy. Famotidine is the only drug substance with the same ATC level IV of ranitidine available on the Italian market. After the detection of a possible cancerogenic impurity (i.e., NDMA) in several batches of drug substance, EMA required the massive withdrawal of ranitidine products U.M. Musazzi, et al. International Journal of Pharmaceutics 579 (2020) 119171 around the EU (EMA, 2019d). Consequently, the famotidine demand has exceeded the MAH production capability, running out the available stocks. In this context, the exportation ban established by AIFA aimed to preserve the National product stocks for removing as long as possible the onset of famotidine shortage.

Here again, the establishing a unique decision tree to support National decision about medicine importation/exportation could be useful to reduce the fragmentation of shortage-mitigating strategies among European countries and to improve the effectiveness of the National strategic plan to satisfy the patients' needs, without altering the National regulatory framework.

As shown in Fig. 5 , a hypothetical decision tree may link the possibility to import or block the exportation to the shortage impact score established with the constant part of the algorithm (Fig. 2) . For high/ medium impact medicinal products, the importation should be allowed during an ongoing shortage. In the case of high-impact products, fast authorization procedures should also be adopted to speed up the importation from other EU (parallel imports) or extra-EU countries with similar pharmaceutical quality standards to the EU, from which the medicine can be exported without any ripple effect. On the other side, it may not be convenient in the case of low-shortage-impact products, since therapeutic alternatives are available at the National level, and they may worsen the shortage situation in other European countries. However, the importation of medicinal products should always be allowed in exceptional cases, such as a substantial reduction of the National supply of all therapeutically equivalents or specific patients' needs justified by their clinical conditions (e.g., allergies to excipients, rare metabolic diseases). Moreover, a similar decision tree can also be used to support National Competent Authorities in the adoption of regulatory pathways such as Art. 126a of Directive 2001/83/EC to ensure the availability of medicines in a specific market, particularly for those with a high shortage impact on public health.

The blockage/restriction of exportations can be feasible as preventing/mitigating strategies in specific circumstances. It is the case of all medicinal products for which the shortage has recently ended, and the supply is lower than a warming level and cannot adequately fulfil the National demand. Here again, the establishment of an exportation ban can be useful for high-impact products since the maintenance of National stocks directly influence the capability of healthcare systems to ensure patient access to therapy. For low-shortage-impact products, such strategies may be a too restrictive provision because of the negligible effects on the continuity of the care. Indeed, therapeutically equivalent medicinal products can be substituted with therapeutic alternatives without compromising the safety of patients. For mediumshortage-impact products, the adoption of exportation restrictions should be evaluated case-by-case based on the availability of other therapeutic options.

Moreover, the promising results obtained by some National competent Authorities (e.g., AIFA) suggested that setting up cooperation schemes between administration and other stakeholders, fostering a responsible approach in managing crisis in medicines supply, also allowing, for instance, surveillance/verification programs that may represent rational and effective preventive strategies to monitor the activities of exporting wholesalers and distributors for reducing the risks of product unavailability. If such tools are strongly necessary for highshortage-impact products, their adoption as a preventive action to monitor medium-and low-impact products is also desirable. However, for low-and medium-impact products, the development of protocols to rationalize the clinical medicine substitution or to activate importation procedures may be more effective. In this context, education programs should be performed by competent Authorities to ensure update of healthcare professionals on these topics.

The shortage impact scores calculated with the constant part of the algorithm can be used to improve and rationalize how to communicate a shortage to the public and patients. Recommendations how to better communicate shortages to patients were introduced by the last HMA/ EMA guideline (EMA and HMA, 2019b) . In this context, it is worth underlining that communication strategies should be carefully planned based on the level of shortage criticality for public health (high, medium, low). Without any doubt, the patients' awareness about shortage is an important aspect that the competent Authorities have to consider in every mitigation action they can adopt. However, it is essential to avoid alarmism in patients, especially when therapeutic alternatives are promptly available. High-profile communication tools (e.g., press release, social network, television, radio) should be U.M. Musazzi, et al. International Journal of Pharmaceutics 579 (2020) 119171 preferred to communicate the shortages of medicinal products with high impact score, whereas drug alerts to healthcare professionals and patient's associations can be sufficient for products with medium and low impact scores. Moreover, the shortage-impact scores can be used as a basis for harmonizing the graphical formats of the communication campaigns in all the EU countries. For examples, red, yellow, and green colours can be used in the systematic National shortage lists, but also in the paper communications, posts on social networks, alerts in the software used by physicians and pharmacists to better identify the medicinal products with high, medium and low impact scores, respectively.

Economic globalization has promoted the delocalization and gathering of manufacturing sites of drug substances and medicinal products, stressing the resilience of the pharmaceutical supply chain of many medicinal products to the shortages. The European countries cannot face the different shortage crisis alone, but the cooperation at EU-level is necessary to provide practical and rational solutions to the problem. For mitigating shortages that affect the Union, the stronger coordination at EU-level and the harmonization of communication, risk-assessment and risk-management strategies are desirable and urgent. Although further studies in real-world settings are needed to complete the validation of the procedure and decision trees, the approach proposed in this article may contribute to improving the information sharing and cooperation among European countries. Indeed, risk-assessment and risk-management strategies adopted by professionals (e.g., pharmacists) and other stakeholders can be rationalized and harmonized based on the medicines shortage impact scores calculated by a competent National Authority.

Moreover, the structure of procedure and decision trees may be adapted and used as a model to build up risk-management strategies for local needs of a specific healthcare assistance setting (e.g., single hospital). However, considering that the shortage-impact assessment is linked to the authorized therapeutic indications of medicinal products in the procedure here proposed, a limitation of the latter, which is also valid for other shortage-mitigation strategies, is that the off-label use of medicinal products in specific patient populations (e.g., paediatrics) is not tracked and, therefore, the shortage impact may not be assessed adequately in this field. In this context, following the example of the provision adopted by several National Authorities to ensure patient access to immunoglobulins treatment (e.g., France, Germany, UK), the establishment of treatment priority rankings, clinical practice guidelines and clinical records may be possible solutions. For these activities, the involvement of EMA scientific committees can be considered an added value for supporting the efforts of competent National Authorities. In parallel, the cooperation among Regulatory Authorities and stakeholders of the pharmaceutical distribution chain should also be improved to set up shared strategies to mitigate distribution-related unavailability in specific geographic regions and to prevent shortages. In this context, the establishment of permanent consultation or task force groups is desirable both at National and European levels for speeding up the resolution of medicine shortage emergency. 

The authors declare the following financial interests/personal relationships which may be considered as potential competing interests:

Paola Minghetti and Umberto M. Musazzi have no conflict of interest. Domenico Di Giorgio states that this scientific publication does not imply any current or potential conflict of interest with the Administration of affiliation; the view and opinions expressed are those of co-author and should not be attributed to AIFA.

",0.6481253959343073
Risk factors for delays between intake and veterinary approval for adoption on medical grounds in shelter puppies and kittens NIH Public Access,"To maximize their capacity to save lives and optimize resource allocation, animal shelters need to identify highly adoptable animals that are unlikely to be delayed on medical grounds before they can be made available for adoption. In this retrospective cohort study, our objective was to identify risk factors for delays from intake to approval for adoption on medical grounds in shelter puppies and kittens. Shelter medical records from 2008 for 335 puppies and 370 kittens were selected randomly at a large metropolitan adoption-guarantee shelter. Data including signalment, source shelter, intake veterinary examination findings, clinical history and days from intake until approval by a veterinarian for adoption on medical grounds were extracted from shelter records and analyzed using multivariate Cox regression. Puppies and kittens with clinical signs of respiratory or gastrointestinal disease at intake took significantly longer to receive approval for adoption on medical grounds (puppies -respiratory p<0.0001; gastrointestinal p<0.0001; kittensrespiratory p<0.0001; gastrointestinal p=0.002). Stray kittens were more likely to be delayed than owner-relinquished kittens or those transferred from other shelters (p<0.01). Older kittens were less likely to be delayed (p<0.0001). Administration of oral or parenteral antibiotics to puppies and kittens with respiratory and/or ocular signs within 24 hours of intake significantly reduced time to approval on medical grounds for adoption (puppies p=0.02; kittens p=0.03). The analyses suggested that puppies and kittens with respiratory or gastrointestinal signs on intake are more likely to experience delays between intake and veterinary approval for adoption on medical grounds. Prompt antimicrobial treatment of animals with respiratory and/or ocular signs may decrease length of stay in the shelter.","Animal shelters care for 6-8 million dogs and cats annually in the United States (HSUS website) and management to optimize the health of shelter pets and minimize the introduction of infectious disease in the face of a constantly changing population, potential overcrowding and limited funds remains a major challenge in shelter medicine (Peterson et al., 2008) . Since shelters are composed of populations of animals, a focus on population management is an essential component of maintaining the health of individual shelter pets. The concepts developed for efficient management of emergency room triage in human hospitals are relevant to animal shelters, since they maximize the utilization of the available facilities for an incoming population (Albin et al., 19750) . Medical triage protocols are used to guide resource allocation during resource scarcities and under ideal circumstances are based on evidence and ethical principles (Christian et al., 2006) . Similarly, individualized evidence-based protocols could be used as management tools to guide animal selection and care in an animal shelter.

It is self-evident that the ability of an animal shelter to maximize the number of homeless animals saved is increased when highly adoptable animals such as puppies and kittens are selected and when the time taken for approval as suitable for adoption on medical grounds is minimized. Our aim was to analyze shelter records from a group of puppies and kittens taken into an adoption guarantee shelter in 2008 to determine risk factors for delays from intake until approval by a veterinarian as suitable for adoption on physical examination findings. We hypothesized that delays were more likely in animals with clinical signs of disease at the intake veterinary examination.

Shelter records for puppies and kittens (6-26 weeks old) taken into PAWS Chicago Rescue and Recovery Center during 2008 were reviewed and analyzed. PAWS Chicago is an adoption-guarantee shelter that sources its intake mainly from the local municipal shelter, Chicago Animal Care and Control, but also accepts some transferred animals from other shelters within the same region. Puppies and kittens are selected for admission to PAWS Chicago daily by shelter staff, who make their selection decisions based on the requirements of the PAWS Chicago Adoption Center regarding breed size for puppies and coat color for kittens. On intake, puppies are housed separately from adult or ill dogs. Kittens are separated from other shelter cats and kittens according to day of intake or by age. All healthy animals are sent to foster homes, depending on availability, within 2-7 days. Foster carers sometimes take animals with mild clinical signs of illness, but only if the animal is still eating and requires only oral medication, if any. Any unwell animals that require injectable medications or daily recheck examinations by a shelter veterinarian are housed at the shelter. Treatments are usually administered to these animals while they are in their cages, but sometimes treatments are given during recheck veterinary examinations in a consulting room at the shelter.

Clinical data was unavailable from any previous stays at other source shelters, but information was collected regarding which shelter the animal came from, and in the case of animals from Chicago Animal Care and Control, whether the animal was stray or ownerrelinquished. Intake veterinary examinations were conducted by a staff of approximately 5 part time veterinarians and occurred within 48 hours of each animal's arrival at the shelter. The following information was recorded -age, breed, gender (including spay/neuter status) and bodyweight (kg) at intake; physical examination findings by a veterinarian at intake; clinical history during the stay at PAWS Chicago; and the number of days from the intake veterinary examination until approval by a veterinarian that the animal was clinically healthy on physical examination and therefore suitable for adoption from a medical perspective.

Veterinary approval for adoption was granted purely on medical grounds and was not influenced by other factors related to the potential adoptability of the animal, such as gender or breed. Approval was granted after the animal had undergone an intake examination, received intake vaccinations, was spayed or neutered, and had been found healthy at a preadoption veterinary examination. The method of random systematic sampling used was to select records alphabetically by the pet's name. Puppies and kittens that were less than 6 weeks old at the time of intake were excluded from the study as it was shelter policy dictated that animals must be at least 6 weeks old before receiving approval for adoption by a veterinarian.

Statistical computations were performed using Stata 11.1 (StataCorp. College Station, TX). The log-rank test was used to evaluate equality of time to veterinary approval for adoption between variable categories for each species. Multivariate Cox regression models for kittens and for puppies, with time to veterinary approval for adoption as the dependent variable, were used to estimate odds ratios adjusted for other variables included in the model. Tests of Cox regression assumption were conducted for each species to ensure that model assumptions were not violated. This test evaluates non-zero slope in a generalized linear regression of the scaled Schoenfeld residual for both individual variables and the global model over time. The null hypothesis (zero slope) of the test is that the log hazard-ratio function does not change over time, indicating that the model assumptions were not violated (Schoenfeld, 1982; Therneau and Grambsch, 1994) . Mann-Whitney U-tests were used to compare the interval between intake and veterinary approval for adoption in groups of puppies and kittens.

During the study period, a total of 1764 dogs and 1343 cats were taken into the shelter. Descriptive statistics and the most commonly recorded problems by body system on the intake veterinary examination are reported in Table 1 . Table 2 reports the number and frequency of single and multiple clinical signs in the population. The results of Cox model assumption testing for each species were not statistically significant (P>0.05), indicating that the model assumptions were not violated.

Of 501 puppies taken into PAWS Chicago in 2008, records from 333 puppies (66.96%) were included in the analysis. Of the 333 puppies, 139 (41.7%) had no abnormalities identified on a physical examination at the time of intake performed by a shelter veterinarian and of those, 1 puppy subsequently became ill before it was granted approval by a veterinarian for adoption on medical grounds. In the multivariate model (Table 3) , puppies with skin diseases, respiratory diseases and digestive diseases at the intake veterinary examination had significantly higher odds of a delay before veterinary approval for adoption (skin OR=1.61; p=0.01; respiratory OR=2.4; p<0.0001; digestive OR =1.55; p < 0.0001).

Of 607 kittens taken into PAWS Chicago in 2008, records from 364 kittens (60.96%) were included in the analysis. Of the 364 kittens in the study, 104 (28.5%) did not have any abnormalities identified on a physical examination performed by a shelter veterinarian at the time of intake. Of the 104 healthy kittens, 5 subsequently became ill before they were eventually granted approval by a veterinarian for adoption on medical grounds. In kittens, the 4 most commonly recorded problems by body system were respiratory (n=162; 44.51%), gastrointestinal (n=143; 39.29%), ocular (n=96; 26.37%) and skin (n=42; 11.54%). In the multivariate analysis (Table 3) , kittens with respiratory disease or digestive disease identified on the intake veterinary examination were more likely to undergo delays before veterinary approval for adoption (respiratory OR=3.95 p< 0.0001; digestive OR=1.50 p=0.002). Kittens transferred from other shelters had significantly lower odds of a delay until veterinary approval for adoption compared with stray kittens (OR=0.57; p=0.01). The odds of delays to approval for adoption reduced by 2% for each week increase in the kitten's age at the time of intake. The test of Cox model assumption indicated that the model assumptions were not violated.

Decisions regarding antimicrobial treatment were made by the attending shelter veterinarian and were based on individual clinical judgment. The most commonly used first-line antimicrobial was amoxicillin/clavulanic acid (puppies -53/107 initial prescriptions; 50.0%; kittens -131/182 initial prescriptions; 72.0%), followed by clindamycin for kittens (35/182 initial prescriptions; 19.2%) and doxycycline for puppies (14/107 initial prescriptions; 13.1%). A single antimicrobial was usually prescribed initially (puppies -84/107 initial prescriptions; 78.5%; kittens -157/182 initial prescriptions; 86.3%). Clinical decisions such as changing or adding antimicrobials to treatment regimens, or commencing other treatments, such as topical preparations and fluid therapy, were made by shelter veterinarians depending on clinical response.

Of 88 puppies that had clinical signs of respiratory and/or ocular disease on the intake veterinary examination (sneezing, coughing, abnormal respiratory tones on auscultation, nasal or conjunctival discharge), 14 (15.9%) commenced a course of antimicrobial treatment prescribed by a shelter veterinarian within 24 hours of the intake examination. Of the remaining 74 puppies, 64 (86.5%) subsequently received antimicrobials prescribed after a veterinary examination during their shelter stay. Of puppies with respiratory and/or ocular signs at intake, those that received antimicrobials in the first 24 hours had significantly reduced time to veterinary approval for adoption (p = 0.02; median time to approval = 12.5 days compared to 20 days). Of 182 kittens with clinical signs of respiratory and/or ocular disease at intake, 19 (10.4%) received antimicrobials prescribed by a shelter veterinarian within 24 hours and of the remaining 163 kittens, 93 (57.1%) went on to receive antimicrobials prescribed after a further veterinary examination during their stay. Commencement of antimicrobial treatment within 24 hours of intake significantly reduced the interval between intake and veterinary approval for adoption in kittens that had respiratory and/or ocular signs on intake (p=0.03; median time to approval = 29 days compared to 39 days).

This study analyzed shelter records to identify risk factors for delays from intake to approval on medical grounds for adoption in shelter puppies and kittens so that recommendations for animal selection could be developed. The shelter used was a well-resourced adoptionguarantee shelter in a large metropolitan area, so animal intake and facilities available for care may not be typical of shelters elsewhere. However, our results could provide a useful starting point for other kinds of shelters where decisions are made about animal selection in the face of limited resources. It would be interesting to repeat this study at an openadmission shelter, or at a shelter in a more rural location to compare the results and draw conclusions about why differences (or similarities) in risk factors for delays to approval for adoption may occur. In order to achieve the end goal of maximizing lives saved, animal selection also needs to take into account the types of animals being presented to shelters and the needs and preferences of potential adopters. The concepts of shelter supply and demand may modify selection against potentially at-risk animals, so the results of this study could be used instead to develop targeted recommendations for extra care in at-risk animals.

Respiratory tract disease is the most common and difficult to manage infectious problem in animal shelters (Foley and Bannasch, 2004; Pedersen et al., 2004) and in dogs and cats, the major syndromes are kennel cough and upper respiratory tract infection (Foley and Bannasch, 2004) . A limitation of this study is the lack of microbiological data for the animals with respiratory and/oral ocular disease on intake, but most shelters do not have the resources for diagnostic testing prior to treatment or final disposition. A previous study on respiratory disease in shelter cats found that the most common pathogens were feline herpesvirus (FHV), feline calicivirus (FCV), Bordetella bronchiseptica, Chlamydophila felis, and Mycoplasma species (Bannasch and Foley, 2005) . Our study confirms the results of another study of shelter cats, which reported that cats with upper respiratory infections (URI) tended to be younger than uninfected cats and that purulent ocular and nasal discharge was commonly associated with all URI pathogens investigated (Bannasch and Foley, 2005) . Canine kennel cough complex is caused by Bordetella bronchiseptica (Bemis et al., 1977) and/or co-infecting viruses or bacteria and is a significant cause of morbidity in shelter dogs (Appel and Percy, 1970) .

Both puppies and kittens in this study that had clinical signs of gastrointestinal disease at intake were more likely to be delayed before they were approved as adoptable on medical grounds (p<0.001). An investigation of disease prevalence in shelter dogs reported that coughing and diarrhea were the most common clinical signs in unwell dogs and that vomiting, diarrhea and parvovirus were more common in puppies than in juveniles or adults (Wells and Hepper, 1999) . Similarly, a paper that compared the frequency of disease among puppies acquired from various sources, reported that pups from pet stores and animal shelters were significantly more likely to have intestinal tract disease (Scarlett et al., 1994) than those acquired from private owners or breeders.

In this population, the presence of skin disease delayed time to veterinary approval for adoption in puppies (P = 0.01), but not in kittens (P = 0.51). This could be related to the relative prevalence of different causative factors for dermatological disease in each species, although unfortunately specific information of this kind was not available. This finding could be explained if skin disease in kittens was due predominately to flea infestation, which is quickly and easily cured, while conditions that required longer courses of treatment, such as Demodex or bacterial infections, predominated in puppies.

Delays before approval for adoption on medical grounds were less likely in kittens that were owner-relinquished or transferred from another shelter rather than stray; female; and/or older at the time of intake (stray status p< 0.001; gender P = 0.029; age p<0.0001). It is possible that kittens that have been owned or have previously been taken into a shelter are more likely to have been vaccinated than stray kittens, thereby reducing their risk of developing feline panleukopenia or the more severe clinical effects of FHV and/or FCV. Additionally, the immune system in older kittens is also more likely to be developed than in younger kittens, although waning maternally-derived immunity could reduce the protection afforded by age. However, a recent study documenting the response to vaccination of feral cats at the time of neutering reported that only some of the cats had protective serum antibody titers against FPV (33%), FHV (21%) and FCV (64%) (Fischer et al., 2007) . It could be speculated therefore that maternally-derived immunity against these common shelter pathogens in stray kittens is not common, in part explaining the higher risk for delays on medical grounds before clearance for adoption in younger kittens.

For both puppies and kittens in this study, prompt oral or parenteral antimicrobial treatment for respiratory and/or ocular disease on intake was significantly associated with reduced time before approval as suitable for adoption on medical grounds puppies (P = 0.02; kittens P = 0.03). The decision to prescribe antibiotics was made purely on the clinical judgment of the attending shelter veterinarian and it is possible that bias could have been introduced if the animals that received antibiotic treatment had initially presented with more severe clinical signs. However, if that was the case, treated animals would be less likely to reach approval on medical grounds for adoption before untreated animals -the opposite of the findings reported here. Our findings agree with the results of previous studies which reported that prolonged shelter time was a major risk factor for feline URI 3 and the development of coughing in dogs (Edinboro et al., 2004) . Early antimicrobial treatment of puppies and kittens with respiratory and/or ocular signs admitted to animal shelters is also likely to be cost effective, as the cost of most antimicrobial drugs compare favorably with the daily costs of providing shelter care.

A limitation of this study is that the outcome variable (approval for adoption on medical grounds by a veterinarian) may not have been based on uniform and objective criteria. Misclassification bias could have been caused by inter-or intra-observer variability in the physical examination findings between different veterinarians working at the shelter. The direction of this potential bias is unlikely to be in only one direction, thereby reducing its net effect, but its magnitude is difficult to predict.

Further prospective studies, performed at a range of shelters with different admission policies and in a range of locations, could enable more targeted recommendations to be made to optimize selection policies in animal shelters. Facilitating a smooth transition from intake to approval for adoption will potentially increase shelter intake, an important factor in reducing pet homelessness. Table 1 Descriptive statistics for information collected at the veterinary examination at the time of intake for puppies (n=333) and kittens (n = 365) entering an adoption-guarantee animal shelter. N/A = Not applicable. Results of multivariable analysis of information collected at the veterinary examination at the time of intake for puppies and kittens entering an adoption-guarantee animal shelter. An odds ratio >1 indicates increased odds of delay from intake to veterinary approval for adoption on medical grounds. NA = Not applicable 1 . 

",0.6470452415738625
"Traditional uses of medicinal plants used by Indigenous communities for veterinary practices at Bajaur Agency, Pakistan","Background: The pastoral lifestyle of Indigenous communities of Bajaur Agency is bringing them close to natural remedies for treating their domestic animals. Several studies have been conducted across the globe describing the importance of traditional knowledge in veterinary care. Therefore, this study was planned with the aim to record knowledge on ethnoveterinary practices from the remote areas and share sit with other communities through published literature. Methods: Data was gathered from community members through semi-structured interviews and analyzed through informant consensus factor (Fic) to evaluate the consent of current ethnoveterinary practices among the local people. Results: In total, 73 medicinal plants were recorded under the ethnoveterinary practices. Most widely used medicinal plants with maximum use reports (URs) were Visnaga daucoides Gaertn., Foeniculum vulgare Mill., Solanum virginianum L., Withania somnifera (L.) Dunal, Glycyrrhiza glabra L., and Curcuma longa L. New medicinal values were found with confidential level of citations for species including Heracleum candicans and Glycerhiza glabra. Family Apiaceae was the utmost family with high number (7 species) of medicinal plants. Maximum number of medicinal plants (32) was used for gastric problems. High Fic was recorded for dermatological (0.97) followed by reproductive (0.93) and gastrointestinal disorders (0.92). The main route of remedies administration was oral. Conclusions: Current study revealed that the study area has sufficient knowledge on ethnoveterinary medicinal plants. This knowledge is in the custody of nomadic grazers, herders, and aged community members. Plants with new medicinal uses need to be validated phytochemically and pharmacologically for the development of new alternative drugs for veterinary purposes.","The historical utilization of plants as health remedies both for human and animal is centuries old. It has been recognized that plants have the capacity to combat several types of diseases ethnoveterinary medicines, a term generally used for folk skills, beliefs, knowledge, practices, methods related to animals' health, and cure of various ailments in the rural areas [1] . Ethnoveterinary practices have achieved immense significance for the last decade owing to the discovery of some effective ethnoveterinary products [2] . The utilization of traditional remedies poses a cheaper, easier, and sustainable alternative to synthetic drugs and pharmaceuticals [3] . It has been reported that due to lack of proper animal husbandry practices, about 30-35% of the losses occur in the animals' breeding sectors especially in developing countries [4] , where the rural people are heavily dependent on livestock farming for their livelihood activities [5] . The Indigenous communities living in rural and mountainous territories of developing world consider livestock a vital source for economy, social security, and food and is thought to be a symbol of prestige for a particular family [6] .

Livestock being as a subsector contributes around 56% of value addition in the agriculture sector and approximately 11% towards the gross domestic product (GDP). About 30 million people living in the rural areas of the country are involved with the livestock subsector [7] . Hence, livestock raring plays a significant role in poverty reduction strategies. According to the report of economic survey of Pakistan [8] , the national herd of Pakistan includes 53.8 million goats, 29.6 million cattle, 27.3 million buffalos, 26.5 million sheep, and 0.9 million camels. People residing in the remote areas utilize medicinal plants for livestock's health. Particularly, the conventional lifestyle of nomadic and pastoralists makes it difficult for them to reach veterinary extension services due to high costs and less availability of allopathic medicines [9] .

In South Asia, several ethnoveterinary studies have been conducted [10] [11] [12] [13] [14] [15] [16] [17] [18] including Pakistan [6, 9, [19] [20] [21] [22] [23] [24] [25] [26] . However, scarce studies on ethnoveterinary medicines have been reported from the Federally Administrated Tribal Areas (FATA) of the country. The tribal areas mainly comprised of mountainous territories where people use medicinal plants to treat livestock's diseases. Traditional ethnoveterinary knowledge is mainly transmitted orally from one generation to another generation in the form of folk remedies, drawing stories, poems, drawing stories, folk myths, songs, and proverbs. This transmission of Indigenous knowledge through oral way faces critical threats of extinction. Therefore, it is necessary to record, document, and encourage the ethnoveterinary medication and integrate them into the existing animal health care services [5] .

Bajaur agency is among one of the Federally Administrated Tribal Areas (FATA) of Pakistan having diversity of medicinal plants being used for the livestock's healthcare services. Due to remote nature and lack of quality education, the area has been little explored for the scientific documentations of traditional knowledge. There is a dire need to explore the folk knowledge about the utilization of herbal remedies for veterinary practices prior to being extinct. Hence, the current study was planned to investigate and document the traditional ethnoveterinary knowledge and practices and release it from the custody of knowledge bearers for sharing it with other communities through publish literature.

Bajaur (Khar: headquarter) is the smallest agency of the FATA having a total area of 1290 km 2 . It shares 52 km border with Afghanistan, which is of great importance to Pakistan and the region. The study area lays at an altitude of 1126 m above the sea level and geographically exists between 34°-30°and 34°-58°latitudes and 71°-11°and 71°-30°longitudes. The Agency is surrounded to the west by Kunar Valley of Afghanistan being separated by the rugged Hindukush hills and other mountain passes known as Nawa Pass, Ghakhi Pass, and Letai Sar being the notable ones. The agency borders on south with Mohmand agency, on east with Lower Dir District and the Panjkora River, and on north with the watershed between Bajaur Agency and District Dir. Moreover, the agency is situated at the extreme end of the Himalayan Range. The areas dominated by agricultural lands are receiving about 800 mm of average rain fall per annum. The two main tribes of Bajaur Agency known as Tarkani and Utman Khel are mainly populated into seven Tehsils including Barang, Nawagai, Khar, Mamund, Salarzai, Utman Khel, and Chamarkand. By profession, mostly, the people are farmers, teacher, drivers, and doing small scale businesses and jobs inside/outside the country. Almost every household has a herd of domestic animals for socioeconomic gains. There are only three degree-level colleges and five higher secondary schools. Moreover, there are only two government hospitals in the study area, while most people are deprived of modern health facilities, which justify their reliance on local herbalists (Hakims). The study area consists of one veterinary hospital and 20 small dispensaries to treat the domestic cattle. However, the local people still rely on traditional recipes due to larger distances from the aforementioned health centers. The dominant vegetation in the area is comprised of Ailanthus altissimo, Eucalyptus camaldulensis, Ficus carica, Melia azedarach, Morus indica, Morus nigra, Olea ferruginea, Pinus roxburghii, Quercus baloot, and Rumex hastatus.

In the month of April, respondents were targeted based on their strong reputation in the field of ethnomedicinal knowledge while field survey was conducted from May to August 2016. Field visits were carried out prior to medicinal data collection in order to acknowledge the cooperation of the Indigenous communities. Mr. Amir Hasan Khan, the local occupant of the area, visited different sites with his volunteer team including a taxonomist and a pharmacist. He arranged several meetings with the local representatives known as Maliks, to whom objectives of the study were presented.

A semistructured questionnaire was developed to gather knowledge on ethnoveterinary plants by following the method adopted by Martin [27] . Mostly, the folk knowledge was gathered from nomads, farmers, and aged community members. The interviews were conducted at various places and in the local language called ""Pashto."" Each informant was acknowledged by presenting the main theme of the study to them in order to gain their consent and trust, which allowed the informants to talk more freely and openly. The recorded information was once again redisplayed to the informants to avoid errors and falsification.

Data was collected from different sites known as Pashat, Tali, Inayat Kali, Ghar Shamozai, Loe Sum, Barang, Mandal, Khar, Mamund, and Salarzai. Accordingly, the sites were categorized into foot hill villages and mountainous villages (Fig. 1) . A total of 80 key respondents were selected belonging to different age groups, i.e., 68 males and 12 females ( Table 1 ). The selection of respondent was based on their high reputation with respect to traditional knowledge on ethnoveterinary plants. Continuous relationships were maintained with the Indigenous communities throughout the course of survey for the strong validation of traditional knowledge.

Surveyed ethnoveterinary medicinal plants were collected and identified by taxonomist at the Department of Botany, Shaheed Benazir Bhuto University Sheringal, District Dir (Upper), Khyber Pakhtunkhwa, Pakistan. Species botanical names and their family names were corrected and verified through the website www.kew.org/mpns. After collection, plants were pressed and dried under the shade, were poisoned (1% HgCl 2 solution), and were mounted properly on the herbarium sheets for future reference. Each herbarium sheet was labeled with a voucher number and submitted to the aforementioned department [28, 29] .

For each of the specie, use reports (URs) (citations) were counted. UR may be defined as the utilization of part of a plant species for a particular disease mentioned by an informant. To determine the informant consensus factor (Fic), the reported species were arranged in various groups according to the ailment treated [11] . Ten ailment categories were prepared from the data. To calculate the Fic, we used the formula, i.e., Fic = Nur − Nt/ Nur − 1. Here, Nur indicates the number of citations in each use category and Nt represents the number of species cited.

Prospects and challenges to traditional ethnoveterinary knowledge Indigenous communities play significant role in reporting traditional uses of medicinal flora. Indigenous knowledge can be used as a tool to conserve and maintain the green diversity, and could be further utilized for scientific validation [12] . During the 32nd session of United Nations Educational, Scientific and Cultural Organization (UNESCO), traditional knowledge on ethnoveterinary medicines was declared an important part of cultural heritage, which is required to be brought under study, sustenance, and protection [30] . Indigenous communities at Bajaur Agency are dependent on livestock for supporting their livelihood. Medicinal plants have a pivot role in the treatment of livestock's ailments in the area. Usually, this treatment process depends either on the traditional knowledge being orally transmitted to the current generation of local people from their ancestors or through personal experiences. Previous scientific literature has focused on the correlation of traditional medical expertise to ethnobotanical knowledge for the treatment of human ailments [31, 32] , although the same plants may be used to treat livestock [33, 34] . In our study, we have observed that the herders, farmers, and older community members are more equipped with traditional knowledge and familiar with veterinary medications, diagnosis process, and treatment.

Indigenous people of the study area are rich in traditional knowledge on veterinary medicines, which may be due to their close observation on domestic animals being considered as an important part of traditional lifestyle. Most commonly, the male community member grazes herds of animal, while females take part in households' management. Figures 2 and 3 showed some of the images of the grazed domestic animals, which are treated with medicinal plant in the area. Other studies have explained this in a different way that men due to close proximity tend to know more about the animal behavior than women [31] .

People of the study area use plants not only for medicinal purposes to treat their domestic animals but also as a fodder. Local community also prevents their animals from such nutrition, which is not healthy in certain conditions and seasons. One may consider this prevention to be a part of ethnoveterinary practices. Nutrition is playing an important role in ethnoveterinary practices in both prevention and cure of domestic animals [35] . Livestock usually ingests some extra and non-important food substances in the green fodder, which could be termed as food medicines or medicinal food [36] . Studies have highlighted the importance of ""food as medicines"" in the context of local traditional knowledge; however, possible health advantages of food in ethnoveterinary methods need further attention [37] . Testing the nutritional status of each traditional ethnoveterinary remedy is not necessary; however, it is essential to evaluate the biological efficacy from the phytochemical, pharmacological, toxicological, and clinical perspectives for wider application. A considerable proportion of the documented uses of plant taxa in our study are in accordance with the established pharmacological effects [36] .

The prevailing Indigenous ethnoveterinary knowledge in the study area is facing certain constrains leading it towards extinction. As an example, the nature of traditional knowledge is making it more difficult to learn and then transfer it in an accurate way. Furthermore, practicing traditional therapies are not being respected by the new generation. Other challenges include low literacy rate in the study area, no proper documentation of Indigenous knowledge, and introduction of modern allopathic medicines, rapid technological advancement, and environmental degradation. Similar kinds of threats have also been reported in other communities across the world [38] [39] [40] . Informants with little education were found less familiar to the traditional knowledge while people having no formal education were more responsive in this regard. Some studies have found that education can be correlated with expertise either positively [41, 42] or negatively [43] , while others found no relationship [44] . Moreover, it is also ambiguous to determine the effect of ""modernity"" on the loss of ethnomedicinal knowledge. Modernity has an established association with greater medicinal competence in Dominica [45] but appeared unrelated to variation in expertise among Tsimane horticulturalists in Bolivia [41] . Furthermore, it is also unclear whether correlation of expertise exists between ethnomedicinal knowledge and ethnoveterinary approaches; however, livestock keepers hold extensive knowledge related to disease prevention, diagnosis, and both traditional and novel biomedical treatments [26] . In summary, despite maintaining knowledge on ethnoveterinary practices by the locals, the tendency to utilize modern pharmaceuticals is increasing day by day. Hence, the conservation of ethnomedicinal knowledge by the local communities is extremely important for the livestock's health in the remote areas.

The use of plants for medical purpose to treat a wide array of maladies emanates traces since the recorded history and even before. In our study, 73 plant species belonging to 43 families were documented. Table 2 presents details on the documented medicinal plants including their botanical names, vernacular names, family names, specimen numbers, parts used, medicinal uses, and use reports. Family Apiaceae (7 species) has the high number of individual species used in ethnoveterinary practices followed by Fabaceae (5 species). Other studies have also reported Apiaceae as the dominant plant family being used in traditional medications [37, 46] . The rationale of high use of Apiaceae species in the current study, though based on traditional evidence, may be referred to their chemical constituents such as phenolics, poly phenolics, lectins, alkaloids, terpenoids, and essential oils, which carry antimicrobial potential [47] . Due to the predominance of sheep, goats, cows, and donkeys in the study area, we have specifically recorded the ethnoveterinary practices used for the treatment of these four types of domestic animals. Key informants declared extensive uses of Visnaga daucoides Gaertn. Table 2) . Medicinal plants with high URs strengthen the concept that such species are more significant to the local population and useful in sharing the traditional knowledge with one another in the area. In our study, V. daucoides is used to treat diarrhea, abdominal pain, and retained placenta in domestic animals. A whole plant is subjected to powder and is combined with flour and black tea to treat digestive problems especially in cow and buffalo.

Amaryllidaceae

Bulb of the herb is crushed and added milk for orally given to animals (6-8 days) for curing digestive complaints.

Bulb is crushed and mixed with way to administered orally for several days in order to rate of fertility in domestic animals.

Narcissus tazetta L. ""SBBU-59""

Gul-E-Nargas Leaves 10 Along with gurr and flour, fresh leaves (¼ kg) are boiled and orally given to livestock for the retained placental removal.

Apiaceae

Tea is prepared from its fruit (¼ kg), and then, it is combined with flour and given to cattle for 3 days in order to treat gastric problems.

Cuminum cyminum L. ""SBBU-63""

Half kilogram of fruit is boiled in black tea and orally given for 15-20 days on daily basis for the expulsion of intestinal worms and treated gastric problems.

Eryngium biehersteinianum (M. Bieb.) Nevski ""SBBU-5""

Stem and leaves and stem 23 Powder of its stem and leaves is orally given for the treatment of liver problems up to the duration of 8-12 days.

Foeniculum vulgare Mill. ""SBBU-61""

Fruit, leaves 47 Decoction is made from it fresh leaves and fruit (150-200 g), and t hen, it is combined with gurr, given orally to livestock for appetite and as sedative for the duration of 5 to 6 days.

Wall. ex DC. ""SBBU-17""

Fresh root of the plant (200 g) is combined with wheat flour and made to paste which is orally given to goat, cow, and sheep as sexual tonic and to enhance the rate of fertility up to 3 days.

Trachyspermum ammi (L.) Sprague ""SBBU-60""

Seeds (¼ kg) of the plant, Allium cepa, wheat flour, and Foeniculum vulgare are thoroughly mixed. The resultant blend is then orally given (15 days) and is considered as good appetizer.

Tea is made from its fruit and given orally to sheep, goat, cow, and buffalo while treating diarrhea, abdominal pain, and retained placenta. The remedy is constantly utilized for the duration of 3 days.

Calotropis procera (Aiton) Dryand. ""SBBU-18""

Plants' fresh leaves are taken and decoction is made, and after that, the decoction is combined with ""Ajuga integrifolia"" and is used for dermal parasites for 3 to 5 days.

Nerium oleander L. ""SBBU-22""

To relieve the external parasite, the decoction of its leaves is used for animal bathing especially goat and cow. The decoction obtained from its bark and is combined with butter which is administered orally to all type of domestic animals to treat skin problems and as blood purifier.

Cassia fistula L. ""SBBU-4""

Amaltas Fruit 13

Fruit of the plant is subjected to boiling along with milk and administered orally up to 3 days to all sort of domestic cattle to relieve fever and gastric complexities.

Glycyrrhiza glabra L. ""SBBU-26""

Khwaga Waly Roots 33 Root (¼ kg) is subjected to paste which is mixed with flour and oil and then is given to goat, sheep, cow, and buffalo to increase milk production and enhance the rate of fertility. The remedy is used for the duration of 5 to 7 days.

Lotus corniculatus L. ""SBBU-72""

Stem and leaves, 19 Stem and leaves are crushed in weight of ¼ kg and orally given to cattle along with bread or dough for 7 to 10 days as sexual tonic and for urinary tract infections (UTI).

Trigonella foenum-graecum L. ""SBBU-11""

Malkhoozi Seeds 9

Seeds (150 g) are crushed and given in dough to animals (5-6 days) against gastric disorders. Decoction is made from the leaves and then gurr is added. This remedy is given orally to cattle for blood purification and as vormifuge. The water is applied topically to treat skin ailments.

Mentha spicata L ""SBBU-58""

Powder is made and decoction is made and then mixed with gurr and taken by animals to cure digestive problems.

Ocimum basilicum L. ""SBBU-9""

Kashmaly Leaves seed 11 Plant leaves and seeds are subjected to decoction and used topically for skin problems.

Salvia moorcroftiana Wall. ex Benth. ""SBBU-13""

Kharghwag Leaves 9

Decoction of its leaves is given orally daily for the treatment of digestive problems.

Gossypium arboreum L. ""SBBU-43"" Pomba Kal 11 About ¼ kg of its powder is mixed with gurr and used for 10 to 12 days. This remedy is administered orally on daily basis as galactagogue.

Grewia optiva J.R.Drumm. ex Burret ""SBBU-62""

Whole plant 6 Dried plant powder is subjected to oil (250 ml), administered orally and topically twice a day for 5 to 7 days for wound healing process. The usage mode of ethnoveterinary plant species by one ethnic community is different from other communities due to difference in traditional knowledge [10, 15] . Previous literature has shown that decoction of the fruit of V. daucoides is used during abdominal pain, which is used to enhance body temperature in the study area [21, 48] . In the same way, the F. vulgare is considered as a strong appetite and sedative. In other cultures across the globe, F. vulgare is used for various livestock problems. For instance, this plant is effective in digestion and diarrhea, when mixed with Camellia sinensis, Trachyspermum ammi, ghee, and sugar [6, 21] . Pneumonia is also being treated by giving its seeds to the animals [24] , while other uses include galactagogue and ruminative [49] . Various parts of S. virginianum are taken for the treatment of cough, fever, milk production, and pain. There is scarce literature on the use of S. virginianum as galactagogue, which shows the unique use of this plant species in the study area and familiarity of local population through longtime experiences. Published literature has indicated that the plant is also used for wound healing process [20] fever, cough, and intestinal infections [20] . Roots and leaves of W. somnifera are given to sheep, cow, and buffalo for milk production and used as antipyretic and sexual tonic. Indigenous populations comprising of various cultures residing in Lesser Himalayas (Pakistan) use W. somnifera for bovine mastitis [6] , while in Ethiopia, this plant is being used to protect animals from bad evils [50] . The plant has carminative effects and is used to remove the flatulence [25] . Additionally, this plant is used as refrigerant and for abdominal pain, digestion, jaundice, skeletonmuscular ailments, and wound healing against sunstroke [26] ; for treating diarrhea [22] ; for trypanosomiasis [9] ; and for anorexia [50] . Informant reported G. glabra as galactagogue and enhances the rate of fertility. Mussarat et al. [22] reported that this plant is culturally used for the treatment of cough by the Indigenous communities residing near the Indus River, Pakistan. However, from the literature, no conclusive evidence was found on the reported uses of G. glabra in our study. Such evidence-based observations could justify the idea of cultural diversity across the regional level in plant remedies. Previous studies related to the human's uses of G. glabra have demonstrates its effectiveness in the treatment of sex hormone imbalances and menopausal symptoms in women [51] . In the current investigation, rhizome of C. longa is used as antiparasitic and treating genital infection and problems. In other cultures, across the country, the dried rhizome of C. longa is mixed with eggs and given for mastitis [22] , jaundice, and skeleton muscular ailments [26] . Decoction of its leaves is mixed sugar, which is used as wound healing agent [6, 19] . A root of C. longa is used for hoof problems and sore joints [52] . In our study, the mustard oil is mixed with whey and is taken orally to relieve abdominal pain. The cultural ethnoveterinary uses from the Lesser Himalayas (Pakistan) include that the oil extracted from B. rapa seeds is utilized for stomach disorders, eye infection, and skin diseases [6] . Furthermore, Brassica rapa L. seeds are used for the retention of fetal membrane, while its oil is effective in treating genital prolepses and sores [53] . This plant is also used in placental retention and mastitis and as antiparasitic [19] ; myiasis, mange, and helminthiasis [20] ; and flatulence [49] . All these researchbased findings showed that the same medicinal plants are being used in different parts of the country; however, their uses differ from area to area and from culture to culture [48] . The ethnoveterinary plants use by one ethnic community is almost different from other communities due to several reasons. To make a comprehensive comparative cultural diversity analysis of plant utilization in ethnoveterinary practices, we have selected a study conducted by Aziz et al. [48] in the FATA region of Pakistan. In comparison, we have found that most widely used medicinal plant species in our study are V. daucoides, F. vulgare, S. virginianum, W. somnifera, G. glabra, and C. longa. While according to Aziz et al. [48] , the ethnic communities in South Waziristan Agency are widely utilizing plant species such as B. rapa, Punica granatum, Capparis decidua, Mentha longifolia, Withania coagulans, and C. longa, during comparative analysis, it was found that only 15 medicinal plants were commonly used in both regions for ethnoveterinary practices, which include Acacia modesta Wall, Allium cepa L., Allium sativum L., B. rapa, Calotropis procera (Aiton) Dryand., Cannabis sativa L., Chenopodium album L. C. longa, F. vulgare, Juglans regia L., Nicotiana tabacum L., Peganum harmala L., Quercus oblongata D. Don, Trachyspermum ammi (L.) Sprague, and V. daucoides. Certain variations in the utilization of these plants and their parts were observed in both areas. For instance, the bulb of A. cepa is used as galactagogue by Waziristanian communities while in Bajaur, it is used to treat digestive problems. A. sativum is utilized for genital prolapsed while the same plant is used as sexual tonic for animals in Bajaur Agency. The seeds of B. rapa are widely used as appetizer and tonic and for cough, seasonal allergies, stomach disorders, and skin infections in South Waziristan Agency, while in the other region, it is used only against gastro-intestinal disorders. The Indigenous communities at South Waziristan Agency consider the leaves of C. procera useful in joint pain while on the other side, the residents of Bajaur Agency used the latex against skin problems. C. album is used for wound healing and flatulence at Waziristan while as stomachic at Bajaur Agency. J. regia is given for the retention of placenta at Waziristan while gastric problems in Bajaur. P. harmala is extensively used for gastrointestinal problems, as antiparasitic, and for skin diseases by Waziristanian communities, while it is used only for the riddance of external parasites in Bajaur. The possible reason for low consensus of the two regions in ethnoveterinary medicinal plants may be due to unique vegetation and distinct socio-cultural values. According to a survey, out of 122 plant-derived pure compounds, 80% (94 plant species) were having the same potential as indicated in traditional medications [54] . As an example, galegine is obtained from Galega officinalis L. and is used in the production of metformin and other bisguanidine-type anti-diabetic drugs [55] ; khellin, extracted from V. daucoides., led to the development of cromolyn in the form of sodium cromoglycate, which is used as a bronchodilator; and papaverine isolated from Papaver somniferum forms the baseline for verapamil, which is generally utilized for hypertension [55] . Survey participants did not describe the standardized dosage and recovery time like other previous ethnoveterinary documentations. The main problem highlighted in other studies is the lack of accuracy in such ethnoveterinary practices, which also push the locals towards modern allopathic drugs for livestock health maintenance [20, 56] . The main reason that veterinarian has always complained is the non-standardized dosage in traditional medicines. Though this is an accusation, one ethnomedicine does not mean that they lack efficacy but require standardization, which could benefit the traditional system by minimizing risks and toxicities. According to Kearns [57] , ethnoveterinary medicines are facing a great intellectual challenge from social theory and postmodernism, and this challenge was focused while detecting variations in animal health practices, beliefs, and experiences of various social groups. Generally, it is not possible for all ethnoveterinary practices to be effective and, at the same time, they have certain weakness in terms of their efficacy as compared to modern medications [58] . Though it is convincing that most of the traditional veterinary medications have clear and sound health effects, many modern allopathic drugs are based on these medicines [59] .

Certain plants in our study were used in single form for more than one disease. For example, Cedrus deodara (Roxb. ex D. Don) G. Don is used in a condition, in which milk obtained from the cattle gives bad smell, then the oil is given orally to the cattle. It is also used as a cooling agent and in treating digestive problems. In large quantity, the oil have the potential to depress the sexual power of male animals [49] . Monteiro et al. [60] also reported similar findings from Pakistan and Brazil, respectively, where they described multiple uses of a single medicinal plant. Utilization of certain plant species for multiple diseases is a widespread practice in ethnoveterinary medications. In contrast, some ethnoveterinary remedies (polyherbal formulations) are being made by combing two or more plants and additives such as whey, ghee, and sugar. This addition is generally followed in remedies to counteract the astringent taste, dilute, and reduce the relative potency of the remedy [61] .

In the study area, a total of 32 plants were reported for gastrointestinal problems with maximum use reports of 433 (Table 3) , which is regarded as the most common disease category in domestic animals being represented by abdominal pain, diarrhea, and digestive problems. These health issues can be easily detected by the respondents and may explain the fact that why the gastric problem category is high in ours as well as in others studies. Different ailments were categorized into 10 groups such as dermatological, gastrointestinal, galactagogue, reproductive, respiratory disorders, tonic, wound healing fever, and miscellaneous. Those medical conditions, which were not fully described by the interviewees, were placed into the miscellaneous category. These include eye problems, weakness, and abnormal conditions related to various organ systems of animal bodies. Highest Fic values were recorded for dermatological problems (0.97) followed by reproductive ailments (0.93) and gastric disorders (0.92) ( Table 3) . Fic value is an indicator of showing the consent of the local people on a specific plant species and efficacy of a certain taxa [62] . Sharma et al. [63] declared that when Fic becomes 1, it means that the local population is exchanging their view, ideas, and information about traditional medications, while on the other side, if the Fic value is 0, then it is vice versa. Fic value in the current study was recorded in between 0.85 and 0.97 for various livestock ailments (Table 3 ). These findings indicate the highest consent among the local people on traditional herbal therapies. Previous research studies conducted in other areas also agreed to high consent of local people on traditional animal therapies. For instance, the reported Fic values for dermatological problems were 0.93, 0.93, and 0.82 [26, 64, 65] ; for reproductive disorders, 1.00 and 0.89 [65, 66] ; for gastric problems, 0.90, 0.70, 0.92, 0.95, and 0.94 [26, [64] [65] [66] ; for galactagogue, 0.83 and 0.50 [6, 65] ; and for wound healing, 0.40 and 0.45 [6, 67] . Heinrich et al. [68] has submitted the idea that high Fic values can be used as a tool to target the plants for the isolation of biologically active components. In our study, most livestock's ailments were mentioned to be seasonal and epidemic due to change in fodder. Furthermore, the concept of hot and cold food is also famous in order to prevent animals from diseases. The local residents change the relative fodder in different seasons in order to minimize the chances of various health problems in cattle. As an example, the seeds of the Nigella sativa L. and kernels of Q. oblongata are given to the cattle to energize them during the cold season. Similarly, the fruits of the Streblus asper Lour. produce cooling effects and considered to be a better remedy during hot summer season. In the same manner, local communities tend to give the infusion of Cannabis sativa L to their livestock in the summer season. Quinlan [69] and Raziq et al. [9] has also mentioned the concept of hot and cold food in traditional veterinary medications.

Drugs derived from plants or their extracts have certain therapeutic properties. To replace antibiotics by suitable therapeutic agents, plants can play an important role in combating with bacterial pathogens. There are several essential oils, which can be used as alternate of antibiotics. These oils can be easily isolated, having low toxicity on mammalian cells, and can be easily degraded in soil and water [70] . In this section, we will analyze the pharmacological evidences of the most utilized studied medicinal plant species in order to check their therapeutic efficacy.

In F. vulgare, phenols, phenolic glycosides, and volatile aroma compounds such as transanethole, estragole, and fenchone are reportedly the key phytoconstituents and responsible for its antioxidant activity. F. vulgare is pharmacologically validated (in vitro and in vivo) in demonstrating activities such as antibacterial, antifungal, antioxidant, antithrombotic, and hepatoprotective [71] . By investigation, it was found that the leaf extracts of S. virginianum is more active against Candida albicans, Salmonella typhi, Staphylococcus aureus, and nematodes [72, 73] . For various extracts obtained in alcohol and water, it was found that W. somnifera has antibacterial potential, antihypercholesterolemic activities as well as diuretic potential [72, 74] . It has been reported that alcoholic and aqueous extracts of C. longa have shown antibacterial activity [74] while its ethanol, petroleum, water, and chloroform extracts are effective against certain strains of viruses, bacteria, and fungi and also have shown anti-inflammatory effects [75] . Researchers have claimed that plant-derived medicines used in traditional systems across the globe can be used as an indicator to consider them more effective than modern drugs [6] . Livestock keepers are using several plant-derived remedies for various acute as well as chronic disorders of cattle. Plant-derived medicines have been used by physicians for hundreds of years in traditional systems, and most of the world population rely on these products for health care systems [76] . There are several thousand plants across the globe being utilized for various therapeutic purposes both animals and humans [49] . Out of these medicinal plants, very low proportion has been investigated and proved scientifically for their Indigenous uses [77] . The essential oils in medicinal plants are having strong antimicrobial potential. As an example, essential oils of cinnamon, thyme, and oregano are therapeutically effective [78] .

Antibiotic resistance is an emerging global concern related to veterinary and human medications [79] . Hence, it is necessary to search for new compounds to combat antibiotic resistant bacteria. Improper therapeutic utilization of antimicrobial medicines in fishery, poultry, agriculture, and animal farming facilitates the emergence and production of drug resistant strains. Additionally, poor prevention and control of unhygienic practices contribute in resistance emergence. The World Health Organization, Food and Agriculture Organization, and World Organization for Animal Health are stressing to promote best practices to avoid the emergence and spread of antibacterial resistance. Continuous attempts are in progress to promote the moderate use of antibiotics in human as well as in animals to tackle the problem of antimicrobial drug resistance [80] .

In general, plants should be used as an alternative to synthetic drugs and investigated for their therapeutic efficacy. Certain plants in our study including Boerhavia erecta L., Celtis australis L., Chamaecyparis obtusa (Siebold & Zucc.) Endl., Eryngium biehersteinianum (M. Bieb.), Gossypium arboreum L., H. candicans Wall. ex DC., Narcissus tazetta L., Opuntia littoralis (Engelm.) Cockerell, and S. asper need comprehensive phytochemical, pharmacological, and toxicological investigations.

Current study, one health concept, and changing environment Current study reports that there are several ailments being treated with medicinal plants by the Indigenous populations. Most prevalent disease categories were dermatological, reproductive, and gastric problems. The dominance of these diseases not only poses threats to the domestic animals but also increases the chances of zoonoses. Local population uses various animal products; hence, there are maximum chances of the migration of infectious diseases from these animals to humans. Linkage of the ethnoveterinary studies with the researches of other disciplines may form an interdisciplinary approach to combat several types of health issues in both animals and plants. This approach mainly led to the concept of one health, which contributes towards understanding the complexities in health problems of living beings [81] . A recent surge in emerging infectious diseases and their putative associated costs to society have reignited interest in the drive of disease emergence. A number of pathogens have emerged in the last 20 years, including the severe acute respiratory syndrome virus, Hendra virus, and Nipah virus. However, there is a growing concern about the H 5 N 1 influenza virus, which fuelled much of the recent debate around emerging infectious diseases (EIDs) [82] . One of the benefits that accrued from the attention on EIDs has been an increased recognition across a range of disciplines that the health of animals (including humans) and the health of the broader ecosystem are inextricably linked, which certainly given momentum to One Health movement. One Health is not all about EIDs, but it also covers important issues of food security and food safety [83] . There is a strong consensus that the climate is changing now and that human activities are the primary cause [84] . However, it is clear that climate change will alter the distribution and incidence of a wide range of diseases either directly or indirectly (e.g., diseases with a development stage outside the host) [85, 86] . The pathways by which climate change can affect host pathogen vector interactions have recently been well described by Gallana et al et al. [86] .

One Health Initiative Task Force (OHITF) [87] defines one health as ""the promotion, improvement, and defense for the health and well being of all species by enhancing cooperation and collaboration between physicians, veterinarians, and other scientific health professionals and by promoting strengths in leadership and management to achieve these goals"". The one health approach plays a significant role in the prevention and control of zoonoses. Approximately 75% of new emerging human infectious diseases are defined as zoonotic [79, 88] . Of the 1461 infectious diseases, approximately 60% are caused by multi-host pathogens, characterized by their movement across various species [89] . This gives significant credence to the importance of examining health effects across species, in order to fully understand the public health and economic impact of such diseases and to help implement treatment and preventive programs.

The application of one health approach has been recognized as a critical need by international organizations as well as the preferred approach to address global health issues. It is also noted that knowledge in veterinary medicine and animal nutrition and husbandry could provide insights into human nutrition and growth.

It is a widespread phenomenon that natural resources including plants are always prone to threats in their natural habitat due to rapid human intervention and destructions of natural resources. The collection process of medicinal plants for ethnic practices and other anthropogenic practices is not only destructing the Indigenous flora but also posing a threat to the traditional knowledge. UNESCO has emphasized on the documentation and preservation of traditional knowledge in South Asia generally and Pakistan and India particularly. However, efforts are going on but they are not sufficient for the conservation of traditional knowledge persistent since several centuries, which can lead to valuable discoveries in modern healthcare system. The local perception of Indigenous communities regarding the threats being faced to the ecological resources especially the medicinal plants was examined in the current study. The lack of awareness has been observed as a major threat to the conservation of plant resources. It was also observed that different factors including time of collection, processing, storage, and herbal preparations are important and necessary steps to be considered for both economic returns and conservation. Mainly, the local healers are involved in the collection of medicinal plants. A study in the Swat region of Pakistan has shown that higher economic outcomes can be obtained from proper harvesting of wild medicinal plants as compared to the standard cash crop [90] . Other studies are supporting our results by showing an enormous potential in improving the harvesting, storage, use, preparation, and marketing of the herbal product as a source of income [91] . In the remote areas of the study region, local inhabitants obtained significant economic advantages from forest products. Similar advantages have been reported for other mountainous communities in the northern parts of Pakistan [26] .

There are certain other threats to the medicinal plant resources of the study area, which include deforestation, heavy grazing pressure, uncontrolled collection of fodder, and other non-timber forest products by the local people and traders. Several studies have reported a decrease in the number of medicinal plants due to over exploitation and environmental degradation [92, 93] . It is therefore a dire need to manage and design the overall grazing system to encourage the sustainable regeneration and protection of medicinal plants. Keeping the observation and findings of the current investigation, proper management steps should be taken with the active participation from the Indigenous communities to conserve this precious flora. It is also important to aware the local people about the market value and sustainable harvesting of medicinal plants. Rapid modernization and urbanization is not only a threat for plant species' degradation but also a threat for the associated folk knowledge. That is why that the disappearance of folk knowledge has been declared more in danger than the natural resources themselves [94] . Therefore, we present a strong recommendation that ethnobotany as a subject should be included into the curriculum to help students in recognizing the endangered and medicinally important species of their respective regions. In addition, incentives may be given to farmers for the cultivation of medicinal plants on marginal lands and home gardens.

Indigenous communities at Bajaur Agency are dependent on medicinal plants for ethnoveterinary practices. Knowledge about the traditional medicinal system is restricted to the herders, farmers, and elder community members. The younger generation is unaware of this traditional treasure and takes no interest due to modernization. Hence, this study is an attempt towards the preservation of traditional ethnoveterinary knowledge from being extinct. There are several medicinal plants, which are being used in traditional herbal system of veterinary disorders. Some of the important are V. daucoides, F. vulgare, S. virginianum, W. somnifera, G. glabra, and C. longa. New ethnoveterinary uses used at the study area were found for H. candicans and G. glabra. Apiaceae is utmost plant family being in use for various livestock ailments. Thorough phytochemical and pharmacological investigations are required by isolating the active compounds and testing the in vitro or in vivo efficacy of the abovementioned plants against the targeted veterinary diseases. Furthermore, critical toxicological investigations are also required to ensure the safe and secure use of documented ethnomedicines. In order to share and further maintain this knowledge, it is direly needed to aware the rural population about the significance of traditional ethnoveterinary knowledge and to motivate them on the conservation of natural flora. 

",0.6466273867092633
Overview Federal public health strategies to minimize the importation of communicable diseases into Canada,"The global spread of communicable diseases is a growing concern largely as a result of increased international travel. In Canada, although most public health management of communicable diseases occurs at the front line, the federal government also takes actions to prevent and mitigate their importation.","According to the World Bank, international travel has been steadily increasing and it is estimated that in 2011, international tourist arrivals exceeded one billion (1) . The continuous movement of people and goods into Canada by air, land and sea has resulted in an ever-present risk of importing communicable diseases.

In Canada, communicable diseases are managed primarily at the provincial, territorial and local levels. However, many federal government departments are also involved in minimizing the importation of human and animal pathogens, toxins, vectors and reservoirs of communicable diseases ( Table 1) . There are extensive federal legislations that direct government departments to address this (2, (3) (4) (5) (6) (7) (8) . For example, Acts and regulations administered by the Public Health Agency of Canada (PHAC) include the Human Pathogens Importation Regulations (2) , the Quarantine Act (3) and the Potable Water Regulations for Common Carriers (4). The objective of this article is to provide an overview of PHAC's roles and responsibilities in minimizing the importation of communicable diseases into Canada through preventive measures taken for travellers and modes of transportation prior to leaving the country as well as measures taken when ill people enter the country. It does not include, nor is it meant to diminish, the roles of local, provincial and territorial authorities or other federal government departments.

The federal public health strategy PHAC minimizes the importation of communicable diseases into Canada through preventive and response activities, aimed at travellers and modes of transportation. Travel health information is available to individuals before they leave the country. Conveyances, or international modes of transportation are inspected and public health risks are addressed. All travellers are screened on arrival to Canada. Other papers in this issue address how PHAC manages new legislation that sets out a risk-based licensing framework to improve federal oversight of human pathogens and toxins in Canada (9) ,and how PHAC can assist in the rapid response to manage and contain an infection when it does enter the country (10).

PHAC provides travel health advice and recommendations to travellers and to health care professionals. Advice is also provided to conveyance operators. Inspections are conducted of conveyances, ancillary services (such as flight kitchens) and terminals.

PHAC's Travel Health Program publishes travel health notices, travel-related disease fact sheets and travel health recommendations by destination on the PHAC website and travel.gc.ca (11) . Travel health notices notify the public about health events including outbreaks that may be of concern to travellers. The notices provide a level of risk and recommend measures that can be taken by travellers ( Table 2 ). 

• Advises travellers to avoid all travel in order to protect the health of the Canadian public. • Would be issued if there is a high risk of spread of disease to the general public regardless of measures taken while travelling. Avoiding travel will limit the spread of the disease in Canada and internationally.

Never issued.

Information on measures to take during mass gathering events is also provided through targeted outreach. For example, specific information is provided to Canadians planning to attend the Hajj, the Muslim pilgrimage to Mecca, Saudi Arabia. During the Hajj, there is increased risk of transmission of certain communicable diseases, such as meningococcal meningitis and respiratory infections (12, 13) . Beginning in 2014, materials such as posters and information sheets were developed and translated into five languages (French, English, Arabic, Urdu and Turkish). These materials were distributed to stakeholders, including physicians and community centres in Muslim communities, mosques, Islamic schools and Canadian travel agencies authorized to issue visas for Hajj pilgrims. In 2014, these travel agencies issued approximately 3,500 visas to attend the Hajj.

Finally, during specific public health events such as the Severe Acute Respiratory Syndrome (SARS),pandemic H1N1, and more recently the Ebola outbreak in West Africa, information was provided to travellers in airports through posters and on airport monitors (14) . Travel health information was also available on the travel.gc.ca website (11) and through social media, such as Facebook and Twitter.

PHAC provides secretariat and epidemiologic support to the Committee to Advise on Tropical Medicine and Travel (CATMAT), an expert advisory body that assists PHAC in developing travel health recommendations for travellers and health professionals (15) . CATMAT produces evidence-based recommendations on the prevention and treatment of infectious diseases and other health hazards that Canadian travellers may encounter internationally. CATMAT statements and recommendations on travel health diseases, conditions and special populations are available on the PHAC website.

A conveyance is a mode of transportation such as aircraft, trains, cruise ships and ferries (4). PHAC's Travelling Public Program works with the international conveyance sector largely aircraft and ships. PHAC conducts risk assessments of conveyances and ancillary services, including flight kitchens and terminals, and works with operators to correct any identified public health risks. Inspections focus on potable water, food, sanitation and vector control. Other activities include potable water sampling on conveyances under the Potable Water Regulations for Common Carriers (4) and food handler and sanitation training for industry employees.

The Travelling Public Program has a compliance inspection program for cruise ships that visit Canadian ports (16) . Unannounced inspections are conducted on participating cruise ships travelling in Canadian waters. The scoring system is based on 41 inspection items with a total value of 100 points. A satisfactory score is 86 points out of a possible 100 points. A score of 85 or lower is not satisfactory and requires a reinspection within the following month. An unsatisfactory score does not mean, however, that the travelling public is exposed to any imminent risk to his/her health.

In . WHO requirements for designated points of entry include having staff and resources available to assess, care for and transport ill travellers and animals and the capacity to inspect conveyances and terminals for public health risks. Furthermore, these IHR-designated Canadian points of entry routinely conduct exercises to assess their response capacity and to ensure that they continue to meet IHR (2005) requirements.

Prior to entering Canadian waters, all international cruise ships report their gastrointestinal illness (GI) rates in passengers and crew to PHAC's Travelling Public Program. Immediate reporting is required if the GI illness rates are above key public health thresholds. PHAC works with the cruise ship operator to mitigate the risk to the other passengers or crew by ensuring the cruise ship has implemented its outbreak prevention procedures. PHAC environmental health officers, who are certified Public Health Inspectors, may conduct an onboard investigation.

Additionally, aircraft are required to report any illnesses or deaths on board to the airport authority prior to arrival. These cases are then assessed by PHAC quarantine officers, who are registered nurses (3).

All travellers arriving in Canada are obligated to present themselves to a Canadian Border Services Agency officer where they are screened for illness as well as for any food, plant and animal products in their possession. As specified in the quarantine Act, if a traveller screens positive for a potential communicable disease that poses a public health risk, he/she is referred to a quarantine officer for further assessment, which includes a history and temperature check (3) . Quarantine officers are stationed in selected airports across the country which have a high volume of international travellers. If the ill traveller arrives at one of these airports, this assessment is conducted in person. If the ill traveller is at a point of entry other than these airports, such as a marine port or a land crossing, this assessment is conducted by telephone.

If the quarantine officer suspects that the traveller has a communicable disease that poses a public health risk, certain measures can be taken, e.g., the traveller can be ordered to report to a nearby hospital for further evaluation or can be ordered to report to the local public health authority within a specified amount of time for follow-up. The Quarantine Officer will coordinate with the local public health authority, emergency services and the hospital to manage the ill traveller.

Environmental health officers also responds to incidents on board conveyances if food, water or sanitation is of concern. For example, environmental health officers may advise aircraft groomers on cleaning spills of body fluids or may investigate reports of vectors, such as rats on ships.

PHAC provides financial support to the Canadian Travel Medicine Network (CanTravNet), a network of Canadian clinical experts in travel and tropical medicine (19) . CanTravNet sites are members of the GeoSentinel Global Surveillance Network, an international network that collects data on returned travellers and immigrants as a sentinel network for global emerging infections.

When a serious infection or outbreak is identified, PHAC has the capacity to deploy a rapid response team to help local and provincial or territorial health authorities to assist in the protection of health care workers and to treat and limit the spread of the disease (18) . Under the IHR (2005), information on certain public health events, including communicable diseases, may be shared with the World Health Organization to facilitate international collaboration during a public health response (17, 18) .

PHAC has a strong federal public health strategy to minimize the importation of communicable diseases into Canada based on health promotion and prevention, early detection and prompt response. PHAC meets its mandate by working in close collaboration with clinical care and public health authorities from a local through to an international level, as well as with other federal departments such as the Canadian Border Services Agency and the Canadian Food Inspection Agency.

One of the components of PHAC's strategy is the entry border screening of travellers. Travellers are screened based on signs and symptoms, rather than laboratory testing. Upon arrival, travellers may be asymptomatic, either in the incubation period or have a subclinical infection, have mild symptoms, use antipyretics to reduce fever, or may not report their symptoms. Because cases may be missed, entry screening has been described as ineffective and resource-intensive (13, (20) (21) (22) . However, the efficacy of border screening is difficult to evaluate (23) . One study has shown that entry screening may delay local transmission of a novel influenza strain by one to two weeks, time which could be used for further community preparation for an epidemic (21) . In addition, visible border screening and control may increase public confidence, awareness and compliance. While entry screening of travellers at borders has limitations, it is only one piece of PHAC's strategy to minimize the importation of communicable diseases into Canada.

New infectious disease threats will continue to emerge in the context of globalization and increased travel, however an integrated system is in place to minimize the importation of communicable diseases at the borders and to protect and promote our national health security. PHAC regularly assesses and develops its strategy. Recent developments with human pathogen legislation (2) and rapid response teams (10) are examples of how PHAC continually strengthens its federal capacity to work with others in the fight against infectious diseases.

Multiple federal government departments work together at the Canadian border to assess people, animals, food, cargo, baggage, conveyances, containers, goods and parcels to prevent infections, toxins, vectors and potential reservoirs of infectious diseases from entering Canada. PHAC will continue to offer and advance health promotion and prevention activities, such as travel health alerts and travel health recommendations for clinicians and participates in inspections, surveillance and response activities to minimize the importation of communicable diseases into Canada.

",0.6450310697254706
Antimicrobial prescribing in Australian hospitals; antimicrobial choice and indications for inappropriate prescribing,"publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.","Background: The Hospital National Antimicrobial Prescribing Survey is an auditing platform that enables facilities to determine the appropriateness of their antimicrobials prescribing. All Australian hospitals can participate in the survey which has been collecting prescribing data since 2011. Methods: Data from 385 facilities and 562 surveys which were submitted between January 1st 2015 and December 31st 2016 were analysed and assessed against an appropriateness assessment guide. The top 10 antimicrobials inappropriately prescribed were determined, both as the total of all antimicrobials and percentage of each prescribed. The top 3 indications with the highest rate of inappropriate prespring were determined. Results: There were 36,502 antimicrobial prescriptions analysed and the top inappropriately prescribed antimicrobial was ceftriaxone with 1123 prescriptions. Amoxycillin-clavulanic acid was the next most inappropriate, with 786 prescriptions, followed by piperacillin-tazobactam, with 575 prescriptions. The antimicrobials with the highest percentage of inappropriate prescribing were cefaclor (58%) Kenacomb (gramicidin-neomycin-nystatin) (56%) and dapsone (50%), although there were only small number of these antimicrobials prescribed, (26, 18 and 2 respectively). The indications for which antimicrobials were most inappropriately prescribed were;

• Ceftriaxone; community acquired pneumonia (29%), infective exacerbation of chronic obstructive pulmonary disease (11%) and urinary tract infection (11%) • Amoxycillin-clavulanic acid; community acquired pneumonia (31%), infective exacerbation of chronic obstructive pulmonary disease (20%) and urinary tract infection (10%) • Piperacillin-tazobactam; community acquired pneumonia (14%), hospital acquired pneumonia (10%) and cellulitis/ erysipelas (7%) Conclusion: The antimicrobials that have both high numbers and percentages of inappropriate prescribing are ceftriaxone, amoxicillin-clavulanic acid, cefalexin and azithromycin, which were most inappropriately prescribed for respiratory tract infections. It is evident that these are obvious targets for targeted quality improvement activities.

of patients with critical illness, but economical and resistant impact of de-escalation strategies for bloodstream infections was rarely reported.

Methods: Patients having community-onset monomicrobial Escherichia coli, Klebsiella species, and Proteus mirabilis (EKP) bacteremia and empirically treated by broad-spectrum betalactams, including 3rd-generation cephalosporins (GC), 4th-GC, or carbapenems, were analyzed and categorized according to the antimicrobial spectrum of definitive therapy: 1st-or 2nd-GC (deescalation group), the same agents as empirical antibiotics (noswitch group), and broader-spectrum antibiotics (escalation group). Clinical information was retrospectively collected from chart record. The propensity score was calculated by the independent predictors of 4-week mortality. Patients in the deescalation group were matched on the 1:1 basis with those in the no-switch group by the closest individual propensity scores.

Results: The eligible 454 adults were categorized as the deescalation (231 patients, 50.9%), no-switch (177, 39.0%), and escalation (46, 10.1%) group. In general, those with de-escalation therapy were more females, had less critical illness and less fatal comorbidity, and had a higher survival rate than the other two groups. After appropriate propensity-score matching for the deescalation and no-switch groups, not only critical illness at onset (Pitt bacteremia score ≥ 4; 16.5% vs. 12.7%; P = 0.34) or day 3 (2.5% vs. 2.5%; P = 1.00), and fatal comorbidity (16.5% vs. 21.5%; P = 0.25), but also the time to defervescence (4.6 vs. 4.7 days; P = 0.89), hospital stays (11.5 vs. 10.3 days; P = 0.13), and 4-week (4.4% vs. 4.4%; P = 1.00) crude mortality rate were similar. However, lower antibiotic cost (mean: 212.1 vs. 395.6 US$, P < 0.001) and less complications of bloodstream infections due to resistant pathogens (0% vs. 5.1%, P = 0.004) was observed in the de-escalation group.

Conclusion: For adults with community-onset EKP bacteremia empirically treated by appropriate broad-spectrum beta-lactams, the de-escalation to narrow-spectrum cephalosporins is safe and cost-effective.

The establishment of auxiliary mechanisms of intimate reminders improves rational antibiotic utilization Hui-Yun Liang 1 , Iig-Ling Chen 1 , Hung-Jen Tang 1,2,3 *. Background: Surgery is to have no time to lose, however, how to effectively manage the use of prophylactic antibiotics is a challenge. There is a reminder mechanism but not like the ad window so that users do not want to see the content and just want to close. To multi-oriented check can not affect the conduct of surgery. The establishment of auxiliary mechanisms intimate reminders, and more humane management, Not only can educate the use of the correct concept of antibiotics, but also to improve of rational antibiotic utilization. Methods: First, ""Time out"" immediately before starting the procedure, include the antibiotic prophylaxis within 1 hour prior to incision, If not, will be asked the reason. Followed by the use of antibiotics in the half-life and surgery time to calculate, and use the music to remind the team to add antibiotics; During the operation, the amount of blood loss, When more than 1000 ml will also be the same way to remind the team to assess and prepare add antibiotics. Results: Up to now, the antibiotic prophylaxis within 1 hour prior to incision arrivals to 98%; The proportion of antibiotics added to the drug in surgery was 80%. From here, we can see the effectiveness of management is significant.

Conclusion: At the right time to use the appropriate antibiotics to prevent the occurrence of surgical site infection, and we use the intimate reminder to achieve this goal, and improve the patient's safety.

Should we continue to implement contact precautions and environmental cleaning for MRSA Hui-Yun Liang 1 , Iig-Ling Chen 1 , Hung-Jen Tang 1,2,3 *. Background: Increased drug resistance is one of the most important issues of global concern, effective way to stop the transmission of drug-resistant strains must have antibiotics stewardship and isolation protection measures, Strains of MRSA were first found in the 1960s there is evidence that concerted efforts that include surveillance cultures, contact precautions and isolation in hospitals can reduce MRSA, but the implementation of isolation measures need to spend the manpower and costs. According to the TNIS System (Taiwan Nosocomial Infections Surveillance System) statistics reported that 51% of MRSA detection, so the implementation of the contact for the hospital Isolation is a great burden, so the policy formulation and implementation is the need for many considerations. Methods: Our hospital for the MRSA patients with standard protection. Beginning in 2016 for the first detection of patients to implement contact isolation, cohorting, and the patient after discharge for the final disinfection. For the first isolated of MRSA patient must immediately check the use of antibiotics and related medical behavior, and the unit who isolated of MRSA to the hand hygiene audit, environment cleaning and disinfection of the joint inspection when the patient was discharged. Weekly fixed view of new cases detected distribution trends. Results: The number of new cases isolated by MRSA was reduced from about 120 in 2013 to about 75 in 2016, while DID usage density of antibiotics ( per 1000 bed-day-occupancy) has been declined 30%, the Penicillins have been declined 35%. Conclusion: Through the continuous monitoring of the trend of drug resistance distribution at the same time prudent use of antibiotics and the use of antibiotics stewardship, the most important thing is to implement contact isolation, hand hygiene and medical environment, such as cleaning and disinfection of infection control measures, Reduce the transmission of drugresistant bacteria.

Pharmacy efficiency of censor inappropriate antibiotic prescriptions with pharmacist and infection specialist in a community teaching hospital Meei-Shoue Kuo 1 *, Huei-Ming Pan 2 . 1 Pharmacy, St. Joseph Hospital, 2 Home Medicine, St Joseph Hostipal

Purpose: Overuse of antibiotics should induce increase number antibiotic resistant bacteria and its side effects, there is evidence to suggest that proper use of antibiotic helps control quality and efficiency. Build up censor mechanisms with pharmacist and infection specialist can increase clinical take care quality. Method: Data were retrospectively collected on 36 items of controlled antibiotics of censor meeting at a community hospital from January to December 2016, A pharmacist analysis appropriate of antibiotic prescriptions within 24 hour, and an infection specialist censor the indication of clinical use.

Conclusion: Total analysis result of inappropriate antibiotic prescriptions of pharmacist are 475 cases, ranking of case first item is wrong adjust of renal function 329 cases (69%), not suitable of dose 79 cases(16.6%), incompatible with culture result 21 cases (4.4%), Mistake frequency 14 cases (2.9%), relative laboratory data 11 cases (2.3%), Improper indication 8 case (1.7%), Ignore drug allegic history 5 cases (1.1%), prolong use duration 5 cases (1.1%), side effects 1 case (0.2%). Total analysis result of inappropriate antibiotic prescriptions of infection specialist are 622 cases, first item is wrong adjust of renal function 498 cases (80.1%), incompatible with culture result and change to narrow spectrum antibiotics 74 cases (12.1), Improper indication25 cases (25%), use more than 3 kinds of antibiotics12 cases (1.9%), Ignore body weight 8 cases (1.3%), prolonguse duration 1 case (0.2%). Compare result of inappropriate antibiotic prescriptions between pharmacist and infection specialist there are same opinion in many ways of total results. Results: Ranking of result of inappropriate antibiotic prescriptions of pharmacist and infection specialist first item is wrong adjust of renal function, although the doctors can adjust antibiotic dose by dose calculate formulars, but during the procedure of censor meeting following within 24 hour after prescription, will helps the doctors use sensible and proper antibiotics and improve communication between pharmacist and doctors. On the other hands, can hoist safety of use antibiotic patients.

Prevalence of fluoroquinolone resistant rectal flora in patients undergoing transrectal ultrasound guided prostate needle biopsy: a prospective multicenter study Myung Soo Kim 1 , Eu Chang Hwang 1 , Ho Song Yu 1 , Seung II Jung 1 , Dong Hoon Lim 2 , Won Jin Jo 2 , Hyun Sop Choe 3 , Seung-Ju Lee 3 , Sung Woon Park 4 . 1 Department of Urology, Chonnam National University Medical School, Gwangju, Korea, 2 Chosun University School of Medicine, Gwangju, Korea, 3 The Catholic University of Korea, St. Vincent's Hospital, Suwon, Korea, 4 Kwangju Christian Hospital, Gwangju, Korea Background: We estimated the prevalence of fluoroquinolone resistant rectal flora in patients undergoing repeat transrectal ultrasound guided prostate needle biopsy and identified high risk groups. Methods: From January 2015 to March 2016 rectal swabs of 557 men from 5 institutions undergoing transrectal ultrasound guided prostate needle biopsy were obtained. Results: A total of 268 patients had cultures positive for fluoroquinolone resistant bacteria for an overall rate of 48.1%. The incidence of fluoroquinolone resistance and extended-spectrum beta-lactamase production was 48.1% and 11.8%, respectively. The most common FQ-R bacteria was E. coli (81% of total FQ-R bacteria, 39% of total rectal flora). Univariable and multivariable analysis of clinical parameters affecting FQ-R showed no factor associated with FQ resistance of rectal flora. The clinical parameters associated with infectious complications after prostate biopsy were operation history within 6 months (RR 6.60; 95% CI 1.99-21.8, p = 0.002). Conclusion: Prevalence of FQ-R rectal flora from the rectum before transrectal prostate biopsy was 48.1% and most common FQ-R bacteria was E. coli (39%) However, antibiotic resistance of rectal flora cannot predicted by clinical factors. So Risk based approach cannot recommended and we should consider targeted antibiotic prophylaxis or extended antibiotic prophylaxis.

The relationship between antibiotic consumption and antibiotic resistance of Pseudomonas aeruginosa Wei-Lung Tsai 1 , Meng-Chiao Lin 1 , Chun-Hsiang Yu 2 , Chun-Ming Lee 2 . 1 Department of Pharmacy, St. Joseph's Hospital, 2 Department of Internal Medicine, St. Joseph's Hospital Background: Antimicrobial resistance is a global concern now. The prevalence of antibiotic resistant species is increasing with time. This makes us to spend more money, more time, and more medical resources to fight with them. Diversify the use of antimicrobial agents seems to be effective in reducing the growth of antimicrobial resistance. Methods: Using retrospective study to evaluate the correlation of anti-pseudomonal antibiotic comsumption and antimicrobial resistance of pseudomonas aeruginosa by Pearson's correlation coefficient. The data were collected from a regional hospital in middle Taiwan from 2014 to 2016. Antibiotic use was expressed as DDD(defined daily dose) per 1000 patients-days developed by WHO ATC/DDD. Results: During the study, the consumption of Ceftazidime, Ciprofloxacin and Levofloxacin increased significantly. The consumption of Gentamicin and Amikacin decreased. There was no significant change in the consumption of Imipenem/cilastatin, Cefepime and Piperacillin/tazobactam. Susceptibility rates of pseudomonas aeruginosa to Ciprofloxacin, Levofloxacin and Imipenem/cilastatin were decreases, the others remained stable. The Correlation between antibiotics and their susceptibility was modestly negatively correlated with Ciprofloxacin (r = −0.36, p = 0.034) and Piperacillin/tazobactam (r = −0.40, p = 0.015). The others, although showed negatively correlated, but p-values (>0.05) were not statistically significant. Conclusion: Our study shows only modestly correlation between anti-pseudomonal antimicrobial usage and pseudomonas aeruginosa resistance. We concluded that the diversity of antibiotic use delayed the antimicrobial resistance development. But with the raising of consumption, susceptibility of pseudomonas aeruginosa to each agent is going down. Therefore, keeping antimicrobial stewardship, diversify the use of antibiotics and good infection control will be important strategies to overcome the problems of antibiotic resistance.

Analysis of the C. difficile associated diarrhea and antibiotic use Hui-Yun Liang 1 , Iig-Ling Chen 1 , Hung-Jen Tang 1,2,3 *.

Background: Patients' expectations for antibiotics in the treatment of upper respiratory tract infections (URTIs) have been observed to increase antibiotics prescribing in primary care clinics, but the phenomenon is less well understood in time-strapped emergency departments. We sought to assess for non-clinical factors associated with antibiotics prescribing in a 1600-bed adult acute-care hospital's emergency department (ED) in Singapore. Methods: We conducted a cross-sectional study on 250 consecutive patients who presented with uncomplicated URTI at the ED between May 2016 and Feb 2017. An interviewer-administered questionnaire was instituted and the medical records of the participants accessed for information on antibiotics prescription. Multivariable logistic regression models were constructed to assess for independent non-clinical factors associated with antibiotics prescribing. Results: Participants had a mean age of 39.4 (SD 14.9) years and tended to be male (64.0%). Almost half (47.2%) had received diplomaand university-level education. 67.6% had no major pre-existing medical conditions. Antibiotics were prescribed for about one-third (35.2%) of the participants, with amoxycillin-clavulanate being the most commonly prescribed antibiotic (72.7%). Although 117 (46.2%) participants reported that they were expecting to receive antibiotics during the ED visit for their illness, only 18.8% informed their physicians about their expectations. Those who expected to receive antibiotics (41.0%) were more likely than those who didn't (30.1%) to be prescribed antibiotics (P = 0.070). Among those who expected antibiotics, there was no difference in the antibiotics prescribing pattern between those who had informed their physicians about their expectations (36.4%) versus those who had not (42.1%) (P = 0.622). After adjusting for age, pre-existing comorbidities, and presenting complaint of fever, expectations for antibiotics (aOR 1.60, 95% CI 0.94-2.72, P = 0.086) and a higher educational level (aOR 1.63, 95% CI 0.94-2.83, P = 0.085) were marginally-significantly associated with antibiotics prescribing. Majority of participants were satisfied with the consultation at the ED, with no difference in satisfaction levels between those who received (90.9%) versus those who did not receive antibiotics (90.7%) (P = 0.965). Conclusion: Patients' higher educational levels and expectations for antibiotics, regardless of whether they were explicitly informed to their physicians, were associated with increased antibiotics prescribing. Further studies are warranted to better understand the influence of non-clinical factors such as these on antibiotic prescribing decision making.

Investigation the relationships of MDR (multi-drug resistant) pathogens isolated and defined daily dosing (DDD) of antibiotics prescription in one teaching hospital at Mid-Taiwan Chung-Hsin Liao, Frank San Fan. Changhua Hospital, Ministry of Health and Welfare Background: Chunghua Hospital is a 700-bed available hospital at Mid-Taiwan. It contains the ability of medical, surgical, intensive and long term care facility. MDR pathogens are progressively becoming the major threats to immunocompromised and non-immunocompromised patients. So, we want to investigate the relationships of MDR pathogens and antibiotics used in our hospital.

Methods: By way of Tamis System, we retrospectively collect the DDD of 33 inject form of antibiotics (from 7 main classes) and 6 important bacterial (including their MDR companions) during the 1st quarter of 2017. The clinical divisions of antibiotics used and regulatory or nonregulatory antibiotics are stratified and recorded. The important bacterial (including E. coli, K. pneumonia, P. aeruginosa, A. baumannii, S. aureus and enterococci) and their MDR companions (CRE, CRPA, CRAB, MRSA, VRE) are collected and analyzed. By way of t-student test, the DDD of antibiotics and the proportion of MDR pathogens are compared. Different cultures of each MDR pathogens are also compared with 7 main classes of antibiotics. Results:

1. HIV-AIDS infected is no more deadly disease under HAART with well adherence, case management and good quality of life 2. MTR (multiple tablets regimen) is gradually repaced by STR.

More and more studies about the outcome, adherence, immunologic recovery and viral suppresion are reported.

Bacteremia, antimicrobial resistance and prescription pattern in pediatric patients: prospective observational study in a teaching hospital in southern Taiwan Bai-Xiu Jian 1 , Wen-Liang Lin 2 , Wen-Chuan Lin 2 , Ching-Lan Cheng 1,2 . Background: Antimicrobial resistance in Taiwan is more studied in adults than in children. This study aims to estimate pediatric antimicrobial resistance, bloodstream infection rates and pathogens, and to describe the antimicrobial prescribing pattern by monthly point prevalence survey (PPS). Methods: We conducted a prospective observational study of all blood cultures reporting studied pathogens from pediatric patients in a tertiary teaching hospital from August 2016 to April 2017. The monthly PPS days were randomly chosen from each month of 2016. Pediatric patients with ongoing antimicrobials on the survey days or who had received antibiotic for surgery on previous days were included.

Results: There were 48 bacteremia isolates from 44 patients enrolled, in which 30 (62.5%) were resistant to at least 1 antibiotic. The most frequent organism was Salmonella species (n = 13) and 85% of them were yeilded from children aged more than 12 months. E. coli (n = 9) was most frequent resistant organism. The susceptibility to ampicillin/sulbactam and cefazolin was 11% and 22%, respectively. PPS were conducted in 2 ICU and 1 general pediatric ward. 371 (65%) of 573 eligible person-times were prescribed antimicrobial. 645 antimicrobials were prescribed ( Fig. 1 ), in which 71% were for therapy and 29% for prophylaxis. The most presribed for prophylaxis were sulfonamides/trimethoprim (31%). Combination of penicillins were most prescribed for therapeutic indication (28%). The most common indication for therapeutic antimicrobials was LRTIs (22%). Background: Our hospital in Tangerang District, Indonesia has an Antimicrobial Restricted Antimicrobial Program (ARCP) implemented since 2015. The rate of bacteria drug-resistance and their antimicrobial susceptibility are extremely crucial to enhance the program. Therefore, through this study, we have shared information about the susceptibilty of 11 (eleven) broad-spectrum antibiotics, including carbapenems, fourth generation cephalosporines, glycylcycline, glycopeptide, and oxazolidinone. Methods: Limitations of broad-spectrum antibiotics therapy are applied to inpatients and outpatients in order to supervise and control prescribing systems by requesting integrated and enclosed information such as clinical and relevant reasons. Futhermore, the rationale for antibiotics utilization should be shown comprehensively in the form provided. Our data were extracted from July 2015 and December 2016 data base retrospectively.

Results: A total of 420 cases of infection, 284 (67.6%) were multidrug resistant gram negatif bacteria. The most common pathogens were Acinetobacter baumanii (23.1%), Klebsiella pneumonia (22.1%), Pseudomonas aeruginosa (13.8%), and Stenotrophomonas maltophilia (8,6%). An increasing susceptibility trend for gram negative bacteria (GNB) was noticed in almost all groups of broad-spectrum antibiotic (5-14%). The drug susceptibility between July 2015 and December 2016 were cefepime (40%, 30%, 48%); cefpirome (45%, 40%, 58%); carbapenems (69%, 65%, 75%); tigecycline (62%, 65%, 67%); piperacillin/tazobactam (52%, 58%, 61%); cefoperazone/sulbactam (81%, 72%, 86%); and amikacin (63%, 64%, 77%). In the other side, the susceptibilty level of linezolide and vancomycin against gram positif bacteria, Staphylococci and Streptococci remained the same at 100%. Conclusion: Our results present the tendency towards improving susceptibility level of broad-spectrum antibiotics, although the number of MDR-GNB still quite high.

Outcome of pharmacist interventions on antibiotics use Yung-Ming Huang, Yi-Shan Chen, Tsai-Yen Liu*. Department of Pharmacy, Hualien Tzu Chi Hospital, Buddhist Tzu Chi Medical Foundation, Hualien, Taiwan, R.O.C Background: The launch of a new antibiotic has become rare in recent years. This is impacting the therapeutic options to treat infections caused by multidrug-or extensively drug-resistant bacteria. As a response to these challenges, optimal use of antibiotics, control the resistance rate is now we need to confront. Methods: Pharmacists participated the Committee of Antibiotics and Infectious Disease Control and implemented some means such as direct interventions, educations, and routine evaluation to improve utilization of antibiotics. IT (Information Technology) was set up to screen each second-line antibiotics prescription and also provided real-time recommendation for poor renal function patients' dose adjustment. The consumption of antibiotics (counted by Defined Daily Dose, DDD) and the resistance rate are reported and evaluated every month and every half year.

Background: To prevent increasing antibiotic resistance among microbes, antibiotic stewardship program (ASP) has been performed in the outpatient clinic (OPD), wards and surgical prophylaxis, but the emergency department (ED) has not been included. However, empiric antibiotics have often been prescribed to patients during their ED stay. Previous publications addressed that initial prescriptions applied in ED often be continued in ward. As a result, we compare the differences of initial antibiotic regimens between ED and ward to evaluate whether the empiric antibiotics has been continued in ward. Methods: We conducted retrospective review of medical records within March 1, 2016 to May 31, 2016. Those records belonged to patients of age less than 18, patients who were not admitted and those without any antibiotic as part of treatment were excluded. The initial antibiotics prescribed in ED, the grade of medical staffs who prescribe antibiotics in ward and the initial antibiotic used in ward were recorded. We made judgement as consistence, deescalation or escalation of antibiotics by comparison of initial antibiotic between ED and ward. IBM SPSS version 22 (2013) was used as the statistics software. Results: During the study period, 604 medical records were included and reviewed. The medical staffs who prescribed initial antibiotics in wards consisted of post-grade year one (PGY-1), residents of first to fifth year (R1-R5) and nurse specialists (NP) who made prescriptions under guidance of visiting staffs. The proportion of PGY-1, R1, R2, R3, R4, R5 were 14.9%, 35%, 30.9%, 13.9%, 4.8%, 0.7% and 0.8% respectively. Among the antibiotic prescribed in ward, the proportion of consistence, escalation and de-escalation in comparison with ED were 69.7%, 16.9% and 13.2% respectively. In consideration of grades of staffs in wards who applied the same antibiotic as in ED, there were 80.57% of PGY-1, 75% of R4, 70.89% of R2, 69.9% of R1, 60% of R5, 52.27% of NP and 37.93% of R3. The antibiotic ordered by PGY-1 has significantly high coherent rate to ED ( p < 0.05) in comparison with R3, R2, R1 and NP. Conclusion: Antibiotics prescribed in ED, as other medications, had been continued in wards around 80% by junior doctors. As a result, implantation of ASP in ED is important.

Evaluation of broad-spectrum antibiotic prescription in a Tunisian infectious diseases department Adnene Toumi*, Ikbel Kooli, Abir Aouam, Hajer Ben Brahim, Chawki Loussaief, Mohamed Chakroun. Department of Infectious Diseases, UR 12SP41, Monastir University Hospital, Tunisia

Background: The emergence of bacterial resistance is a public health problem, leading to broad-spectrum antibiotic prescription. Consequently, particular attention must be paid to broad-spectrum antibiotic prescription, in order to preserve their effectiveness. In our hospital, no recommendation exists for broad-spectrum antibiotic use. The aim of this study was to describe the modalities of broad spectrum antibiotic prescriptions in an infectious diseases department and confront them to the guidelines. Methods: We carried out a professional practice evaluation to assess broad-spectrum antibiotic prescriptions made at the infectious diseases department (Fattouma Bourguiba teaching hospital, Monastir, Tunisia), between January and December 2016. We evaluate 6 broad-spectrum antibiotics: imipenem, ertapenem, fluoroquinolones, aminoglycoside, tigecycline and colimycin. Evaluation was done by two infectious disease specialist based on national and international guidelines. Results: Ninety eight prescriptions were included for 95 patients. The mean age was 53 years (14-84 years). Sex ratio was 0.86. Broadspectrum antibiotics were mostly used for skin infections in 41 cases (41.8%) and urinary tract infection in 33 cases (33.7%). It was community acquired infections in 90 cases (91.8%) and nosocomial infections in 8 cases (8.2%). Antibiotic combinations were used in 68 cases (69.4%). The broad-spectrum antibiotic was first line prescribed in 58 (59.2%). Prescriptions were documented only in 24 cases (24.58%). Extended-spectrum betalactamase producing Escherichia coli was isolated in 7 cases (25.9%). Fluoroquinolones were used in 64 cases (65.3%), imipenem in 15 cases (15.3%), aminoglycoside in 8 cases (8.1%), colimycin in 6 cases (6.1%), tigecycline in 2 cases (2%) and ertapenem in 1 case (1%). Broadspectrum antibiotic usage was judged adequate in 65 cases (66.3%). Otherwise, antibiotic duration was adequate in 89 cases (90.9%), posology in 97 cases (99%) and the use antibiotic combinations in 40 cases (57.1%). Twenty-six cases (26.8%) of the initial prescriptions seemed to be adapted to the identified bacteria. Conclusion: In our department, global evaluation showed that more than third of prescriptions were inadequate. Moreover, broad-spectrum antibiotics were prescribed in communityacquired infections in more than 90% of cases. In order to improve the quality of broad-spectrum prescriptions, standardized protocols must be implemented, supervised by an antibiotic stewardship team. study was to evaluate the stability of daptomycin in serum at various temperatures. Methods: Daptomycin serum samples were prepared and stored long-term in a refrigerator (4°C) or in freezers (−80°C and −20°C), as well as at assumed body temperatures (35°C, 37°C, and 39°C). The stability of daptomycin under various conditions was evaluated by repeatedly measuring concentrations. Results: Although the decrease of daptomycin concentration in serum samples stored in freezers was less than 10% after 168 days (6 months), the concentrations in samples stored in a refrigerator decreased by more than 70% over the same period. Furthermore, daptomycin concentrations in serum samples stored at close to body temperature decreased by more than 50% after only 24 h ( Figure 1 ). The rate of daptomycin elimination from serum samples increased with the rise of temperature, especially under conditions that assumed living body. 

The results of the present study demonstrate that the measurement of serum concentrations of daptomycin needs to be performed rapidly or the samples collected from patients should be stored in freezers until analysis. At close to body temperatures, daptomycin was unstable and its stability was affected by variations in temperature. The daptomycin elimination from living body and its pharmacokinetics may be affected by the instability in serum.

Hsin-Wen Ko 1 , Wei-Jen Chen 1 , Shu-Wen Lin 1,2 . 1 Department of Pharmacy, National Taiwan University Hospital, 2 Graduate Institute of Clinical Pharmacy, National Taiwan University Background: The therapeutic range and pharmacokinetic (PK) properties of vancomycin have been addressed in many researches. To ensure the optimal antibacterial efficacy and to avoid the potential toxicities, various dosing methods have been developed for non-obese patients. However, the available information is scant when it comes to the obese patients. In our study, we aimed to analyze the data from a medical center in order to further improve the current dosing method for obese patients.

Methods: In our hospital, a vancomycin dosing calculator based on the Bauer's method was built to assist pharmacists in dosing vancomycin. The essential information, included patient profile, predicted PK parameters, and measured concentrations, was documented in a fixed format. We retrospectively collected all the records from Jan. 2016 to May 2017. The definition of obesity is body mass index ≥ 30 kg/m2. All adults (≥18 years old) with at least 1 measured steady state trough levels were included. During treatment period, patients who had acute kidney injury, serum creatinine >1.5 mg/dL, dialysis, concomitant use of mannitol, or missing data were excluded. To measure the performance of this dosing calculator in obese patients, the percentage of patients whose predicted Ctrough was within 2.5 and 5.0 mg/L from measured Ctrough were calculated. Results: Overall, 69 obese patients were included. Sixty-two percent of obese patients whose trough concentration fell into the range of 10 to 20 mg/L after initial dosing. The mean dose per kg for those who achieved the levels of 10-20 was significantly lower in obese patients (9.7 mg/kg Q8H and 11.0 mg/kg Q12H). Comparing the difference between predicted and measured concentrations, 35.2% of obese patients had the predicted concentrations that are within 2.5 mg/L from measured concentrations; 66.2% had the predicted concentrations that are within 5.0 mg/L from measured concentrations. Conclusion: Lower dose per kg was needed to achieve therapeutic range in obese patients which may imply the PK change of vancomycin clearance doesn't correlate linearly with total body weight. Therefore, when empirically giving the guideline recommended initial dose of 30-45 mg/kg/day, lower daily dose per kg might be considered for obese patients. The performance of our calculator in obese patients was well comparable to that in nonobese patients. and epidermal lesion due to toxidermia in all cases. Drugs imputability was established by pharmacovigilance investigation: allopurinol (n = 6), salazopirin (n = 3), carbamazepine (n = 3), ceftazidime (n = 2), ampicillin (n = 2), meglumine antimoniate (n = 1), cefotaxime (n = 1) and lamotrigine (n = 1). The discontinuation of the incriminated drug was indicated in all cases with good progress without prescription of corticosteroid therapy. Conclusion: Our study showed that drug reaction with eosinophilia and systemic symptoms (DRESS) syndrome is a severe and rare disease. Clinically, It seems to be an autoimmune-like reaction. Corticotherapy should not be systematically used. Early diagnosis allows an adequate care and a favorable outcome.

Effect of antimicrobials and antifungals against human drugmetabolizing cytochromes P450 Toshiro Niwa. School of Pharmacy, Shujitsu University Background: Cytochromes P450 (CYP) are a superfamily of enzymes that catalyze the oxidation of a wide variety of endogenous and exogenous chemicals. Although CYP2C subfamilies (CYP2C8/9/19) share more than 82% amino acid sequence identity, they have rather distinct substrate specificities. Piperacillin, a penicillin-based antibiotic, is partially metabolized by CYP2C8. CYP3A4 is the predominant form expressed in the human liver and intestine, whereas CYP3A5 is expressed polymorphically. Azole antifungals inhibit CYPs including CYP3A4, and macrolide-based antibiotics exhibit mechanism-based inhibition against CYP3A4. However, there have been a few reports describing the effect of these antimicrobials and antifungals on human drugmetabolizing CYP activities. Methods: CYP2C8/9/19, CYP2D6, and CYP3A4/5 expressed in recombinant Escherichia coli (Bactosomes) were obtained from Cypex Ltd. Aminopyrine N-demethylation, tolbutamide hydroxylation, dopamine formation from p-tyramine, and testosterone 6βhydroxlation activities were determined as reported previously. Results: Inhibition of amoxicillin, ampicillin, and piperacillin at 0.5 or 1 mM concentration was not observed for CYP2C9, CYP2D6, CYP3A4 and CYP3A5, however, amoxicillin and piperacillin inhibited CYP2C8-mediated aminopyrine N-demethylation at 50% inhibitory concentration (IC 50 ) of 0.83 and 1.14 mM, respectively. The predicted free fractions of the maximum inflow concentration of the inhibitors into the liver calculated with reported pharmacokinetic parameters are much lower than the concentration investigated in this study and estimated IC 50 values for CYP2C8. Although fluconazole, itraconazole, and voriconazole at a concentration of 2 or 10 µM neither inhibited nor stimulated CYP2C8mediated aminopyrine N-demethylation activity, ketoconazole and miconazole noncompetitively inhibited CYP2C8-mediated aminopyrine N-demethylation with the inhibitory constant values of 1.98 and 0.86 µM, respectively. Erythromycin inhibited CYP3A4mediated testosterone 6β-hydroxylation preincubation-timedependently but had little influence on the CYP3A5-mediated metabolic activity. Conclusion: The interactions between these three penicillin-based antibiotics and other drugs that are metabolized by CYPs investigated would not be clinically significant. Ketoconazole and miconazole might inhibit CYP2C8 in addition to other CYPs clinically.

resistant Gram positive pathogens, such as S. pneumoniae, Staphylococcus aureus, and Enterococcus, will be used to assess in vitro resistance development to nemonoxacin, in comparison to ciprofloxacin, levofloxacin, and moxifloxacin. Methods: Four strains of each bacterial species, include S. pneumoniae (one will be penicillin resistant S. pneumoniae), Staphylococcus aureus (one will be methicillin resistant S. aureus), Enterococcus faecium (one will be vancomycin resistant Enterococcus faecium) and Enterococcus faecalis (one will be ampicillin resistant Enterococcus faecalis) will be isolated for testing from patient specimens. All clinical isolates will be confirmed wild-type at GyrA, GyrB and ParC, ParE before further in-vitro test. A stepwise resistance selection will be conducted at 1×, 2× and 4× MIC. DNA sequencing for the QRDR of gyrase and topoisomerase genes were performed. Results: Inducible in vitro antimicrobial susceptibility test for ciprofloxacin, levofloxacin, moxifloxacin and nemonoxacin against each three different strains of Staphylococcua aurues, Enterococcus faecium, Enterococcus faecalis, and Streptococcus pneumoniae. Nemonoxacin has shown the lowest MIC among all quinolones. For example, the MICs of three S. aureus strains are 0.03, 0.06, 0.03 μg/ mL for nemonoxacin, compared with 0.016, 0.125, 0.047 μg/mL for moxifloxacin, 0.125, 0.19, 0.19 μg/mL for levofloxacin, and 0.19, 0.38, 0.25 μg/mL for ciprofloxacin, respectively. In the MIC inducible test, nemonoxacin has shown low resistance potential after quinolone exposure. The MICs of nemonoxacin did not increase for S. aureus, E. faecium, E. feacalis and 4-fold increase of S. pneumoniae over three cycles of selection. Conclusion: Our in vitro inducible antimicrobial susceptibility test revealed that nemonoxacin has a low propensity for selecting resistant Gram positive pathogens compared to other fluoroquinolones.

Chu-Chung Lin*, Hung-Chuan Chen, Chia-Hui Wang, Chiayn Chiang. TaiGen Biotechonolgy Co., Ltd, Taipei, Taiwan

Background: The increasing of multidrug resistance in serious Gram-negative bacterial pathogens has posted an emerging threat to public health. Novel targets or mechanisms of action against multidrug resistance are needed to avoid the preexisting antibiotic resistance mechanisms. Lipopolysaccharide (LPS) is the major component of the outer membrane of Gram-negative bacteria (GNB) and serves as a permeability barrier that protects the bacterium from many antibiotics. For most GNB, such as Pseudomonas aeruginosa and members of the Enterobacteriaceae, the inability to synthesize the LPS has a bactericidal effect. And thus the interference of LPS assembly by a small molecule inhibitor would result in antibacterial activity. The UDP-3-O-(R-3-hydroxymyristoyl)-N-acetylglucosamine deacetylase (LpxC), encoded by lpxC, is an essential enzyme that controls the first committed step in LPS biosynthesis in Gram-negative bacteria. Moreover, there is no counterpart enzyme in human. Methods: Through computer-aid drug design and rational drug design, TaiGen has discovered a proprietary series of selective potent LpxC inhibitors. Results: In this report, we demonstrated a novel class of LpxC inhibitors showed good and equal potencies against P. aeruginosa, K. pneumonia and E. coli in bacterial growth inhibition assay (MIC: <1 ug/mL) and reduced LPS biosynthesis (LpxC enzyme assays IC: <50 nM) significantly.

Conclusion: Our novel chemical class of LpxC inhibitors provide the potential as new antibacterial agents for treatment of Gramnegative infections.

In vitro killing curves of tedizolid and linezolid against clinical strains isolated from patients in China Shan Wang, Yun Li, Yuan Lyu*, Feng Xue, Jian Liu, Wei Yang, Jia Zhang. Institute of Clinical Pharmacology, Peking University First Hospital, Beijing 100191, China

Background: Tedizolid is a novel antibacterial oxazolidinone derivative, which is approved in both oral and intravenous formulation for the treatment of acute bacterial skin and skin structure infection with Gram-positive pathogens (including methicillin-resistant Staphylococcus aureus [MRSA] ) in certain countries. This study determined the in vitro killing activity of tedizolid and linezolid against recent clinical strains isolated from patients in China. Methods: Time-kill curves were generated for recent clinical isolates of MRSA, penicillin-resistant Streptococcus pneumoniae [PRSP] , and linezolid-resistant Enterococcus faecalis [LREF] , using also ATCC control strains. Bacterial cultures (inoculated at approximately 1 × 10 6 CFU/mL) were grown at 35°C in the appropriate liquid medium for up to 24 hours. Tedizolid and linezolid were added to cultures at baseline, at clinically relevant concentrations based upon standard dosage for humans (200 mg once-daily and 600 mg twice-daily respectively), and also based upon recent minimum inhibitory concentration (MIC) data. Samples were removed at fixed time points, plated onto standard culture medium and then incubated at 35°C for 18 hours. Curves were plotted using the colony counting method, based on CLSI Guideline M26. Results: Key data are displayed in the table. The control and clinical strains were susceptible to both drugs, except a linezolid-resistant E. faecalis clinical strain had an MIC = 8 µg/mL. Tedizolid and linezolid displayed notable bactericidal effects against PRSP and bacteriostatic inhibitory effect against MRSA at clinically relevant concentrations. Against LREF, both drugs exerted an inhibitory effect independent of concentration. Conclusion: Tedizolid exerted a greater bactericidal effect than linezolid against streptococcal species in vitro and exerted a bacteriostatic against MRSA and LREF. Background: During the last decade, colistin ( polymyxin E) has been used as the last line defense against infections caused by multidrug resistant pathogens. Unfortunately, the re-use of this drug has been obviously followed by the emergence of colistin resistance, mediated by chromosomic gene mutations into pmrA/ pmrB, phoP/PhoQ, ccrA/ccrB, lpxA, lpxC, lpxD, and mgrB. These mutations induce a complex gene regulation cascade leading to the synthesis and addition of the 4-amino-4-deoxy-L-arabinose (L-Ara4N) to lipid A of the membrane inducing therefore the colistin resistance. However, in 2015, it has been described for the first time, a plasmid-mediated colistin resistance gene mcr-1. Nowadays, a worldwide dissemination of mcr-1 has been reported. Here, using a phylogeny approach, we investigate the mcr-1 gene origin.

Methods: All described mcr-1-harboring plasmids were collected from the NCBI database. Plasmid typing was performed from PlasmidFinder 1.3 database. The sizes and %GC content were estimated using infoseq. Homologous sequences of MCR-1 were collected by BlastP analysis. Phylogenetic analysis was performed using MAFT and FastTree. The ""Rhizome"" analysis of MCR-1 was performed by segmented this latter into 10 fragments of 50 amino acids, and then phylogenetic analysis of each fragment was conducted.

Results: Comparative genomics of the 59 mcr-1-harboring plasmids reveals a wide diversity of these latter composed by different incompatibility plasmid groups. These plasmids can be distinguished into three groups according to their sizes ranged from 6.36-Kb to 369.3-Kb and their %GC content ranged from 41% to 50%. They are identified into several Enterobacteriaceae and Gramnegative bacteria. Mcr-1 gene is shared by all plasmids and located into a transposon (3'442-bp; 46 .34%GC) composed by an insertion sequence ISApl1, mcr-1, and a sulfatase encoding gene, pap2. The blastP analysis reveals a total of 9'025 homologous sequences to MCR-1. The phylogenetic analysis of these sequences reveals a huge number of phosphoethanolamine transferases within the Campylobacter genus. Moreover, this analysis reveals that the most closely related homologous sequences to MCR-1 were those identified in Moraxella species, with high similarity (≥98%). This finding was confirmed by a ""Rhizome"" analysis of MCR-1 which clearly shows that this latter is a chimeric sequence from Moraxella catarrhalis, Moraxella pluranimalium, and Psychrobacter urativorans and the most part of the gene was from M. pluranimalium. Conclusion: Mcr-1 gene is disseminated and transmitted through a transposon. Campylobacter could be potential source of new phosphoethanolamine transferase genes. The origin of this mcr-1 gene was Moraxella genus, especially, M. pluranimalium.

Identification of the molecular support of the intrinsic resistance to colistin in Proteus vulgaris by a genomic comparative approach and in vitro tests Sophie Baron, Jean Marc Rolain. Background: For several years, facing, colistin has been used as the last line antimicrobial drug for the treatment of multi-drug resistant Gram negative bacteria infections. Unfortunately, the renewed interest for this antimicrobial peptide led to the emergence of acquired resistance especially in Enterobacteriaceae, but also the increase of infections due to intrinsic colistin-resistant bacteria. Mechanisms of resistance to colistin are complex and misunderstood, particularly in these bacteria. While all Proteus bacteria are naturally resistant to colistin, we were surprised to recently isolate a P. vulgaris clinical strain susceptible to colistin, and we propose in this work to understand the molecular support of colistin resistance in this species by a comparative genomic and phenotypic approach. Methods: Genomes of P. vulgaris susceptible strain CSUR P1868_S and a resistant one CSUR P1867_R were sequenced and compared to select groups of interesting genes that were absent, truncated or mutated in the susceptible strain. Among the group of mutated genes, we selected those that were already known to be putatively involved in antimicrobial peptide resistance and we realized a SIFT score to predict the probability mutation could alter protein function. Use of electronic microscopy and lipid A analysis by mass spectrometry after lipid A extraction help us to understand pathways involved in colistin susceptibility/resistance. Results: Genome of CSUR P1868_S was 4.28 Mb in length with a GC content of 37.8% whereas CSUR P1867_R genome was 3.9 Mb and 38.1%. It contained numerous resistant genes particularly against aminosides, β-lactamines, fluoroquinolones, and cotrimoxazole, carried by various plasmids. We found significant difference in outer membrane glycoside structures in the susceptible and resistant strains, and this was corroborated by MALDI-TOF analysis, which showed the absence of 4-amino-4-deoxy-L-arabinose (L-Ara4N) in outer membrane lipid A moiety of the susceptible strain. Comparative genomic analysis with others resistant strains of Proteus vulgaris available in local database found mutation in arnBCADTEF operon of studied susceptible strain. Transcriptomic analysis of genes belonging to arnBCADTEF operon showed significant decrease in mRNA expression level of tested genes in the susceptible strain, supporting that addition L-Ara4N in the outer membrane of lipid A moiety can explain the increase in colistin susceptibility. Finally, the insertion of the arnD gene in the susceptible strain led to a 32 folds increase of colistin MIC, showing the role of this gene in Proteus vulgaris colistin resistance. Conclusion: We showed that the arnD gene of the arn operon was mutated in the susceptible P. vulgaris. It is known that sugars fixation on lipid A is a key pathway for polymyxin resistance. However, it is certainly not the only one because lots of enigmas on colistin resistance remain. Other genes that were absent in the susceptible strain could be linked to colistin resistance including a whole operon involved in sialic acid metabolism. We are currently developing knock-out techniques to study these different hypothetical targets and to discover new pathways involved in colistin resistance. Bacterial defense system and antimicrobial peptides coexist for a long time, and mechanisms of resistance to colistin are complex and implied a lot of genes. This is the first report that describes genomic differences between this particular susceptible strain and a typical P. vulgaris opening new research pathway to understand colistin resistance.

1 Bunkyo Gakuin University Graduate School, 2 Bunkyo Gakuin University

Background: Pseudomonas aeruginosa (P. aeruginosa) is a common cause of chronic airway infections in patients with pulmonary disorders such as diffuse panbronchiolitis (DPB) and cystic fibrosis (CF), and DPB and CF had a poor prognosis. Long-term, low-dose macrolide treatment has markedly increased 5-year survival rate of patients with DPB. Consequently, researchers are interested in using macrolides such as erythromycin and clarithromycin that not included in the anti-pseudomonal therapeutic weapon because of the absence of bactericidal or bacteriostatic activity. Previous studies have demonstrated that macrolides influence P. aeruginosa virulence. However, most studies evaluated the regulatory effects of macrolides on P. aeruginosa virulence factors after treatment over a short period (<48 hours). In this study, we subcultured P. aeruginosa for two years in the presence of low-dose macrolides, and evaluated virulence factors such as total protease, elastase, and pyocyanin. In addition, the effect of macrolide exposure strain on A549 cell was investigated. Methods: P. aeruginosa PAO1 was repeatedly subcultured 104 times over two years on Mueller-Hinton agar containing 1.6 μg/mL erythromycin or 0.8 μg/mL clarithromycin. The macrolide concentration was determined by referencing the maximum drug concentration (Cmax) at the time of oral administration. Strains that subcultured in the presence of low-dose macrolides were frozen at 2 month intervals. Macrolide exposure strains were used to examine the time-dependent change of total protease, elastase, pyocyanin and A549 cell survival rate.

Results: The production of the virulence factors gradually decreased with macrolide exposure, and elasetase and pyocyanin were maximally affected after only 2 months. Also, as the period of macrolide exposure to the P. aeruginosa PAO1 increased, the death of A549 cells to which the bacterial supernatant was added decreased, and the survival rate increased.

Conclusion: We showed that three virulence factors gradually are inhibited by macrolide and the cytotoxicity declines as the macrolide exposure period to bacteria becomes longer. These findings exhibit that long-term, low-dose macrolide exposure may gradually reduce the pathogenicity of P. aeruginosa via timedependent inhibition of virulence factors. We suggest that longterm, low-dose macrolide treatment for chronic airway infection should be administered for at least 2 months or more to achieve optimal effects against P. aeruginosa.

isolates is associated with carbapenem treatment of ESBL-and/or AmpC-producing bacterial infection. One of the mechanisms is the insertion in outer membrane protein genes ompK35 and ompK36 by various insertion sequences. Here, we investigated the effects of different carbapenems on development of carbapenem resistance associated in ompK35 and ompK36 mutations of carbapenemsuceptible ESBL and non-ESBL producing K. pneumoniae. Methods: 10 carbapenem-susceptiblee isolates, including ESBLproducing isolates 7, 8, 17, 18 , and 21 and non-ESBLproducing isolates CLKP-2, 32, 36, 56, and 60, were incubated with ertapenem (ETP), imipenem (IMP) and meropenem (MEM) induividually at 1X and 1/6X concentrations for 36 h. The isolates in the inhibition zone of ETP, IPM and MEM were collected for ompK35 and ompK36 analysis of insertion sequence types ans carbapenem resistance.

Results: Higher carbapenem concentration (1 X) stimulated more resistant isolates for both ESBL and non-ESBL producing isolates. Under high concentration, MEM and ETP treatment induced resistance to both carbapenems for ESBL producing isolates, while ETP treatment only developed resistance to ETP for non-ESBL producing isolates. Among selected ESBL isolates induced by carbapenems, insertion sequences were found mostly in ompK36, not ompK35. Few isolates with normal PCR sizes of both genes demonstrated carbapenem resistance, suggesting involvement of other mehcanism for carbapenem resistance. Conclusion: Both ESBL and non-ESBL producing isolates could develop carbapenem resistance associated with carbapenem types and ompK35 and ompK36 mutations.

Novel aminoglycosides that evade bacterial resistance mechanisms Dev P. Arya. Clemson University

Background: Aminoglycosides inhibit bacterial growth by binding to the A-site decoding region of the bacterial 16s ribosomal RNA (rRNA) within the 30S ribosomal subunit. Previous work has shown that there is approximately a five-fold difference in the affinity of neomycin for the human A-site model and the E. coli model. The methodology for synthesizing, screening for both ribosomal binding/selectivity and bacterial growth inhibition, and rapid analysis of the data provides a systematic method for identification of bacterial ribosome specific antibacterials that can evade bacterial resistance pathways.

Methods: We have developed rapid synthetic and screening methods that rapidly identify compounds that discriminate between the two model rRNA structures. Results: Novel potent aminoglycosides that show high selectivity for the bacterial ribosome over mammalian ribosome were identified. Conclusion: Our approach, coupled with a rapid solid phase synthesis of peptidic aminosugars, has identified active aminoglycosides that show large differences in binding affinity for the E. coli A-site vs. the human A-site.

Activation of efflux pumps accounts for low-level moxifloxacin resistance in Mycobacterium tuberculosis isolates causing urinary tract infection Xiang-Jun Chen 1 , Jung-Yien Chien 2 , Chong-Jen Yu 2 , Po-Ren Hsueh 1,2 . 1 Department of Laboratory Medicine, National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan, 2 Department of Internal Medicine, National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan

Background: To evaluate the prevalence of Mycobacterium tuberculosis isolates causing urinary tract tuberculosis (UTB) and the rates of resistance to moxifloxacin in Taiwan. Methods: Of the 3034 patients with culture-confirmed TB at the National Taiwan University Hospital in Taiwan during the period 2005-2012, 47 (1.5%) patients had UTB and were included in the study. All isolates were assessed for mutations in the gyrA and gyrB genes. Minimum inhibitory concentrations (MICs) of moxifloxacin were determined in the presence or absence of efflux pump inhibitors (EPIs), including verapamil, reserpine and carbonyl cyanide 3-chlorophenylhydrazone. Responders to EPIs were defined as isolates that showed at least a 4-fold reduction in MICs in the presence of EPIs.

Results: During the study period, there was an increasing trend in prevalence of UTB (P = 0.029). Among the 47 isolates, 2 (4.3%) were resistant to rifampicin, 5 (10.6%) were resistant to isoniazid, 24 (51.1%) were resistant to ofloxacin, 22 (46.8%) were resistant to moxifloxacin and 7 (14.9%) were resistant to streptomycin. Among the 22 isolates with moxifloxacin resistance, 19 (86.4%) had lowlevel resistance (MIC = 1.0 to 2.0 mg/l). Patients with prior exposure to fluoroquinolones (FQs) were more likely than non-exposed patients to have moxifloxacin-resistant isolates (14/22, 63.6% v.s.

Background: Klebsiella pneumoniae (KP), a member of the family Enterobacteriaceae, is a pathogenic gram-negative bacterium commonly found in respiratory and intestinal tract in human. In recent years, multidrug-resistant K. pneumoniae (MDRKP) is rapidly increasing because of antibiotics overuse and it has been implicated in a variety of nosocomial infections. MDRKP cause a problem in the clinical treatment of patients and some references demonstrate that antibody-immune response is essential for host defense against bacterial pathogens. In this study, we've developed a monoclonal antibody for the treatment of MDRKP. DCB's human single B cell antibody discovery platform allows screening of single cells to rapidly isolate and sequence fifty of antibodies with known binding specificity or functional properties. Methods: Fully Human Monoclonal Antibodies from Single Peripheral Blood B Cell Using K. pneumoniae (KP) as a sorting model, we established a platform technology that can obtain human mAbs against KP directly from each single human plasma or memory B cell by cloning the VH and VL variable regions. The peripheral blood mononuclear cells were isolated from human whole blood, and KPspecific single memory B cells were sorted into 96-well PCR plates by fluorescence-activating cell sorter. The antibody genes of these single cells were then amplified by RT-PCR, and expressed by FreeStyle 293 expression system. Opsonophagocytic killing assay (OPK assay) The target K. pneumoniae was grown in TSB. HL-60 cells was differentiation by DMF (N,N-dimethylformamide). For the OPK assay, contains 1 × 10 3 CFUs of bacteria, antibody, 4 × 10 5 differentiated HL60 cells and complement. The reaction was plating on TSA plate incubate for overnight at 37°C, then count colonies and analyze data. Conclusion: We demonstrate an established platform, which generates a panel of human monoclonal antibodies from human PBMCs. Twenty-five antibodies clones were selected high affinity with K. pneumoniae in single B cell technology. The average KD value is up to 10 −10 . In vitro opsonophagocytic assays demonstrate that twenty anti-KP antibodies can increase the antibody-mediated phagocytosis in K. pneumoniae. Our results suggest that the selected antibodies can be potential lead drugs for further testing in a K. pneumoniae infection animal model. The recombinant Abs might play a crucial role in treatment of MDR-KP infection.

Background: The rapid emergence of multidrug resistant (MDR) bacterial strains is a global public health challenge, which has accelerated by the overuse and misuse of antibiotics. There is an urgent demand for antimicrobial agents effective against these super bugs. The natural chemopreventive compounds isothiocyanates (ITCs) are widely found in cruciferous vegetables have been shown to possess cancer chemopreventive efficacy, antioxidative properties and immunomodulatory activity. ITCs such as allyl isothiocyanate and benzyl isothiocyanate are revealed to be bactericidal through an unidentified mechanism. Our previous study indicated that sulforaphane (4-methylsulfinybutyl isothiocyanate) significantly reduced the growth of Staphylococcus aureus, Escherichia coli and Vibrio vulnificus. Though ITCs are effective in bacteria killing, the application of ITCs on MDR bacteria is still unknown. We aimed to evaluate the application of ITCs on MDR E. coli.

Methods: Antibacterial activity of ITCs against MDR E. coli was assessed by minimal inhibitory concentration (MIC) assay and growth inhibition assay. Bacteria survival rate was evaluated by plate count and calculated as the percentage of the control group. The experiments were done in triplicate. All results were presented as the means ±S.D. and analyzed by Student's t-test.

Results: The MICs of benzyl isothiocyanate and sulforaphane for MDR E coli were 1.25 and 0.28 mM respectively. MDR E. coli was effectively killed by BITC within 1h at 37°C. BITC treatment also inhibited MDR E coli growth, this inhibition could be reversed by adding thiol reducing agents including N-acetyl cysteine (NAC), L-Glutathione (GSH), Dithiothreitol (DTT) and 2-mercaptoethanol (2-ME) to the culture medium. Background: Economic pressures and legal constraints increasingly force hospital pharmacies to purchase generic drugs rather than original branded products. Yet, differences in key properties have been noted between suppliers of antibiotics with the same denomination (INN), even if all approved by local health authorities. This has raised concerns among clinicians about drug quality when dealing with patients in critical conditions such as in Intensive Care Units where ease of preparation, efficacy, and stability of the prescribed antibiotics are of paramount importance. Methods: We compared (i) the rate of dissolution (by experienced nurses), (ii) the coloring (spectrophotometry) and degradation (LC-MS-MS) over ward storage of the original meropenem (MERONEM ® ; AstraZeneca) vs. 4 generics of meropenem authorized in Belgium, France and/or Spain (Sandoz, Hospira, Fresenius-Kabi, Aurovit). Samples were prepared following manufacturer's recommendation and exposed to fixed temperatures for up to 8 h.

Results: A summary of key results is presented in the Table   Product Mimicking administration by motor-activated pumps for extended or continuous administration in Intensive Care Units for severely-ill patients (drug dissolved in 0.9% NaCl);

Background: It is suggested that susceptibility results for cephalosporins be reported according to the minimum inhibitory concentration (MIC), regardless of whether or not the isolate produces an extended-spectrum β-lactamases (ESBL). The revised breakpoint MIC defining susceptibility to cefotaxime (1 mg/L) might be a surrogate for ESBL detection. Cefotaxime-resistant Enterobacteriaceae are challenging organisms with little treatment options. Carbapenems such as imipenem, ertapenem or meropenem are frequently used, but the emergence of carbapenem resistant Enterobacteriaceae is a concern issue, which may hinder the use of carbapenems. Although, cephamycins such flomoxef are effective against cefotaxime-resistant Enterobacteriaceae in vitro,

there are a few clinical data demonstrating effect against bloodstream infection (BSI) caused by these organisms. Methods: We performed a retrospective observational study on adults with BSI caused by flomoxef-susceptible, cefotaximeresistant Enterobacteriaceae to investigate the efficacy if flomoxef compared with ertapenem. The outcome assessment was evaluated with propensity score-based matching and logistic regression analysis.

Results: Demographics and clinical characteristics of 246 adults with BSI caused by flomoxef-susceptible, cefotaxime-resistant Enterobacteriaceae treated by flomoxef (n = 58) or ertapenem (n = 188) were compared. Crude mortality at 28 days was found in 41 (16.7%) patients. After adjustments in the multivariate model, patients with severe sepsis (adjusted OR = 3.84, 95% CI = 1.16-12.78, P = 0.03), high BSI mortality score (adjusted OR = 5.59, 95% CI = 2.37-13.21, P < 0.01), ultimately or rapid fatal co-morbidity (adjusted OR = 10.60, 95% CI = 3.43-32.75, P < 0.01) and pneumonia (adjusted OR = 10.11, 95% CI = 3.43-29.81, P < 0.01) were independently associated with 28-days mortality. By the propensity scores, 58 patients with flomoxef therapy were matched with 116 treated by ertapenem therapy at a ratio of 1:2, and there were no significant differences in terms of BSI severity, co-morbidity, or BSI sources between groups. Moreover, crude mortality rates at 28 days (20.7% vs. 13.8%, P = 0.28) did not differ significantly. However, the length of hospitalization stay was shorter in the ertapenem group (10.2 ± 8.5 vs. 14.6 ± 9.4 days, P < 0.01).

Conclusion: Ertapenem therapy is associated with shorter time of hospitalization than flomoxef therapy following flomoxef-susceptibile, cefotaxime-resistant Enterobacteriaceae BSI in adult patients with both guarded prognosis.

(p = 0.04) and lower 30-days mortality ( p < 0.01) when compared with initial daptomycin group. By the propensity scores with age and Pittsburgh bacteremia score ≥4, 25 patients in initial high maintenance teicoplanin dosage group were matched with 25 in initial daptomycin group at a ratio of 1:1. There was no different significance in overall 30-days mortality rate in these two groups (p = 0.26). However, the Kaplan-Meier curve for 30-days mortality reveled patients in initial high maintenance teicoplanin dosage group had better survival than those in initial daptomycin group (p = 0.02). Conclusion: After propensity score match with age and patient's severity, the clincal outcome in initial high maintenance teicoplanin dosage group was not inferior than in initial daptomycin treatment group. • 18 years old when antibiotics initiated.

• Baseline serum creatinine (sCr) measurement within 48 hours of antibiotics initiation. • Received minimum one dose of PT and V within 24 hours when either PT + V initiated concurrently or if one agent initiated first followed by addition of another.

Exclusion criteria: Patients excluded if medical records incomplete, receiving dialysis, had history of chronic kidney disease (CKD) (Stage IV or higher) or recent history of acute kidney injury (defined as increase in sCr of 0.5 mg/dL or greater or increase of sCr of 1mg/ dL or greater for CKD). Data collection: Medical records were reviewed to collect demographic data, antibiotic use, sCr concentrations, comorbid conditions and concomitant nephrotoxic agents. In the event of nephrotoxicity, management and outcome recorded. Primary and Secondary Endpoint: Primary endpoint was incidence of nephrotoxicity defined as ≥ 50% increase in baseline sCr concentration or increase from baseline sCr of = 0.5 mg/dl. Nephrotoxicity was based on Rifle classification, focusing on change of sCr. Secondary endpoint was association of nephrotoxicity with potential confounding factors that may increase risk of nephrotoxicity.

Statistical Analysis: Baseline demographics and clinical characteristics presented as mean standard deviation (SD) or median (interquartile range [IQR] ) for continuous variables and frequencies ( proportions) for categorical variables. Secondary endpoints assessed with univariate analysis (Chi-square/Fisher's exact).

Simple logistic regression attempted.

Results: 103 patients were screened for eligibility with 21 patients excluded from study. Final study sample included 82 patients. 94 patient visits recorded. Incidence rate of nephrotoxicity was 22.3%. Nephrotoxicity group had marginally significant lower baseline sCr when PT + V were initiated (mean 54.7 vs 69.1, p = 0.037). Nephrotoxicity group had higher mortality than non-nephrotoxicity group (Death rate 33.3% vs 9.6%, p = 0.013). Conclusion: Nephrotoxicity incidence rate was 22.3%. Figure: (abstract PS 046).

Abstracts of the 30th ICC 2017 / International Journal of Antimicrobial Agents 50 S2 (2017) S69-S278 S86 Background: Tissue stroma is known to be important in regulating Hp-mediated inflammation, but the molecular basis of its interaction with Hp and its potential crosstalk with dendritic cells (DCs) remain to be determined. To this end, the potential crosstalk between H. pylori (Hp) infected gastric stromal cells (Hp-GSCs) and DCs were investigated. Methods: The expression of IL-23 and IL-17 indicative activation of DCs and Th17 cells, respectively, was evaluated as the primary experimental readouts. Primary CD90 + GSCs from cancerous and adjacent normal tissues were generated from gastric cancer patients, and monocyte-derived DCs were obtained from healthy individuals. Levels of cytokines and prostaglandin E 2 (PGE 2 ) were measured by ELISA, and C-type lectin expression in GSCs was assessed by flow cytometry and immunohistochemistry. Results: In a trans-well co-culture system, significantly upregulated DC-derived IL-23 expression was found when DCs were cocultured with Hp-infected GSCs (Hp-GSCs). Further, PGE 2 from Hp-GSCs was discovered to possess the priming effect, which could be inhibited, by anti-COLEC12 (Collectin subfamily member 12) blocking Abs, COLEC12 knockdown or when alpha3-fucosyltransferase-null ( futB; HP0651) strain of Hp was used. Also, the expression of COLEC12 was co-localized with CD90 + stromal cells in cancerous tissues. Furthermore, Hp-GSCs-conditioned DCs were able to induce the expression of IL-17 from CD4 + T cells, which could be inhibited by IL-23-neutralizing Abs. Conclusion: These results suggested the importance of COLEC12 as a receptor involved in Hp-stromal cell interaction and its subsequent conditioning effect on DCs.

cellulose, releasing cellobiose from either non-reducing or reducing ends and β-glucosidases hydrolyze cellobiose to glucose. Methods: The exo-1, 4-β-D glucanase gene CBH from the thermophilic bacterium Thermotoga maritima MSB8 ATCC DNA was placed under the control of T5 promotor in the plasmid pQE-30 (3.46 bp) and integrated into the genome of Escherichia coli Top10F by transformation. The high enzymatic activity strain was selected and designed as E. coli cel-3A and cultivated in shaking flasks with IPTG induction at different concentrations and time. SDS-polyacrylamide gel electrophoresis assay for cell bound and supernatant revealed a single band at about 52 kDa. Moreover, the recombinant E. coli cel-3A manifests to possess the ability of carboxymethyl cellulose utilization as a carbon source. Results: CBH with improved catalytic and kinetic characteristics is a matter of utmost importance to withstand the current robust industrial environment. The recombinant cellobiohydrolase produced by E. coli exhibit maximum activity at pH 4.0 and temperature 80°C, indicating it was a thermophilic acidic. Exo-1, 4-β-D glucanase is worthy for denim biofinishing/washing. Further research was carried out in appropriate medium in 200L batch fermentation. Conclusion: IPTG induction magnified the exo-1, 4-β-D glucanase activity to 32.5 IU/ml. The exo-1, 4-β-D glucanase secreted by recombinant E. coli represents an appealing potential for both research and textile industry application.

The effect of post translational phosphorylation on halophilic methanoarchaeal molecular chaperone DnaK ATPase and substrate protein refolding activities Wei-Ling Chen, Shih-Shan Chen, Shih-Wen Fang, Mei-Chin Lai*. Department of Life Sciences, National Chung Hsing University, Taichung, Taiwan, R.O.C Background: Molecular chaperone plays an important role to assist folding and assembling of nascent polypeptides, preventing protein misfolding and aggregation which were damaged by temperature, pH and osmotic stresses. DnaK belong to the heat shock protein 70 of molecular chaperone and cooperates with cochaperone DnaJ and nucleotide exchange factor GrpE. ATP is required for DnaK to active and remodel their substrate client protein. Although it was generally recognized that the molecular chaperone DnaK system is highly conserved within the Three Domains of life, investigation on the molecular chaperone system in Archaea are limited. Genes related with molecular chaperone in halophilic methanogen Methanohalophilus portucalensis FDF1 T were analyzed and the dnak, dnaJ and grpE genes are induced by the heat and salt stresses. Moreover, our phosphoproteomic analysis in strain FDF1 T indicated that there are four phosphorylated sites in DnaK (Wu et al., 2016) . Methods: The phosphorylated sites of MpDnaK were mutated to alanine as the dephospho-mimic state and also to aspartic acid as the phospho-mimic state with the site-directed mutagenesis method by PCR. The overexpression of wild type and mutants MpDnaK were induced by IPTG in E. coli DE3(RIL)BL21 and were further purified by nickel affinity chromatography. The ATPase activities of MpDnaK was measured by colorimetric assay for inorganic phosphate. The structure modeling of MpDnaK was constructed by Easy modeler with highest identity species as template. Results: The ATPase kinetic assay revealed that substrate ATP hydrolytic velocity in phospho-mimic type MpDnaK was similar with the wild-type which suggested that the phosphorylation on Ser518 did not affect the ATP hydrolytic activity. Further structure modeling analysis revealed the phosphorylated Ser518 is located in the lid domain of MpDnaK which relate to the open/close of the substrate binding pocket and initiate/terminate the disaggregation process. Distance measurement within the wild type Ser518, dephosphomimic Ser518A and phosphomimic Ser518D to the substrate binding pocket residue Arg414 showed that after phosphorylation on Ser518, the distance are the shortest (1.8 Å). The shorter distance may contribute more stable conformation for lid closing and promote more efficient disaggregation process. This analytical result suggest that post-translational phosphorylation may involve the regulation of the substrate binding and disaggregation. Conclusion: The phosphorylation on Ser518 did not affect the ATP hydrolytic activity of MpDnaK, whereas the PTM-phosphorylation on lid domain-Ser518 promote the lid-substrate binding pocket interaction, which may further enhance the refolding activity.

Xue-Hua Ling 1,2 , Shang-Kwei Wang 2 , Chang-Yih Duh 1 *. 1 Department of Marine Biotechnology and Resources, National Sun Yat-sen University, Kaohsiung, Taiwan, 2 Department of Microbiology and Immunology, Kaohsiung Medical University, Taiwan

Background: Therapeutic drugs, Bortezomib, Carfilzomib and Ixazomib, target ubiquitin-proteasome system (UPS) have been licensed in treating multiple myeloma. Moreover, UPS inhibitors are demonstrated to attenuate the progression of neural degenerative diseases. UPS is a versatile complex composed of multiple subunits. Proteolytic function of UPS is involvoed in diverse and important biochemical pathways in cell. Therapeutic prospect of UPS inhibitor is promising and valuable. We aimed to establish a high-content image-based drug screening assay directing UPS. Our natural compounds are purifed from Formosan soft corals with distinctive structure elucidated. Methods: Human cytomegalovirus UL76 interacts with proteasome regulatory subunits of 26S proteasome. In HEK293T cells, the transient-expressing EGFP-UL76 fusion protein exhibits distinct nuclear aggresome. Fluorescence intensities and the phenotypic behaviors of UL76-aggresome are used as markers for proteasome inhbition. The image acquisition is accomplished using ImageXpress Micro widefield high-content screening system (Molecular Device). The imagng data were analyzed by MetaXpress software package. Proteasome inhibitors, Bortezomib, MG132, and clasto-lactacystin β-lactone are used as paradigm control to validate the screening efficacy Results: The phenotypic behaviors and intensity profiles of UL76aggresome are responsive to the treatment of the three known proteasome inhibitors in a dose-dependent manner. This cellbased high-content screening system is established and employed in search of proteasome inhibitors from marine natural products. One hundred and forty structural elucidated natural products were assessed. Among them, three groups of structural disctinct groups purified from Formosan soft corals were shown with inhibitory activity for proteasome. Conclusion: We established an efficient high-content system for screening UPS inhibitor. waste water. Taiwan area is small, so little arable land can be used, if the land is subjected to heavy metal pollution and water irrigation, have accumulated in the soil. the residues of heavy metals in the soil, resulting in reduced the area of cultivated land, the main is to assess P. chrysosporium to treated heavy metals ability. This experiment using SDB as basic medium, carbon source respectively sawdust and starch, and adding calcium sulfate 0.5 g and calcium carbon 1 g in the medium was the best, tolerance test of white rot fungi. We also study the influence of removing heavy metals in different pH, culture period, calcium ion, anion and common metal. The experiment of adding calcium in this test, calcium ion in the liquid culture that can promote the degradation of white rot fungus of heavy metals, especially for copper ion and lead ion, copper ion degradation in the concentration of 10 ppm to 1 ppm and lead ion degradation in the concentration of 20 ppm to 1.5 ppm, then explore the degradation ability of zinc, copper, mercury, lead, all have good results, but only the zinc ions in the zinc copper exception. The ion in the mixed solution, zinc ions can be degraded by this strain. Experiment that starch medium than in sawdust medium, can provide more nutrients, prompting the white rot fungi to produce catalase, may be because in adversity, this strain can secrete more peroxide dismutase to help themselves to resist adversity and sawdust medium, Although the activity of SOD in sawdust medium was low, the degradation rate was better, and it could buffer the pH value of polar acid, and stable and sustained supply of nutrients.

Bacterial genotoxin enhances radiosensitivity in cancer stem cells by regulating autophagy Yu-An Chen 1 , Yu Chen 2 , Chih-Ho Lai 1,3 *.

Background: Cytolethal distending toxin (CDT) produced by Campylobacter jejuni is a genotoxin that induces cell cycle arrest and apoptosis in mammalian cells. We recently reported that CDT can overcome the radioresistance of prostate cancer (PCa) cells by intervening in the repair of radiation-induced double-strand breaks (DSB). However, the detailed mechanisms underlying the effects of CDT on radioresistance in PCa cells require further investigation.

Methods: Recombinant C. jejuni CDT subunits were cloned, purified, and then subjected to functional analysis. CDT impairs radiation-induced autophagy flux was investigated using immunofluorescence microscopy. The molecular mechanisms underlying CDT regulates autophagy to enhance radiosensitivity in PCa cells were explored. Results: Our results demonstrated that C. jejuni CDT suppresses the IR-induced autophagy pathway in PCa cells by attenuating c-Myc expression and therefore sensitizes PCa cells to radiation. We further showed that CDT prevents the formation of autophagosomes via decreased high-mobility group box 1 (HMGB1) expression and the inhibition of acidic vesicular organelle (AVO) formation, which are associated with enhanced radiosensitivity in PCa cells. Conclusion: Our findings reveal that C. jejuni CDT inhibits c-Myc and reduces HMGB1, resulting in prolonged IR-induced DSB and impaired autophagy, thus converting radioresistance to radiosensitivity in PCa cells.

Shih-Wen Cheng, Yu-Tze Horng, Po-Chi Soo. Department of Laboratory Medicine and Biotechnology, Tzu-Chi University, Hualien 97004, Taiwan Background: Klebsiella pneumoniae, an important opportunistic pathogen in nosocomial infection, usually causes urinary tract infection, pneumonia, and pyogenic liver abscess (PLA) in human. K. pneumoniae produces many virulence factors that contribute to pathogenicity in humans. Bacterial biofilm represents a key virulence determinant in promoting bacterial persistence and resistance to antibiotics. The ability of bacterial adherence to biotic or abiotic substrate is an essential step for biofilm formation. Such adherence can be mediated by bacterial fimbriae, which are also important virulence factors in many bacteria. In enterobacteria, cyclic AMP (cAMP) receptor protein (CRP) plays a vital role as a global regulator. CRP protein regulates several essential bacterial virulence gene expression, including fimbriae and biofilm formation, responsing to intracellular concentration of cAMP. It is reported that CRP-cAMP complex regulates type 1 fimbriation in Escherichia coli. Methods: Here, we used CRISPR/Cas9 system to perform targeted gene knockout with a transgene intergration by homologous recombination (HR) using a suitable donor sequence.

Results & Conclusion: Our preliminary data indicated that the gene KPN_00353 encoding EIIA homolog up-regulates biofilm formation. Thus, our purpose is to clarify how the CRP-cAMP complex mediate the biofilm formation. Loss-of-function mutagenesis is an important tool characterize gene functions, and CRISPR (clustered regularly interspaced short palindromic repeats)/Cas9 (CRISPR-associated protein 9) system has emerged as an efficient and precise gene editing tools for targeted mutagenesis in microorganisms in recent years. In the present study, we show the designation of crp mutant of Klebsiella pneumoniae by using CRISPR/Cas9 system followed by evaluating the efficacy of CRISPR/ Cas9 system in K. pneumoniae genome editing. Then, we will examine the importance of CRP in K. pneumoniae.

PS 055 Type VI secretion system of Proteus mirabilis N2 Yi-Hwa Chou, Zhong-Ting Chiu, Yilin Tsai, Shwu-Jen Liaw*. Department and Graduate Institute of Clinical Laboratory Sciences and Medical Biotechnology, College of Medicine, National Taiwan University, Taipei, Taiwan Background: Proteus mirabilis is a common human pathogen causing urinary tract infections (UTIs), especially in patients with indwelling urinary catheters. Many regulatory mechanisms have been evolved for bacteria to adapt to changes of diverse environments. Type VI secretion systems (T6SS) are known to contribute to bacterial pathogenicity by exerting toxic effects on host cells or competing bacterial species and are involved in a broad variety of functions such as biofilm formation and stress sensing. This large array of functions is reflected by a diversity of regulatory mechanisms. The regulation of P. mirabilis T6SS remains unclear. Recently, our transcriptome analysis revealed P. mirabilis T6SS was regulated by a global regulator Crp. Methods: We first confirmed the regulation of T6SS by Crp using real-time RT-PCR and the promoter reporter assay and identified one T6SS main structure operon and four Hcp/VgrG effector operons by using sequence specific RT and then PCR. We investigated whether Crp can bind directly to T6SS promoter region using a DNaseI footprinting assy. Overexpression of the effector toxin was performed to disclose its function. The growth predominance test and killing assay were used to observe the competition between wild type and crp mutant. We contructed vipAB (encoding the T6SS sheath) mutant to confirm the Crpregulated killing of T6SS. We also purified the possible toxin to assess its function in killing of E. coli. Results: Crp positively regulated T6SS by binding directly to T6SS promoter region and also positively regulated the expression of the effector toxins. Overexpression of the effector toxin in wild-type P. mirabili resulted in killing of both crp mutant and E. coli. Accordingly, the growth predominance test and killing assay demonstrated that P. mirabilis T6SS conferred growth predominance on the wild-type over both crp mutant and E. coli. Conclusion: Crp positively regulated the expression of P. mirabilis T6SS and affected its killing function. T6SS contributed to growth predominance over crp mutant and E. coli, suggesting P. mirabilis T6SS as an antibacterial weapon.

Epinecidin-1 protects mice from LPS-induced endotoxemia and cecal ligation and puncture-induced polymicrobial sepsis Bor-Chyuan Su 1 , Han-Ning Huang 1 , Tai-Wen Lin 2,3 , Chwan-Deng Hsiao 2 , Jyh-Yih Chen 1 *. Background: The antimicrobial peptide, epinecidin-1 (Epi), was identified from Epinephelus coioides and may have clinical application for treating sepsis. Epi has been shown to ameliorate antibiotic-resistant bacteria-induced sepsis in mice, but further evaluation in mixed-flora models and a description of the protective mechanisms are essential to establish this peptide as a potential therapeutic. Methods: Therefore, we first tested the protective effects of Epi against polymicrobial sepsis-induced bactericidal infection, inflammation and lung injury that result from cecal ligation and puncture in mice. Furthermore, since lipopolysaccharide (LPS) is a key inducer of inflammation during bacterial infection and sepsis, we also tested the LPS-antagonizing activity and related mechanisms of Epi-mediated protection in mice with LPSinduced endotoxemia and LPS-treated Raw264.7 mouse macrophage cells. Results: Epi rescued mice from both polymicrobial sepsis and endotoxemia after delayed administration and suppressed both lung and systemic inflammatory responses, while attenuating lung injury and diminishing bacterial load. In vitro studies revealed that Epi suppressed LPS-induced inflammatory cytokine production. Mechanistically, Epi disrupted the interaction between LPS and LPS binding protein, competed with LPS for binding on the cell surface, and inhibited Toll-like receptor 4 endocytosis, resulting in inhibition of LPS-induced reactive oxygen species/p38/Akt/NF-κB signaling and subsequent cytokine production. Conclusion: Overall, our results demonstrate that Epi is a promising therapeutic agent for endotoxemia and polymicrobial sepsis.

An open-label, randomized and comparative study to evaluate the efficacy and safety of cefoperazone/sulbactam in comparison to cefepime for the treatment of hospital acquired pneumonia and healthcare-associated pneumonia Fu-Der Wang 1 , Jien-Wei Liu 2 , Yen-Hsu Chen 3 , Wen-Sen Lee 4 , Jung-Chung Lin 5 , Ching-Tai Huang 6 , Hsi-Hsun Lin 7 , Yung-Ching Liu 8 , Hung-Jen Tang 9 , Yao-Shen Chen 10 , Wen-Chien Ko 11 , Min-Chi Lu 12 . 1 Taipei Veterans General Hospital, 2 Kaohsiung Chang Gung Memorial Hospital, 3 Kaohsiung Medical University Hospital, 4 Wan Fang Hospital, 5 Tri-Service General Hospital, 6 Chang Gung Memorial Hospital-Linkou, 7 E-Da Hospital, 8 Shuang Ho Hospital, 9 Chi Mei Medical Center, 10 Kaohsiung Veterans General Hospital, 11 National Cheng Kung University Hospital, 12 Chung Shan Hospital Background: To compare the efficacy and safety of cefoperazone/ sulbactam and cefepime in treatment of hospital acquired pneumonia (HAP) and healthcare-associated pneumonia (HCAP).

In this open-label, randomized phase 3 trial, we randomly assigned (1:1) patients aged 18 years and older with HAP and HCAP to receive cefoperazone/sulbactam or cefepime. The primary endpoint was to compare the clinical response to cefoperazone/sulbactam with cefepime at the test of cure (TOC) visit evaluated within 7-21 days after last dose. Secondary endpoints were overall survival and objective response in all patients assessed by intention-to-treat (ITT). During the study period, all subjects who received at least one dose of study drugs were included in the safety population and monitored for safety issues. Results: From Sep. 22, 2009 to Aug 06, 2012, a total of 174 subjects in 12 hospitals were screened, A total of 147 subjects were included in the evaluable population. The clinical response rate at TOC visit was 81.8% (54/66) in the cefoperazone/sulbactam group, while it was 79.0% (64/81) in the cefepime group. In the ITT population, the clinical response rates were 82.1% (55/67) in the cefoperazone/sulbactam group, and 79.0% (64/81) in the cefepime group. Non-inferiority of cefoperazone/sulbactam to cefepime was demonstrated in both populations with a noninferiority margin at 20% ( p-value <0.001). Thirty-five adverse events were judged as related to study medications. No statistically significant difference regarding adverse events was observed between the two groups. Conclusion: This study indicated that the combination of cefoperazone/sulbactam was not inferior to cefepime when used to treat patients with HAP/HCAP. All the reported serious adverse events were not related to study drugs, and the incidences of adverse drug reactions were low. Therefore, cefoperazone/sulbactam (2g/2g every 12 hours) was tolerable and effective to treat patients with HAP/HCAP.

Study about the risk and prognostic factors of invasive Streptococcus pneumoniae infection Hiroki Kurata 1,2 *, Naoki Iwanaga 1,2 , Yosuke Nagayoshi 1,2 , Shinnosuke Takemoto 2 , Tadayoshi Oono 1 , Keiko Hisatomi 2 , Seiji Nagashima 2 . 1 Department of Infectious Diseases, National Hospital Organization Nagasaki Medical Center, 2 Department of Respiratory Medicine, National Hospital Organization Nagasaki Medical Center

Background: Streptococcus pneumoniae (the pneumococcus) is the most frequently isolated causative pathogen in communityacquired pneumonia, approximately 10% of it is considered invasive pneumococcal disease (IPD). However, there have been only a few reports about the influence on risk and prognosis factors of IPD. Methods: 37 cases of pneumococcal isolation from two sets of blood cultures in Nagasaki Medical Center from April 2009 to March 2016 were designated as IPD. We compared the IPD cases with 49 cases of pneumococcal pneumonia without isolation from blood cultures about the risk and prognostic factors. We defined pneumococcal pneumonia in which pneumococci were isolated in sputum culture or urine antigen test was positive, in addition to respiratory symptom such as fever, cough, sputum, and dyspnea, elevated inflammatory response and chest abnormal shadow. Comparative study was done backwardly about the underlying disease, blood test, quick Sequential Organ Failure Assessment (qSOFA) score, antibiotics regimen and prognosis. Results: Pneumonia rate in adult IPD was 62.2%, and the mortality rate of the cases was 16.2%. In the IPD group, there was a tendency to select broad-spectrum antimicrobials as an initial treatment of pneumococcal pneumonia. In the multivariate analysis, several independent risk factors of IPD, including malignant tumors [odds ratio (OR), 19.60; 95% confidence interval (CI), 4.58-84.04; p = 0.0001], and initial qSOFA (OR, 19.90; 95% CI, ; p = 0.0009), were associated with IPD. Initial qSOFA (OR, 5.60; 95% CI, 1.14-27.13; p = 0.0343) was also elucidated as a prognostic factor in the multivariate analysis. Conclusion: High initial qSOFA could be a risk and prognostic factor of IPD. We might suspect IPD earlier by using qSOFA and lead to the improvement of prognosis, because qSOFA was extremely easy scoring on bedside. Background: Anaerobes are rare causes of spinal infections and constitute less than 4% of all cases. Methods: In this report, we presented a case with Parvimonas micra, a gram-positive anaerobic coccus, spinal infection. We also searched the PubMed database using the terms ""spondylodiscitis,"" ""discitis,"" ""spondylitis,"" ""osteomyelitis,"" ""spinal,"" ""vertebral,"" ""Peptostreptococcus,"" ""Micromonas,"" ""Parvimonas"" to find similar cases in English literature. The clinical information of all cases, including ours, was extracted and analyzed using SPSS 18 software. Results: Totally, 14 additional cases with P. micra spinal infection were found in English literature. Table 1 shows the demographic data, co-morbidities, and outcomes of patients with P. micra spinal infection. The median age of these patients was 70 years. Over half of the patients were male and nearly half of all cases (46.7%) had dental problems or a prior history of dental work. Back pain (80%), but not fever, was the most common symptom and the median duration of symptom was 6 weeks. Thoracic and lumbar vetebrae were the most commly involved sites. Increased values of inflammation markers were recorded in all cases. Five cases (33.3%) received surgical intervention. The median duration of antimicrobial treatment was 8 weeks and all but one case were cured without clinical relapse. Abbreviation: IQR-interquartile range. a Including malaise, anorexia, fever, and headache.

Conclusion: In summary, P. micra spinal infection should be considered in patients with a medical history of dental problem or work acccompanied with an insidious onset of back/neck pain and elevated inflammation markers.

Acute respiratory distress syndrome associated with invasive pulmonary aspergillosis caused by influenza A virus: a case report Mei-Yu Su*, Wen-Liang Yu, Che-Kim Tan. Department of Intensive Care Medicine, Chi-Mei Medical Center, Tainan City, Taiwan

Background: Invasive pulmonary aspergillosis (IPA) in severe complicated influenza A has been reported in Taiwan. We report refractory acute respiratory distress syndrome (ARDS) due to Aspergillus pneumonia in a patient with influenza A infection. Methods: A 56-year-old man of type 2 diabetes mellitus, hypertension, and old cerebrovascular accident suffered from dyspnea for days. He was brought to emergency department on February 20, 2016. Laboratory data revealed WBC, 5200/μL; platelet count, 22,800/μL; procalcitonin, 19.89 ng/ml; lactate, 5.8 mmole/L; and creatinine, 0.68 mg/dL. Arterial blood gas showed pH, 7.29; PCO 2 , 44.5 mmHg; PO 2 , 80 mmHg; HCO 3 , 21.1 mmol/L; Base excess, −5.1 mmol/L; FiO 2 , 65%; and P/F ratio, 123 mmHg. Real Time PCR for FluA showed positive. Antimicrobial therapy included oseltamivir, tamiflu, levofloxacin and piperacillin/tazobactam. CXR showed diffuse mixed interstitial and alveolar infiltration over bilateral lungs, suspicious of pneumonia. He was intubated and then was admitted to intensive care unit. Initial hemodynamic support with fluid resuscitation and infusion of vasopressor were administered. ARDS occurred with persistent hypoxemia and inhaled nitric oxide therapy on February 25. Aspergillus antigen index was high (>4.43). Intravenous voriconazole was used on February 26. However, spiking fever was noted. Sputum culture yielded Acinetobacter baumannii and Eenterobacter aerogenes. We adjusted antibiotic therapy to imipenem/cilastatin and teicoplanin. Results: After aggressive treatment, poor prognosis was informed to his families about the limit of medical treatment. The patient was critically discharged on March 14, 2016. Conclusion: IPA may complicate influenza pneumonia in a previously healthy adult. Early diagnosis and treatment of Aspergillus infection complicating influenza is mandatory as high mortality. Given the rapid symptom-onset and deterioration, the clinician must be vigilant in promptly diagnosing and treating influenza-like illness. Antibiotics for bacterial co-infection and antifungal therapy should be considered. Finally, corticosteroids in patients with influenza may pose a risk for invasive fungal infections.

Concurrent cytomegalovirus colitis and Aspergillus pneumonia in a patient with plasmacytoma Mei-Yu Su*, Wen-Liang Yu, Che-Kim Tan. Department of Intensive Care Medicine, Chi-Mei Medical Center, Tainan City, Taiwan

Background: Cytomegalovirus (CMV) infections of the gastrointestinal tracts most commonly affected the colon, followed by the small bowel and esophagus. The clinical presentation is largely dependent on the digestive site of infection. Hematemesis can occur in CMV esophagitis, gastritis and duodenitis. We report lower gastrointestinal bleeding caused by CMV infection in a patient.

Methods: This 68-year-old woman had a history of type 2 diabetes mellitus, right femoral neck plasmacytoma with pathologic fracture post bipolar endoprothesis on September, 2016. She suffered from dyspnea for two days. She was brought to emergency department on March 12, 2017. Laboratory data revealed WBC, 5500/μL with 8.9% bandemia; platelet count, 60,000/μL; C-reactive protein, 75.4 mg/L; procalcitonin, 1.28 ng/ml; lactate, 3.1mmole/L; creatinine, 1.4 mg/dL; and K, 6.62 mmol/L. CXR showed consolidations in bilateral lungs. Intubation was done for desaturation. Then she was transferred to intensive care unit. Piperacillin/tazobactam and levofloxacin were used. Levophed and fluid resuscitation were given. Initially treatment showed improvement. Bloody stool was noted and colonoscopy revealed ulcers of ileocecal valve and proximal colon on March 20. However, bloody stool persisted, repeated colonoscopy showed the same lesions and diluted bosmin was injected with hemoclipping. Colon biopsy showed ulceration of ileocecal valve. Blood CMV-PCR showed positive. CMV viral load showed 559,808 IU/mL on March 23. Ganciclovir was given. Bloodly stool was improved. But fever and hypotension occured. CXR showd increased infiltration at both lungs, more at right side. We shifted antibiotics to piperacillin and tigecycline because sputum culture yielded Proteus mirabilis, Klebsiella pneumoniae, and carbapenem-resistant Acinetobacter baumannii. Results: Unstable blood pressure with abdomen guarding was noted. Left decubitus showed moderate free air on liver surface, hollow organ perforation with peritonitis was impressed. Profound shock requred high dose vasopressor. We discussed with surgeons and family, and informed poor prognosis and high mortality rate. Family members declined surgery and requested DNR. She was expired on March 25, 2017. Later, the blood Aspergillus antigen index was 7.3 (normal, < 0.5).

Conclusion: CMV disease is a major cause of morbidity and mortality in immunocompromised patients. CMV colitis in patients with plasmacytoma is quite rare but challenging. Broader use of sensitive CMV detection methods will certainly bring about more chance to early diagnosis of the infection. Although ganciclovir resulted in symtomatic improvement of blood stool, but hallow organ perforation developed, suggesting colon ulcer perforation. Concurrent Aspergillus pneumonia was unexpected.

Higher mortality of severe influenza patients with probable aspergillosis than those with and without other co-infections Wen-Liang Yu. Department of Intensive Care Medicine, Chi Mei Medical Center, Tainan, Taiwan; Department of Medicine, Taipei Medical University, Taipei, Taiwan

Background: Aspergillus-associated infection might comprise up to 23-29% of severe influenza patients from the community throughout stay in an intensive care unit (ICU). In Taiwan, cases of severe influenza with aspergillosis are increasingly reported. Therefore, we describe the relative risk of mortality among severe influenza patients with aspergillosis and other coinfections compared to severe influenza patients without Aspergillus coinfections. Background: Respiratory infections are commonly seen clinically. We hereby report a young male diagnosed with Lemierre syndrome that initially presented with sore throat and dyspnea.

Methods: A previously healthy 20-year-old soldier visited our emergency room (ER) with sore throat, dyspnea, and fevers for five days. When presented to our ER, physical examination revealed bilaterally enlarged tonsils without exudates, and right lower lung crackles; the rest of his physical findings are otherwise unremarkable. Lab showed neutropenia, metabolic acidosis, and acute kidney injury. A chest computed tomography (CT) without contrast showed a right lower lung consolidation, suggesting pneumonia. We thus treated him with intravenous moxifloxacin empirically upon admission. Results: Despite promptly starting antibiotics, intermittent fevers and progressive dyspnea persisted. Blood culture grew anaerobes in preliminary reports on hospital day 3. We shifted his antibiotics to intravenous imipenem. However, his dyspnea and fevers did not resolve after switching to imipenem, and a follow-up CT scan on the hospital day 5 showed a worsening bilateral pneumonia. His final blood culture report yielded Fusobacterium nucleatum. An ambulatory ultrasonography examination of the persistently painful right neck revealed an intravascular mass in right internal jugular vein, suggestive of thrombosis. The diagnosis of Lemierre syndrome was made. He was intubated on hospital day 6, and was added ampicillin/sulbactam in addition to imipenem. A neck CT scan on hospital day 8 (Figure 1 ) clearly demonstrated his right internal jugular vein thrombosis, along with surrounding fat tissue stranding. Right internal jugular vein ligation was performed on hospital day 10. He was successfully extubated after the surgery, and had no more fevers. He was transferred to the general ward on hospital day 11. We then de-escalated his antibiotics to ampicillin/sulbactam. The subsequent clinical course was smooth, and the patient was discharged on hospital day 24. Conclusion: Leimerre syndrome has decreased in incidence and been described as ""the forgotten syndrome"" as antibiotics advances. In a young patient with acute pharyngitis accompanied with signs of septic embolization, Lemierre syndrome should always be considered. Background: Mycoplasma hominis, a well known cause of neonatal infection, has been reported as a pathogen in urogenital infections in adults; however, central nervous system (CNS) infections are rare. We report here the first case of M. hominis meningitis in China, post neurosurgical treatment for an intracerebral haemorrhage in a 79-year-old male. Case presentation: We describe a 79-year-old who developed M. hominis meningitis after neurosurgical treatment and was successfully treated with combined azithromycin and minocycline therapy of two weeks' duration, despite delayed treatment because the Gram stain of cerebrospinal fluid (CSF) yielded no visible organisms. The diagnosis required 16S rDNA sequencing analysis of the cultured isolate from CSF. Literature review of M. hominis CNS infections yielded 16 cases (13 instances of brain abscess and 3 of meningitis). Delay in diagnosis and initial treatment failure was evident in all cases. With appropriate microbiological testing, antibiotic therapy (ranging from 5 days to 12 weeks) and often, multiple surgical interventions, almost all the patients improved immediately. Conclusion: Both our patient findings and the literature review, highlighted the pathogenic potential of M. hominis together with the challenges prompted by rare infectious diseases in particular for developing countries labs with limited diagnostic resources.

Pulmonary mucormycosis in a kidney transplant recipient Yu-An Chen 1 , Chia-Hui Chou 2 , Mao-Wang Ho 2 . Background: Fungi of the order Mucorales are ubiquitous in nature but rarely cause human infection. However, because of the rising amount of immunocompromised patients, the incidence of invasive fungal infection also increased. There was only one case report of pleural empyema related to mucormycosis to date. Here we report the first case in Taiwan and review the literature of the previous reports. Methods: A 54-year-old man with a history of end stage diabetic nephropathy presented to our hospital 35 days after receiving kidney transplantation, complaining progressive dyspnea and pleuritic right chest pain for 5 days. The initial laboratory data showed leukocytosis (WBC 16,200 cells/micro liter) and chest plain film showed right side ill-defined opacity with pleural effusion. Broad-spectrum antibiotic was initiated but the clinical improvement was limited. The pleural effusion Gram's stain and further analysis yielded mold of Rhizopus species. Results: Pig-tail was inserted for pleural effusion drainage, and we started Liposomal Amphotericin B for mold of Rhizopus species. Fever subsided and the respiratory condition also improved. The pig-tail was removed 40 days later when the drainage amount decreased to 30 ml. Liposomal Amphotericin B was discontinued after 25 days of therapy, and was shifted to oral Posaconazole for maintenance. Currently the patient is doing well and received regular follow-up in our outpatient clinic.

The advance of solid organ or hematologic stem cell transplantation, improvement of survival in patients with hematologic malignancy, and the increasing number of HIV-infected patients all contributed the enlarged number of immunocompromised patient. And the number of previously rarely, opportunistic infection will rise with time. Obtain the adequate sample and make the right diagnosis will reverse the potentially life-threatening infection of the patients and is of paramount importance to achieve long-term survival in these patients.

A retrospective analysis of the posaconazole prevention for acute myeloid leukemia patients receiving induction chemotherapy-induced invasive fungal infection Chia-Hsuan Wu, Hui-Chen Su. Department of Pharmacy, Chi-Mei Medical Center, Tainan, Taiwan Background: Acute myeloid leukemia (AML) consists of a group of hematopoietic neoplasms. The treatment includes induction chemotherapy, consolidation chemotherapy and allogeneic hematopoietic cell transplantation. Induction chemotherapy induces severe neutropenia and such immunocompromised patients often become septic shock because of infection. Some clinicians prescribe posaconazole to prevent invasive fungal infection before induction chemotherapy. We want to analyze the posaconazole prevention for acute myeloid leukemia patients receiving induction chemotherapy-induced invasive fungal infection. Method: This is a retrospective study. Patients were diagnosed AML and received induction chemotherapy from 2014 to 2017 at a medical center in southern Taiwan are included. We compare the incidence of invasive fungal infection, the period of neutropenia and the survival rate between two groups, one group consist of patients with posaconazole prevention and the other group consist of patients without posaconazole prevention. Results: There were 25 patients included for analysis. There were 18 patients received posaconazole prevention and 7 patients didn't. The incidence of invasive fungal infection is both 0% in two groups and the period of neutropenia in the prevention group is 17.4 days and 16.1 days in the group without prevention. The survival rate is 59% in the prevention group and 100% in the group without prevention. Conclusion: This retrospective analysis reveals that the incidence of invasive fungal infection and the survival rate show no statistically significant difference between two groups. The one month cost of posaconazole is 59769 NT dollars. Suggest clinicians evaluate the necessity of posaconazole prevention for AML patients with induction chemotherapy according to disease severity or the risk rate of infection. The limits of this retrospective analysis include the small sample size and the absence of disease severity. Therefore, we need more cases to evaluate the long-term efficacy.

Valganciclovir for treatment of cytomegalovirus disease in kidney allograft recipients Hsin-Lin Lin 1 *, Yu-Feng Tian 2 , Ling-Hsien Lee 2 , Li-Ling Chu 1 , Hui-Chen Su 1 . 1 Department of Pharmacy, Chi-Mei Medical Center, Tainan, Taiwan, 2 Division of Transplantation surgery, Department of Surgery, Chi-Mei Medical Center, Tainan, Taiwan Background: Cytomegalovirus (CMV) infection is a frequent complication after transplantation, with its associated morbidity and poor transplant outcome, represents a serious threat to transplant recipients. Valganciclovir is off-label use to CMV infection after renal transplant. Retrospective chart review of patients with CMV disease treated with valganciclovir. Methods: We retrospectively analyzed data on the diagnosis and treatment of CMV infective complications in 20 patients transplanted between 2008 and 2016 (98 men and 95 women), age at the time of transplantation 21-80 years. Results: The overall CMV infection rate was 10%. The cumulative incidences of CMV infection at 3, 6, 9, 12, 60, 120 and more than 120 months after transplant were 2.6%, 3.6%, 4.1%, 5.2%, 7.3%, 9.8% and 10.4% respectively. Patient survival rate and graft survival is 70%. Duration of Valganciclovir were 78.7 ± 38.7days. 70% of patients with renal function is not affected by CMV infection (P < 0.05). The most combination of immunosuppressant were Tacrolimus plus sirolimus and prednisolone. Conclusion: CMV infection after renal transplant recipients due to high mortality rate. CMV infection did not directly affected graft survival rate, but if immunocompromised patients can be due to patient expire. CMV infection with renal transplant recipients should be closely monitor and active treatment.

Increasing risk of Pneumocystis pneumonia among newly diagnosed HIV-infected patients with Pneumocystis colonization Nan-Yao Lee 1,2,3 , Ming-Chi Li 2,3 , Po-Lin Chen 1,2,3 , Chia-Wen Li 2,3 , Wen-Chien Ko 1,2,3 . 1 Departments of Medicine, College of Medicine, National Cheng Kung University, 2 Department of Internal Medicine, 3 Center for Infection Control, National Cheng Kung University Hospital, Tainan, Taiwan Background: Pneumocystis jirvecii pneumonia (PJP) is a lifethreatening opportunistic infection, and develops primarily among HIV-infected persons with a low CD4 count. The epidemiology of pneumocystis colonization among such patients is unknown. The aim of this study was to estimate the incidence of Pneumocystis colonization and risk factors of Pneumocystis pneumonia among newly diagnosed HIV-infected patients. Methods: A prospective study was conducted at a medical center in southern Taiwan with a one-year follow-up after enrollment. P. jirovecii DNA was detected by real-time PCR and sequencing in oral swash samples. Clinical data were collected from medical charts in a uniform data record form. Results: Of 82 enrolled cases, P. jirovecii DNA was detected in 37 (45.1%) asymptomatic cases, who were recognized as colonizers. There were no significant differences in terms of age, sex, comorbidity, risk factor of HIV infection among participants with or without colonization. Eight (9.8%) cases developed PJP during follow ups and there was no fatal case. Pneumocystis colonization was associated with a lower CD4 count (median [IQR]: 144 (42-308) cells/mm 3 vs. 320 (169-492) cells/mm 3 ; P = 0.007), a higher HIV plasma viral load (1.97 × 10 5 [1.02 × 10 5 -4.24 × 10 5 ] vs. 3.59 × 10 4 (1.28 × 10 4 -7.89 × 10 4 ) HIV RNA copies/mL; P < 0.001), and receipt of steroid treatment (P = 0.03). The incidence of Pneumocystis pneumonia for participants with or without colonization was 239.0 and 22.3 cases per 1000 person-year respectively. By the Kaplan-Meier analysis, the cases of Pneumocystis colonization were associated with a higher risk of PJP than those without colonization (Log-rank test, P = 0.04). Conclusion: Pneumocystis colonization is not uncommon in newly diagnosed HIV-infected patients and associated with an increasing risk of PJP.

Increasing urinary tract infections among chronic kidney disease patients with asymptomatic bacteriuria Nan-Yao Lee 1 , An-Bang Wu 2 , Yu-Tzu Chang 2 , Chin-Chung Tseng 2 , Wen-Chien Ko 1 *. Background: Chronic kidney disease (CKD) is more common, and its prevalence is increasing over the world. Asymptomatic bacteriuria, also referred to as bacteriuria or asymptomatic urinary infection, is a common finding among CKD patients with varying prevalence by age, sex, sexual activity, and in both men and women with abnormalities of the genitourinary tract. The aims of the present study were to determine the prevalence of and the risk factors for asymptomatic bacteriuria among CKD patients. Methods: A prospective study will be conducted at a center in Tainan, Taiwan. Participants if they are aged >20 years with chronic renal disease attending the outpatient clinic will be enrolled in this prospective, observational study. At the initial visit, all subjects were interviewed with a standardized questionnaire and submitted a midstream urine specimen for evaluation of bacteriuria and pyuria. During the subsequent follow-up for one year, the care of participants will be followed the treatment guidelines and clinical practices for CKD according decision of attending physicians. Clinical data will be obtained from a standardized medical chart abstraction by use of uniform data abstraction forms. Results: 482 participants were enrolled, asymptomatic bacteriuria was identified among 41 participants (8.5%). The incidence was 85.1 per 1000 person-years. Participants with diabetes were risk for asymptomatic bacteriuria (P < 0.001). Of 19 (4.0%) cases developed urinary tract infections during following period and there was no fatal case. The participants with asymptomatic bacteriuria (P = 0.017) and diabetic (P = 0.014) participants were risk for development of UTIs. Escherichia coli (30, 70.7%) following Enterococcus species (6, 14.6%) and Klebsiella pneumoniae (3, 7 .3%) were most common identified bacteria of asymptomatic bacteriuria. Conclusion: The diabetic was risk factor associated with asymptomatic bacteriuria and developing UTIs among participants with chronic kidney disease.

PS 074 CMV colitis with coexisting or false-positive amebic colitis in a HIV patient Wen-Chi Hsu 1 *, Liang Yu 2 . 1 Department of Nursing, Chi Mei Medical Center, Tainan, Taiwan, 2 Department of Intensive Care Medicine, Chi Mei Medical Center, Tainan, Taiwan Background: Ameba and Cytomegalovirus (CMV) virus are common pathogens causing gastroenteritis in a human immunodeficiency virus (HIV)-infected patient. Symptoms include abdominal pain, watery diarrhea and bloody stool. We report on a HIV patient with delayed diagnosis of CMV colitis, who was initially treated for amebic colitis but with relapse. Rapid diagnosis and appropriate treatment are therefore particularly important in these contexts. Methods: Case study Results: A 27 year-old man with HIV infection had water diarrhea for 2 weeks. Fever with general skin rash was noted for 3 days. Then, he visited our hospital. On examination, the temperature was 38.1°C; the blood pressure, 107/68 mm Hg; and the pulse, 84 beats per minute. Diphenhydramin and methylprednisolone were used for suspected allergy. Initial laboratory data showed WBC, 11.200/ μL; CD4 counts, 478/uL; creatinine, 0.65 mg/dL; S-GOT, 664 U/L; S-GPT, 619 U/L and total bilirubin, 2.82 mg/dL. Amoebiasis antibody titer was 1:4096 by using indirect hemagglutination assay (IHA). Metronidazole had been used for 5 days. However, acute hepatitis (S-GOT, 1377 U/L and S-GPT, 1696 U/L), elevated bilirubin (total, 7.77 mg/dL and direct, 5.85 mg/dL) and severe coagulopathy ( prothrombin time, 31.6 seconds) occurred. He was transferred to the intensive care unit for suspected sepsis and acute liver failure. Colonoscopy showed mucosal hyperemia with edema in the whole colon and multiple ulcers in the transverse colon, descending colon, sigmoid colon and rectum, favoring amebic colitis. Piperacillin/tazobactam and metronidazole had been given for one week. His condition improved and he was transferred to general ward. However, worsening water diarrhea relapsed and episodes of hypotension occurred. The pathology of mucosal biopsy revealed CMV colitis. Meanwhile, blood CMV viral load was 889 IU/mL and CMV antigenemia was 1 cell per 200,000 PMN. The CMV-PCR results were positive in blood and stool samples. Therafter, intravenous ganciclovir achieved rapid improvement of diarrhea and liver function. He was discharged after 3 weeks of hospital stay. Conclusion: False-positive amebic IHA was ever reported. Our case highlights the importance of histopathological diagnosis for watery diarrhea in HIV patients. Blood and stool CMV-PCR might help presumptive diagnosis. However, the possibility of CMV colitis alone, following or coexisting with amebic colitis was difficult to discern. CMV activity might be enhanced by steroids, which therapy should be careful in HIV patients even without low CD4 count.

Mycobacterium avium complex lung infection in a patient with anti-interferon-γ autoantibodies and previous Mycobacterium abscessus neck lymphadenitis Lih-shinn Wang 1 , Chia-ching Jackie Wang 3 , Cheng-lung Ku 4,5 , Jen-jyh Lee 2 , Chia-jung Chiang 6 , Po-ren Hsueh 7 . Background: Mycobacterium abscessus is extremely difficult to treat and considered incurable. Pulmonary disease caused by M. avium complex (MAC) usually occurs in patients with chronic lung disease or HIV/AIDS. Here we report a case of MAC lung disease with a previous history of M. abscessus lymphadenitis in a 50-yearold female chef. Methods: The patient presented with chronic cough and dyspnea in April, 2013. Sputum was negative for acid-fast bacilli (AFB). Neck computed tomography (CT) revealed multiple cervical lymphadenopathy ( Figure) . Chest HRCT revealed a RUL alveolar lesion and right hilar lymphadenopathy. Multiple fixed enlarged cervical LN's were found along level II-V, right lateral neck with desmoid adhesion to surrounding soft tissue at surgery. Pathology revealed granulomatous inflammation, and AFB was positive. Antituberculosis (TB) treatment was initiated with HERZ (isoniazid, ethambutol, rifampin, pyrazinamide). Sputum culture later grew a non-tuberculous mycobacterium (NTM), and pyrazinamide was switched to azithromycin in June 2013. She had free T4 of 0.87 ng/ dL, TSH 0.260 uIU/mL, anti-TG 95.1 IU/mL, and anti-TPO 146.1 IU/ mL. Fine needle aspiration in Dec., 2013 revealed Hashimoto's thyroiditis. She completed her anti-TB treatment in Jan., 2014. Pus culture grew M. abscessus, and anti-mycobacterial treatment was switched to clarithromycin and ciprofloxacin at National Taiwan University Hospital in Feb., 2014. The patient could not tolerate ciprofloxacin, which was changed to levofloxacin and doxycycline. The patient did not respond to the new anti-NTM therapy, so she was admitted to Tzu Chi Hospital in April 2014. Results: Chest film revealed RLL infiltrate, and Mycoplasma IgM was positive. Anti-NTM treatment was switched to intravenous tigecycline, levofloxacin and amikacin, in addition to oral clarithromycin and sulfamethoxazole/trimethoprim treatment. Antiinterferon (IFN)-γ autoantibodies were positive. Sputum culture grew a NTM, which was identified as MAC by an official TB Center at Tainan. Because the patient responded to the new anti-NTM therapy, we didn't change the regimen. Eventually the neck lymphadenopathy relapsed when we changed her oral anti-NTM treatment to clarithromycin, levofloxacin and sulfamethoxazole/ trimethoprim. Unfortunately, the patient succumbed to this infection. Conclusion: Patients of M abscessus lung disease could have coexisting or previous history of MAC pulmonary infection. Anti-IFN-γ autoantibodies may play a role in mycobacterial infections, particularly in persons without predisposing conditions. Patients with M. abscessus pulmonary disease should be treated with multidrug anti-mycobacterial therapy based on susceptibility test results whenever available, and surgical resection may offer a more favorable response in selected cases.

Mycobacterium mucogenicum infection presenting with generalized follicular hyperplasia and prurigo nodularis in an immunocompetent patient Lih-shinn Wang 1 , Ting-shu Wu 2 , Chia-ching Jackie Wang 3 , Cheng-lung Ku 2,4 , Yung-hsiang Hsu 5 , Pau-yuan Chang 6 . Background: Mycobacterium mucogenicum is a rapidly growing mycobacteria (RGM) frequently isolated from tap water or from respiratory specimens. It is associated with a wide spectrum of clinical manifestations in both immunocompetent and immunocompromised hosts. Methods: A 69-year-old male florist had a medical history of BPH, anemia, and hypertension. He started taking a health drink called ""Imumio Miracle Water,"" which was hydrogen enriched water with ""Super Lutein"" produced by Naturally Plus Company, in mid Aug., 2015. He then developed nodular skin lesions involving bilateral upper limbs 2-3 weeks thereafter. He lost 3 kg of weight, and had intermittent fevers since Sep., 2015. He stopped taking the health drink in late Sep., 2015. Whole body Gallium scan revealed bilateral hilar lymphadenopathy, and chest computed tomography (CT) revealed cervical, axillary and mediastinal lymphadenopathy. ESR was 103 mm/hr, CRP 5.85 mg/dL, cold agglutinin 1:128 (+), and CMV IgG: 144 AU/mL. He was referred to Infectious Diseases service under the impression of Kikuchi disease. Results: The skin lesions were consistent with prurigo nodularis. ( Figure. ) Sputum culture grew a non-tuberculous mycobacteria (NTM). He was started on azithromycin, rifabutin and ethambutol for possible MAC, although HIV test was negative. MAC PCR was negative. Anti-interferon-γ autoantibodies were also negative. Axillary biopsy did not show lymphoma, or toxoplasmosis. Additional lymph node biopsy by mediastinoscopy revealed lymphoid hyperplasia. He remained febrile, and anti-NTM treatment was switched to tigecycline and meropenem for possible RGM. Bone marrow biopsy was negative for lymphoma. Clarithromycin was added, and the patient became afebrile. The isolate was finally identified as M. mucogenicum, and susceptibility testing showed susceptible to clarithromycin, tigecycline, imipenem, sulfamethoxazole/trimethoprim, and linezolide. The patient had oral clarithromycin, sulfamethoxazole/trimethoprim, and linezolide at OPD, but we soon discontinued sulfamethoxazole/ trimethoprim due to allergy. The skin lesions resolved a few weeks after initiating the combination treatment, and repeat Ga scan and chest CT demonstrated resolution of lymphadenopathy, but follow up ESR and CRP levels were still high after anti-NTM treatment for a total of 18 months. The patient did not have thrombocytopenia with linezolid treatment throughout the course. Testing of the health drink did not show M. mucogenicum.

Conclusion: M. mucogenicum is often of no clinical significance, but it can cause systemic illness even in an immunocompetent host. The basis of combination antimicrobial therapy should be based on susceptibility test results whenever available. The optimal duration of therapy is unknown.

Fulminant liver failure secondary to systemic lupus erythematosus reactivated in a chronic hepatitis B Hui-Chun Chao. Department of Intensive Care Medicine, Chi-Mei Medical Center

Background: The frequency of liver dysfunction or abnormal liver enzyme values during the course of systemic lupus erythematosus (SLE) ranges from 19% to 60%. Patients with SLE have a 25-50% chance of developing abnormal liver tests in their lifetime. In many cases, the prevalence of the concomitant hepatopathy is higher when associated with SLE than alone. Use of these drugs in patients with previous HBV infection can result in severe liver damage if the virus is reactivated. Case Report: This 52-year-old man, a business man working in China previously, had hepatitis B and SLE. He began to have leg edema and bowel habit change and hyperbilirubinemia was found. Gastroenterologist suspected ""acute cholestatic hepatitis or chronic hepatitis B with reactivation, decompensated status."" Baraclude was started, however bloody stools passage occurred. Colonscopy were repeated and showed a large nearly circumfrential rectal ulcer with out-pouched sac and large blood clots, and blood-covered mucosa from rectum to terminal ileum with much blood clots at terminal ileum. After admitted to our ICU, there were only intermittent bloody stools in the first days. Dyspnea aggravated and was suspected due to lung edema r/o pneumonia by CXR. He received endotracheal intubation and mechanical ventilation after failure of diuresis/albumin and high flow nasal cannula use. Fever was present on admission and recurred up to 40 degree Piperacillin/ tazobactam and moxifloxacin were given since admission. Pre-liver transplant computed tomography scan showed suspicious lower rectal perforation and then sigmoid loop colostomy performed on 3/31. After further discussion with radiologist, SLE vasculitis involving several major vessels was suspected, which rendered liver transplant not feasible according to the radiologist's suggestion. His liver function also continued to deteriorate. Due to severe coagulopathy, recurrent bleeding occurred. ICH was suspected when dilated and fixed pupils were found. Brain image could not be performed due to unstable oxygenation and hemodynamics. After discussion with his wife, she requested for comfort care only. He finally passed away after 2days later. Conculsion: Liver dysfunction in the presence of SLE can be caused by many factors, but when at the time of SLE onset, either SLE itself or drugs can be the cause. The features of liver dysfunction and its causes need to be elucidated and considered before treating SLE.

Multiple opportunistic infections in a patient of eosinophilic granulomatosis with polyangiitis Pin-Chuan Lin 4 , Li-Shinn Wang 1 , Shih-Tzu Tsai 2 , Ya-Ju Wu 5 , Chih-Bin Lin 3 , Teng-Yi Lin 6 . 1 Division of Infectious Diseases, 2 Division of Rheumatology and Immunology, 3 Division of Chest Medicine, 4 Department of Internal Medicine, 5 Department of Intensive Care Medicine, and 6 Department of Laboratory Medicine, Buddhist Tzu Chi General Hospital, Hualien, Taiwan Background: Eosinophilic granulomatosis with polyangiitis (EGPA; also known as Churg-Strauss syndrome) usually presents as recurrent asthma that cannot be controlled without systemic steroids, which, unfortunately, increases the risk of opportunistic infections. Methods: We report a 36-year-old male electronics processing staff presented to the Emergency Department with a rapid onset fever with productive cough. He'd been hospitalized for many times in recent 5 years due to recurrent asthma requiring oral prednisolone, and pneumonia, including a Pneumocystis jiroveci pneumonia 2 years ago and an aspergillosis pneumonia 6 months ago He recovered from 4 months of voriconazole treatment, which had been stopped for 2 months. Complete blood count showed peripheral eosinophilia during the attacks. He also developed palpitation and neuropathy since 3 months ago. Physical examination demonstrated a fever of 39.7°C, tachycardia at 122 bpm, wheezing and basal crackle breath sounds, elastic nontender purplish nodules at fingers and palms, and paresthesia and muscle atrophy predominantly at both thighs. Chest X-ray showed interstitial infiltration and lab tests demonstrated leukocytosis of 21,940/μL, eosinophilia of 65%, elevated procalcitonin of 67.54 ng/mL, and elevated CRP of 13.37 mg/dL. Results: The patient was given piperacillin/tazobactam as empiric treatment. He then developed ventricular fibrillation on hospital day 2, and was intubated and transferred to medical ICU after CPCR. Chest X-ray showed consolidation with air bronchogram in both lungs ( Figure 1 ). Cardiac echo demonstrated global hypokinesia. EGPA was diagnosed according to the 1990 ACR diagnostic criteria, treatment with high dose intravenous methylprednisolone and rituximab started, and his eosinophilia, leukocytosis, and pulmonary infiltration on chest X-ray gradually subsided. His intermittent fevers persisted, though. Blood, urine, and stool cultures were negative for common pathogens, and influenza, HSV, CMV, Cryptococcus, P. jirovecii, and HIV were negative. A sputum culture grew Candida albicans, and Aspergillus DNA tests were equivocal in blood and sputum. The patient became afebrile and procalcitonin and CRP levels returned to normal limits after antibiotics were shifted to the combination of meropenem, tigecycline and caspofungin after several attempts. Finally, the patient was confirmed to have CMV replication by positive CMV pp67, and was discharged after completing 14 days of ganciclovir treatment. Conclusions: Opportunistic infections should be considered when fever with abnormal procalcitonin and CRP levels persists despite that EGPA was grossly under controlled. Cardiac involvement may lead to life-threatening arrhythmia in EGPA.

Background: Cytomegalovirus (CMV) infection is increasingly recognized in critically ill immunocompetent patients because of previous studies demonstrated an association between CMV disease and increased mortality rates, prolonged intensive care unit and hospital length of stay, prolonged mechanical ventilation, and nosocomial infections. Molecular assays are now considered to be the ""gold standard"" for assessment of human CMV infection and disease in those at risk from severe associated clinical manifestations. The nucleic acid sequence-based amplification assay, CMV pp67 late Gene transcripts (CMVpp67) is suggested by published reports for reliable prognosis. Methods: We performed a retrospective review of the medical records of patients in medical, surgical and burn intensive care units (ICU) with positive and negative detection of human CMVpp67. We compared the characteristics of 18 patients with positive CMVpp67 with those of 64 patients with negative CMVpp67. Results: Under the univariate analysis, patients with positive CMV pp67 are associated with higher proportion of sepsis (56.2:83.3%, p = 0.036), longer ICU lengths of stay ( p = 0.004), and longer hospitalization days ( p = 0.002). However, after considering all the variables in the multivariate analysis, there were no statistical significances among all the variables and CMV pp67 status. In addition, we also found that CMV pp67 status was not associated with the mortality rate (OR = 0.07, 95%CI = <0.001-9.09). But those who died were associated with more different antibiotics use (OR = 2.3, 95%CI = 1.05-5.05), higher percentage of vasopressor use (OR = 206.5, 95%CI = 5.52->999.99), higher prevalence of end stage renal disease (OR = 195.1, 95%CI = 2.27->999.99) but shorter hospitalization days (OR = 0.96, 95%CI = 0.93-1.00). Conclusion: In conclusion, in our cross sectional study, within critically ill patients, there were no correlation of positive CMV pp67 with the characteristics we assumed to be; therefore, we couldn't tell if there is any parameter with higher positive CMV pp67 rate. In addition, positive CMV pp67 wasn't related to higher mortality rate. Owing to controversial results compared to previous studies, further larger sample size studies are necessary to develop more accurate correlation results. Background: The establishment of bacterial species identification through gene sequencing triggered an increase in the reported cases of non-tuberculous mycobacterial infection caused by new bacterial species, which prompted the urgent need for the establishment of diagnosis and treatment regimens. Methods: The patient was a man at age 64. Upon being observed with minor hemoptysis in December 2011, the patient visited our hospital. Granular patterns were observed dominantly in the rightlung, along with bronchodilation and partial cavitary lesion, which led to the suggested diagnosis of pulmonary mycobacteriosis.

Sputum and bronchoscopically-extracted bronchoalveolar lavage fluid were examined for mycobacteria. While both specimen were identified with mycobacteria specularly±and in culture positive, TB and MAC-PCR found it negative, and it was confirmed unidentifiable through DNA-DNA hybridization (DDH). The case was then followed up as pulmonary non-tuberculous mycobacterial infection but observed with gradual progression of the lesion, which led to two courses of sputum cultivation to identify the mycobacteria in September 2016. While the cultures were identified positive, both TB-LAMP and MAC-PCR returned negative results, and DDH failed to identify it. In order to identify the causal bacteria, 16S rRNA gene sequencing was conducted. Results: 16Sr RNA gene sequencing found it 100% phasic with M. shinjukuense. As the clinical and bacteriological findings suffice the criteria by American Thoracic Society/Infectious Diseases Society of America Guideline, the patient was diagnosed with pulmonary non-tuberculous mycobacterial infection triggered by the said bacteria. Conclusion: M. shinjukuense, initially reported in Japan in 2011, is a new type of non-tuberculous mycobacteria belonging to the 3rd bacterial group under Runyon classification. Because tuberculosis complex rapid diagnostic techniques like Mycobacterium Tuberculosis Direct (MTD) or Transcription-reverse transcription concerted method (TRC) occasionally return a false-positive result, diagnosis requires an extra caution. There are reports of chemotherapies using CAM/RFP/EB and INH/RFP/EB, suggesting the treatments' effectiveness.

A probable case of Mycobacterium gordonae infection presenting as cervical lymphadenopathy and fever of unknown origin Sy-harn Lian 1 , Lih-shinn Wang 1,2 , Jen-jyh Lee 1,3 , Ying-huei Chen 4 , Huei-jen Chao 4 , Chia-jung Chiang 5 . 1 School of Medicine, Buddhist Tzu Chi University, Hualien, Taiwan, 2 Division of Infectious Diseases, 3 Division of Chest Medicine, Department of Internal Medicine, 4 Department of Laboratory Medicine, Buddhist Tzu Chi General Hospital, Hualien, Taiwan, 5 Ministry of Health and Welfare Chest Hospital, Tainan, Taiwan Background: Mycobacterium gordonae, a scotochromogenic nontuberculous mycobacteria (NTM), is ubiquitous. Many isolates represent colonization or contamination of the specimen, but not true infection. Methods: This 24-year-old woman, a nurse of a local clinic, presented with a 3-month history of fevers. She started with intermittent fevers of 40°C, with blood-tinged sputum, sore throat, vertigo, hearing loss, tinnitus, nausea, vomiting and headache. She also noted a mass on the right neck, 3 centimeter in size. Additionally, she had recurrent oral ulcers. Her grandmother had a history of tuberculosis. She had a history of exposure to a dog. No drug, toxin or travel history was identified. Results: The complete blood counts with differential count and platelet count, blood cultures, blood chemistries, ESR, CRP, urinary analysis, purified protein derivative skin test, abdominal sonography, chest radiograph and high resolution computed tomography of the chest revealed unremarkable findings. The acid-fast stain of sputum showed equivocal result in only one of the initially three specimens. Only one of the three sputum cultures grew a NTM. The head and neck CT showed bilaterally enlarged neck lymph nodes over level II/III, especially right level II. The survey of Streptococcus A, Cryptococcus, M. pneumoniae, syphilis, HIV, CMV and HSV, revealed unremarkable results, and no infectious sources were identified despite the investigations by the dermatologist, gynecologist, otolaryngologist, dentist and gallium-67-labeled leukocyte scanning. No malignancies were found by examining CEA, CA15-3, CA125, CA19-9, and by colonoscopy . The surveys of  autoimmune diseases, including T3, T4, TSH, anti-TG, anti-TPO, C3,  C4, ferritin, IgA, IgG, IgM, rheumatoid factor, antinuclear antibodies, anti-Jo-1, anti-SSA, anti-SSB, anti-CENPB, anti-Scl-70 , anti-Sm, anti-RNP and anti-nDNA, were negative. The anti-interferon-γ autoantibodies were also negative. The NTM was further idntified as M. gordonae by PCR at an official Mycobacterium reference laboratory. The diagnosis was established by one equivocal AFB stain and only one positive culture result, which did not fit American Thoracic Society Diagnosis Criteria for pulmonary NTM infection. However, the patient became afebrile with rifampicin, ethambutol and isoniazid treatment, and the cervical lymphadenopathy resolved a few weeks later. All the associated symptoms were gone and the patient cured without relapse with a total of 11months of the anti-NTM treatment and 12 months of follow-up. Conclusion: The most effective treatment regimen for M. gordonae has not been established. However, the clinical response to specific antimycobacterial therapy indicates possible disease presence. The recommended duration of therapy is not known.

The accuracy of GeneXpert test as the first-line mycobacterium tuberculosis identification tool in a regional hospital in south Taiwan Chih-Hsiung Kao, Chiung-Yin Chuang, Tzu-Wei Juan. Department of Laboratory Medicine, St. Martin De Porres Hospital, Taiwan

Background: The GeneXpert test is a new automated molecular test which detects the DNA in Mycobacterium Tuberculosis (MTB), as well as testing for the genetic mutations associated with resistance to the drug Rifampicin (RIF). Some organizations have claimed that the GeneXpert test is going to revolutionize the diagnosis and care of patient with MTB. It provides results directly from sputum specimen in less than 2 hours. WHO recommended that the test should be used as the initial diagnosis test in individuals suspected of having Multi-Drug Resistant Tuberculosis (MDR TB). The purpose of our study was to assess the performance to detection of drug resistant MTB of the GeneXpert Test compared to conventional culture methods when implemented into the normal clinical workflow of a regional hospital in south Taiwan. Methods: WHO emphasized that the GeneXpert MTB/RIF test does not eliminate the need for conventional microscopy culture and drug sensitivity testing (DST), as it detects both live and dead bacteria. We assessed the performance of GeneXpert test from April 2015 to April 2017 with all of new directly sputum smear acidfast bacilli (AFB) stain positive tuberculosis suspects. Each new AFB smear-positive patient's specimen was processed with reflecting GeneXpert test and then sent to our referral laboratory for conventional MTB culture and DST. Results: A total of 40 patients with suspected pulmonary MTB who had a smear positive AFB sputum specimen were enrolled into the study. Among these 40 patients, 39(97.5%) had no prior history of MTB and 1(0.25%) were retreatment MTB case. Overall, 32(80%) samples were culture positive, 8(20%) samples were culture negative. Both GeneXpert and conventional DST revealed 32 (100%) MTB isolates had no RIF resistance. There was 100% agreement between the GeneXpert and conventional DST results in detecting RIF resistance MTB. Conclusion: This study showed and verified the accuracy of GeneXpert test, and this result was similar to those of some early experimental findings and previous reports. The main advantages of the GeneXpert test are, for diagnosis, reliability when compared to sputum microscopy and the speed of getting the result when compared with culture. For diagnosis of MTB, although sputum microscopy is both quick and cheap, it is often unreliable. Although culture gives a definitive diagnosis, to get the result usually takes weeks rather than the hours of the GeneXpert test.

Multiple skin nodules in a post-renal-transplantation woman Jia-Yin Guo, Mao-Wang Ho, Chia-Hui Chou, Yuan-Ti Lee, Chiu-Ching Huang, Wen-Shin Hsih. Department of Internal Medicine, China Medical University Hospital, Taichung, Taiwan Background: Nontuberculous mycobacteria (NTM) are ubiquitous in soiil and water. Mostly, NTM only cause disease in whom with impaired host defense, such as immunosupressed patients. Mycobacterium haemophilum is belonging to the group of NTM, which frequently found in environmental habitats, and known to cause cutaneous and subcutaneous infections. Mycobacterium haemophilum infection is hard to treat and there is no standard guidelines are available now. Hence, we share a patient infected with cutaneous Mycobacterium haemophilum responded well to the treatment. Methods: A 75-year-old female with a history of type 2 DM and end stage renal disease received renal transplantation 18 years ago, was under mycophenolate mofetil and prednisolone as immunosuppression drug for years. She got a left wrist contussion injury and received tranditional chinese herbal medicine applied on the wound 6 months before. Initially, the wound swelled with erythematous base, later much erythematous papules developed along forearm, fingers and face, then progressed to tender nodules, ruptured with odorous pus, and poor healing ulcers left. She was admitted under the impression of cellulitis, which pus culture yield NTM, Mycobacterium haemophilum by molecular test. She received antibiotics with rifampin, moxifloxacin and clarithromycin. Immunosuppression was adjusted to low dose prednisolone only. However, due to persistant poor healing wound, antibiotics was adjusted to rifabutin, ciprofloxacin, clarithromycin and doxycycline. Clarithromycin was withdrawn on the second week for suspected side effect of vomiting. Tigecycline was added for secondary wound infection. In addition, plastic surgeon suggested wound care with hypochlorous acid and Microdacyn 60 and performed local debridement once. Results: After admission for one month, there was progressive epithelialization over skin defects of left hand. No more new nodules were found over the integument. She was discharged with oral rifabutin, ciprofloxacin, and azithromycin. After 3 months of treatment, her skin lesion had totally recovered. Conclusion: In immunocompromised host with ulcerating skin lesion, M. haemophilum infection should be taken into consideration, especially when AFB is seen by direct microscopy while routine mycobacterial cultures remain sterile. In most cases, recovery after prolonged antimycobacterial treatment is common.

Control of community outbreak of multidrug-resistant tuberculosis in three aboriginal villages of Hualien County in eastern Taiwan Cheng Po-Pin 1,2 , Lee Jen-Jyh 1,2 , Lin Chih-Bin 1,2 , Sun Hung-Chieh 4 , Wang Lih Shinn 1, 3 Spoligotyping identified the Beijing strain as the predominant genotype (n = 49, 65.4%), followed by Haarlem H3 (n = 19, 25.3%), and T1(n = 1, 1.3%). Six (8.0%) isolates did not match any spoligotype in the spolDB4 data base. We have determined that 18 (24.0%) isolates were determined to have a unique pattern and 57 (76.0%) were clustered pattern strains (classifying into 6 clusters). The largest cluster (E cluster) belonged to the Beijing genotype and included 26 cases, 12 of whom lived in the same community (ShilinTownship) of 1346 inhabitants with close relationship (relatives, neighbors or friends). The second largest cluster (F cluster) belonged to the Haarlem H3 genotype and comprised 19 patients, 18 of them lived in another community (Hechung Township) of 470 inhabitants with close relationship (relatives, neighbors or friends). All 75 MDR-TB patients in three aboriginal villages were enrolled in the DOTS-Plus program. Five cases were still in treatment, and among 70 patients with final outcomes, 56 (80%) were treated successfully. The prevalence of MDR-TB patients improved dramatically over the last ten years.

The proportion of MDR-TB patients with a clustered pattern strain in the three aboriginal villages of Hualien County in eastern Taiwan was very high. The control of community outbreak of MDR-TB is successful over the last 10 years. (See figure) PS 086 Tuberculosis in patients with active malignancy and transplant recipients Louisa J Sun, Heather L Isenman, Catherine WM Ong. Division of Infectious Diseases, National University Health System, Singapore Background: Transplant recipients and patients with malignancies are at high risk of developing tuberculosis (TB). Diagnosis may be delayed in this group of patients as they often present with atypical symptoms. We reviewed the clnical characteristics and treatment outcomes cancer patients and transplant recipients who developed TB in our center in a moderate TB burden country. Methods: Patients with malignancies and transplant recipients were identified from a database of total 724 active TB cases from January 2013 to December 2015. Medical records were reviewed we analysed clinical characteristics including mortality outcomes, site of infection, chemotherapy regimes, duration from transplant to infection, TB drug regimens used and treatment complications. Results: In 3 years, we found 73 cancer patients and 6 transplant recipients who were diagnosed with TB.

Lung and haematological cancers accounted for 22% each of the total number of TB cases diagnosed in cancer patients. The median age was 59 years old. Pulmonary TB was the most common site of TB (82%). Other sites included lymph node and gut. Of concern, a large majority of patients with pulmonary TB were smear negative (39 out of 60). Diagnosis was made by positive GeneXpert or culture. 89% of patients had received chemotherapy, including steroids. Patients were treated with standard first-line TB drugs, except for 5 haematology patients who received rifampicin-sparing regimes and 8 oncology patients who received levofloxacin instead of pyrazinamide. 11 patients developed treatment-related complications. All cause-mortality was 22.2%. The transplant group comprised of 4 renal and 2 haematopoietic stem cell transplants (HSCT). There were no liver transplant recipients. The median age was 53 years old. 4 patients had pulmonary TB, of which 3 had positive sputum smears, indicating high infectivity rate. 3 of four renal transplant recipients developed TB at a median of 6 years post-transplant (range 5-7 years). Both HSCT patients developed TB within 6 months post-transplant. All but 1 patient were still on immunosuppression at the time of TB diagnosis. Of the 4 patients who completed treatment, rifampicinsparing regimes were used, with drug-associated complications occurring in 2 patients. The median treatment duration was 9.5 months (range 9-15 months). All-cause mortality was 14.3%. Conclusion: Transplant patients and patients with active malignancy who develop TB are at risk of complications of disease and treatment, and have a high mortality rate. Efforts to decrease active TB rates in this group of immunocompromised patients can reduce patient morbidity and mortality.

Serum ferritin levels predict disease severity in pediatric dengue infections Tzong-Shiann Ho 1,2,3 , Han Lee 1 , Ning Chung 1 , Fu-Chun Kuo 1 , Ching-Fen Shen 1 , Shih-Min Wang 1,2,3 , Ching-Chuan Liu 1,3 *. Background: As one of the most prevalent mosquito-borne infectious diseases, dengue results in various clinical outcomes ranging from mild fever to fatal hemorrhagic shock. There is continuously an unmet clinical need for biomarkers of dengue severity. Macrophage activation has been recently observed in severe dengue patients. Serum ferritin is also one of the macrophage activation markers in vivo. This study aims to clarify the relationship between serum ferritin levels and the development of severe dengue in children. Methods: Patients younger than 18 years admitted to a tertiary medical center in southern Taiwan with laboratory-confirmed dengue infections during the 2015 dengue outbreak were included in the current study. Epidemiological characteristics, clinical manifestations and laboratory data of these patients were analyzed by electronic chart record. Serum ferritin levels were determined in patients with available specimens by commercial enzyme-linked immunosorbent assay kits. Results: Totally seventy-two pediatric dengue patients (40 males) were grouped by the updated World Health Organization classification. Among them, 53 patients (73.6%) were with warning signs, 6 (8.3%) were classified as severe dengue. Serum ferritin levels was significantly increased in patients with clinically severe dengue (7298 ± 1497 ng/mL) compared with dengue with warning signs (1008 ± 261.3 ng/mL) and age-matched healthy participants. Serum ferritin levels are higher in severe cases on 4th day of dengue infection, a cut-off value of 2377 ng/mL also predicted severe dengue in children with area under curve 0.9853 in receiver operating characteristic curve analysis. Results: 40 patients were histopathologically confirmed as KFD, and the mean age of the patients was 13.9 ± 3.1 years. The male to female ratio was 1:1. The lymph node involvements were often cervical (95%) with features of unilateral predisposition (75%), polyadenopathy (84.4%) and tenderness (56.3%). Fever, cough, rhinorrhea, and tonsillitis were other common presentations. Laboratory findings included leukopenia (56.5%), monocytosis (63.6%), and positive results of EB-VCA IgG (88.9%), EB-VCA IgM (22.2%), EBEA IgG (22.2%) and EBNA IgG (88.9%). The univariate analysis of prolonged fever with lymphopenia, monocytosis, thrombocytopenia and necrotizing type in histopathology was disclosed as statistically significant (P < 0.05). Glucocorticoids and Hydroxychloroquine were administered in 15.6% of patients respectively, along with symptomatic treatments for the rest. Recurrence occurred in 11.1% of patients without Hydroxychloroquine treatment. In situ hybridization was performed using internal repeat 1 fragment DNA and EBV-coded small RNA (EBER-1) for EBV, and showed that cases were negative for EBV genome. Conclusion: KFD should be suspected in children with febrile cervical lymphadenopathy, especially when concomitant with leukopenia, monocytosis and past Epstein-Barr virus infection. Lymphopenia, monocytosis, thrombocytopenia and necrotizing type in histopathology are reliable predictors for prolonged fever. Our results suggest EBV may not have causative role in the pathogenesis of KFD.

Epidemiology of RSV and Non-RSV acute bronchiolitis and effectiveness of palivizumab Chia-Wan Tang 1 *, Kai-Shen Hsieh 2 . 1 Department of Pediatrics, An-Tai Medical Care Corporation Tian-Sheng Memorial Hospital, 2 Chang-Gung Memorial Hospital, Taiwan.

Background: Respiratory syncytial virus (RSV) causes 50-90% of acute bronchiolitis and 4-50% of pneumonia. Almost 40-60% of all infants infected in the first year and 85-90% infected by age two. Reinfection is common, even within one respiratory season and children tend to have increased risk wheezing episode after RSV infection. The aim of this study is to study the epidemiology of RSV and non-RSV acute bronchiolitis and the effectiveness of Palivizumab, a monoclonal antibody produced by recombinant DNA technology. Methods: A retrospective review of the medical records of children under 2 years of age with acute bronchiolitis hospitalized in a tertiary medical center (VGHKS) in southern Taiwan between January 2005 and December 2014. The patients were grouped in two: RSV positive and RSV negative. We checked the gender and age distribution from the two groups and in addition, we reviewed the duration of hospitalization as well. Results: There were 1727 patients hospitalized for acute bronchiolitis, and (16%) were RSV infections. The male to female distribution was 148 vs 96 in the RSV positive group and 944vs 539 in the RSV negative group. RSV bronchiolitis had longer hospital stays (9 vs. 7 days, P < 0.0001), and the patients suffer from RSV related bronchiolitis were younger of age (0.23 vs 0.5 years, P < 0.0001). Conclusion: RSV related acute bronchiolitis has different seasonality in Taiwan. This disease cause higher morbidity in younger infants and has a more prolonged length of hospital stay. Children who received Palivizumab had less RSV related bronchiolitis.

Fever of unknown origin as the first manifestation of malignancies in Albanian adults Migena Qato, Najada Como, Arjan Harxhi, Pellumb Pipero, Iris Koshovari, Dhimiter Kraja. Infectious Diseases Service, UHC ""Mother Theresa"", Tirana, Albania Background: In the last 60 years, there have been many researches on the various causes of fever, as the definitions and patterns of disease have changed and serological and imaging tools are improved estabilishing in this way the diagnose sooner. Tumoral pathologies are often presented as prolonged febrile syndromes, with fever as the only dominant clinical pattern. In everyday practice they also represent a considerable number of misdiagnoses, mainly infectious nosologies. Considering that tumors and fever are already a solid clinical binomial, the febrile profile has become the most attractive part of tumoral pathologies. Methods: Our study includes 123 cases, during 2010-2015, age groups 20-70 years. The epidemiological study is based on a survey, according to gender, age group and residence; the clinicaldiagnostic study is based on detailed anamnesis, epidemiological survey, clinical, biological, biochemical, microbiological and serological research. Results: Epidemiological Males were 79 cases and females 44 cases. Mean age was 45 years old, resident in town 82 and countryside 41. Clinical A. Topography/tumor Head and neck 9 (Cerebral tumor 7,thyroid 2) Thorax 11(pulmonar adenocarcinoma 8, breast 3 ) Gastrointestinal tract 49 (gastric cancer 4, colon 16, pancreas 15, liver 8, billiary tract 6) Blood 46 (leucosis 19, lymphoma 17, myeloma 8, Hairy Cell Leukemia 2) Urogenita tract 8 (renal cancer 2, prostatic 2, seminoma 1, ovarian3) B. Type of fever/ tumor Continuous (cerebral 3, thyroid 2, pulmonar 6, colon 6, intestinal 1, pancreas 5, acute leucosis 8, billiary tract 2, renal 1, seminoma1, breast 1) Remmitent (cerebral 2, colon 5, intestinal 2, pancreas 4, liver 2, billiary tract 2, acute leucosis 2, HCL 2, seminoma 1,breast 2) Intermmitent (cerebral 2, pulmonar 2, liver 6, billiary tract 2, pancreas 6, colon 5, renal 1) Recurrent (lymphoma 7, acute leucosis 6, myeloma 2) Ondulant (lymfoma 10, acute leucosis 3, myeloma 6). Conclusion: We identified 17 types of febrile tumors, with fever as the first clinical manifestation. Tumor fever debuted in 5 different types; continuous fever dominated with 29.3% of cases. Diagnostic orientation to malignances followed a detailed anamnestic, laboratory and imagery screening which excluded important infectious disease in the first place. At the end of this study we are convinced that the febrile profile is also an important diagnostic tool for a rapid diagnosis of malignancies.

Epidemiological, clinical, laboratory and therapeutic survey on HCV infection in hemodyalisis patients Najada Como, Migena Qato, Esmeralda Meta, Pellumb Pipero, Arjan Harxhi, Elda Zogu, Dhimiter Kraja. Faculty of Medicine, University Hospital Center ""Mother Theresa""

Background: Patients of hemodialysis actually represent a target group closely related to infection by hepatitis C virus. Efforts to manage this group of patients are strongly focused on both; the clinical profiling of infected patients and their adecuate treatment, always aiming at the efficacy and maximum safety for these patients Methods: During a one year period, out of 863 hemodialysis patients, we selected 55 patients who resulted anti-HCV IgM positive, agegroup 20-75 years old who had different underlying chronic diseases. Patients evidenced a creatine clearance of <10 ml/ min, detectable anti-HCV antibody and HCV-RNA in the serum, and elevated serum alanine aminotransferase (ALT). Patients HIV and Hepatitis B coinfections and patients with decompensated hepatic cirrhosis were excluded from the study. Patients underwent monotherapy with pegylated interferon Alfa 2a 135mcg per week for 24 weeks. Results: Epidemiological and clinical findings: 55 patients with mean age 43.5 years, F / M 1.16 ratio, average time in hemodialysis 10.5 years and underlying diseases: chronic renal failure −55 cases, acute pielonephritis −1, renal polycystosis −6, congenital renal atrophy −1, nephrolithiasis-2, transplant −2, nefrectomy-3, arterial hypertension-2, and spondiloarthrosis −1 cases. Laboratory data evidenced elevated serum level of ALT which varied from 98 to 2047UI/L and bilirubine levls up to 2.3 mg/dL. Laboratory data also evidenced anemia in 53 cases, leucopenia 43 cases and thrombocytopenia 46 cases Therapeutic: Peginterferon Alfa 2a monotherapy resulted with sustained virological response in 60% of cases (mean HCVRNA at baseline 3.6 × 10 (5) and in the end of 24 weeks resulted undetectable). Conclusion: HCV infection in hemodyalisis patients remains the major cause of chronic liver and its complication. Each of our patients evidenced a mild clinical form of hepatitisprobably related to immunocompromised status and HD procedure and the adverse effects of peginterferon therapy appeared minimal compared to the efficacy of treatment in our patients. Nevertheless, at the end, the HCV infection negatively impacts survival on dialysis.

Monomicrobial necrotizing fasciitis predicting factors: a prospective case series study in a single center Tsung-Yu Huang 1,2 , Yao-Hung Tsai 3 , Hui-Ju Chuang 3 , Wei-Hsiu Hsu 3 *.

1 Division of Infectious Diseases, Department of ascertain whether the clinical presentation and outcome for patients with this disease differ for those infected with a gramnegative as compared to gram-positive pathogen.

Methods: Seventy patients with monomicrobial necrotizing fasciitis were examined prospectively from April 2015 to May 2017. All patients received prompt radical debridement, adequate broad-spectrum antibiotic therapy, aggressive resuscitation with or without adjuvant hyperbaric oxygen therapy. Thirty-nine patients were infected with a gram-negative pathogen (Group 1) and 31 patients with a gram-positive pathogen (Group 2). Results: Group 1 was characterized by a higher incidence of marine related events, hemorrhagic bullae, tachypnea and septic shock, higher Sequential Organ Failure Assessment (SOFA) Score, a higher rate of bandemia, bacteremia, hypoalbumenia and a lower prevalence of Laboratory Risk Indicator for Necrotizing Fasciitis score (LRINEC score). Higher International Normalize Ratio (INR), lower level of C-reactive protein (CRP) and fibrinogen were more prevalent in Group 1. Shorter time to diagnosis, fewer delay diagnosis for necrotizing fasciitis, a higher rate of acute respiratory failure and higher admission rate of intensive care unit (ICU) admission were more common in Group 1. In a multivariate analysis, marine related events (odds ratio ( Background: Varicella zoster virus (VZV) is widely known as the pathogen that usually causes chickenpox and shingle in primary infected patients and reactivation of endogenous latent virus host respectively. However, VZV-caused pneumonia is quite rare comparing to the skin lesion.

Methods: This is a 42-year-old male without any chronic illness was admitted because of skin eruptions, fever up to 40 degree, coughing with little sputum, general malaise and muscle soreness. Laboratory data revealed leukocytosis with monocyte predominate, percentage of band form neutrophil and atypical lymphocyte elevation, and liver enzyme was two times higher than the upper limit.

Results: On admission, chest X-ray film showed coarse lung markings in the bilateral lungs with diffused reticulonodular density which developed into interstitial pneumonitis. Under the medication of dexamethasone 5 mg Q12H and acyclovir 250 mg Q8H for 5 days, his skin lesion started crusting and his pulmonary symptoms improved. Conclusion: This is our first proven case of VZV pneumonia in this year. Although pulmonary involvement of VZV is uncommon, we should be kept in mind in order to prevent patient from severe VZV pneumonia causing disaster which often require ventilator support.

The Background: Diclofenac, one of non-steroidal anti-inflammatory drugs (NSAIDs), was reported to be associated with the alleviation in the severity of Clostridium difficile infection (CDI), due to its inhibitory effect on phospholipase A2 and cyclooxygenase-2 (Cox-2), which play important roles in disruption of epithelial integrity and gut inflammatory response induced by C. difficile toxin A. The aim of this study is to investigate the impact of NSAID exposure on CDI.

Methods: A published mouse model of CDI, which was induced by prior exposure to antibiotics and proton pump inhibitor, was used.

The mice were fed by diclofenac at the dose of 5 μg/kg or 10 μg/kg or phosphate buffered saline (as the control) by oral gavage prior to CDI. The severity of CDI, including body weight change, stool consistency, cecum and colon length, and bacterial burden of C. difficile in the cecum and stool, were evaluated. The mRNA and protein levels of prostaglandin E2 (PGE2) and Cox-2 were determined in the colonic tissues.

Results: Cox-2 and PGE2 were highly expressed in the colonic tissues during CDI. PGE2 levels were decreased after treated with diclofenac. The severity of colitis induced by C. difficile, including body weight loss, cecum weight loss, and cecum length shortening, was decreased in the diclofenac-treated mice as compared the vehicle-treated control mice. Besides, treatment with diclofenac at either dose can decrease the bacterial burden of C. difficile in the cecum and stool, and also reverse the ratio of Firmicutes to Bacteroidetes in the microbiota of cecum and stool. Conclusion: Treatment of diclofenac attenuates the severity of CDI in a murine model.

Use suppository form of metronidazole to avoid encephalopathy side-effect Huai-En Lu 1 , Li-Hsin Tsao 2 , Yuarn-Jang Lee 3,4 , Shu-Chen Chien 1 . Background/Case Presentation: A 65-year-old man was diagnosed rectal cancer in 2011 and recurrent perianal abscess since 2012 for more than 4 years. All his pus cultures yield Proteus mirabilis, Escherichia coli, Finegoldia magna, Eggerthella lenta, Anaerococcus prevotii, and Bacteroides uniformis. He was followed in infectious outpatient department and taken metronidazole plus ceftibuten for relapse perianal abscess recently since Dec. 2016. However, dizziness and ataxia developed about one month later. Metronidazole-induced encephalopathy was diagnosed by clinical symptoms and image study, and reversibly improved symptoms after metronidazole cessation. But, untolerable side-effect and uncontrolled infection noted when received other antibiotics for anaerobic pathogen covering. Thus, we tried to re-challenge metronidazole to achieve well infection control and to avoid it associated side-effect. Discussion: Our problem was how to avoid metronidazole related encephalopathy when re-challenge metronidazole. Our hospital has different dosage form of metronidazole, including entericcoated tablets, injection, suppository and gel. We thought suppository form (supp.) is a good choice base on pharmacokinetics of metronidazole for perianal infection. The rectal absorption of metronidazole was about one-half or even less of that oral administration. Hepatic first-pass elimination of metronidazole may partially avoid after rectal administration. Therefore, we used metronidazole 250 mg supp. every night for few days then titrated to 500mg supp. every six hours. The discharge got better and no more metronidazole related encephalopathy symptoms observed. Conclusion: Encephalopathy has been reported in patients who received metronidazole or some antibiotics. Different dosage form of drug is another choice to avoid side-effect. And we also need to consider site of infection, severity of infection and bioavailability of drug.

Salmonella osteomyelitis: a single center, retrospective case series with literature review Chin-Kang Wan 1 , Da-Wei Wang 1 , Cheng-Hua Huang 1 *, Wen-Chun Hsu 2 , Ming-Hsueh Tsai 2 , Li-Min Chang 2 . 1 Department of Internal Medicine, Cathay General Hospital Taipei, 2 Clinical Pathology, Cathay General Hospital Taipei Background: Salmonellae cause a broad spectrum of illness including gastroenteritis, typhoid fever, or extraintestinal manifestations through hematogenous metastases. Salmonella osteomyelitis is an uncommon extraintestinal manifestation and especially rare in patients without predisposing factors such as sickle cell disease, systemic lupus erythematosus or immunocompromised. Previous studies reported that Salmonella osteomyelitis accounts for only 0.8% of all Salmonella infection and 0.45% of all type of osteomyelitis. We herein present 5 immunocompetent cases of Salmonella osteomyelitis without sickle cell disease. Methods: 128 patients were diagnosed of Salmonella bacteremia between January 2006 and April 2017 at Cathay General Hospital, and clinical data were retrospectively analyzed. Results: All patients were older than 60 and three of them were female, showing age predilection. Three patients were diagnosed of vertebral osteomyelitis, one patient was diagnosed of tibial osteomyelitis and the remaining one patient was suspected to be sacroiliitis. All patients received protracted antibiotics treatment according to susceptibility and two patients received prolonged antibiotic administration with add-on probenecid for recurrent salmonella osteomyelitis. No patients underwent surgical intervention and all patients survived the disease. Conclusion: Salmonella osteomyelitis is rarely seen in immunocompetent adults. Duration of antibiotics is of paramount in treating these patients to avoid relapses and failure. Add-on therapy with probenecid in relapses of salmonella osteomyelitis has never been reported in preceding studies and its efficacy is encouraging in our study. Further studies are needed to investigate this approach in patients of salmonella osteomyelitis.

Distribution of extended-spectrum β-lactamases, AmpC βlactamases, and carbapenemases among Enterobacteriaceae isolates causing intra-abdominal and urinary tract infections in the Asia-Pacific region from 2008 through 2014: results of the study for monitoring antimicrobial resistance trends (SMART) Shio-Shin Jean 1 , Po-Ren Hsueh 2 . Background: To investigate the antimicrobial resistance and assess the molecular characteristics of β-lactamases (ESBL, AmpC βlactamases, carbapenemases) among Enterobacteriaceae isolates that caused intra-abdominal infections (IAI) in patients hospitalized in the Asia-Pacific region during 2008-14. Methods: Multiplex PCR assay was applied for detecting the specific types of β-lactamases in 2893 isolates with ertapenem MICs >0.5 g/L. In-hospital acquisition times for most isolates were also delineated. Results: Among 2728 isolates proven with β-lactamase production, the rates of non-susceptibility to imipenem were low (average= 7.9%) among IAI Enterobacteriaceae isolates from all Asia-Pacific countries except Vietnam (17.7%) and the Philippines (10.2%). A stepwise and significant increase in annual rates of carbapenemase production among these isolates was noted. CTX-M-15, and CTX-M-14 were the dominant ESBL variants in most IAI Enterobacteriaceae species. The most abundant AmpC β-lactamase variants were bla CMY-2 among isolates of Escherichia coli and bla DHA-1 among isolates of Klebsiella pneumoniae. In addition, the IAI Enterobacteriaceae isolates harboring the bla CMY-2 and bla DHA-1 alleles were associated with high community-acquired rates (38.0% and 42.6%, respectively). AmpC ACT and MIR variants were mostly detected in IAI Enterobacter species. The IAI bla NDM-1, 4, 5, 7harboring isolates of E. coli, K. pneumoniae and Enterobacter cloacae were mostly commonly identified among IAI isolates from Vietnam and the Philippines. Also of note, bla OXA-48 -harboring IAI Enterobacteriaceae isolates were detected in the Vietnam. Conclusion: The high resistance burden in Vietnam and the Philippines warrants aggressive control policies to combat the worsening resistance trend in antimicrobial resistance among Enterobacteriaceae isolates causing IAIs. Background: The Brugada syndrome is a well-known autosomal dominant genetic disorder (sodium channel SCN genes: SCN5A or SCN10A…etc) which predominates in male, and one of the major reasons for sudden cardiac death in middle age adults. Brugada syndrome had been widely discussed but the prevalence has not been well studied, and the risk factors are still in debate. We found a case of influenza A infection induced Brugada syndrome. Methods: Patient was a 64-year-old previously healthy nonsmoker man who had been traveled to Taitung four days prior to this admission. He denied any contact experiences but suffered from dry cough with rhinorrhea and myalgia for three days. He visited Fenglin Veterans Hospital where rapid flu test was positive for influenza A, and the patient received Relenza treatment. Two days before the admission, he had a new onset of intermittent retrosternal pain which was not aggravated by exertion or relieved at rest, and could be radiated to upper back and last 5-10 minutes. There was no fever, no nausea or vomiting during chest tightness attacked. At our Emergency room, he was afebrile with 36 o C, blood pressure was 118/53 mmHg; Heart rates was 63 beats per minute. On physical examination, he had cold sweats, regular heart beats without murmur. Chest x-ray film showed bilateral lower lungs infiltration. Serial ECG showed intermittent AV dissociation, downsloping ST-elevation at V2-4. Results: On admission, troponin I levels were elevated from 0.05 to 0.1ng/ml. He had no obvious chest pain or cardiopulmonary distress. Repeat throat swab for influenza PCR test confirmed influenza A infection. The serial ECG showed Brugada pattern and cardiac echo revealed mild anterior wall hypokinesis. Coronary angiogram revealed non-significant stenosis of left main artery and left anterior descending artery. Aspirin and statin were administered. His general condition was much improved after treatment, and chest x-ray showed decreased bilateral infiltrations. He was discharged with much improvement six days after admission, and followed up at Infectious Disease and Cardiology OPD. Later, the gene laboratory at National Taiwan University confirmed SCN5A gene mutation. Conclusion: Brugada electrocardiographic changes has not yet been associated with Influenza in current medicine, only two case reports were found on the Pubmed. However, this time we had genetic with clinical evidence shows that Infuenza A may had related to induce Brugada syndrome, which gives more information in study groups for Brugada syndrome in Taiwan.

Herpes zoster sacral with dysfunction of the urinary bladder and anorectal muscle Como Najada, Meta Esmeralda, Kraja Dhimiter, Pipero Pellumb, Harxhi Arjan. Infectious Diseases Departament UHC ""Mother Theresa"" Tirane Introduction: Affect of visceral organs during herpes zoster is rare phenomena, althought very important because can occur diagnostic and therapeutic errors, exacerbated the progress of diseases. Case Presantation: A 61 years old men was hospitalized in our clinic as herpes zoster sacral multidermatomes. He presents vesicular eruption in the sacral region including 4 dermatomes, associated with urinary retension and combustion, preceded by pain of lower limbs started a week before. After 5 days abdominal pain occours, pain in the right testicle, fever, the hipersensitivity during palpacion on right part of abdomen; urinary catheter settled as result of urinary retension. In lab exams resulted high level of leukocytes, pyuria, (blood culture sterile, urinary culture P aeruginosa), in abdominal imaging (ultrasound, CT scan) identified fluid in duglas, and hydrocele at the right epidedimis, which resulted an orchioepididimitis. EMG was in its normal physiological parameters. The case is being treated with acyclovir oral and locally, antibiotics, NSAI. Defecation was settled in 6th day of hospitalization;eruption began to disappear by the 13th day, while during persist urinary retention and urinary catheter was removed after 5 weeks. Conclusions: Case was considered Herpes Zoster sacral complicated with urinary bladder dysfunction, the extended urinary retension, urosepsis by P Aeruginosa, anorectal dysfunction with constipation and erectile emphatic.

Erysipelothrix rhusiopathiae bacteremia: an uncommon zoonotic pathogen Chu-chun Lin 1 , Lih-shinn Wang 1,2 , Huei-jen Chao 3 , Si-shiuan Peng 3 , Teng-yi Lin 3 . 1 School of Medicine, Buddhist Tzu Chi University, Hualien, Taiwan, 2 Division of Infectious Diseases, Department of Internal Medicine, Buddhist Hualien Tzu-Chi Hospital, Taiwan, 3 Department of Laboratory Medicine, Buddhist Hualien Tzu-Chi Hospital, Taiwan Background: Erysipelothrix rhusiopathiae usually infects animals but not humans. Patients with history of wild animals contacts should be considered as having an infection caused by an uncommon zoonotic pathogen. Methods: Patient was a 44-year-old man with HBV infection, alcoholic liver cirrhosis, gastric and esophageal varices, and hypertension. He just had an admission for cellulitis and blood culture grew Streptococcus equinus, which is always abundant in the feces of horses, but uncommon in humans. He was sent to Yu-Li Tzu Chi hospital for altered consciousness with bloody vomiting for a few hours. At ER, he had a mild fever of 38.5°C, with blood pressure of 169/72 mmHg, and pale conjunctivae and icteric sclerae were noticed. Then he was transferred to Hualien Tzu Chi Hospital.

Lab data showed normocytic anemia with hemoglobin of 7.3 g/dL, thrombocytopenia with platelet count of 38,000/uL, and a white blood cell count of normal range but with bandemia (7%). Elevated NH3, AST, TBI, DBI, PT and aPTT, and decreased albumin were noted. He was diagnosed of upper gastrointestinal (GI) bleeding and a systemic infection with unknown source, and was given PRBC and platelet transfusion, pantoprazole, tranexamic acid, and terlipressin. Flomoxef was initiated as an empirical antibiotic treatment. Results: Panendoscopy revealed gastric varices and small erosive shallow bleeding ulcers at antrum and body, and he had histoacryl injection and Argon plasma coagulation. Because of previous S. equinus bacteremia, the patient might have a unique contact history. On the second hospital day, blood culture grew an uncommon gram-positive bacillus, with borderline gram-negative staining ( Figure A) . Clear rod-shaped bacteria were seen on blood smear ( Figure B ). The colonies grew selectively on blood agar plate ( Figure C) , and the H 2 S precipitation was seen on triple sugar iron media ( Figure D) . The isolate was identified as Erysipelothrix rhusiopathiae. The patient indeed had a recent exposure to wild boars, deer and Formosa muntjac. Flomoxef was switched to ceftriaxone treatment. Echocardiography did not reveal vegetation, and Gallium-67 whole body inflammation scintigraphy did not demonstrate any source of inflammation. The patient was afebrile after admission. All laboratory data were improving during hospital stay. Then he was discharged after 15 days of ceftriaxone treatment. Conclusion: E. rhusiopathiae occurs most commonly as an occupational disease. Penicillins and cephalosporins are the drugs of choice for treatment.

A fatal case of primary sternal osteomyelitis presenting as anterior wall abscess Chen Peichen 1 , Wang Lishin 1,2 , Chang Paoyuan 1,3 . 1 School of Medicine, Tzu Chi University, Hualien, Taiwan, 2 Division of Infectious Diseases, Department of Internal Medicine, 3 Department of Radiology, Buddhist Tzu Chi General Hospital, Hualien, Taiwan

Background: Primary sternal osteomyelitis (PSO) is rare, yet potentially carries a devastating outcome. It is generally caused by Staphylococcus aureus, but can be caused by S. epidermidis, gramnegative bacilli, and Mycobacterium tuberculosis. Clinicians need to consider PSO when a patient presents with anterior chest pain, swelling, and bacteremia. We report the case of a 41 years old female presented with PSO without previous chest surgery or trauma.

The patient was a 41-year-old woman with a medical history of diabetes mellitus and liver cirrhosis, Child C. She presented a painful, anterior chest wall swelling for one month. The patient denied any cardiac surgery, trauma or acupuncture history. She began with a fever, and had progressive dyspnea 2 days prior. The patient was sent to the emergency room of another hospital, and had a debridement of the chest wall abscess than admitted to the intensive care unit. Pus and blood cultures grew Klebsiella pneumoniae, and cefoperazone/sulbactam was administered. Liver abscess was considered as a possible source, but not confirmed. She was then transferred to our hospital. Results: On physical examination, there was a 7 cm long, 4 cm wide, and 3 cm deep open, clean surgical wound with some cauterized granulation tissue. White blood count was 11,400/uL. Wound culture grew oxacillin-resistant S. aureus (ORSA), and sputum culture grew Pseudomonas aeruginosa. Cefoperazone/ sulbactam was switched to pipercillin/tazobactam and tigecycline. The patient had a second debridement on hospital day 14. Whole body inflammation scan and chest computed tomography ( Figure. ) showed sternal osteomyelitis, mediastinitis and pericarditis. Tigecycline was switched to daptomycin for coverage of ORSA. She had sternectomy on day 47, and wound culture grew K. pneumoniae. Pipercillin/tazobactam was switched to cefmetazole. However, the infection wasn't well controlled. She had an altered consciousness and died on hospital day 90. Conclusion: Due to rare clinical experiences, we have no standard approach for PSO. However, the presentation of PSO is insidious, and a diagnosis can often be delayed. PSO should be borne in mind when a patient presents with an anterior wall abscess.

Streptococcal toxic shock syndrome presenting as purpura fulminans Yuh-shyan Wu 1 , Lih-shinn Wang 1,2 , Ya-ru Wu 1,3 , Si-shiuan Peng 4 . Background: Purpura fulminans is a rare syndrome of intravascular thrombosis and hemorrhagic infarction accompanied by vascular collapse and disseminated intravascular coagulation (DIC). It is typically associated with meningococcal infection and is rapidly progressive. Our patient is a case of streptococcal toxic shock syndrome presented as symmetric peripheral gangrene and necrosis of large areas of the skin of limbs. Methods: Patient was a 59-year-old obese woman. She came to the hospital with yellowish productive cough for one week, and had fevers for 2 days. Septic shock developed during hospital stay, and she was transferred to intensive care unit for resuscitation. She had progressive swollen face, eyelid and neck. Head and neck, chest and abdomen CT disclosed extensive cellulitis of the neck. Results: Laboratory data at admission revealed leukocytosis with left shift, acute kidney injury, and lactate acidosis. Blood cultures showed group A beta-hemolytic Streptococcus. Examination of pleural fluid disclosed gram-positive cocci. Cultures of sputum, pleural fluid and urine showed negative findings. The patient was treated with vancomycin, meropenem and clindamycin. 5 days after admission, she had a fever of 40°C, and blood culture grew Candida albicans. Sputum culture also grew Candida albicans and Candida tropicalis. Catheter and urine cultures were sterile. Antifungal agent was administered. She had gangrene of bilateral upper and lower limbs, and right upper chest on hospital day 14. The manifestations were consistent with purpura fulminans and DIC, and she was treated with platelet transfusion and anticoagulant. 23 days after admission, she had bilateral below knee amputation. 13 days after admission, DIC developed and laboratory data revealed prolonged aPTT (>180 sec), low platelet (91 × 10 3 /uL), and elevated level of D dimer (8261.93 ng/mL). Amputation of the hands was also suggested, but the family refused further management. Then the patient had hospice and was discharged due to terminal condition. Conclusion: Purpura fulminans is rare and most commonly occurs in babies and small children, but can be more rare in adults. In severe sepsis, purpura fulminans typically develops in the distal extremities and progresses proximally affecting the whole body surface. 40-70% of adult patients with sepsis-associated purpura fulminans die. histoplasmosis. It is usually self-limiting or localized in immunocompetent individuals. Disseminated disease usually occurs in immunocompromised patients or in patients with chronic illness. We describes a Taiwanese who have human immunodeficiency viral infection and histoplasmosis. Methods: A 23-year-old man, denied major systemic disorder or foreign country travel before admission, presented to emergency room with 2-week fever. Cough, abdominal fullness and body weight loss were associated. Thrombocytopenia (Platelet 19,000/ ul), cavity in left upper lung zone and hepatosplenomegaly were noted initially. After admission, acute human immunodeficiency viral infection was diagnosed. The laboratory data also revealed hypertriglyceridemia and high ferritin. He received antiretroviral therapy immediately due to suspected virus-associated hemophagocytic syndrome. The bone marrow studies were also arranged. Hypotension, metabolic acidosis with respiratory failure still developed on the fifth day of hospitalization. Results: The bone marrow studies revealed abundant yeast-like microorganism in histiocytes and hemophagocytosis. The serum Galactomannan test was positive result also. Liposomal Amphotericin B was administered for invasive fungal infection. Three weeks later, the blood fungal culture which collected when admission and the bone marrow fungal culture all yield Histoplasma spp. The Liposomal amphotericin B was switched to Voriconazole orally. He was extubation successfully and discharge on the 42th day of hospitalization Conclusion: Histoplasmosis is a rare disease in Taiwan, and few microbiologically documented cases have been previously reported. It is commonly diagnosed in areas of endemicity but frequently overlooked in areas of nonendemicity due to a low index of suspicion among physicians. In HIV-infected patient who present with thrombocytopenia, hemophagocytotic syndrome and cavitary pulmonary lesion, a differential diagnosis of histoplasmosis should be kept in mind.

Imaging in sepsis-associated encephalopathy Masaki Fujioka 1 *, Akihiro Shindo 2 , Kei Suzuki 1 . 1 Mie University Hospital, the Emergency and Critical Care Center, 2 Mie University Graduate School of Medicine, Department of Neurology Background: Sepsis-associated encephalopathy (SAE) refers to acute neurological dysfunction that arises in the context of extracranial sepsis. Despite use of variety of diagnostic techniques, however, SAE remains a clinical diagnosis and could represent a variety of pathophysiological processes that are both related and unrelated to coexistent sepsis. Herein we describe impressive imaging in patient with SAE. Methods: We experienced a 66-year-old man with prolonged fever and diarrhea progressed into a coma, who brought to our hospital. Laboratory tests revealed multiple organ failure, and a computed tomography scan showed severe intestinal inflammation. Initial resuscitation was initiated for septic shock due to severe enteritis. His general condition gradually improved, without consciousness restoration. On day 15, his Glasgow Coma Scale (GCS) score was 3, and MRI was continuousuly performed. Results: MRI showed enhanced fluid-attenuated inversion recovery (FLAIR) signal in the white matter. Susceptibility-weighted imaging revealed multiple microbleeds. These findings indicated SAE, which is associated with extracranial sepsis. The pathophysiology is thought to involve blood-brain-barrier leakage. His GCS score improved to 7, and FLAIR signal enhancement in the white matter decreased on day 36. Furthermore, MRI showed decreased enhancement on day 60, without further improvement of consciousness.

The diffuse and severe white matter abnormalities like those we observed in our patient are rare and have been associated with poor outcome. This case shows that severe encephalopathy with extensive white matter lesions might be a reversible condition. Clinicians should realize that this condition and give cautious eyes to the first signs of improvement of the clinical condition. We deem it necessary to do follow-up and repeated MRI study.

A scheme to test Clostridium difficile and rapid to improve performance of detection for C. difficile-associated diarrhea (CDAD) Jiann-Yuan Chen, I-Jen Wang, Ling-Na Shih, Jiin-Chyr Hsu. Department of Laboratory Medicine, Taipei Hospital, Ministry of Health and Welfare, Taiwan

Background: Clostridium difficile (CD) is a common cause of hospital-acquired diarrhea, which is usually associated with previous antibiotic use. C. difficile-associated diarrhea (CDAD) is most strongly associated with fluoroquinolones, cephalosporins, carbapenems, and clindamycin. most common cause of pseudomembranous colitis (PMC). Clostridium difficile should be considered in diarrhea cases with a history of antibiotic use within the last 8 weeks (community-associated CDI) or with a hospital stay of at least 3 days, regardless of the duration of antibiotic use (hospitalacquired CDI). Methods: 43 stools were collected from the suspected CD infection cases (all > 25 years of age) admitted in two regional hospitals in north Taiwan. The samples during the period of November 2016 to May 2017. The effectiveness analysis of glutamate dehydrogenase antigen (GDH)and toxin A/B combined assay (brand A: RIDA ® QUICK) versus a two-step algorithm for optimal detection of toxigenic CD. Each two-step algorithm, stools were using the RIDA ® QUICK GDH for screening, if positive, tested for toxin by RIDA ® QUICK Toxin A/B and tested for Toxin A &Toxin B single each other (brand B:Mascia Brunelli) to double check, then were underwent anaerobic stool culture on Cycloserine Cefoxitin Fructose Agar (CCFA). Results: Over 7 months, we can calculate the laboratories' expenses were 37% less than if combined assay alone. The brand A&B testing kits provide a rapid turnaround time than anaerobic stool culture (1 hour vs 72 hours). A total of 43 suspected CDI patients, 34.9% (15/43) were GDH positive and Toxin A/B positive was 9.3% (4/43). Distribution of sex and age with CD colonization (GDH + ) were 6 males and 4 females in age group ≧65 years (66.7%, 10/15). Two (13.3%, 2/15) were GDH-positive and toxin A/B positive by both tests. The risk of toxigenic CD (GDH + , Toxin A/B + ) infection between male and female was not significant (2.3% vs 2.3%). Conclusion: Our study suggested one or two-step algorithm for the detection of CD infection is a reliable, cost-effective, and timesaving strategy. The prevalence of toxicgenic CD infection based on a two-step algorithm is similar to that reported in previous literature in Taiwan. When GDH antigen positive had 100% match compared to the culture on CCFA agar was indentified Clostridium difficile. Also proved that C. difficile strains appear to produce the cell wall-associated enzyme glutamate dehydrogenase antigen (GDH).

Clinical characteristics of bacteraemia caused by Burkholderia cepacia complex species and antimicrobial susceptibility of the isolates in a medical centre in Taiwan Ying-Chun Chien 1 *, Chun-Hsing Liao 2 , Wang-Huei Sheng 1 , Jung-Yien Chien 1 , Yu-Tsung Huang 2,3 , Chong-Jen Yu 1 , Po-Ren Hsueh 1, 3 Background: Data about bacteraemia caused Burkholderia cepacia complex (BCC) among patients without cystic fibrosis are limited. This study is intended to investigate the clinical characteristics and outcomes of patients with bacteraemia due to different species of BCC. Methods: Patients with BCC bacteraemia who were treated at a medical centre from January 2013 to February 2015 were evaluated. BCC isolates were identified to species level by Bruker Biotyper MALDI-TOF MS and sequencing analysis by 16S rRNA and recA genes. Antimicrobial susceptibilities of the isolates were determined by the agar dilution method. Results: Sequencing of the recA gene in the 54 blood isolates revealed 37 (68.5%) isolates of B. cenocepacia, 9 (16.7%) B. cepacia, 4 (7.4%) isolates of B. multivorans and one isolate each of B. arboris, B. pseudomultivorans, B. seminalis, and B. vietnamiensis. The overall performance of the Bruker Biotyper MALDI-TOF MS system for correctly identifying the 54 BCC isolates to the species level was 79.6% that was better than that (16.7%) by 16S RNA sequencing analysis. Bacteraemic pneumonia (n = 23, 42.6%) and catheterrelated bacteraemia (n = 21, 38.9%) were the most common types of infection. Higher rates of ceftazidime and meropenem resistance were found in B. cepacia isolates (33.3% and 22.2%, respectively) than in isolates of B. cenocepacia (21.6% and 10.8%, respectively) and other species (12.5% and 12.5%, respectively). Overall, the 30-day mortality rate was 38.9% (21/54). Bacteraemia caused by BCC species other than B. cenocepacia and B. cepacia (adjusted odds ratio [aOR] 20.005, P = 0.024) and high SOFA score (aOR 1.412, P = 0.003) were predictive of higher 30-day mortality. Conclusion: Correct identification of BCC complex to species level is important because different BCC complex species was associated with different outcomes of bacteraemia and exhibited different susceptibility patterns.

Evaluation of TPLA as an initial syphilis-screening test Ryosei Murai 1 , Koji Yamada 1 , Nozomi Yanagihara 1,2 , Satoshi Takahashi 1,2 . 1 Division of Laboratory Medicine, Sapporo Medical University Hospital, 2 Department of Infection Control and Laboratory Medicine, Sapporo Medical University School of Medicine Background: Syphilis serological tests are divided into nontreponemal and treponemal tests, and neither is sufficient alone for diagnosis. Traditionally, initial syphilis screening begins with the nontreponemal test (traditional algorithm); however, recent advancements in instrumentation have enabled more automated and objective approaches to the treponemal test (reverse algorithm). The reverse algorithm for initial syphilis screening was limited to enzyme immunoassay (EIA) or chemiluminescence immunoassay (CIA). Recently, a TPLA method that uses Treponema pallidum components coated on latex particles using a chemical wide use automatic analyzer was developed. We evaluated the use of this reagent, instead of EIA or CIA, for initial syphilis screening in the reverse algorithm.

Methods: A total of 90 stored samples were reacted with Mediace ® TPLA (Sekisui Medical Co, Tokyo, Japan) or Mediace ® RPR (Sekisui Medical Co, Tokyo, Japan) and tested. We evaluated four different automated syphilis serological tests, including Architect Syphilis TP ® (Abbott Diagnostics, Tokyo, Japan), Lumipulse ® presto TP (Fujirebio Inc, Tokyo, Japan), Serodia ® -TP·PA (Fujirebio INC, Tokyo, Japan), Esplain ® TP (Fujirebio Inc, Tokyo, Japan). Samples were stored at −40°C and thawed immediately prior to analysis. We confirmed the results with FTA-ABS and TPPA. Results: In the analysis, 46 of 90 samples tested positive with Mediace ® TPLA, and the false positive rate was 0%. However, the false-negative rate of Mediace ® TPLA was 2.27% (1 of 44), whereas that of Lumipulse ® presto TP was 0% and that of Architect Syphilis TP ® was 2.27%. Conclusion: The performance of Mediace ® TPLA is nearly equivalent to that of Lumipulse ® presto and Architect Syphilis TP ® . The cost per test for Mediace ® TPLA is lower than that for other tests. Because Mediace ® TPLA has various advantages such as low cost, high IgM-detection power, and easy operability, it should be recommended for the initial screening test.

Using a new method to culture the C. difficile from environment Chih-Chen Lin 1,2 , Chang-Pan Liu 1,2 , Fu-Chieh Chang 1 . 1 Infection Control Center, MacKay memorial Hospital, 2 Department of Infection, MacKay memorial Hospital Background: C. difficile is a gram-positive bacillus that belong anaerobic bacteria which contain spore. In previous studies, this bacterium can survive in hard situation for 3 months or longer. In clinical, if we don't disinfect complete, it will provide a chance to spread it. So, if we had a efficient method that can culture this bacterium from environment, it would be useful for infection control. Methods: In this study, we used Banana broth as a selective broth. We collected the sample from the environment that CDI patient used before. We use the swab and 0.9% normal saline to touch the surface then put the swab into the broth. Then we culture it on 35°C without anaerobic incubacted. After 48 hours, we checked the color of the broth, if C. difficile growth on this broth, the color will become to yellow. Results: In this study, we collected 12 sample from the environment. And there were 2 broths changed to yellow. After PCR checked, all of the bacterium were C. difficile. That's to say, this broth can be a selective broth for C. difficile. Conclusion: In this study, we found the Banana broth was a great selective broth to culture C. difficile without anaerobic incubated. Otherwise, through the change of color, its very easy for staff to check the result and don't need to culture by using CCFA agar.

Burkholderia pseudomallei causing urinary tract infection and fever: a case report Yin-Tai Tsai 1 , Tzu-Ying Lee 1 , Ruo-Tzu Li 1 , Chuan-Ru Wang 1 , Chia-Hsin Hsieh 1 , Jui-Hua Hsu 1 , Meng-Ting Hsieh 1 , Wen-Shyang Hsieh 1,2,3 , Hsaio-Wei Wang 4,5 , Yung-Ching Liu 4,5 . We present a case of urinary tract infection and fever caused by Burkholderia pseudomallei who was admitted at a Northern Reginal hospital in Taiwan. The case is presented to highlight the identification of this rare pathogenic organism in the laboratory from the samples collected and to start the treatment early for better prognosis since the mortality is high without treatment. Methods: 46-year-old male presented to Emergency Department with symptoms of breathe heavily and hematuria and fever. On examination patient suspected urinary tract infection. Samples of urine, blood and pus collected using sterile swabs from the discharge of cellulitis was sent to microbiology laboratory for culture and antimicrobial susceptibility testing. Microbiological culture and identification revealed the etiologic agent to be Burkholderia pseudomallei. The organism was susceptible to ceftazidime (30 μg), trimethoprim/sulfamethoxazole (1.25/ 23.75 μg). The strain was resistant to amikacillin (30 μg), and colistin (10 μg). His fasting blood glucose was 233 mg/dl. Results: Patient was started with Ciprofloxacin, Ceftazidime, or Doripenem/Imipenem/Meropenem at least 14 days. He was put on antidiabetic therapy. Second sample of blood collected after five days was negative for culture. Conclusion: Urinary tract infection is common in diabetics, and in a diabetic patient B. pseudomallei, a rare pathogen can cause infection. This organism can be overlooked in routine cultures as contaminant especially if the bacterial growth on the culture plate is polymicrobial. Identification requires a great deal of clinical suspicion as well as alertness on the part of laboratory personnel. Melioidosis caused by should be considered in the differential diagnosis in diabetic men with urinary tract infection, especially if they come from areas where melioidosis is prevalent. Appropriate treatment must be initiated to prevent complications.

Diagnostic accuracy of Interferon-γ Release Assay for Latent Tuberculosis Infection in children above the age of 5 years Fang-Ching Liu 1 , Yuan-Hsin Chu 2 . 1 Division of Infectious Disease, Jen-Ai Hospital, 2 Department of Infection Control, Jen-Ai Hospital Background: Taiwan CDC promoted the TB preventive program which introducing international recommended new LTBI diagnostic tests such as IGRAs (Interferon-γ release assays) and treatment prescription for TB contacts of all age groups from March 1, 2016. IGRAs have been widely used for the diagnosis of latent and active tuberculosis in adults, but their role in diagnosing LTBI in children younger than 5 years remains debating. Here we perform an EBM approach to evaluate diagnostic accuracy for LTBI in children above the age of 5 years. Methods: A PICO was formulated, P: children above 5 years old; I: (IGRAs OR interferon-gamma OR Interferon-γ); O: diagnostic accuracy for LTBI. These terms and synonyms were engaged with Boolean character (AND, OR) into a keyword to search Cochrane Library and PubMed databases. Clinical diagnostic questions, according to the Oxford Level of Evidence 2011, systematic review (SR) of cross-sectional studies with reference standard would be the best evidence. In PubMed a relevant SR was retrieved out of 87 searching results for critical appraisal. Conclusion: Though Taiwan CDC still use TST as diagnostic tool for LTBI in children younger than 5 years. Probably due to blood draws can be more difficult in young kids, or other considerations such as cost-effectiveness. However, this Level 1 evidence showed that the diagnostic performance of IGRAs were slightly higher than TST and can be used as supporting assays to detect LTBI in children above the age of 5 years.

PS 114 10 cases of hematologic malignancy that caused gastrointestinal perforation Kazuhiko Natori, Daisuke Nagase, Akiko Shibuya, Yurika Mitsui, Yasunobu Kuraishi, Haruka Fujino. Division of Hematology and Oncology, Toho University Medical Center Oomori Hospital Background: It is common that malignant lymphoma infiltrates the digestive tract. It may also cause gastrointestinal perforation after starting chemotherapy, resulting from its good response to the therapy. The occurrence of gastrointestinal perforation during chemotherapy often leads to a fatal outcome. We report our experience with 10 patients who developed gastrointestinal perforation while being observed for malignant lymphoma. Methods: Among patients with histopathologically diagnosed hematologic malignancy between 1988 and 2015, 10 patients with gastrointestinal perforation were selected. The histopathological diagnosis was consistent with the WHO Classification of Tumours of Haematopoietic and Lymphoid Tissues, fourth edition (2008). Results: The ratio of men to women was 8:2 and the median age was 64 years (range: 40-77 years). The histopathologic type was non-Hodgkin's lymphoma in all patients, with a subtype of T-cell leukemia/lymphoma (ATLL) in 1 patient, follicular lymphoma in 1 patient, diffuse large B-cell lymphoma in 6 patients, diffuse large Tcell lymphoma in 1 patient, multiple myeoma 1 case. About malignant lymphoma, according to the Ann Arbor staging system, the clinical stage was IE in 2 patient, IIIE1 in 1 patient, IV in 5 patients. Perforation occurred prior to chemotherapy in 2 patients, after completion of the first cycle of chemotherapy in 6 patients, after 4 cycles in 1 patient, after 6 cycles in 1 patient, during the advanced stage of chemotherapy-refractory lymphoma in 1 patient, and during recurrence after complete response in 1 patient. Conclusion: According to our experience, it was suggested that curative resection is not mandatory in patients undergoing surgery for gastrointestinal perforation and it is important to start chemotherapy in the early postoperative period and select a safer surgical procedure, such as limited surgery, flexibly assessing the necessity of removing the lesions. Background: Serum procalcitonin (PCT) has been touted as a diagnostic marker to predict severe bacterial infection these days. This study was performed to evaluate the usefulness of PCT in diagnosis of various types of bacterial infection in children, by comparing with other inflammatory markers. Methods: We retrospectively reviewed medical records of 447 patients under the age of 18 years who were admitted to Chung Ang University Hospital between 2011 July and 2014 June, for whom PCT value had been obtained. Patients were categorized into bacterial infection group and non-bacterial infection group. Furthermore, bacterial infection group were categorized into invasive bacterial infection (IBI), mucosal bacterial infection (MBI), toxigenic bacterial infection (TBI), and mycoplasma infection (MI) groups. IBI was defined as the infection of sterile body site such as bacteremia, meningitis, osteomyelitis, and pyelonephritis. Staphylococcal scalded skin syndrome and scarlet fever were included into TBI. MI was defined as positive mycoplasma IgM (≥770 IU/mL) or mycoplasma PCR. Results: Of 447 patients, 104 (23.3%) patients were diagnosed with bacterial infection. Among bacterial infection group, 73 cases (70.2%) were identified to IBI, 17 cases (16.4%) were MBI, 7 cases (6.7%) were TBI and 9 cases (8.4%) were MI group. The values of PCT (9.53 ± 32.10 vs. 0.60 ± 2.13), C-reactive protein (CRP; 57.86 ± 55.63 vs. 33.66 ± 51.55), and white blood cell (WBC; 12388.29 ± 6805.38 vs. 10439.30 ± 7437.23) count were higher in bacterial infection group than non-bacterial infection group (P < 0.05 for all). However, only the value of PCT was higher in IBI group than other bacterial groups (13.11 ± 38.01 vs. 1.26 ± 1.65 in MBI, 1.10 ± 1.87 in TBI, and 1.69 ± 2.06 in MI, P < 0.05 for all). In the Receiver Operating Characteristic (ROC) analysis, the area under the curve was 0.706 for PCT, 0.691 for CRP, 0.603 for WBC and 0.501 for erythrocyte sedimentation rate (ESR). The specificity for serum PCT (80%) in diagnosing invasive bacterial infection was significantly larger than those of other inflammatory markers (0.35 for CRP, 0.54 for WBC, 0.22 for ESR). Conclusion: PCT is a highly effective diagnostic marker of IBI but not effective to differentiate MBI, TBI, or MI from non-bacterial infection. Also, PCT is more useful marker to predict bacterial infection than CRP, WBC, ESR.

Investigate the incidence and antimicrobial susceptibility test of Elizabethkingia meningoseptica in a teaching hospital Yao-Shen Tung 1,2,3 , An-Chi Chen 1,2 , Shen-Min Huang 1,2 , Yui-Yein Yang 1,2,3 , Li-Hung Wu 1,3 . 1 Show Chwan Memorial Hospital, 2 Departments of Medical Laboratory, 3 

Background: The increasing emergence of antibiotic-resistant nosocomial infections is a worldwide concern. E. meningoseptica infection inhumans have been presented as meningitis outbreaks in premature newborns and infants. It has a unique antibiotic susceptibility pattern. It is resistant to many antibiotics commonly used to treat infections caused by Gram-negative bacteria. This is a challenge for clinicians to treat patients. Methods: In this study, 100 E. meningoseptica clinical isolates were collected from teaching Hospital from 2012 to 2016. The phoenix 100 Automated Microbiology System (Becton Dickinson) with the GN panel was used to identify bacteria and confirm antimicrobial susceptibility. The antimicrobial susceptibility against 14 kinds of antibiotics were tested. Results: The incidence of E. meningoseptica isolates increased from 7 in 2012 to 41 in 2016. Among them, 52% from sputum, 33% from blood, and more than 50% samples isolated form intensive care units (ICUs). The in vitro antimicrobial susceptibility test results are presented in Table 2 . E. meningoseptica was most often susceptible to trimethoprim-sulfamethoxzole (97.3%) and fluoroquinolones, including levofloxacin (23.7%) and ciprofloxacin (28.5%). However, most antibiotics that target Gram-negative bacteria were not very effective against. Conclusion: E. meningoseptica had a high resistance to various antimicrobial agents. Pneumonia was the most common infection caused by E. meningoseptica, and the mortality rate was quite high. However, despite the high mortality rate, the rate of appropriate antibiotic use was notably low. Clinicians should pay attention to the disease progression and evaluate the response to current antibiotics, and if possible, consider using effective antibiotics based on culture reports.

Co-resistance to antimicrobial agents of extended-spectrum β-lactamases producing E. coli in a regional teaching hospital in southern Taiwan: implication on clinical treatment Yi-Ping Chen 1 , Po-Liang Lu 2 , Chun-Chi Huang 1 , Jeng-Yih Wu 1 , Chen-Min Kuo 1 , Tung-Nan Liao 3 Background: Extended-spectrum β-lactamases producing E. coli (ESBL-E. coli) is an important nosocomial pathogen in Taiwan. It has been reported that ESBL-E. coli isolates were resistant to multiple antimicrobial agents. The related data is less known in regional hospitals in Taiwan. Methods: We retrospectively analyze the antimicrobial resistance profile of E. coli collected from Sep. 2015 to Aug. 2016 in a regional hospital in southern Taiwan. Only the first isolates recovered from one person is included for further analysis. The ESBL phenotype and the drug susceptibility testing were performed with Vitek 2 compact system. Odds ratio was estimated with SAS JMP software 12th version. Results: Among the 2113 E. coli isolates, 286 (13.52%) isolates produced ESBL. The susceptibility rates to antimicrobial agents of ESBL-E. coli were ampicillin: 0%, amikacin: 99.30%, ceftazidime: 0%, cefazolin: 0%, cefepime: 0% 2.47 ). Co-resistance to ESBL phenotype is observed for ampicillin, ceftazidime, cefazolin, cefepime, gemtamicin, sulfamethoxazole, levofloxacin, and ampicillin/sulbactam. Conclusion: The epidemiology data in a regional hospital in Taiwan revealed ESBL-E. coli were most susceptible to amikacin, ertapenem, tigecycline, colistin and piperacillin/tazobactam. Besides the co-resistance of ESBL phenotype to all tested beta-lactams, Coresistance was also observed for gemtamicin, sulfamethoxazole, ampicillin/sulbactam, and levofloxacin. The extent of co-resistance may impact the choice of empirical antimicrobial therapy.

Acinetobacter calcoaceticus-A. baumannii complex, Taiwan Tsai-Ling Yang Lauderdale*, Hui-Ying Wang, Yih-Ru Shiau, Jui-Fen Lai, I-Wen Huanag, TSAR Hospitals. National Institute of Infectious Diseases and Vaccinology, National Health Research Institutes Background: A. calcoaceticus-A baumannii (ACB) complex comprises four genospecies: A. baumannii, A. calcoaceticus, A. nosocomialis, and A. pittii. Considerable differences exist in their epidemiology and clinical relevance but routine differentiation of these genospecies is not commonly done. The present study investigated the distribution and antimicrobial susceptibility of the ACB complex genospecies in the Taiwan Surveillance of Antimicrobial Resistance (TSAR) program. Methods: Acinetobacter isolates were collected biennially as part of the TSAR program between 2010 and 2016 from 26 to 28 hospitals using the same collection protocols. Susceptibility to different antimicrobial agents was determined by broth microdilution in every study year but colistin was not tested in 2010. All ACB complex isolates were subjected to gyrB genospecies typing to differentiate the 4 species. Data from 2010 to 2012 were grouped together to compare with combined data of 2014 and 2016. Results: A total of 1321 ACB complex isolates were identified, including 337, 405, 290, and 289 isolates from 2010 337, 405, 290, and 289 isolates from , 2012 337, 405, 290, and 289 isolates from , 2014 337, 405, 290, and 289 isolates from , and 2016 337, 405, 290, and 289 isolates from , respectively. From 2010 337, 405, 290, and 289 isolates from -2012 337, 405, 290, and 289 isolates from to 2014 337, 405, 290, and 289 isolates from -2016 , the proportion of A. baumannii decreased from 85.7 to 79.4% while that of A. nosocomialis increased from 4.4 to 11.2% ( p < 0.001 for both), with A. pitti remaining stable at around 9%. The specimen distribution differed among A. baumannii, A. nosocomialis, and A. pitti isolates, with 55.1, 51.0, and 34.5%, respectively, from respiratory tract, and 13.9, 43.9, and 35.3%, respectively, from blood. Carbapenem nonsusceptibility in ACB complex was 64.6% in 2010-2012 and 59.9% in 2014-2016 overall ( p = NS), but was significantly higher in A. baumannii (71.1%) than in A. nosocomialis (23.5%) and A. pitti (15.1%). Colistin resistance was not detected in 2012 and was 3.1% in 2014-2016 but no mcr-1 was detected. Conclusion: The prevalence of carbapenem resistance in A. baumannii remains very high in Taiwan. The changes in ACB complex genospecies distribution in recent years plus differences in their antimicrobial susceptibility and specimen distribution indicate the need for continued surveillance of this important group of pathogens.

Study of integrons in multidrug-resistant Escherichia coli isolates Chao-Tsai Liao*, Ya-Chi Shin, Tien-Yu Yeh. Department of Medical Laboratory Science and Biotechnology, Central Taiwan University of Science and Technology Background: Integrons are genetic elements capable of integrating or mobilizing gene cassettes encoding antibiotic resistance determinants. Escherichia coli is often resistant to multiple antimicrobial agents and is often associated with the production of extended-spectrum β-lactamases (ESBLs), which are frequently described as a part of complex integrons, facilitate their horizontal transfer to other related and unrelated microbes. The aim of this study was to define the current prevalence and phenotypes of 100 multidrug-resistant (MDR) E. coli isolates from clinical specimens in central Taiwan and to investigate associations between multidrug resistance and existence of integrons. Methods: Non-duplicate 100 MDR E. coli isolates from January to July 2013 were used in this study. Firstly, susceptibility of 100 isolates to 12 antibiotics was determined by the Vitek 2 system. The extended-spectrum β-lactamases production was detected by disk diffusion confirmatory tests. Then, prevalence and class of integrons was detected in MDR strains by polymerase chain reaction-restriction fragment length polymorphism (PCR-RFLP). Finally, the gene cassette arrays of class 2 integrons were identified by PCR, DNA sequencing and restriction enzyme digestion. Results: The extended-spectrum β-lactamases production was detected in 82% (82/100) of E. coli isolates by disk diffusion confirmatory tests. Antibiotic resistance patterns were as follows: ampicillin 100%, ampicillin/sulbactam 89%, amikacin 96%, ceftazidime 100%, ciprofloxacin 82%, ceftriaxone 100%, cefazolin 100%, imipenem 0%, gentamicin 87%, cefoxitin 38%, sulfamethoxazole/ trimethoprim 87%, and piperacillin/tazobactam 5%. PCR-RFLP results showed that the prevalence of integrons was confirmed in 20% of MDR isolates. Class 1 and 2 integrons were detected respectively in 5% and 20% of MDR strains. Moreover, 5 strains were found to contain both class 1 and 2 integrons. No integron class 3 was detected in any of the isolates. All of these integron 2-positive isolates contained constant gene cassette arrays of dfrA1 + sat2 + aadA1 which confer resistance to trimethoprim and streptomycin. Conclusion: In this study, MDR E. coli isolates with resistance to four or more different antibiotics were common. The data suggest that multiple and complex mechanisms involving mobile genetic elements in class 1 and class 2 integrons and antibiotic resistance have been developed in the evolution of E. coli strains in Taiwan.

Clinical characteristics of patients with bloodstream infection caused by metallo-β-lactamase-producing Enterobacteriaceae in a tertiary hospital in Japan Nobuaki Mori 1 , Yasuko Aoki 1 , Akiko Higuchi 2 , Shoko Hirose 3 , Shiko Seki 4 . 1 Department of General Internal medicine, National Hospital Organization Tokyo Medical Center, 2 Departmet of Clinical Laboratory, National Hospital Organization Tokyo Medical Center, 3 Department of Pharmacology, National Hospital Organization Tokyo Medical Center, 4 Department of Surgery, National Hospital Organization Tokyo Medical Center Background: Infections caused by carbapenemase-producing Enterobacteriaceae have been reported worldwide and have become a great threat to public health. However, clinical characteristics of patients with bloodstream infections (BSI) due to metallo β-lactamase (MBL)-producing Enterobacteriaceae are not well studied. Therefore, we investigated the clinical characteristics of patients infected with the MBL-producing Enterobacteriaceae. Methods: We retrospectively conducted a chart review of the patients with MBL-producing Enterobacteriaceae, which were isolated from blood samples, in a tertiary care hospital in Japan between April 2013 and March 2017. MBL-producing Enterobacteriaceae was defined as isolates from blood culture showing decreased susceptibility to meropenem (minimum inhibitory concentration, ≥2 mg/dL), and a positive result in the test using metallo-β-lactamase SMA Eiken (Eiken Chemical Co., Japan). Clinical and microbiological characteristics and therapeutic outcomes of these patients were investigated. Results: During the study period, a total of 6 adult patients were identified. The median age was 55 years, and 3 (50%) were women. All patients had severe underlying disease at the time of admission (cardiopulmonary arrest (n = 3), invasive pneumococcal infection (n = 1), bleeding esophageal varices (n = 1), and peritonitis due to diverticulitis perforation (n = 1)). Of the patients, 5 were admitted to the intensive care unit. The sources of MBL-producing Enterobacteriaceae BSI were as follows: catheter-related BSI (n = 2), pyelonephritis (n = 1), cholangitis (n = 1), bacterial peritonitis (n = 1), and bacterial translocation (n = 1). Of the 6 isolates, 4 were Enterobacter cloacae and 2 were Klebsiella pneumoniae. The immunochromatography assay showed that all isolates were IMPtype. No isolates were resistant to levofloxacin and aminoglycoside. All patients had a previous history of antibiotic treatment and all isolates were isolated after more than 12 days of hospitalization. All patients were treated with either levofloxacin only or levofloxacin and aminoglycoside. Microbiologic response rates based on the results of culture tests using samples from the original infection sites were 100%. However, the all-cause 30-day mortality rate was 50%. Conclusion: This study showed MBL-producing Enterobacteriaceae BSI was fatal, although rare, in patients with severe disease and longterm hospitalization. Further research is necessary to study the appropriate treatment options for MBL-producing BSI, including levofloxacin treatment based on antimicrobial susceptibility test results. Objective: With the increasing of methicillin-resistance Staphylococcus aureus (MRSA) all over the world, the surgical sit infection caused by MRSA also increased. Health care workers (HCWs) carried by MRSA is the role of transmission of MRSA in the hospital. Our study will investigate the carriage rates of S. aureus and MRSA of surgical HCWs in our hospital, and have a molecular epidemiology analysis for all S. aureus strains. Methods: There were 242 pharynx swabs collected from 242 HCWs working in surgical wards, medical wards and an operating room during March-April 2016. The swabs were cultured on mannitol salt agar for choosing suspicion colony. The isolates were identified as S. aureus based on morphology, coagulase test, and VITEK test. Antibiotic susceptibility testing of all isolates was performed by modified Kirby Bauer disc diffusion method. The isolates were recognized as MRSAs if they were resistant to oxacillin. All of isolates were analyzed for epidemiological relatedness by pulsedfield gel electrophoresis (PFGE). Results: Out of the 242 healthcare workers, 70 (28.9%) carried S. aureus, and 10 (4.1%)carried MRSA. MRSA carriage rate was highest among nurses (7.5%), while carriage rate among doctors was 1.9%. However, the difference was not significant (P = 0.812). The carriage rates of surgical ward A, ward B, ward C, ward D and the operating room were all above 30%. Similarly, the difference of departments and wards was not significant (P = 0.420). After the PFGE analysis, the isolates were grouped into five similarity groups: A-E. The largest number of isolates belonged to the group D and E, which accounted for 78.2%. On the other hand, two MRSA isolates of surgical ward C were identical in the term of PFGE denderogram. Conclusion: The mean carriage rate of S. aureus is 30% among our healthcare workers, which is similar to the other countries' in the world. The MRSA carriage rate is lower than other developing countries'. The carriage rates of surgical wards are not significantly different to other wards', so the surgical sit infection cause by S. aureus is not related with the difference of HCWs' S. aureus carriages. The PFGE denderogram gave the popular gene groups of S. aureus, which might help the surgical sit infection control.

Ribotypes and antimicrobial susceptibility among clinical toxigenic Clostridium difficile isolates in Taiwan Yuan-Pin Hung 1,4,5 , Yuan-Ti Lee 6 , Hung-Jen Tang 7 , Hsiao-Ju Lin 1,4,5 , Hsiu-Chuan Liu 2,3 , Jen-Chieh Lee 4 , Pei-Jane Tsai 8 , Bo-Yang Tsai 8 , Po-Ren Hsueh 9 *, Wen-Chien Ko 4 *. Background: Some Clostridium difficile hypervirulent strains, including RT027 and RT126 had been reported in Taiwan. The data of toxin gene or ribotype distribution of toxigenic C. difficile isolates in Taiwan remain limited. Methods: The study was conducted from January 2015 to December 2016 in five hospitals in Taiwan. Stool culture for C. difficile was done in fecal sample from patients with diarrhea. Multiplex PCR was used to detect tcdA, tcdB, cdtA, cdtB, and tcdC deletion, and antimicrobial susceptibility was investigated in toxigenic C. difficile isolates. Ribotypes (RTs) of those isolates with tcdC deletion and tcdA-/tcdB+ were determined. Results: Of 1199 C. difficile isolates collected from five hospitals, 904 isolates were toxigenic: tcdA+/tcdB+ (794, 87.8%) and tcdA-/ tcdB+ (110, 12.2%). Of the toxigenic isolates, 81 (9.0%) isolates had tcdC deletion and binary toxin, i.e., hypervirulent isolates. RT078 family (61 isolates: 75.3%, including RT033, RT078, RT126, and RT127) and RT027 family (15: 18.5%, including RT027, RT034, and RT075) were predominant. Of 110 tcdA-/tcdB+ isolates, 100 (90.9%) isolates were identified as RT017. Metronidazole and vancomycin resistance, though rare, was present in 1.2% and 2.0%, respectively, of toxigenic isolates, and of note there were 9 isolates resistant to both metronidazole and vancomycin. Irrespective of toxigenic or non-toxigenic isolates, MIC 90 of doxycycline (8 μg/mL) was higher than that of tigecycline (≤0.0625 μg/mL), suggestive of more potent antibacterial activity of tigecycline than doxycycline. Conclusion: Hypervirulent isolates accounted for 9% of toxigenic isolates in Taiwan, and the dominant ribotypes were RT078 and RT027 family. Metronidazole or vancomycin remains active against the vast majority of C. difficile isolates.

Methicillin-resistant Staphylococcus aureus (MRSA) clones: preferences for nasal and extranasal sites and acute and subacute healthcare settings Angela Chow 1 , Jani Ong 2 , Jia-Wei Lim 1 , Aung-Aung Hein 1 , Grace Tin 1 , Vanessa Lim 1 . 1 Department of Clinical Epidemiology, Tan Tock Seng Hospital, 2 Infection Control Unit, Tan Tock Seng Hospital Background: Methicillin-resistant Staphylococcus aureus (MRSA) is the most common healthcare-associated multidrug-resistant organism. Intermediate-care (ITC) and long-term care (LTC) facilities have been observed to have greater diversities of MRSA clones than in acute-care hospitals (ACH). Differential factors associated with colonization of different MRSA clones have not been well studied. We sought to compare and contrast factors associated with colonization with various MRSA clones among patients contemporaneously hospitalized across acute and subacute healthcare settings.

Methods: Over two consecutive years during June-July 2014 and June-July 2015, we screened randomly-selected inpatients with >48 hours stay at a 1600-bed ACH and all inpatients at its 6 affiliated ITC and LTC facilities in Singapore. Separate nasal, axillary, and groin swabs were cultured for MRSA and isolates subject to wholegenome sequencing. Clinical data were subsequently obtained from medical records. To account for clustering, multivariable 2-level multinomial regression models were constructed to assess for factors associated with colonization of major MRSA clones, using SAS Proc Glimmix. Results: A total 10,367 samples from 3483 patients were screened for MRSA. MRSA was isolated in 9.4% of the samples. Sequence type ( However, the groin was 7 times as likely as the nares to be colonized with ST45 (aOR 7.19, . Conclusion: Colonization of MRSA clones differed between nasal and extranasal sites and between acute and sub-acute healthcare settings. Regardless of healthcare facility, contact precautions should be taken for recent MRSA-colonizers/infected to prevent MRSA transmission.

Epidemiology of Methicillin-resistant Staphylococcus aureus (MRSA) colonization in different health care associated workers Hsin-I Shih 1 , Hsiang-Chin Hsu 2 , Chia-Yu Chi 3 *.

Background: Methicillin-resistant Staphylococcus aureus (MRSA) is a main cause of health care-associated disease burden. Health care workers (HCWs) have been identified as the source of MRSA in numerous outbreak investigations. Epidemiology of MRSA nasal colonization in a range of HCWs have been studied, and yet out-ofhospital health care providers have limited data. This study aims to characterize the risk factors and molecular epidemiologic characteristics of MRSA colonization in out-of-hospital health care providers and HCWs in a teritary hospital. Methods: This was a prospective study. Participants included outof-hospital health providers such as Emergency Medical Technicians, paramedics, nurses working in the long term care facilities and in-hospital health care providers such as physicians and nurses. Swabs were obtained from nares and processed using standard laboratory techniques. Standard microbiologic methods were used to identify methicillin-susceptible S. aureus and MRSA. Molecular epidemiologic methods included pulsed-field gel electrophoresis, PCR detection of Panton-Valentine leukocidin, and SCCmec and multilocus sequence typing. Results: A total of 112 out-of-hospital health care providers and 137 in-hospital HCWs participated. The combined prevalence rate of MRSA was 7%. The majority of the MRSA colonization were in female (16/17, 94%) and in-hospital nurses (12/17, 71%). Five (29%) of the out-hospital providers had MRSA colonization. Nurses (OR: 9.33, 95% CI: 2.03-42.90) and clinical service year between 5 and 10 years (OR: 3.66, 95% CI: 1-13.67) had higher risk for MRSA colonization. Eight different strains of MRSA were identified by pulsed-field gel electrophoresis. The majority of the MRSA was SCCmec IV and V strains, three of which carried Panton-Valentine leukocidin. A unique ST typing 398 from a hospital nurse who ever traveled to Europe was identified in this study. Conclusion: The findings suggest that there is substantial colonization of MRSA among in-hospital and out-hospital health care providers. Implementations of infection control strategies should be applied high priority in those HCWs.

Trends in Carbapenemase-producing Klebsiella pneumoniae, Taiwan, 2012-2015 Sheng-Kang Chiu 1,2 , Ling Ma 3 , Ming-Chin Chan 4 , Chang-Phone Fung 5 , Tsu-Lan Wu 6 , Yin-Ching Chuang 7 , Po-Liang Lu 8 , Jann-Tay Wang 9 , Jung-Chung Lin 1 , Kuo-Ming Yeh 1 *. Background: Before 2011, the prevalence rates of carbapenemaseproducing Klebsiella pneumoniae (CPKP) among carbapenem nonsusceptible K. pneumoniae (CnSKP) isolates were below 10% in Taiwan. We present the dissemination trends of CPKP from January 2012 to August 2015. Methods: Twenty-one hospitals in Taiwan were enrolled in the study. K. pneumoniae isolates with minimum inhibitory concentrations (MICs) of >1 mg/L for imipenem or meropenem were collected for carbapenemase genes screening and antimicrobial susceptibility testing. Results: A total of 1457 CnSKP isolates were collected and screened for carbapenemase genes, and tested for antimicrobial susceptibility. Among 457 CPKP isolates, three classes of carbapenemase genes were identified: class A ( Background: Lancefield group B streptococci (GBS) are increasing as a cause of invasive disease among non-pregnant adults. We set out to characterize GBS isolated from adults in Portugal in 2009-2015.

Methods: All GBS isolates (n = 555) were serotyped, assigned to clonal complexes (CCs) by multilocus sequence typing and characterized by surface protein and pilus islands (PI) gene profiling. Antimicrobial susceptibility testing was done by disk diffusion and resistance genotypes identified by PCR. High throughput sequencing of representative isolates was performed. Results: Overall, serotype Ia was the most frequently found in the population (31%), followed by serotypes Ib (24%), V (18%), and III (13%). Serotype Ib increased significantly throughout the study period ( p < 0.001), to become the most frequent serotype after 2013. Over 40% of the isolates belonged to CC1, including most isolates of serotypes Ib (n = 110) and V (n = 65), all sharing surface protein gene alp3 and PI-1 + PI-2a. Overall erythromycin and clindamicin resistance rates were 35% and 34%, respectively, both increasing throughout 2009-2015 ( p < 0.010). Macrolide resistance was associated with CC1 ( p < 0.001) and serotype Ib ( p < 0.001). Genomic analysis revealed that the Ib/CC1 lineage probably resulted from the acquisition of the type Ib capsular operon in a single large recombination event (≈300 Kb) by a representative of the V/CC1 macrolide resistant lineage. Conclusion: The serotype Ib/CC1 genetic lineage was detected for the first time and expanded in Portugal in the last six years and is now dominant among the GBS population causing invasive disease in adults.

High percentage of multidrug-resistant pathogens among long-term care facility residents hospitalized due to urinary tract infections in southern Taiwan Chia-Ming Chang 1,2 , Hsin-I Shih 3 , Chi-Chang Huang 1,2 , Deng-Chi Yang 1 , Yu-Wei Chen 1 , Wen-Chien Ko 2 . 1 Division of Geriatrics and Gerontology, Department of Internal Medicine, 2 Division of Infectious Disease, Department of Internal Medicine, 3 Department of Emergency Medicine, National Cheng Kung University Hospital, College of Medicine, National Cheng Kung University, Tainan, Taiwan Background: Fever and infections are the major causes of transferal to hospitals among residents of long-term care facilities (LTCFs). There were rare studies on the infections in LTCFs and pathogens if these residents were hospitalized. This study aims to investigate the clinical characteristics, pathogens and antimicrobial susceptibility from LTCF residents who were hospitalized due to urinary tract infections (UTIs). Methods: During February 2013 to January 2015, patients from the LTCFs that were sent to emergency room (ER) due to diagnosis of UTIs were enrolled in a medical center in southern Taiwan. Clinical information including demographic data, co-morbid conditions, functional status, catheterization status, will be collected. Urine cultures were obtained at emergency room within 24 hours of admission. The cultured gram-negative rods from urine and their susceptibility results by disc diffusion methods were also obtained.

Results: A total of 185 patients were enrolled between February 2013 and January 2015. Among these patients, 90(48.6%) were male, mean age was 77 years (range, 20-100 years),156 (84%) were totally dependent in ADLs, 129 (70%) were bed bound and 96 patients (52%) used urinary catheters. E. coli (117 isolates), K. pneumoniae (45), and P. aeruginosa (28) were the most common pathogens. Extended-spectrum beta-lactamase (ESBL) producers accounted for about 38% of E. coli and 24% of K. pneumoniae. The non-ESBL isolates also showed lower percentage (<60%) of susceptibility to 2nd generation cephalosporins, fluoroquinolones and sulfonamides, but susceptible to fosfomycin and carbapenem. Multidrugresistance (MDR, resistance to ≥3 classes) accounted for 57% and 70% of non-ESBL E. coli and K. pneumoniae isolates, respectively. P. aeruginosa also showed high percentage (>80%) of resistance to fluoroquinolones, but lower resistance (<20%) to ceftazidime and carbapenems. Two isolates of Acinetobacter baumannii were pandrug-resistant (PDR). All 7 A. baumannii isolates were fluoroquinolone resistant, 30% were resistant to carbapenems. Conclusion: Our findings highlight MDR and resistance to commonly used oral antimicrobial agents among LTCF residents with UTIs, especially to fluoroquinolones and cephalosporins. Empirical antimicrobial therapy might be optimized in these patients.

Molecular epidemiology and antimicrobial susceptibility of Enterococcus faecium clinical isolates in a Japanese hospital Asami Matsumoto 1 , Yuka Yamagishi 1,2,3 , Hiroyuki Suematsu 3 , Kentaro Oka 2 , Motomichi Takahashi 2 , Hiroshige Mikamo 1,2,3 . Background: Enterococcus spp. are well-known indigenous bacteria that often isolated from human and/or animal specimens. Generally, these species are non-pathogens but it can sometime induce infectious diseases especially in compromised host. The purpose of this study was to assess the molecular typing of Enterococcus faecium which isolated from single site to evaluate the association and/or differentiation of genotype, infection site, patient group and antimicrobial sensitivity. Methods: We extracted DNA from 159 of E. faecium isolates at Aichi Medical University Hospital in 2015 and assesed random amplified polymorphic DNA (RAPD)-PCR method to analysed genotypeing. The antimicrobial sensitivity were examined by the RAISUS automated method (Nissui Pharmaceuticals Co., Ltd, Tokyo). Retrospectively analysed with the patients background and/or infection site by medical records to characterize with the correlation between genotype. Results: The results of RAPD-PCR showed that 159 clinical isolates of E. faecium classified into 8 genotypes (Type 1; 52.8%, Type 2; 4.4%, Type 3; 20.8%, Type 4; 11.9%, Type 5; 8.2%, Type 6, 7 and 8; 0.6%). As a result of classification, more than 50% of strains isolated from pus, urine, urinary catheter and venous blood were categorized into Type 1 and more than 80% of strains isolated from choler and ascites were categorized into Type 1 or Type 3 (choler; Type 1 : Type 3 = 39.5% : 44.2%, ascites; Type 1 : Type 3 = 47.4% : 36.8%). In addition, the results of the comparison between genotype and the clinical department where the strains isolated has shown the statistically significant correlations. Moreover, the result of antimicrobial sensitivity, resistant to ampicillin, levofloxacin and minomycin was 69.2%, 67.9% and 30.8%, respectively. However, all isolates were sensitive to linezolid, vancomycin and teicoplanin. Furthermore, more than 90% of strains were categorized into Type 1, 2 and 4 were resistant to ampicillin and levofloxacin. Conclusion: These results clearly suggested the correlation of genotype and infection site of the E. faecium and need to be further analysis of detailed virulence/antimicrobial resistant genes in each genotype.

Epidemiological study of Salmonella serogroup from November 2010 to December 2014 in a regional teaching hospital in middle Taiwan Fang-Ching Liu, Chien-Sheng Cheng. Pediatrics Jen-Ai Hospital Tali

Background: The frequency with which Salmonella cause diarrhea and antibiotics resistant was increasing in these years. We attempted to determine the pattern of Serogroups and antimicrobial susceptibility of Salmonella infections in a regional teaching hospital during the period of November 2010 to December 2014. Methods: Children with Salmonella enterocolitis were retrospectively reviewed and epidemiology, age, sex, clinical manifestations, laboratory results were recorded between November, 2010 to December, 2014. The isolates and susceptibility of these Salmonella were carried out accordingly to standard microbiological methods. Results: Case records from 205 children were reviewed beteen November 2012 to December 2014. Of the 199 patients whose stool underwent complete testing for Salmonella Serogroup and the majotiry was D1 group (42%), then B group (39%), C1 (10.2%), D2 (5.9%) and C2 (1%). Eight stools (3.5%) tested positive for Salmonella and Rotavirus infections. Overall, 75.6% of cases were under three year of age, with 6.3% being younger than 1 years of age. Fever (95.5%), diarhea (87.4%), vomiting (25.6%) and bloody stool (15.1%) were the most common symptoms for Salmobella infections and abdominal pain (( p < 0.041)were associated with Salmonella Serogroup D1 (17%) and C1 (19%) infections. The resistance rates were 48.2%, 30.7%, 2.5%, 1.5%, and 2.5%, to ampicillin, trimethoprim-sulfamethoxazole, ceftriaxone, cefixime and ciprofloxacin, respectively. The majority (98.5%) of CRP was elevated with 53.1% between 0.05 and 5 mg/dl, and 45.5% >5 mg/dl. The symptoms with Salmonella infection to admission was about 2.58 ± 0.96 days and the average hospital date with Salmonella infections was 6.07 ± 1.35 days. Conclusion: Salmonella gastroenteritis is the important causing pathogens in infants and children and 75.6% in our study was less than 3 years age. The prevalence of Salmonella Serogroup B decreased from 48% in 2007, 36.8% in 2010 and 39% in 2015 and Serogroup D2 (42%) was predominated in our study. 3.4% of our patients had co-infections with Rotavirus and Salmonella infections. Most of the salmonella serogroups isolated in this study were highly susceptible to the third cephalosporins (97.5%) while resistant to ampicillin (48.2%) and bactrin (30.7%). The rising resistant antimicrobial agents informed us the rational antibiotics uage and the importance of infection control. Background: There are many currently available epidemiologic strain typing methods including antibiogram, pulsed-filed gel electrophoresis, restricted fragment length polymorphism and multilocus sequence typing (MLST). Among them, MLST is the most widely used method for the evaluation of clonality. MLST is an unambiguous procedure for characterizing isolates using pairwise sequence analysis of the internal fragments of seven housekeeping. However, major disadvantage of MLST is high-cost and labor-intensity. We designed Microsoft Excel-based macro system for the automated analysis of DNA sequences of seven housekeeping genes. Methods: Using excel-based macro, sequencing data were called to the spreadsheet as string-type text. Next, sequences with reverse primer were replaced with a complementary base sequence. Forward sequence and complementary base-replaced reverse sequence were assembled and aligned by matching 100-consecutive 20-identical sequences. Finally, assembled sequence were compared to the pre-built database constructed from online MLST database (http://www.mlst.net/databases/) to determine the type of allele. In case of non-typable allele, we manually checked through the online database. To evaluate the accuracy, we compared the result of our program with the result of pre-manually determined sequence typing. Results: We chose the fasta-format text files as the raw data which should be called to the spreadsheet for analysis. For process of the raw data calling, all the fasta files in the specified folder was automatically loaded, which takes most of the running time, however, is the main advantage of our program. The running-time of the whole process was 10 minutes per 50 strains, which is much faster than the manual typing method, which would generally take several hours. Comparing with the results of manual typing, concordance rate was 100%. All the non-typable alleles through our program were revealed as new allelic type when confirmed manually. Conclusion: MLST is a good method for epidemiologic evaluation, however, labor-intensity is the major disadvantage. We designed macro-based automated analysis, and the concordance rate with manual typing was excellent. Also, this program may prevent clerical error of manual typing. Excel-based macros can be used by anyone, and can be constructed through uncomplicated coding. This might be useful for the large scale epidemiologic studies.

Accuracy of bleaching concentration used in a medical center in Taiwan Pei-Chun Pan 1 , Ying-Chun Chen 1 , Hui-Mei Huang 2 , Zhi-Yuan Shi 1 . 1 Infection Control Center, Taichung Veterans General Hospital, Taichung, Taiwan, 2 Nursing Department, Taichung Veterans General Hospital, Taichung, Taiwan Background: The cleaning and disinfection of contaminated equipment and environment are essential for the prevention of healthcare-associated infections. The most common chemical used to clean the hospital environment is bleach (sodium hypochlorite). The concentration of bleach will decay by chang of temperature and light. As a result, the bleach with suboptimal concentration cannot kill the microorganism effectively. However, the concentration of bleach is not routinely checked in the daily cleaning practice. The aim of this study is to investigate the concentrations of various brands of bleaches used in hospital, and ensure the optimal concentration of bleaches can be achieved. Methods: Twelve bleaches from 5 chemical companies were purchased during February 2016 to July 2017. The bleaches were diluted with tap water to a presumed concentration of 500 PPM, according to the procedures suggested by the manufacturers. The diluted concentrations were determined by a bleach concentration meter.

1. The concentration of 12 bleaches varied from 234 to 466PPM. 2. The average diluted concentration was 370 PPM.

The concentrations of the bleaches could decay gradually. We are concerned about that the suboptimal concentration will not kill the pathogens effectively. For quality assurance, the manufacturers should focus on 3 aspects. First, they should provide product certificate that meets the required specification in contracts. Second, they should assure appropriate storage, transport, and dispensing of the products. Third, they should perform the product monitoring system. The hospitals have to carefully inspect the quality of the bleaches that are reliable for use.

Could the HOCl as a high level disinfection solution for environment cleaning Wen-Jun Yeh 1 , Fu-Chieh Chang 1 , Ya-Ling Chen 1 , Chang-Pan Liu 1,2 , Nan-Chang Chiu 3 , Hsin Chi 3 . 1 Infection Control Center, MacKay Memorial Hospital, 2 Department of Infection, MacKay Memorial Hospital, 3 

Background: HOCl is a unstable weak acid. In vitro, this production can kill pathogen. So in this study, we wanted to investigate the function of this produce in clinical. As we know, if this can kill pathogen from surface, then we could not culture any pathogen from the surface. So we used the traditional method to check its fuction. Methods: In this study, we choice 15 beds for culture. Before the terminal disinfection, we collect the sample from environment. Then we used the HOCl as a high level disinfection solution to cleaning and disinfection. After that, we still collected the sample from the environment then culture it. After incubated 48 hours, we check the result. Results: The result showed 26% culture positive after HOCl cleaning and disinfection. Otherwise, the culture positive ratio of chlorine solution was 21% before this study. So, the result showed there were no different between each method. But the price of HOCl are higher than chlorine solution. Conclusion: In this study, the HOCl solution function were not good enough to replace chlorine solution. Even though the experiment were great in vitro. So, according to this study, we didn't suggest the hospital use HOCl solution as a High level disinfection solution for environment Cleaning.

The improvement of food safety through tableware washing at hospital in Taiwan Yu-Hong Tsai, Lin-Lin Pan. Department of Laboratory Medicine, Chang Gung Memorial Hospital, Chiayi

Background: The quality of meals provided in hospitals is essential to patients. The nosocomial food contamination can cause various food-borne illnesses of patients and healthcare staff. The Ministry of Health and Welfare (MOHW) abrogated and amended the regulations about tableware in 2014. We examine whether the regulation is enough to monitor the food safety base on our experience about non-conformity in 2016. Methods: The surveillance was performed based on the standard ""Methods of test for food microorganisms-test of coli-form bacteria"" announced by MOHW in Taiwan. The lauryl tryptose broth (LST) and eosin-methylene blue (EMB) agar were used to measure the most probable number (MPN) of coli-form bacteria and Escherichia coli in tableware and meals.

Results: The spoiled meal was informed from wards in 2016. At first, the negative result was reported in the MPN of Escherichia coli from tableware and meals. However, the result revealed that 1150 MPN/g of coli-form bacteria in the meals was more than the threshold (1000 MPN/g) from the regulation ""Sanitation Standard for General Foods"" approved by Food and Drug Administration (FDA) in Taiwan. Klebsiella pneumonia was isolated from contaminated simultaneously tableware and meals through further identification. Thus, the infection control team-driven intervention was activated immediately to correct the procedure of tableware washing performed by subcontractor. We implement the condition with dry-heat 110°C for 30 min from food industry to disinfect the tableware. The surveillance was continued monitored to confirm the cleanness of reusable tableware. Conclusion: In summary, we identified the contaminated meals were resulted from inadequate tableware washing and improved abnormal procedure. We suggest that the regulation about pathogens isolated from reusable and disposable tableware should be considered and established in hospitals to improve the food safety of patients and healthcare staff.

Investigation of an abnormal result in dialysis water Ya-Ling Chen 1 , Jih-Roung Lu 2 , Fu-Chieh Chang 1 , Chang-Pan Liu 1,3 . Background: According to TSN2013 guideline, dialysis water have to exam every month, and the reference range was lower than 200 CFU/mL. If the data was under 200 CFU/mL higher than 50 CFU/mL, clinical staff or engineer have to find the possible reason that caused the data higher than 50 CFU/mL. In this investigation, we report a situation that caused the data higher than 50 CFU/mL for two months. And after finding the possible origin, we did some change, then the result showed its useful that no cases present after the change. Methods: In this study, we drew a picture to check the possible site of the RO system. Otherwise, we collected 35 dialysis water from RO system. Then we used the TGEA agar as a culture media. After 168 hours incubated in 20°C, we check the colony count. Results: After incubated, we found the RO system's dialysis water were qualified, but the outlet of RO system's dialysis water was unqualified. Otherwise, we found all the unqualified water were cultured positive for fungus. The result showed the fungus belonged to Candida and Aspergillus. After checked the data, we tried to find possible source. Conclusion: Finally, we found all the positive culture site that something inside or something nearby the outlet. And the goods were tooth mug、toothbrush and some personal belongings. So we suggested the clinical staff clean the space that empty the space of outlet. Thanks to the supporting of nurse, there are no more culture positive data present.

Systemic analysis of the culture data after terminal disinfection Ya-Ling Chen 1 , Fen-Mei Chen 2 , Li-Chuan Kao 2 , Fu-Chieh Chang 1 , Chang-Pan Liu 1,3 . 1 Infection Control Center, MacKay memorial Hospital, 2 Department of Nursing, MacKay memorial Hospital, 3 Department of Infection, MacKay memorial Hospital Background: As we know, the terminal disinfection is very important to prove new patient avoid bacteria from environment. In clinical, we suggested to use the disposable wipe for clean. And the chlorine solution had to over 500 ppm for terminal disinfection. Otherwise, we taught the cleaner the way to clean. So, in this study, we wanted to check if the environment still have Multi-drug resistant organisms after cleaning or not. Methods: In this study, we chose 41 beds 492 sites that contain MDRO before cleaning. Then we used the chromogenic agar as a culture media. After 48 hours incubated, we can check the bacteria growth or not. Finally we did the Antibiotic susceptibility test to check the MDRO still present or not. Results: After experiment, we found 21% (9/41) of bed that culture positive for MDRO. And the bacteria were Carbapenem-Resistant Acinetobacter baumannii (8/9) and Carbapenem-Resistant Enterobacter cloacae (1/9). Most culture positive bed were from Medical Intensive Care Unit. Conclusion: According to this study, we did the education of the staff in Medical Intensive Care Unit. Besides that, we ask the cleaner used the disposable wipe every time. Then have to check the concentration of the chlorine solution before clean. Background: According to the chemotherapy bleach detection of the bacteria assessment SOP. After the environment is clean to perform a bacteria test. The culture dish needs to be left 1 hour and then sent to the laboratory culture. To facilitate environmental clean monitoring. But after the petri dish inspection, often more than the laboratory to work time. So for the evaluation of postharvest petri dish placed in the chemotherapy room until the next morning to the laboratory culture. And the original work manual implementation process is different. The traps are evaluated after the test feasibility of inspection for the next day. Methods: Each time the implementation of the bacteria test will be divided into the control group (immediately check after the colletion) each time 8 pieces, test group ( petri dish every other day checked) each time 8 pieces. Observe the results of training. The control group was tested on October 13, November 17, 2016 and January 5, February 23, March 9, April 13, 2017. The test group was on October 14, November 18, 2016. January 6, February 24, March 10 April 14, 2017 respectively implementation training. Results: In the chemotherapy room to blood agar plate (BAP, 90 mm) for 1 hour and the collection was complete sent to laboratory training. The control group was submitted six times (48 pieces of specimen). The test group submitted six times (48 pieces of specimen). Total of ninty-six samples no colonies were grown. The results of the control group and the test group were consistent ( Figure 1 ). Conclusion: From the experimental results that whether the postharvest petri dish is immediately sent to the bacterial chamber culture or after the inspection of the petri dish every other day. As long as the chemical deployment room environment clean thoroughly And colony culture dish inspection and disposal appropriate. Can get a good environment for the deployment of chemotherapy drugs. To protect the patient's medication safety.

Reducing HAI by implementation of environment clean separation management measures using 4 different color disposable wipes Swee Siang Wong 1 *, Chiu Chu Yang 2 , Yi Pei Hsieh 2 , Chen Ni Kuo 2 , Yi Ru Chen 2 , Li Ching Chen 1 , Ko chung Tsui 1,3,4 , Cheng Hua Huang 1 . Background: Environmental cleaning is a fundamental principle of infection control in healthcare settings. However, the comparative application methods and contamination assessment techniques is unknown, and no consensus exists around benchmarks for cleanliness. We sought to determine whether implement environment clean separation management measures in a medical intensive care unit (MICU) could reduce the density of healthcare associated infection. Methods: We performed a 4-month prospective cohort intervention study, August ∼ December 2013, at medical intensive care unit in a 700-bed tertiary care hospital. Following one month of preintervention phase, implementing education and training for all cleaning staff and related personnel to use environment clean separation management measures by using 4 different color disposable wipes to clean high tough surfaces. The cleaning staff was asked to use red wipe to clean the toilet door knobs, isolation room and waste room, yellow wipe to clean the patient's bedside, green wipe to clean the nursing station and meeting room, blue wipe to clean the procedure tables, refrigerator handle and water dispenser button. The cleaning effect of the MICU environment was measured by using ATP bioluminescence. Then the density of health-care associated infection of the MICU was monitored every month. Results: The ATP bioluminescence value <100 is determined as pass and >200 is determined as fail. Within the study period, the value of pass increase from 13% to 53%, the value of fail decrease from 47% to 20%; numerical difference trend of the pass value increase from −7% to 20% and fail value decrease from 13% to −20%, showing the use of different color disposable wipes and the correct wiping method or wiping process can improve the environment cleanliness. The density of health-care associated infection one month before the intervention phase is 12.8‰ and one month after the intervention phase is 12.6‰. Conclusion: Using environment clean separation management measures is effective in cleanliness of MICU environment. However, there is no decline of the density of health-care associated infection.

Experience sharing of improving surgical hand scrub water quality Chih Hao Tseng. Infection Control Office; Clinical Laboratory Department Background: According to 2006 New Hospital Accreditation: The Surgery and Anesthesia departments should regularly implement bacteriological examination, yet relevant literatures did not suggest regular inspections, and these inspections should be implemented only when associated epidemiology data was required to confirm outbreaks. However, in consideration of healthcare quality and patient safety, infection control (IC) team had routinely collected and cultured surgical hand scrub water samples from total 4 faucets in our operation room (OR) by IC technician every month. The results had been less than 50 cfu/ml. On Sep 23, 2015, the cultured result of 4 water samples showed abnormal increase: from 70 to over 200 cfu/ml Methods: IC team had discussed with OR head nurse and ruled out anthropic factors but we were not conscious of ultraviolet germicidal lamp (UVGL) factor. After further discussion with public works department, ""decay of UVGL"" was recognized as the primary cause of decreased germicidal efficacy. Improvement measures were subsequently developed and implemented as described below: (1) Check and wipe off the dust on the surface of UV lamp every week to avoid penetration reduction that might affect germicidal efficacy (executed by sanitary staff, and included the practice in the routine clean work). (2) Staffs should always be aware of the status of UVGL, and once UV system is in alarm, beeping, immediately report to the OR leader nurse to call the manufacturer. (3) Renew UVGL and filter cartridge every year and every six months, respectively, to ensure the germicidal ability. Results: The measures were taken and carried on. We follow up in the next 6 months and the water quality of all 4 faucets were normal (<50 cfu/ml). Conclusion: UV germicidal lamp is widely used for sterilization. It is generally thought that UVGL has sustained germicidal intensity until the lamp is full dimmed. Thus, decay of UVGL tends to be neglected in such case. In fact, its germicidal ability is associated with Irradiation intensity (I) and Time (T). Therefore, the dust and grease on the surface of UV lamp may reduce penetration and germicidal efficacy. Hopefully, the experience sharing can remind IC healthcare workers of the link between decay of UV lamp and incomplete sterilization that is closely related to patient safety.

The experience of ameliorating abnormal reverse osmosis water quality in hemodialysis room of the reginal teaching hospital Hui-Ying Chuang 1,2 , Yea-Fang Wu 2 , Shu-Jiuan Tsai 1,2 , Ya-Lan Chen 1,2 , Mei-Lan Shen 3 , Chorng-Jang Lay 1,4 , Chen-Chi Tsai 1,4 . Background: After typhoon in July, 2014, we found that the 67% of dialysis machines had >50CFU/ml bacterial count of the reverse osmosis (RO) dialysate in the hemodialysis room. Methods: After survey, we found that the typhoon-induced turbid tap water contributed shortening life of RO membrane. So, we made some provisions for this problem, including changing RO membrane, disinfection of pipeline every week, and sampling RO dialysis after disinfection. When the result of RO dialysis sampling achieves <50CFU/ml for 6 months, the frequency of pipeline disinfection was prolonged to every other week. In addition, RO dialysis sampling must be performed immediately after typhoon.

Results: Three months after these procedures, <5% of dialysis machines had >50CFU/ml of RO dialysate. In 2017, there were no dialysis machines with >50CFU/ml of RO dialysate. Conclusion: After typhoon, sampling of RO dialysate for bacterial count should be performed immediately. When water quality is abnormal, it is necessary to change RO membrane and reinforce disinfection of pipeline for guaranteeing good quality of RO dialysate.

Effectiveness of an ultraviolet-C disinfection system for reduction of multidrug-resistant pathogens Jui-Hsuan Yang 1 , Un-In Wu 1 , Huei-Min Tai 1 , Wang-Huei Sheng 1,2 .

1 Department of Internal Medicine, National Taiwan University Hospital, Taipei, Taiwan, 2 Center for Infection Control, National Taiwan University Hospital, Taipei, Taiwan

Background: Healthcare-associated infections caused by multidrug-resistant (MDR) pathogens are significantly associated with increased mortality and morbidity. Environmental cleaning can reduce transmission of these pathogens but is often inadequate. Adjunctive, no touch methods are warranted to enhance the effectiveness of disinfection particularly in hospital settings where healthcare-associated infections are of major concern.

Methods: The study was conducted at National Taiwan University Hospital, a 2400-bed acute care medical center in Taipei, northern Taiwan, during the period October 2015 to March 2016. We examined the effectiveness of a mobile, automatic device, Hyper Light Disinfection Robot (model: Hyper Light P3), which utilized ultraviolet-C (UV-C) (254 nm) to kill clinical strains of MDR-Pseudomonas aeruginosa, MDR-Acinetobacter baumannii, vancomycin-resistant Enterococcus faecium (VRE), ATCC strains of methicillin-resistant Staphylococcus aureus (MRSA), Mycobacterium abscessus and Aspergillus fumigatus in experimental setting. The device was wheeled into different strategic position that was 1 meter from the agar plate. Then, the device was wheeled to a distance of 2 and 3 meters from other sets of agar plates with the same number of colonies, respectively. The above experiments were repeated with 5, 10 and 15 minutes of exposure time. The performance of this device in disinfecting three hospital rooms previously admitted by patients harboring MRSA and VRE was also assessed. Results: In vitro, more than 3 log 10 reduction of vegetative bacteria colonies was observed after UV-C irradiation of 5 minutes at a distance of 3 meters from the device, except for VRE and M. abscessus. At the distance of 1 meter, substantial and comparable reduction of colonies was observed across all tested bacteria regardless of exposure time. The killing effect was less pronounced for A. fumigatus particularly at the distance of 2-3 meters. In uncleaned hospital rooms, there was significant reduction in the number of bacteria colonies sampled from different surfaces after UV-C irradiation for 15 minutes (24 hours incubation: 35 CFUs vs 0 CFUs, p= 0.0005; 48 hours incubation: 165 CFUs vs. 0 CFUs, p < 0.0001). Conclusions: The Hyper Light P3 device was effective in killing MDR pathogens and could be considered as an adjunctive method in disinfecting hospital environment.

Urinary tract infections associated with cross-contamination of ureteroscopes Jung-Hui Chen 1 , Mei-Hua Lin 1 , Wei-Rong Huang 1 , Yi-Ching Hsieh 1 , Chun-Yi Lee 1,2 , Mei Huang 1,2 *. Background: Ureteroscope is a minimally invasive procedure widely used for examination and treatment of the urinary tract conditions. Cleaning and disinfection of the ureteroscopes used for the sterile procedure should follow standardized guidelines to prevent iatrogenic infections associated with cross-contaminations. In May 2016, the urology department of a hospital found that 4 patients fevered and urine cultures were positive about 1 to 3 days after ureteroscope. The Infection Control Group investigated and took relevant measures in order to avoid a similar situation happen again. Methods: The urology department of a hospital informed that 4 patients underwent ureteroscope during 17, 18 and 23 May 2016. They had fever and urine cultures were positive about 1 to 3 days after returning home. The Infection Control Group tracked data and found that there were 5 people fevered and urine cultures were positive among those patients who underwent ureteroscopy since 1 May, and 4 of them use the same ureteroscope. Furthermore, their urine cultures were Pseudomonas aeruginosa (CRPA*3). By preliminary judge, it's a cluster event. Further investigation revealed that ureteroscope cleaning staffs didn't performe in accordance with standard procedures. The Infection Control Group intervened immediately and implemented the infection control measures as follows:

1. To modify the disinfection method, and soak ureteroscope into the enzyme for 1 minute before cleaning; 2. To purchase ureteroscope brushes, and clean lumens by using the brushes while cleaning all ureteroscopes; 3. To strengthen the examination of ureteroscope cleaning process; 4. To suspend the use of problem ureteroscope until the test is qualified.

Results: After the intervention, there were no cases of infection due to ureteroscope from that day to 21 June 2016. But 4 cases of infection appeared when the problem ureteroscope was re-used on 22 June. Therefore, prompted another investigation which indicated that the ureteroscope was suspected to be damaged and should no longer be used on any patient. Conclusion: Contaminated ureteroscope could lead to transmission of infections resulting in increased healthcare cost and even life-threatening adverse events. Competency training, periodic auditing and biologic testing, and maintenance of ureteroscope are important to ensure the quality of ureteroscopy and the safety of the patients.

Investigation of a suspected outbreak of nontuberculous mycobacteria infected associated with bronchoscopy Chun-Ming Lee 1 , Hsiu-Tzu Chiang 2 , Fu-Chieh Chang 2 , Chang-Pan Liu 2,3 . Background: Genus Bacillus, especially Bacillus cereus is a pathogen in patients who are immunocompromised or have underlying conditions. Also contamination of reused hand towels by Bacillus cereus spores sometimes causes severe sepsis, wound infections following surgery. However, treatment of the towels with disinfectants in the washing process hardly removes the contamination. In Japan, beauty salon also use the steamed reused towel for removing the cream after facial treatment and healing. However, there are few reports about the bacterial contaminations for the steamed towel. In this study, we investigated the contaminations of bacteria from reused towel and steam warmer for beauty salon in Japan.

Methods: The five steamed towels before using were randomly selected from the steam towel warmer. The test objects were cut off 1 piece (1 cm 2 ) from each towel and extracted bacteria by saline.

For examination for contamination of steam warmer, the samples were collected by cotton swab in saline. The extracts were spread on the blood agar and incubate them on 37C for overnight. Then, the bacterial identification were done by 16sRNA sequencing. Results: Cultures from towels showed Bacillus spp. contamination of all of 5 towels. Each bacterial number of towels were from 6.8 × 10 2 to 3.4 × 10 3 cfu/cm 3 . No bacteria were detected from the steam warmer.

Conclusion: The steamed towels from beauty salon were heavily contaminated by Bacillus spp. Accordingly, we recommend the use of disposable hand towels for treatment in beauty salon.

The impact of regular urine drainage bag outlet disinfection on the urine cultures Yu-Yi Yang, Mei-Yu Wu, Chun-Hsing Liao. Infection Control Center, Far Eastern Memorial Hospital

Background: In the Taiwan CDC ""Prevention of catheter related urinary tract infection bundle care"" program, the board asks participating hospitals to disinfect the outlet of the urinary drainage bag after each opening. This recommendation is based with limited evidence. In order to understand the effect, we perform a prospective study to analyze the urine cultures from urinary bags with/without outlet disinfection.

Methods: Due to significant risk differences of contamination for short-term and long-term urinary catheters, we separated the study group into short-term catheter (<3 days) and long-term catheters (>7 days). For test group, we asked the nurses and family members to disinfect the outlet with alcohol swab every time the outlet is opened. For control group, no disinfection was performed. The samples for urine cultures from urine bag and swab culture for urine bag outlet (cotton swab inserted into the outlet) were collected when the urinary catheter was removed.

Results: For short-term catheter, the positive urine culture rate was 0% (0/10) for the test group and 10% (1/10) for the control group. The outlet swab positive rate was the same. For long-term catheter, the positive urine culture rate was 80% (8/10) for the test group and 40% (4/10) for the control group. The outlet swab positive rate was 40% for both control and test group. The most common isolated microorganisms were E-coli, Enterococcus spp. Conclusion: Disinfection of urinary drainage bag outlet with alcohol swab after each opening is not found to be useful in this small group of patients. Background: Observation and feedback is a core strategy for hand hygiene (HH) improvement. Direct overt observation is currently the most common method to monitor HH. Observation bias, also known as the Hawthorne effect, was one major disadvantage of the method. Although the magnitude of the Hawthorne effect has been described in several studies, subgroup analysis of the Hawthorne effect has not been clear yet. Methods: A case-control study was conducted in a tertiary teaching hospital during a 15-month period. The HH overt observation results were matched for category, department, observation time and location with those observed covertly at a ratio of 1:1. The matched pairs were then analyzed for the Hawthorne effect by different variable. Results: Among the study period, a total of 31,522 HH opportunities were observed (4581 by overt observation, 26,941 by covert observation). There were 3047 matched pairs after 1:1 case control matching. The overall HH compliance by overt and covert observation was 78.2% and 54.6%, respectively (the overall Hawthorne effect 23.6%). The Hawthorne effect in nurses (30.0%) was almost three times as large as that in physicians (10.5%), and tended to be more obvious in outpatient clinic (40.6%) than in the intensive care units (11.4%). Further analysis revealed the Hawthorne effect was significantly different between by categories of healthcare workers and observation location (p value both <0.001), but not different by department, observation time, and HH indications. Conclusion: The heterogenicity of the Hawthorne effect in hand hygiene may influence the interpretation of overt observation results. We suggest covert observation be performed at least once in health facilities in which HH observation are routine. Realizing the heterogenicity of the Hawthorne effect may help identify target population with lower HH compliance.

A project to improve rates of correct Hand Hygiene among healthcare workers in intermediate care nursery Chia-Ying Wu*, Liang Ling Ling, Shu-Ting Chang, Sheng-Fen Hung, Shu-Ling Chen, Mi-Hua Lee. Yung Kung Campus Chi-Mei Medical Center Background: Since 2009, the WHO has launched the ""Save Lives: Clean Your Hands"" campaign promoting the provision and use of alcoholic hand sanitizer in patient care areas to enhance accessibility of hand hygiene facilities and developed guidelines, including ""Five Moments for Hand Hygiene,"" aiming for minimal complexity of the process and time spent on hand hygiene to ensure maximum protection and embed hand hygiene into an integral practice. To promote hand hygiene in hospitals, the CDC of the Taiwan Ministry of Health and Welfare established hand hygiene model centers and implemented a hand hygiene accreditation program from 2010 to 2011, in an effort to improve hand hygiene behavior and practices among healthcare workers.

Methods: A quality improvement project was conducted and the fishbone diagram was used to identify problems. The main problems identified to such as lack of awareness and knowledge regarding hand hygiene timing, the boundaries between patient care and non-patient care areas are not well-defined, steps for hand hygiene are not performed properly… and other seven major issues. Improvement in educational and training, evaluation and feedback, and systematic strategies was implemented. In-ward medication dispensing procedure was changed to distinct patient care and non-patient care areas, the importance of hand hygiene was promoted to health-care workers at ward or division meetings, standard hand-washing was repeatedly demonstrated by hand hygiene auditors to reinforce hand hygiene compliance, and crossaudits were conducted by infection control link nurses/hand hygiene representatives on a monthly basis to assess compliance of Five Moments for Hand Hygiene and Seven Steps of Hand Washing in the Intermediate Care Nursery, with provision of feedback to the teams for continual improvement. In addition to increasing the number and locations of hand sanitizer placement, all mouse pads were replaced with the ones printed with hand washing diagram to strengthen memories and foster retention. Results: After implementation of the project, rates of correct hand hygiene improved from 81% to 100% for eight consecutive months and the compliance rates rose from 91% to 100%. Conclusion: Interventions and approaches used in the project successful improved correct hand hygiene and hand hygiene compliance rates, both achieving 100%, for 8 consecutive months. We hope that this model can be extended to other pediatric units to enhance the quality of care, reduce healthcare resource consumption and social costs, and ensure the safety of patients and healthcare workers.

A data review of staffs' hand hygiene compliance in a local hospital Tsao Ya Ching. Longtan Min-Sheng Hospital Background: Since past decades, hand hygiene in infection control prevention has been proof by research. It is one of the most effective methods in decreased healthcare associated infection transmission among patients' care. Therefore, the attitudes of healthcare staffs and hospital workers perform an essential role. The purpose of the survey was to discover the relationship between hand hygiene compliance and attitudes of the employees in a local hospital. 41% staffs of it have been worked over 10 years. The yearly average rate of its hand hygiene compliance was decreased from 77% to 71% between 2015 and 2016.

Methods: The study was conducted by means of a questionnaire. It divided into three sections within 26 questions. The first section was dealt with personal background and hand hygiene training experiences. The second part of the questionnaire was concerned with hand hygiene knowledge and attitudes. The third section was related to hand hygiene behaviors. Questionnaires were given to the staffs in two different periods: September 2015 and September 2016. 73% of staffs in the hospital were submitted their questionnaires during these two periods. Hand hygiene surveillances also applied before and after the surveys submission. Results: 251 surveys were collected. 93% of participant explained they had been to a hand hygiene training. 99% of questionnaires followed 5 moments for hand hygiene. It showed significantly higher than the handwashing surveillances. Indeed, only 73% staffs washed hands in hand hygiene 5 moments during the observations. In the second section of questionnaire discovered, 86% staffs had positive attitudes toward hand hygiene and infection control prevention. By contrast to hand hygiene compliances, just 67% staffs completed hand washing process while they provided healthcare. Moreover, 34% interviewers answered they wore dual gloves when they cared patients. From these interviewers, the results found out they only changed gloves by handwashing opportunities as well.

Conclusion: This study investigated, most staffs knew hand hygiene is one of the effective and easy methods for infection control preventions. Virtually, they are difficult to perform it. They even wore gloves to replace handwashing moments. However, a regularly hand hygiene training and handwashing audit programs are required.

The investigation of clusters of diarrhea in the staffs Hui-ju Hu, Meng-Chuan Lu, Hsiu-Wen Yu. Infection Control Unit, Cheng Ching Hospital Background: There were twenty staffs attending a staff trip to Yilan on 6-7 January, 2016. The symptoms of vomiting or diarrhea were found continually in some staffs. The clusters of diarrhea were assumed and we reported to the Department of Health, Taichung City Government. The investigation and samples collection were implemented soon and we also educated the staff to keep caution to avoid spreading. Methods: There were twenty staffs attending a staff trip to Yilan on 6-7 January, 2016. They had lunch on the second day at a seafood restaurant in Gonglian and sashimi was served. There were three staffs suffered from vomiting and diarrhes on 8 January, 2016. We reported to our trade union of welfare, and the Department of Health, Taichuny City Government as diarrhea clusters. There were eleven staffs suffered from gastrointestinal symptoms and one staff even resulted in hospitalization until 14 January, 2016. All the eleven staff received examination of stool bacterial culture and three stool samples were sent to the laboratory of Centers for Disease Control. Our implementing measures are as followings: (1) Educate the staff with symptoms should keep aerosol and contact isolation, rest at home. (2) The other staff who took care of these patients should wear masks and keep hand hygiene. (3) The environment that the patients worked should enhance cleansing. (4) Watch out for the possible symptoms of infection and informed infection control department. Results: All three stool samples revealed Norovirus infection. Conclusion: Norovirusis a very contagious virus, it can live in stool or vomitus for a long time. The virus is transmitted by fecally contaminated food or water, by person-to-person contact, and via aerosoli zation of vomited virus and subsequent contamination of surfaces. Norovirus is rapidly in activated by either sufficient heating or by chlorine-based disinfectants and polyquaternary amines, but the virus is less susceptible to alcohols and detergents. Early implementation is helpful and important to prevent spreading. We hoped to share this special experience to prevent possible clusters and save the resources and costs.

Outcomes of implementing infection control strategies in a psychiatric unit Ching-Feng Huang 1 , Shiu-Feng Lu 2 . 1 Department of Chang Gung University of Science and Technology at Chiayi Campus Kaohsiung Veterans General Hospital, Chiayi, Taiwan, 2 Department of Nursing Kaohsiung Veterans General Hospital, Kaohsiung, Taiwan

Background: A psychiatric unit is usually closed space; there are many interactions among patients and their family. The chance of cross and cluster infection is higher. Infection control strategies are implemented to prevent cross and cluster infection among patients, visitors, and health professionals. Methods: The infection control strategies were conducted in the year of 2016. The strategies include four aspects. (1) Patient and their family: The poster was posted to remind individuals suffering from fever, cough, and cold should wear a mask, or educate them not to visit. The knowledge of hand hygiene and mask was promoted through patient group education session monthly and family forums every two month. Chest X-ray was performed for each inpatient. (2) Health professionals: The staff should take three-hour lessons to strengthen his or her knowledge regarding infection control and reporting system. The unit audited staff's compliance with hand washing to reinforce practice and develop good habits. Each staff checked own body temperature (BT). The staff should be arranged off if his or her BT is above 37.5. The staff could on duty after no fever for 24 hours. The unit also evaluated the infection signs and symptoms of inpatients who were implemented an intrusive cauterization. Validity period of all healthcare products should be examined. Staff should take at least one-hour lesson of needlestick and sharps injuries prevention yearly; new staff should complete this course in three months. One registered nurse (RN) was assigned to be responsible for infection control; another acted as an infection control consultation liaison RN. They were responsible for monitoring safety use of healthcare products and reporting suspected cases. (3) Equipment: Hand sanitizer was provided at fixed places. The gate of the unit was equipped a vending machine for masks. If there was diarrhea or vomiting, it is important to strengthen environment clean and disinfection. (4) Suspected cases: The suspected patients should be held on isolated rooms in order to decrease contact with others. Related library tests or examinations should be followed up.

1. Patients and visitors can compliance with hand hygiene and masks. There was no reported cluster infection in year of 2016. The rate of completing chest X-ray examination was above 95%. 2. The rate of compliance with hand hygiene correctly among staff is 100%.

Conclusion: It is effective to decrease and prevent cross infection by promote infection control strategies for patients, visitors, and health care providers. It further enhances healthcare quality.

The effectiveness of environmental cleansing to improve infection control of upper respiratory infection in a psychiatric center Yu-Ping Liu 1 , Shiun-Yin Jeng 1 , Huei-Lan Lu 1 , Tso-Jen Wang 1 , Chung-Shin Liao 2 . 1 Jianan Psychiatric Center, 2 Chang Hua hospital Background: Environmental cleansing was a key point in the prevention of medical care, psychiatric patients generally had poor personal hygiene and habits, therefore while infectious disease occurred, indirect easy to make the environment contaminated by bacteria, when cleaning staff cannot effectively clean the disinfection environment, and contact with the patient with a infectious disease, would increase the medical care-related infections, in order to put an end to the breeding of infectious diseases, environmental cleansing to improve intervention is very important. Methods: Retrospective monitoring data found that the average upper respiratory tract infection rates were 2.5‰, 1.9‰ and 2.8‰ in a psychiatric center in 2013-2015. A investigating of the cleaners' cleanliness status in acute and rehabilitation ward in 2016, the result showed that the cleaner was not familiar with the process of cleaning, the bleach concentration is not correct, the cleaning equipment is not used separately. Environment cleaning intervention was provided for improving, the content included: (1) Education and training: through the video, to strengthen the priority of the clean process, from clean area to the contaminated area, the correct bleach concentration of the way and reply to the effectiveness of teaching acceptance. (2) The provisions of unified cleaning appliances: measuring cups, bucket clearly marked. (3) Clean appliances management: planning area and mop to the red tape clearly identified to use the division of cloth wiping. (4) Conducting internal and external audits: audit by the ward staffs and infection control division to understand the cleanliness and implementation of the cleaners' environment. Results: After the improvement of environmental cleanliness, the incidence of upper respiratory tract infection from 2.8‰ down to 2.24‰, cleaning staff also expressed that through the in-service education, unified cleaning appliances and management, can more clearly implement the environmental clean disinfection of the operating process. Conclusion: Environmental improvement can effectively reduce the upper respiratory infection, the in-service education will be included in the cleaning staff in future, to develop cleaners on the environment clean operation process familiarity, and continue to audit the implementation of the situation in order to maintain the best hospital environment clean quality.

Pseudo-outbreak of nontuberculous mycobacterium infection caused by bronchoscope contamination Yi-Chuan Chen 1 , Ting-Ying Chung 1 , Chun-Sui Lin 1 , Mao-Cheng Ge 1,2 , Ting-Shu Wu 1,3 *.

1 Infection Control Committee, 2 

Background: The growth rate of non-tuberculous mycobacterium (NTM) from bronchial wash specimen reached 9 cases/month (infection control threshold: 6 cases/month) was reported by the microbiology laboratory in Chang Gung Memorial Hospital, Linkou branch, in March 2017. Five out of 9 cases were M. chelonae complex.

Methods: Infection Control Committee investigated the outbreak by enhancing hand-washing in the bronchoscope room, performing culture of individual bronchoscopes and the surfaces, filtered water, cleaning chemicals and tubing of automatic washing machines.

Results: There was no growth of NTM from culture of the bronchoscopes. After the investigation, handwashing compliance of the bronchoscope staff reached 100%. All specimen (N = 4) from the filtered water of 3 automatic washing machines and tap water culture showed growth of NTM, however, the strain was different from the outbreak M. chelonae complex strain. After re-sterilizing all tubing system and staff re-education, there was no more NTM outbreak in the following months.

Conclusion: Threshold monitoring and alertness from microbiology laboratory and clinical staff are important measures in infection control in the hospital.

The vancomycin-resistant enterococci (VRE) case study and disposal experience of intensive care unit in a regional hospital Chun-Feng Chiu 1,2 , Min-Han Hsieh 1,3 , Tun-Chieh Chen 1,3 . Background: In a regional hospital in southern Taiwan, vancomycin-resistant entorococci (VRE) was isolated from four patients in intensive care unit (ICU) in one month. The sites where VER was isolated were bloodstream, urine, pus and ascites. One of the four patients was healthcare-associated bloodstream infection. This study was intended to analysis the possible cause of VRE transmission and to prevent outbreak. All of the four patients were at the risk of VRE infection. About the hand hygiene in medical staff, the mean compliance rate was 70% in previous 3 months, which is lower than the stander 80%. In this ICU, there's no audit for the terminal cleaning of the ward when patient transferred to ward. Therefore, once culture result of multidrugresistant organism (MDRO) was obtained after patient transferred out of ICU, the ward can't accomplish the terminal cleaning according to the standard of MDRO-exposed ward. Methods: First, patients from whom VRE was isolated were placed in single ward for isolation and selected cleaning tools were exclusively to this ward. Second, hand hygiene of medical staff was regular monitored. The goals of accuracy rate and compliance rate were 90% and 85%, respectively. Third, take preventive segregation measures and regular check. Fourth, the terminal cleaning of the ward was regularly monitored. Fifth, the education of appropriate antibiotic use was held for medical staff.

Results: The results of following six months after these measurements included the followings. No more VRE isolated in this ICU. The mean accuracy rate and compliance rate of hand hygiene in medical staff were 92.8% and 88.3%, respectively. The mean rate of monitoring terminal cleaning was 96.7%.

Conclusion: With the increasing use of antibiotics, the vancomycin-resistant in enterococci was kept increasing in ICU in Taiwan region hospital. In this study, all four isolates of VRE were from different site and the antibiogram was also different, which means outbreak is not likely. During the investigation and intervention measurements, we found that reduction of healthcare associated infection can be achieved by improvement of hand hygiene and making sure of environment cleaning.

Outbreak investigation of influenza at pediatric intensive care unit of a medical center in southern Taiwan Shu-Ching Hsu 1 , Iig-Ling Chen 1 , Hung-Jen Tang 1,2,3 *. Background: A total of 16 medical staff had symptoms of upper respiratory tract infection, including 12 nurses, 3 physicians and 1 nursing caregiver, who were diagnosed with a group of pandemics in a pediatric intensive care unit of a medical center in southern Taiwan in mid-May 2015. A type of influenza confirmed a total of 8 (50%) symptoms, including fever (≧38°C) 7 (43.8%), cough 14 (87.5%), sore throat 14 (87.5%) and other symptoms. Which involved in the relevant infection control measures, making the influenza group sudden effective control. Methods: The hospital for the southern Taiwan 1273 bed of medical center, pediatric intensive care unit a total of 34 medical staff. Received units in charge of the notification unit recently staff have fever, upper respiratory tract and other symptoms, and the rapid diagnosis of influenza A type. The investigation interval from 5/18 to 6/5. In order to avoid the expansion of the incident, the Infection control division visited the site and discussed with the unit supervisor, and initially judged the A-type influenza group. In order to avoid the event, The incident was conducted to describe, analyze and review the epidemiological changes in the course of treatment, the control measures for segregation infection, and the influenza vaccination of the staff, and actively informed the health bureau. Results: Through the intervention investigation, the daily monitoring of the epidemic and return to the Health Bureau, a comprehensive staff health monitoring log, and the implementation of measures, including strengthening the environment disinfection, unnecessary time should be fully wear masks, strengthen the implementation of hand hygiene. After the relevant intervention measures, this influenza cluster event was effectively controlled. Conclusion: Medical personnel when there is a history of exposure to influenza respiratory symptoms should seek medical diagnosis and early treatment. Personnel should implement abnormal body temperature notification. In order to provide a safe medical environment, the Infection control division by monitoring temperature feedback system and daily active surveillance, as soon as possible to find the ward and unit influenza abnormal cluster events and early control of the epidemic.

The efficacy of infection control courses for caregivers at a hospital in southern Taiwan Hsiuo Kuo 1,2 *, Huiwen Sung 1,2 , Tepei Tang 1,3 , Chelung Su 1,3 , Shufeng Chang 1,3 . 1 Infection Control Committee, 2 Department of Nursing, 3 Department of Medicine, Mingjong Hospital, Pingtung, Taiwan

Background: The study is to discuss education and training to promote the efficacy of caregivers' infection control recognition. Methods: Thirteen caregivers at a southern hospital were studied to use ""infection control recognition evaluation chart"" as a tool. Tests were performed two weeks before and after the education and training. Results: At this study, the average work year of these caregivers is 6.6. The nationalities are 7 Taiwaneses (53.9%) and 6 Vietnameses (46.2%) 38.5% of them felt by themselves the related knowledge of infection control inadequate. The general recognition of infection control progressed to reach 9.5% (238/25) (Table 1) . Conclusion: The intervention of education and training could promote a caregiver's infection control recognition and protect the safety of a patients hospitalization.

An investigation regarding a tuberculosis cluster happened in intensive care unit of a tertiary hospital in eastern Taiwan Yun-cheng Wang 1 , Xin-yi Zheng 2 , Hui-Li Jiang 1 , Lih-Shinn Wang 3 , Hsiu-tzy chiang 4 , Soon-Hian Teh 1,3 *. To reduce this high rate of CLABSI an action plan was worked out: 1. Intensive training session of the nursing staff 2. Implementation of the Clear MicroClave, a needle free connector system 3. Implementation of a custom made closed IV system, with less risk of introducing infection in the system The whole process was intensively supported by the infection control department and the multi-professional vascular access team. Results: These actions resulted in a dramatic decrease in CLABSI, from 7.6/1000 tunneled catheter days to 0 at the end of 2015, and from 5.74/1000 port catheter days to 0 at the end of 2015. Conclusion: Intensive staff training, introduction of a new IV prepared system and the introduction of a needle free system resulted in a dramatic fall of central line infections on a pediatric hemato-oncology ward.

Applying system-oriented event analysis model to the safety of center catheter care systems Lin Shi-Hua 1 , Chuang Sheu-Wen 2 , Hsueh Kuei-hsia 1 , Chen Han-Hsing 1 . 1 Taipei Hospital, Ministry of Health and Welfare, 2 Taipei Medical University Background: At 1990 Institute of Medicine (IOM) proposed ""To Err is Human: Building a Safer Health System,"" the construction of safe sanitation systems have become a modern medical industry objectives pursued. Central line (CL) bundles to prevent central line-associated bloodstream infections (CLABSIs) are widely promoted. However, in 2010 the studies also a policy for the CL bundle is often present but frequently not well implemented by National Healthcare Safety Network (NHSN). Therefore, understanding the center catheter care systems during the depth of the interaction cross relationship becomes a very important issue. In addition, in recent years, to develop resilience engineering bringing new opportunities to enhance patient safety. Therefore, the study Applying System-Oriented Event Analysis Model (SOEA) and resilience engineering to the safety of center catheter care systems. Methods: Used SOEA and qualitative research interviews in an Intensive Care Unit of regional hospital. Results: The study found the goals of central catheter care systems to save patients live and control the CLABSI, in system-wide, there are three important sub-systems of working system functions to achieve system goals. Use SOEA, we found that there are 9 kinds of hazards in the central catheter care system. However, sampling a total of 15 cases of exposure to hazard among those in January 1, 2015 to March 31, 2015, only 3 cases to get the CLABSI. In view of this situation, interview doctors and nurses, found that in the same system hazers caused by the various states, have the same or different reaction practices to maintain their system and can implement functions continue to work, and may reduce the risk of patient injury. In the other hand, the SOEA can clarify the relationship between the various elements of central catheter care systems, combined with resilience engineering to understand HealthCare worker that how to respond the system in daily hazards, and under the anticipated and unanticipated occurrence of various adaptively adjust its central catheter combined practices, rather than blindly follow, and to maintain patient safety practices. Conclusion: Applying SOEA in addition to the ability to the structure of the system can complete presentation, including relevance of the system, orderly, purposeful, providing system improve important information. SOEA analysis of the resilience of the system, allowing the HealthCare worker to understand the occurrence and learning control method reaction only after full cognitive system structure in order to achieve the goal to Building a Safer Health System.

Staffs' cognition, attitude and behavior for central venous catheter bundle care program in medical intensive care units of a medical center in southern Taiwan Wan-Ching Wu 1 , Po-Lin Chen 2 , Wen-Chien Ko 1 , Nai-Ying Ko 1,3 . Background: To promote the quality of care for central venous catheters (CVCs) and reduce the incidence of catheter-related bloodstream infections, a CVC bundle intervenion program was implemented in the electronic health management system in 2012. In order to understand the staffs' cognition, attitude and behavior for CVC bundle care program, an investigation with questionnaire was underwent in July, 2016. Methods: The staff members, including doctos and nurses of 54bed medical intensive care units (ICUs), were invited to annyomously fill out the questionnaire for CVC bundle culture awareness. The itmes incldued in the study questionnaire were based on the Taiwan CDC guideline for CVC bundle care, which addressed the critical procedures for CVC placement and care. The study was ethically approved by The Institutional Review Borad of National Cheng Kung University Hospital (IRB no.: B-ER-105-82). Results: A total of 138 members (128 nurses and 10 doctors) of medical ICUs participated this study. The average year of service of the nurses was 5.1 years. Most of the study subjects (123, 89.1%) admitted ""catheter-related bloodstream infections is zero-tolerance."" All of them agreed ""the doctors should evaluate the necessity of catheter for patient every day."" The majority (116, 84.1%) of the participants agreed that CVC must be removed within 48 h, if it was inserted in an emergent situation. In addition, a significant proportion (83, 60.1%) ever proposed removal of CVC catheters inserted in such situations. Major contributing factors for catheter-releated bloodstream infections recognized by our colleagues included critical patients who need multiple venous lines for treatment (n = 42), overt bleeding during the process of CVC placement, prolonged procedure time (n = 23), incomplete disinfection of CVC conector (n = 34), and patients with multiple chronic diseases (n = 30). However, 63.8% of the respondents conisdered that the bunde care would increase their work load. Most of the study subjects (108, 78.3%) admitted the bundle program would fail, if the practioners did not take it seriously. Conclusion: All respondents recognized CVC bundle care program would reduce the risk of catheter-related bloodstream infections. However, most of the practitioners worried that heavy workload would be an obstacle to practice the bundle care.

Investigation of outbreak caused by Elizabethkingia meningoseptica at a respiratory care center in Taiwan Guan-Jhou Chen 1 , Ning Ting 2 , I-Chen Hung 2 , Jann-Tay Wang 1,2 , Wang-Huei Sheng 1,2 *, Yee-Chun Chen 1 , Shan-Chwen Chang 1 . Background: Elizabethkingia meningoseptica is a ubiquitous pathogen that could be found in clinical environment and equipment surfaces. E. meningoseptica had the nature characteristics of resistant to various antimicrobial agents, such as carbapenem. Increasing frequency of human infection, especially from healthcare associated infections, had been reported in recent decade. However, efforts of infection control of outbreak caused by E. meningoseptica remains less reported. Methods: The Respiratory Care Center (RCC) at the National Taiwan University Hospital is a 15-bed unit which provides care for ventilator-dependent patients. During October 2016 to November 2016, 3 patients developed bacteremia due to E. meningoseptica during the stay at RCC, which was significantly higher comparing to background incidence. All three isolates had identical antimicrobial susceptibility pattern. An environmental investigation and infection control measures of outbreak, such as strict hand hygiene, contact isolation and environmental cleaning were conducted. Molecular typing by using pulsed-field gel electrophoresis (PFGE) was used to investigate the relatedness of isolates and isolates with more than 90% of similarity were considered the same cluster. Results: In the investigation of outbreak, eight clinical isolates were cultured from six patients, including 6 isolates from blood and two isolate from sputum. Two additional isolates were cultured from the hospital environment. PFGE of the 10 isolates revealed three different clusters, including three, three and two isolates in each cluster. A blood isolate from one patient was identical to one of the isolates from the medical environment (Cluster 3, Figure 1 ). There was no linkage to the environmental isolates of the other two clusters indicate the possibility of cross transmission by healthcare personnel (Cluster 1 and 2, Figure 1 ). After use of strict cohort nursing, hand hygiene and environmental cleaning, the outbreak was controlled. Conclusion: We reported the investigation of an E. meningoseptica outbreak at the RCC of a medical center in Taiwan. Our results demonstrate potential transmission of E. meningoseptica via medical environment and person-to-person contact. Vigorous infection control measures should be applied to prevent spreading of E. meningoseptica at hospitals.

Implementation of care bundle to reduce the ventilatorassociated pneumonia Yu-Hsia Hen 1 , Ying-Chun Chen 1 , Hui-Mei Huang 2 , Zhi-Yuan Shi 1 . 1 Departments of Infection Control, 2 Nursing, Taichung Veterans General Hospital, Taichung, Taiwan Background: According to previous studies, some 5% of hospitalized patients had healthcare-associated infections. Ninety percent of the healthcare-associated pneumonia in the intensive care unit were ventilator-associated pneumonia. Compared with pneumonia which are not ventilator-associated, the risk of using ventilators gets six to twenty times higher than patients not using ventilators. Each ventilator-associated pneumonia was estimated to increase by 19.2 days of the length of hospital stay and US$5,335 of the cost in Taiwan. To prevent ventilator-associated pneumonia and improve the quality of medical care, we launched a campaign to promote the care bundle of ventilator-associated pneumonia. Methods: We implemented many strategies to promote the care bundle of ventilator-associated pneumonia from June 2015 to December 2016. The daily checklist was developed for five elements, including elevation of the head of the bed to at least 30 degrees, daily sedation vacation, daily assessment of readiness to extubation, mouth care with 0.2% chlorhexidine and emptying the water from the tubing of ventilators. Furthermore, we educated the healthcare workers and monitored the compliance with care bundle. Results: Compared to the data in 2015, after introducing the care bundle of ventilator-associated pneumonia in 2016, the infection rates of ventilator-associated pneumonia reduced from 2.31 to 0.86 per 1000 ventilator days. The utilization rates of ventilators decreased from 28.98 to 26.80 per 100 resident days. The compliance rate of care bundle increased from 80% to 95%. By the decrease of ventilator-associated pneumonia from 32 to 14 each year, we saved medical cost about US$ 100,000 each year. Conclusion: There are many benefits to adopted care bundle, including improving the quality of medical care for the hospitals, avoiding suffering from infections for patients and reducing medical cost for social insurance.

Outbreak of surgical site infections related to mastectomy with/ without reconstruction at a medical center Ju-Chun Lo 1 , Ying-Ying Chang 2 , Ya-Hui Huang 1 , Jann-Tay Wang 1,2 , Wang-Huei Sheng 1,2 . 1 Center for Infection Control, National Taiwan University Hospital, Taipei, Taiwan, 2 Department of Internal Medicine, National Taiwan University Hospital, Taipei, Taiwan Background: The National Healthcare Safety Network classifies breast operations as clean procedures with an expected cumulative incidence of 1%-2% surgical site infection (SSI). SSI will increase hospital days, mortality, medical costs and the workload of staff. The prevention of healthcare-associated infections have always been the important topic of medical institutions. The incidence of mastectomy-related SSI increased from Nov. 2016 at a medical center. An outbreak investigation and some relevant interventions were conducted. Methods: In recent four years, the incidence of mastectomyrelated SSI at the medical center ranges from 0.1 to 0.2%. An active surveillance by infection control nurse found that the SSI incidence increased since Aug., Sep., and Nov. 2016, and even went higher in Dec. 2016 and Jan. 2017. Outbreak was therefore suspected. In order to realize the cause for the unusually increased SSI, an outbreak investigation was conducted to analyze the probable factors, including staff's daily work, environment and equipment cleaning, the operation procedures, and post-operation wound care processes.

Results: It was found that the concentration of antimicrobial detergent was not enough, the prophylactic antibiotics were not used properly, surgical hand preparation using alcohol-based handrub was not adequate, wound coverage when re-draping was also not proper, alcohol-based hand hygiene equipment was insufficient in the treatment room, and there was no standard procedure for tissue expander injection. After discussion with the operation team, we establish a new standard procedure of for tissue expander injection, and improve the hand hygiene and aseptic technique. The SSI incidence data was thereafter also feedback to the operators. After these interventions, the incidence of mastectomy-related SSI decreased since April 2017. Conclusion: A timely implemented outbreak investigation and effective intervention can stop the outbreak of SSI.

PS 164 10 years' experience in implementing VAP bundle in the medical intensive care unit of a regional hospital to reduce ventilatorassociated pneumonia incidence Chin-Te Lu 1 , Hui-Chen Kuan 2 , Chia-Chun Chi 2 , Liang-Wen Ding 3 . (1) readiness-to-wean assessment daily, (2) check endotracheal cuff pressure daily, (3) diary ""sedation vacation"" (4) hand hygiene, (5) oral care every 8 hours, (6) head-of-bed elevation above 30 degrees, (7) suctioning of both the oropharynx and the endotracheal tube. We correct the data including: compliance rate of VAP bundle, ICU stay days, rate of ventilator usage, case number of VAP cases, VAP rate. For analysis the effects of VAP bundle, we divide into 4 periods: (1) Pre-intervention period (2004) (2005) (2006) , (2) Intervention Period (2007) Results: The compliance rate of VAP bundle was 16.7% in the Preintervention period increasing to 80.0% after the implementation of first year. The ventilator usage rate decreased 10.3% after implanted VAP bundle, and ICU stay days decreased 1.96 days of each patient from Pre-intervention period to Confirmation Period. The VAP cases decreased from annual average of 8 cases before VAP bundle implementing to an average of 1.33 cases a year after VAP bundle implementing (decrease of 83.3%). The implementation of the VAP bundle during three years got a total reduction of 20 cases of VAP. A record of 20 month period of zero VAP case from March 2008 to October 2009 in Confirmation Period, and 37 months form January 2011 to January 2014 in Maintenance Period had achieved. Conclusion: Our experience has shown that the implementation of VAP bundle, can be effective in reducing ventilator-associated pneumonia occurred.

Upgrade bundle care to reduce the catheter-associated urinary tract infection rate Pai Mei Chao 1 , Mei Fang Liang 2 , Yu Chen Yang 2 , Pao Tsuan Kao 3 .

Director of Nursing Department, 2 Infection Control Center, 3 

Background: According to literature review, urinary tract infection (UTI) is the most common infection site in many hospitals. UTI exists 30-40% of all health care associated infection (HAI), also one of common sites. Urinary catheter is utilized in about 15% to 20% of inpatients. Every more one day use of urinary catheter can increase UTI rate by 3% to 8%. During 2015-2016, in medical ward and intensive care unit, overall urinary catheter utilization rate is 27.1% to 28.3% resulting in UTI rate ranged from 0.83‰ to 0.72‰. In order to implement more complete and effective strategy for reducing UTI rate, we participate in Invasive Health Care Procedure Program in 2017. The previous strategy for prevention of UTI is not adequate, and we expect for reducing of UTI rate by implementation of current UTI bundle care. Methods: Since February 2017, we start to implement the strategy as following:

1. Educate all health care providers and selected special personnel, arrange small unit training for basic and new knowledge of urinary catheter application. The selected health care personnel are trained for application of urinary catheterization and skills of catheter care. Through this training course, they can share these skills to other members. 2. In order to promote the facility and procedure method, use the appropriate disposable urinary catheter package which can facilitate health care providers to apply and shorten the duration of catheterization, also reduce contamination rate. There is utilization of 2% chlorhexidine for disinfection, preconnection of urinary catheter and urine collecting bag, using sealed valve designed urine collection bag if urine specimen is necessary to be obtained which can avoid for damaging the closed circuit, wiping the catheter opening with alcohol based paper before and after collection of urine. 3. Warning for hand hygiene before and after contact with urinary catheter and urine bag by using small card attached to urine bag, and warning for catheter care by using picture in language of Chinese, English and Vietnam easily to be recognized by main care giver. Catheter placement checklist and daily care checklist are monitored for effective implementation.

Results: Followed from February, 2017 to June, 2017, in medical ward and ICU, the utilization of urinary catheter rate is 28.4%, the catheter related UTI rate is 0.46‰. Although the utilization rate is not differed from previous rate, the UTI incidence is reduced significantly after implementation of current UTI bundle care. Because health care providers strictly follow the strategy in all aspects, the incidence of UTI reduced significantly. At the same time, nursing department adjust and standardize the urinary catheter placement procedure and urinary catheter care manual. Conclusion: During this UTI bundle care strategy, reusable urinary catheter package is replaced by disposable urinary catheter package supported by hospital authority. All health care providers strengthen to implement all aspect of UTI bundle care, keep awareness of catheter care knowledge, keep proper disinfection before urinary catheterization, and assess daily for catheterization requirement, avoid unnecessary urinary catheterization. Therefore, catheter related UTI rate can be reduced significantly.

The survey for the effectiveness of implementing VAP (Ventilator-associated Pneumonia) bundle in the ICU of a regional hospital in Taiwan Catheter utilization ratios and microbiological characteristics were also analyzed.

Results: The incidence rates of both CLABSI and CRBSI decreased significantly from the baseline to the intervention periods (from 9.27 to 7.66 per 1000 CL-days, and from 1.51 to 0.89 per 1000 CLdays, respectively, with both p < 0.001). The yearly incidence rate decreased by up to 31% (incidence rate ratio [IRR], 0.69; 95% confidence interval [CI], 0.59-0.81) for CLABSI and 59% (IRR, 0.41; 95% CI, 0.26-0.65) for CRBSI since the initiation of the interventions. The incidence rate by quarter also showed a trend of decrease for both CLABSI and CRBSI ( Figure 1 ). The catheter utilization ratio decreased from 0.71 to 0.63 ( p < 0.001). Microbiological analysis showed that among all CLABSI isolates, the proportion of coagulase-negative staphylococci significantly decreased during the intervention period.

Conclusion: Implementing multimodal interventions focusing on CL bundle improvement was effective in reducing the incidence rates of CLABSI and CRBSI in Taiwan's adult ICUs.

Effectiveness of insertion and maintenance bundles to prevent central-line-associated bloodstream infections in critically ill patients: experiences and challenges in a medical intensive care unit Ya-Wen Chang 1 *, Miao-Fang Tai Background: To determine the effectiveness of a care bundle, with a novel line maintenance procedure, in reducing the rate of central line-associated bloodstream infection (CLABSI) in the intensive care unit (ICU). We aimed to quantify the effectiveness of centralline bundles (insertion or maintenance or both) to prevent these infections.

Methods: This was a prospective interventional quality improvement project aiming to decrease CLABSI in one adult intensive care unit. The intervention study was conducted over a period of 7 months. ""Plan-Do-Study-Act"" cycles, checklists for insertion and a standardized real-time ultrasound-guided cannulation of the internal jugular vein was adopted to implement the central line insertion bundle in clinical practice. The compliance of health care staff with central venous catheter bundle elements was also measured. The implemented CLABSI prevention bundle that included: implementation of central-line insertion and maintenance bundles that proper hand hygiene, personal protective equipment, maximal barrier precautions and iodine alcohol skin preparation during line insertion, daily discussion of catheter necessity, and meticulous site and tubing care. The rate of CVC related infections during the intervention period (1 October 2016 to 30 April 2017) was compared with the average of the 6 months prior to implementation.

Results: The rate of CLABSI decreased from 10.86 to 5.02 per 1000 catheter days ( p < 0.05). The length of ICU stay and catheter duration was associated with a higher risk of infection associated to these devices ( p < 0.05). Annulation of the internal jugular vein was achieved in all patients by using ultrasound. Average access time (skin to vein) and were significantly reduced in the ultrasound group of patients ( p < 0.05). The staff adherence to the bundle was over 95%. Conclusion: Improving the reliability of the central line insertion bundle has reduced infections. Key steps in the process were setting clear aims and ensuring staff understand the change process and measurement of results. This is fundamental to the success of any quality improvement process.

Using ATP bioluminescence technique to reduce surgical site infection in orthopaedic surgeriesthe clinical experience by a regional hospital Liang-YI Wu 1 , Yueh-Ying Hsieh 2 , Wen-Chuan Lin 1 , Ya-Fang Wang 1 , Hsiao-Yun Wen 3 , Yang-Hwei Tsuang 2 , Yong-Ching Lau 4 . 1 Infection Control Office, 2 Department of Orthopedics, 3 Operating Room, 4 

Background: The annual rate of surgical site infection associated with orthopedic operation in our hospital was 1.43‰ (25/17468) in 2015, which increased by 0.91‰ when compared with 0.52‰ (9/ 17128) in 2014. According to the investigation and analysis, it was found that part of the surgical instruments used during the implantation of prosthesis and implant were provided by the orthopedic implant suppliers who were also in charge of the cleaning and sterilization of the surgical instrument after the operation. After intense testing the passing rate of the adenosine triphosphate [ATP] bioluminescence assay test was only 38.5% (5/ 13) for the instruments from the orthopedic implant suppliers.

Methods: The followings are the methods we used to improve the infection rate. 1st, develop the protocol for the use of Managing Rental Equipment in the Operation Room. 2nd, standardize the protocol for the process of surgical instrument cleaning and sterilization. 1. Develop the processing protocol of surgical instruments which includes: itemization, decontamination, soaking, cleaning, inspection, packaging, and sterilization. 2. Conduct the educational training program for the personnel who are responsible for equipment cleaning and sterilization. 3. Use of 3M™ Clean-Trace™ ATP Surface Test and 3M™ Clean-Trace™ NG Luminometer as the testing tools, and randomly test the operation equipment after cleaning. The passing value is <150RLU, and the passing rate was calculated as the number of qualified items divided by the total number of testing items (%), to make sure that the extent of cleanliness has reached our goal of effective sterilization. Results: The passing rate of ATP randomized testing was increased from 72.5% (50/13, 2015/01) to 72.9% (140/192, 2015/20-2015/12) and continued to increase to 92.8% (205/221, 2016/01-2016/12). The infection rate of SSI decreased from 1.43‰ (25/17468, 2014) to 1. 07‰ (20/18628, 2015) and kept improving to 0.82‰ (15/18203, 2016) . Conclusion: This study used ATP bioluminescence assay as an objective and rapid tool to determine the environmental cleanliness and effectively demonstrated a decrease in the rate of surgical site infection caused by the surgical instruments. Therefore, we now include this protocol in our standard operating procedure.

Creating an health environment by the implementation of bundle care for prevention of catheter-associated urinary tract infections at a medical center Hsin-Hsin Chang 1 , Jann-Tay Wang 1,2 , Yee-Chun Chen 1,2 , Wang-Huei Sheng 1,2 . 1 Center for Infection Control, National Taiwan University Hospital, Taipei, Taiwan, 2 Department of Internal Medicine, National Taiwan University Hospital Background: According to the 2006-2016 Taiwan Nosocomial Infections Surveillance System (TNIS), the most common HAI is catheter-associated urinary tract infections (CAUTIs). CAUTIs result in significant morbidity, mortality, and medical cost. This prompted us to investigate the impact of infection control measures, such as CAUTI bundle care. Bundle care has been proven to effectively reduce CAUTI in many literatures. Well-structured infection control programs are imperative in order to the prevent CAUTI. Methods: Prospective surveillance was performed on all patients admitted to the National Taiwan University Hospital (NTUH). All patients developing healthcare-associated uninary tract infection (HAUTI), including CAUTI while their staying at NTUH during 2008 to 2016 were enrolled for analysis. A CAUTI bundle care was implemented in the entire hospital, particular focus on ICUs, since 2010. The bundle care emphasized 4C elements: clean practice, clean products, clean equipment, and clean environment. A comprehensive educational program including optimization of the catheterization process, demonstrating by teaching videos for learning purposes, clinical audit, Objective Structured Clinical Examination (OSCE) for nurses and interns, and competitions to test the operators' practical knowledge and skills was also conducted at the same time. A computer-based infection surveillance system and checklist was also developed to monitor the adherence to the CAUTI bundle care. The outcomes were analyzed on Wilcoxon-Mann-Whitney test. A p value less than 0.05 was considered as statistically significantly. Results: During the study period, there were 6557,971 patient-days with a total of 9845 episodes of HAUTIs in the entire hospital identified. CAUTIs accounts for about 56.7% of HAUTIs. Among them, 1922 episodes of CAUTIs were assessed in 620,019 patientdays over 353,609 catheter-days in adult ICU. Comparing the incidence density of CAUTI in adult ICU patients before and after the implementation of bundle interventions, CAUTI incidence density remarkably decreased from 7.9 to 4.8 per 1000 patient-days ( p = 0.02). Foley utilization was seen significantly decreased, from 1148.5 catheter-days to 956.1 catheter-days ( p = 0.028) monthly between the pre-CAUTI and post-CAUTI bundle care. Conclusion: CAUTI incidence density decreased in response to the implementation of an effective bundle care intervention. Multidisciplinary teamwork, education, a comprehensive checklist and from time to time audit feedback to improve healthcare workers' compliance are the keys to success.

Efficacy of daily bathing with 2% chlorhexidine for reducing healthcare-associated bloodstream infections in hematology patients Kuei-Lian Tien 1 *, Jann-Tay Wang 1,2 , Wang-Huei Sheng 1,2 , Chi-Tai Fang 2,3 . 1 Center for Infection Control, National Taiwan University Hospital, 2 Department of Medicine, National Taiwan University Hospital, 3 Institute of Epidemiology and Preventive Medicine, College of Public Health, National Taiwan University Background: Hematology patients undergoing immunocompromising cytotoxic chemotherapy are at risk for healthcareassociated infections. It remains unclear whether daily bathing with 2% chlorhexidine can reduce the risk. The aim of this study is to compare the incidence of healthcare-associated bloodstream infection (HABSI) between patients using of 2% CHG daily bath and those using general antibacterial cleansers daily bath. Methods: This was a single-arm prospective interventional study. Patients admitted to two hematological units during the period from December 2015 to June 2016 were the study population. We recruited patients to receive 2% CHG daily bath (intervention group). Those who refused to participate were provided with general antibacterial cleansers daily bath (control group). The incidence rate of skin flora-related bloodstream infection (SKIN_BSI), central venous catheter (CVC)-associated bloodstream infection (CLABSI_CVC), and HABSI of the intervention group were compared with that of the control group. Results: In total, 485 patients were enrolled as the intervention group, and the other 408 patients served as control group. Compared with the control group, the incidence rate of first SKIN_BSI was reduced by 65.5% (1.0 vs 2.9 per 1000 patient-day, p = 0.012), the incidence rate of first CLABSI_CVC was reduced by 61% (6.6 vs 16.9 per 1000 patient-day, P = 0.005) and the incidence rate of first HABSI was reduced by 34.2% (7.9 vs 12.0 per 1000 patientday, P = 0.042) in the intervention group. After adjusting for effects of confounding variables by logistic regression, the 2% CHG daily bath reduces the risk of first SKIN_BSI by 70% (adjusted OR = 0.3, P = 0.02), the risk of first CLABSI_CVC by 68% (adjusted OR = 0.32, P = 0.01), and the risk of first HABSI by 59% (adjusted OR = 0.41, P = 0.002). In contrast, the risk of gut-origin bacteremia, such as Escherichia coli and Klebsiella pneumoniae-related bloodstream infection, did not change (adjusted OR = 0.993, P = 0.99). Conclusion: Daily bathing with 2% CHG is effective in reducing skin flora-related bloodstream infection and cathether-related bacteremia. We recommend routine adoption of this simple and low-cost intervention to reduce BSI among patients with hematological malignancy.

Investigation of orthopedic arthroscopic surgery of Staphylococcus aureus infection outbreak Li Chen Chen 1 *, Tuzz Guang Young 1 , Wan Chen Lin 2 , Hui Chien Hung 3 , Hsiu Yu Peng 1 , Yuh Chyun Chiang 4 . 1 Department of Infection Control, 2 Department of Laboratory Medicine, 3 Department of Nursing, 4 Administration Office Background: October 2016 three patients underwent arthroscopic surgery have Staphylococcus aureus surgical site infection in our hospital. We used Fisher's exact test and found there's significance ( p < 0.05). Infection control team immediately responded and investigated environmental survey, the relevant infection control measures was also initiated. Methods: In our hospital, orthopedic department has a total of 35 patients underwent arthroscopic surgery in October 2016. Infection control nurses reviewed daily bacteriologic reports and medical records routinely. We have surveyed doctor and nurses with total of seven nasal swabs and hand specimens before and after hand scrub to identify the source of pathogen, including carrier and colonizer. All the pathogen specimen will compare with RAPD (Random amplified polymorphic DNA) to reconstruct ribonucleic acid and to distinguish whether the strains were identical. Results: We found all 3 infected patients received their surgery on October 3. The causative pathogen is S. aureus. Thirty nine of the environmental specimens were negative for S. aureus. Three of seven staffs have been found S. aureus in the nasal cavity. Two circulating nurses showed positive identification of S. aureus before and after they had scrubbing their hands. We conducted RAPD Analysis Typing for the pathogens and found that all patient's pathogens and pathogens from 2 circulating nurse were identical. The outbreak of surgical site infection has been confirmed. Conclusion: During investigation, we found that the circulating nurse was not required to wear gloves. Furthermore, we have confirmed S. aureus moved from the nurse's hands to surgical site, because of ineffective clean process while the nurses has severe eczematous skin lesion in her hands. After group meetings and discussions, we have revised the process for circulating nurse, which including hand hygiene and personal protective equipment. From November 2016 to May 2017, there was no more S. aureus surgical site infection. It can be well explained that our efforts for this outbreak are effective and successful.

The effect of bundle care intervention on catheter-associated urinary tract infections rate in medical intensive care unit Meng-Yi Liu, Chien-I Su, Yi-Hsuan Chen. Registered Nurse, Department of Nursing, Taipei Tzu Chi Hospital, Buddhist Tzu Chi Medical Foundation, New Taipei City, Taiwan

Background: Catheter-associated urinary tract infections (CAUTI) are the most common health care-associated infections (HAI) in medical intensive care units (MICU), accounting for 30-40% of all HAI. CAUTIs prolong hospital stay and exacerbate mortality rate. According to the 2016 nosocomial infection records provided by the Infection Control Center of Taipei Tzu Chi Hospital, the infection density of CAUTIs in MICU was elevated to 4.17‰ and medical costs increased. Therefore, the collaboration practice team assembled from Infection Control Center and Center of Quality Management promote the bundle care intervention. Methods: The collaborating team surveyed the bundle care intervention through brainstorming and cause-and-effect analysis. After reviewing literatures, the team investigated the current states of indwelling catheter implementation of three aspects: the ""care knowledge,"" ""placement,"" and ""care checklist."" Different understandings of the bundle care process were survey for medical team. The completion rate of accurate catheter care calculated from nursing team scoring. Daily evaluation instruments for catheter placement and care investigated. Finally, the team invited external infection control experts to MICU for intervention measures of bundle care implementation, standards of the placement and daily care, professional training sessions, instructional films for placement, daily care for patients, and a digitized platform for daily care evaluation. The team expected that these measures would alleviate the CAUTI rate. Results: The research team enhanced health care professionals' knowledge of bundle care intervention for CAUTI through revised standard operating procedures of indwelling catheter placement and care, lecture instructions, and self-learning via instructional films. Furthermore, this study ensured accurate implementation through an objective structured clinical examination (OSCE), group discussions, and subsequent audits. The CAUTI infection rate was decreased from 4.17‰ to 1.68‰, indicating the effectiveness of the intervention.

The OSCE results demonstrated that after educational training, both nurses in the nurse postgraduate nursing year training program (NPGY program) and the clinical nursing teachers had higher levels of knowledge on bundle care intervention of CAUTI than clinical nurses. The NPGY nurses received 2 years of dedicated training, whereas the clinical nursing teachers must receive relevant training continually when mentoring NPGY nurses. Therefore, clinical nurses should regularly receive the latest information to enhance their expertise and improve health care quality.

Lung cancer post-surgical with pleural effusion caused by Aspergillus infection: a case report Ching I Ting. ChiMei Medical Center Intensive Care Unit Background: Lung infections can be severe consequences of chemotherapy-induced immune defects. Aetiological causes of infection include bacteria, viruses and fungi. Most infections are caused by bacteria, but viruses are being increasingly identified. Diagnosis is difficult and frequently time-consuming. Other immunosuppression conditions, such as diabetes mellitus, malnutrition, alcoholism, connective tissue diseases and prolonged corticosteroid therapy, are also situations of increased risk. Case Report: This 76-year-old female, patient has past history of (1) Right side Low Lung bronchioalveolar carcinoma post operation on 88-04 (2) Hepatitis B related with liver cirrhosis (3) Hepatocellular carcinoma (4) Tachy-brady syndrome post Permanent pacemaker in 105-04 (5) Chronic kidney disease. She was just discharged due to 1. Adenocarcinoma, right upper lobe post CT guided biopsy. In this hospitalization, chest CT revealed Right Up Lung mass (size: 3 × 2.7 cm) and chest CT-guided biopsy was performed on 4/14. The pathology showed Adenocarcinoma, right upper lobe. This time, she was admitted for surgical treatment. We arranged Video-Assisted Thoracic Surgery operation on 5/2. Post operation transfer to intensive treatment. In the ICU, empiric antibiotic treatment with Piperacillin Sodium for infection control. Successful ventilator weaning, extubation on 05/03. However, sudden onset of PEA and apnea with cyanosis were occurred. Emergent intubation and resuscitation were provided. suspected pulmonary embolism further survey. May 19th the Chest CT was arranged that impression. Discussion the Chest CT report to her family and suggest insertion pig tail, they are agree then Chest CT guide for Pigtail drainage was performed on 5/23. 5/29 right pleural effusion: Aspergillus species. We add voriconazole 250 mg q12h therapy. 6/5 right pleural effusion: Aerobic culture: No Growth. voriconazole treatment course (5/29-6/9). However, unstable hemodynamic was done with renal, liver function failure, condition poor response, discharged on 6/11. Discussion: The definite diagnosis is made through the histological demonstration of tissue invasion by the fungus and the growth of Aspergillus species in a culture. Therapy with voriconazole or itraconazole has emerged as the first-line treatment and is safer than amphotericin B. The long-term prognosis for patients with CNPA is not well documented. The ideal treatment duration has not yet been defined and depends on the extension of the disease, the patient's response to treatment, the base disease and the patient's immunological condition. In some cases, lifelong therapy may be required.

Enhancing the quality of bundle care in patients with Ventilator associated pneumonia through crossdisciplinary teamwork Wan-Ling Hsu, Ni-Huang Lin, Yi-Hsuan Chen. Department of Nursing, Taipei Tzu Chi Hospital, Buddhist Tzu Chi Medical Foundation Background: Ventilator-associated pneumonia (VAP) commonly occurs in intensive care unit patients who have a tracheal tube. In Taiwan, approximately 13.7-25.3% of hospitalized patients contract VAP, which has a mortality rate of 20-33%. Hospital infection control center reported the VAP infection rate in our medical intensive care units was 1.17‰ in 2015, which was higher than the nationwide in medical centers (0.6‰) and which has steadily increased. Accordingly, hospitalization days and medical costs in Taiwan have risen. For this study, we collaborated with the infection control center, respiratory therapist team, information management center, and medical quality management center to develop and implement a bundle care intervention to reduce the VAP infection rate. Methods: A medical intensive care unit that has 45 patient beds and which averages a 96.4% occupancy rate was adopted for this study. Through cross-team cooperation and brainstorming, we analyzed several crucial factors related to VAP, and developed a VAP bundle care scale by referring to previous studies for investigating medical employees' knowledge of VAP bundle care procedures. The results showed that their knowledge varied, and that they did not adopt consistent care standards or have adequate daily assessment tools. In response, we invited external infection control experts to train hospital medical personnel on how to provide bundle care, modify daily VAP care standards (e.g., raising bedhead to 30°-45°, adopting daily sedation cessation, using 0.2-0.12% chlorhexidine gluconate for oral care, draining ventilators water, and washing hands), provide training, develop a daily care assessment form, and establish an information sharing platform. Results: In this study, we modified VAP bundle care standards and the medical team enhanced their knowledge of VAP bundle care through classroom learning and digital training videos. Through visual management and reminders from an information system, the accuracy of the VAP bundle care increased from 44.6% to 95.9%. Moreover, the VAP infection rate reduced from 1.17‰ to 0.44‰. We found that when bedheads were raised to 30°-45°, alert conscious patients could not fall asleep and experienced sacrum pain; accordingly, these patients refused this care procedure.

Conclusion: The bundle care information system not only improved the comprehensiveness of the bundle care that patients received, but also reminded medical team members or helped them to keep track of its tasks and the quality of care provided. The effectiveness of the medical team was critically enhanced through the information-sharing platform.

Using bundle care approach to reduce the ICU VAP and CAUTI infection density Yu-Wen Huang 1 , Yih-Giun Cherng 2 , Ya-Fang Wang 1 , Yu-Lan Wang 1 , Yong-Ching Lau 3 , Chun Mah 4 , Chien-Ling Su 5 , Wen -Chuan Lin 1 . 1 Infection Control Office, 2 Dean Chamber, 3 Infectious Diseases, 4 Department of Nursing, 5 Respiratory Room Background: We implemented an invasive medical intervention bundle care quality improvement program in 2015-2016, the VAP had remarkably decreased from 3.10‰ (2011) to 0.65‰ (2015). The incidence rate of CAUTI was 1.77‰ (2015), a significant reduction relative to the pre-intervention period 2.34‰ (2013). Methods: Comparing to the Taiwan Nosocomial Infections Surveillance System (TNIS) in 2010, the VAP for medical centers and regional hospitals was 0.9‰ and 1.2‰, respectively, but our ICU was 3.1‰ in 2011. The incidence of CAUTI in ICU was increased from 1.29‰ (2012) to 2.34‰ (2013). Because of the results, our hospital has implemented bundle care in the adult ICU with special focus on VAP and CAUTI prevention 2015-2016. Several wrong interventions were noted after quality control circle for VAP (2011): (1) Not implemented hand hygiene (2) lack of the recognition of appropriate sputum suction timing (3) did not elevate the head of the bed to an angle of 30-45°(4) did not drain and discard any condensate that collects in the tubing. For CAUTI, the problems were included: (1) did not implement sterilize procedure of catheter insertion. (2) hand hygiene adherence and accuracy before and after 5 moments. (3) the audit for CAUTI bundle was 58%, the implementation arte for daily insert urinary catheters was 60%. Five intervention measures (1) video for hand hygiene and hand hygiene activity was performed (2) superintendent, vice superintendent and director of nursing department were participated for video performance. (3) monthly audit was done (4) certificate activities for VAP and CAUTI bundle care prevention. (5) monthy feedback. Results: 1. Hand hygiene adherence and accuracy rates was 85% and 80%. 2. Implementation of VAP+ CAUTI was above 90%. 3. The incidence rate of VAP and CAUTI of our ICU in 2015 was 0.65‰ and 1.77‰, which was lower than other medical centers (0.9‰) and regional hospitals (1.4‰) in 2015.

Conclusion: Bundle care is an effective measure for promoting medical care quality, monthly meeting and surveillance system should be applied to evaluate its efficacy.

Using Hepac Lock Flush for reducing Port-related bloodstream infection Shu-Jiuan Tsai 1,2 , Chen-Chi Tsai 1,3 , Chorng-Jang Lay 1,3 , Hui-Ying Chuang 1,2 , Ya-Lan Chen 1,2 , Lin-Jia Lin 1,2 , Hsiu-Lun Su 2 . Background: Ventilator-associated pneumonia (VAP) is defined as pneumonia that developed 48 hours or longer after the use of mechanical ventilator. According to previous studies, oral hygiene care using 0.2% chlorhexidine gluconate (CHG) as an element of the ventilator bundle is supposed to decontaminate the mouth, avoid aspiration of contaminated secretions into the respiratory tract, and prevent VAP. The aim of the study was to evaluate the antiplaque efficacy by using oral care with 0.2% CHG gel in an adult intensive care unit (ICU).

Methods: This study was conducted in an adult ICU at a regional hospital from February, 2016 to April, 2016. During the study period, VAP bundle care was implemented, including (1) head-ofbed elevation 30°−45°, (2) maintenance of ventilator circuit free from accumulated water, (3) daily interruption of sedation, (4) daily assessment of readiness to extubate, (5) hand hygiene, and (6) performance of oral hygiene with 0.2% CHG gel twice daily. Plaque disclosing solution was used to evaluate the amount of plaque. Modified Quigley-Hein Plaque Index (MQHPI) score was recorded before and after each oral hygiene care with 0.2% CHG gel. Independent t test was perfomred to evaluate the significance of difference.

Results: There were 15 patients enrollwed in this study during the 2-month period. All of the study subjects were male with a mean age more than 75 years. The mean difference of MQHPI score was 0.28 ± 0.38 (95% C.I. 0.07-0.49, p = 0.01) before and after 0.2% CHG gel use. Conclusion: Oral hygiene care with 0.2% CHG gel can significantly lower the MQHPI score in adult critically ill patients. Further largescaled investigations are warranted to evaluate whether it had impact on reducing the incidence of VAP.

To evaluate the effectiveness of Ventilator-associated pneumonia (VAP) care bundle and to enhance the ratio of recognition and compliance in a district hospital Chia-Yun Pan 1 , Yu-Chiao Chen 1 , Yu-Jen Chang 2 , Hui-Ling Tsai 2 , Man-Ling Yu 2 , Li-Chu Lee 2 , Ming-Chi Tsai 3 , Wen-Chi Cheng 4 , Wen-Chih Wu 4 , Cho-Chin Sun 5 . Background: According to previous studies, there were many evidences suggested that bundle care can reduce healthcare related infection effectively. However, clinical staff didn't know the meaning of bundle care clearly and didn't obey the measures well. That's why we implemented ventilation-associated pneumonia (VAP) bundle care since June 1, 2015 to now. In the beginning, the accuracy of bundle care was only 76.6%, which may cause in low compliance rates of bundle care. Compliance rates of VAP were merely 57.1% respectively. The lack of recognition can affect the compliance of performing the measures. Methods: The VAP bundle care intervention include: head-of-bed elevation above 30 degrees, twice ""oral hygiene"" for patients daily, patency control on ventilator circuit, daily assessment of sedation cessation, hand hygiene, and evaluation ventilator need for patients every single day. We made up a special group that led by our assistant administrator. After observation of promotion to bundle care in clinical unit, we found that most of clinical staffs were poor recognition about bundle care and were not wellknowing in the measures. We elaborated corrective measures, including holding education training activity and bench-marking, instituting bundle care checklist and demonstration, hold ""King of Knowledge"" responder game, making informational materials for promotion and carrying out examination seriously to promote bundle care smoothly.

Results: After promotion of corrective measures, compliance rates of VAP bundle care were from 83.3% to 100%, respectively. Improvement rate of the test before and after education training reach 20%. Correction rate of performing policy was 100%. And the most important was that our VAP achieved zero tolerance for 16 months continued. Conclusion: Improving recognition of clinical staffs can enhance compliance rates of bundle care, leading to reduced healthcare related infection. So, we will still follow this study and hope to achieve the goal of zero tolerance.

Cephalosporin-glycopeptide combinations for use against clinical methicillin-resistant Staphylococcus aureus isolates enhanced in vitro antibacterial activity Hung-Jen Tang 1,2 , Chih-Cheng Lai 3 , Chi-Chung Chen 4,5 , Chun-Cheng Zhang 1 , Tzu-Chieh Weng 1 , Wen-Liang Yu 1,4 , Hung-Jui Chen 1 , Yu-Hsin Chiu 6 , Wen-Chien Ko 7,8 *, Yin-Ching Chuang 4,6 *.

Background: The empirical combination of both a beta-lactam and glycopeptide to counter potential staphylococcal pathogens may improve the clinical outcomes for cases of Staphylococcus aureus bacteremia.

Methods: We reported comparative in vitro studies of combination effects of different cephalosporins (i.e., cefazolin, cefmetazole, cefotaxime, and cefepime) combined with glycopeptides for 34 randomly selected methicillin-resistant S. aureus (MRSA) isolates by three methods, including the checkerboard, time-killing, and combination MIC measurement methods. Results: Thirteen SCCmec type III isolates with a cefazolin MIC of ≥128 µg/mL were classified as the high-cefazolin MIC (HCM) group, whereas 13 SCCmec type IV and 8 SCCmec type V isolates were classified as the low-cefazolin MIC (LCM) group. With the checkerboard method, synergism was present for vancomycinbased combinations at 30.8-69.2 and 13.6-66.7%, as well as teicoplanin-based combinations of 38.5-84.6 and 0-47.6%, of the HCM and LCM isolates, respectively. No antagonism was noted. The in vitro inhibitory activity was evident even at a low concentration of 1/512x MIC of cephalosporin combined with sub-inhibitory concentrations (1/2x MIC) of a glycopeptide. With time-killing assays, synergism was noted at 1/2x or 1x susceptible breakpoint concentrations (SBCs) of a cephalosporin combined with 1/4 or 1/2 MIC of a glycopeptide. In the presence of 1/2 SBC of a cephalosporin, vancomycin or teicoplanin MICs decreased an average of 2.0-to 6.6-or 1.6-to 5.5-fold, respectively. With 8 µg/ mL cephalosporin, the decline of glycopeptide MICs was most obvious in the presence of cefmetazole. Conclusion: Cephalosporin-glycopeptide combinations at clinically achievable concentrations can exhibit in vitro synergistic antibacterial activity against clinical MRSA isolates. Such combinations require more clinical data to support their application for use in human MRSA infections.

The impact of inoculum size on the activity of cefoperazonesulbactam Chih-Cheng Lai 1 , Chi-Chung Chen 2,3 , Ping-Chin Chang 4 , Yin-Ching Chuang 2,4 *, Hung-Jen Tang 5,6 *. Background: This study was performed to assess the in vitro activity of cefoperazone alone and different cefoperazonesulbactam ratios against different inoculum sizes of ESBL-E. coli and K. pneumoniae. Methods: Minimum inhibitory concentrations (MICs) of cefoperazone, cefoperazone-sulbactam at fixed ratio of 1:1 and 2:1 against a normal inoculum size of 5 × 10 5 CFU/mL CFU/mL and a high inoculum size of 5 × 10 7 CFU/mL CFU/mL were measured. Results: After the addition of sulbactam at a 1:1 ratio, most MIC 50 and MIC 90 values decreased. The susceptibility rate of cefoperazone-sulbactam at a 1:1 ratio is greater against most organisms, except P. aeruginosa; for P. aeruginosa, the susceptibility rate of cefoperazone-sulbactam was at a 2:1 ratio. For extended-spectrum β-lactamase (ESBL) E. coli, the susceptibility rate of cefoperazonesulbactam at ratios of (1:1) and (2:1) decreased from 97.0 to 87.9% and 90.9 to 60.6%, for normal to high inoculum, respectively. For ESBL K. pneumoniae, both susceptibility rate of cefoperazonesulbactam at ratios of (1:1) and (2:1) decreased from 75.8%, and 63.6% at normal inoculum to 51.5% and 42.4% at high inoculum. Conclusion: Cefoperazone-sulbactam at a 1:1 ratio has greater in vitro activity against most multidrug resistant organisms than cefoperazone-sulbactam at a 2:1 ratio. Such combinations were not influenced by the inoculum size of ESBL E. coli and K. pneumoniae and could be a therapeutic option for treating severe infections.

Clinical experience of patients receiving doripenem-containing regimens for the treatment of healthcare-associated infections Chien-Ming Chao 1 , Chi-Chung Chen 2,3 , Yin-Ching Chuang 2,4 , Chih-Cheng Lai 1 , Hung-Jen Tang 5,6 *. Background: In this study, we retrospectively reviewed the clinical experience of patients receiving doripenem-containing regimens for the treatment of healthcare-associated infections (HCAIs) in a tertiary care center and assessed the clinical usefulness of doripenem therapy in this clinical setting. Methods: In this retrospective study, the medical records of all adult patients who had ever received doripenem-containing therapy for the treatment of HCAIs were reviewed between September 1, 2012 and August 31, 2014, and the following data were extracted: age, gender, type of infection, disease severity, underlying comorbidities or conditions, and laboratory results. Additionally, we also extracted data regarding the rates of mortality and clinical and microbiological response. Results: A total of 184 adult patients with HCAIs who had received doripenem-containing therapy were included in this study. Respiratory tract infections (n = 91, 49.5%) were the most common type of infection, followed by urinary tract infections, intra-abdominal infections and skin and soft tissue infections. The mean APACHE II score was 14.5. The rate of clinical success was 78.2%, and the overall in-hospital mortality rate was only 13.0%. Among patients, in-hospital mortality was independently and significantly associated with APACHE II score (odds ratio (OR), 1.2825; 95% CI, 1.1123 ± 1.4788) and achieving clinical success (OR, 0.003; 95% CI, 0.0003 ± 0.409). Conclusion: The overall in-hospital mortality rate was low and the clinical success rate was high among HCAI patients receiving doripenem treatment. These results suggest that doripenem may be judiciously used for the treatment of patients with HCAIs.

Combination of cephalosporins with vancomycin or teicoplanin enhances antibacterial effect of glycopeptides against heterogeneous vancomycin-intermediate Staphylococcus aureus (hVISA) and VISA Chih-Cheng Lai 1 , Chi-Chung Chen 2,3 , Yin-Ching Chuang 2,4 *, Hung-Jen Tang 5,6 *. Background: Catheter associated urinary tract infections (CAUTI) is one of the most common health care associated infection. The most imprtant risk factor for development of CAUTI is prolonged use of a urinary catheter. Active surveillance and infection control are needed to disclose in each hospital and to deal with this problem effectively. Methods: Prospective surveillance was conducted according to the US Centers for Disease Control and Prevention (CDC) procedures at a major teaching hospital from 2013 through 2016. Fisher's exact probability test and Kruskal-Wallis test ware performed. Results: During the study period, 3,229,898 patient-days and 659,165 catheter-days were evaluated, and 2,144 CAUTI occurred. The overall rate of CAUTI was 3.25 per 1000 catheter-days. The mean age of 71 ± 19 years, and female gender accounted for 44.8%. The length of hospital stay was 75 (1-105) days in median. Parts of patients had past history of diabetes mellitus (31.9%), cancer (27.8%) and bedriddern (30.2%). Those patients who were admitted to intensive care units (ICUs) was 31.7%, and the length of ICU stay was 14±11.4 days in average. Conclusion: The cases of CAUTI and the distribution of risk factors were stable. Then, we intervene infection control measures such as using computer reminder to reduce usage of urinary catheterization, that can be attributed to obvious decrease in the rate of urinary catheter utilization ratio.

Early detection of healthcare-associated urinary tract infection by using an electronic active surveillance system at a medical center Hui-Chi Lin 1 , Yi-Ju Tseng 2,3 , Hsin-Hsin Chang 1 , Ming-Yuan Chen 4 , Jann-Tay Wang 1,6 , Wang-Huei Sheng 1,6 , Yee-Chun Chen 1,6 , Feipei Lai 4,5 , Shan-Chwen Chang 6 . 1 Center for Infection Control, National Taiwan University Hospital, Taipei, Taiwan, 2 Department of Information Management, Chang-Gung University, Taoyuan, Taiwan, 3 Department of Laboratory Medicine, Chang Gung Memorial Hospital, Linkou, Taoyuan, Taiwan, 4 Information Systems Office, National Taiwan University Hospital, Taipei, Taiwan, 5 Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan, 6 Department of Internal Medicine, National Taiwan University Hospital and College of Medicine, Taipei, Taiwan

Objectives: We apply a hospital-wide electronic automatic system to early detection and intervention for healthcare-associated urinary tract infections at a medical center. Methods: A real-time information technique integrated clinical and laboratory data into a signal entry point user interface to assist infection control personnel in making decision for healthcare-associated urinary tract infections. We use web services techniques based on Service-Oriented Architecture, which are suitable for integrating heterogeneous platforms, protocols, and applications. Results: Provider of Quality Indicator: During study period, 269 episodes of ICP-detected HAUTIs and 325 episodes of computerdetected HAUTIs were found throughout the 20 departments. The correlation of HAI events counted with and without the HAUTI surveillance system in different department is displayed. The square of the sample correlation coefficient was 0.995 (by departments) and 0.958 (by weeks). Accuracy of the Detection Rule: A total of 396 episodes were verified during June 1, 2015 and November 30, 2015. The episodes of computer-detected HAUTIs correlated closely with the infection control personnel detected (P < 0.001). Compared with reference standards, this system performed excellently with regard to sensitivity (84.89%), specificity (99.87%), positive predictive value (93.65%), and negative predictive value (99.65%). Decrease in the Delay of HAUTI Confirmation. The system shortened the delay in confirmation of HAUTI cases, on average, by 30.78 days (59.9 days versus 29.1 days, P < 0.001). Conclusion: An electronic active HAUTI surveillance system simplifies the workflow of hospital infection control, and provides convenient service and is time-saving for detecting HAI. It allows specialists to have more time to deal with ad hoc occurrences. In the process of improving efficiency, was also manage to strengthen the quality of medical care as well as patient safety.

Shu-Ling Chen, Chi-Fen Tsou, Ying-Ling Chen, Hung-Jen Tang.

Background: July 7, 2016, the neurosurgery resident informed infection control center. He found that was an increase in the number of patients who had suspected postoperative infection in neurosurgical outpatient clinics. He suspected that It related to construction work of operating room. The construction period of the operating room was from June 8 to July 24, 2016. Based on chi-square test, the null hypothesis was that the operation of neurosurgery in different operating rooms, and postoperative surgical site infection (SSI) chance was the same during construction.

Methods: During 2016 June to September, on healthcareassociated infection's SSI numbers were the highest in July, there were 13 cases. SSI patient surgery divisions distributed in the largest number of neurosurgery, it was 7 cases (53.8%). From the control chart was not difficult to find that SSI (0.92%) had exceeded 2 times the standard deviation of the control chart in July. The department of neurosurgery had 177 knife times (7th of the all surgical numbers). There were 87 knife times in No. 2 operating room and 47 knife times in No. 9 operating room in the majority, the rest operating rooms only 43 knive times. In No. 2 operating room SSI was 6 cases and In No. 9 operating room SSI was 5 cases, other operating rooms were without infection. Based on the chisquare test, the event of SSI was not significant with construction work of operating room (P = 0.2570, >α = 0.05). Results: Definition from the Taiwan CDC 2009 edition healthcareassociated infection surveillance, SSI's follow-up survey is divided into implants and no implants. So, there are implants to track until July 7, 2017 so far. Up to now SSI has not increased. The construction range of the operating room was In No. 8 operating room, adjacent to No. 7 and No. 9 operating rooms. No. 9 operating room mainly perform neurosurgical surgical procedures. The result of the survey was no correlation between postoperative infection and construction work of operating room. Conclusion: Here is the suggestion, surgery room if the construction plan, it should notice surgeon to evaluate the surgical schedule postponed or canceled, or to make a mandatory stop disposed of operating by the surgical committee. Moreover, after the completion of the construction, it should to commission qualified manufacturers for dust detection and bacteria sampling. Until normal environmental testing result to begin to surgery. In order to ensure the quality of surgery. Background: The incidence of carbapenem-resistant Pseudomonas aeruginosa (CRPA) related healthcare-associated infection (HAI) has increased in recent year worldwide. This study is to investigated the risk factor of CRPA infections in a universityaffiliated teaching hospital in Taiwan in order to provide more information under the current infection control system. Methods: A retrospective cross-sectional study included P. aeruginosa HAIs events from January 1, 2009 to June 30, 2014, and divided to CRPA infected case group and carbapenem-susceptible P. aeruginosa (CSPA) infected reference group. The medical records were collected to identify risk factors for CRPA HAI and mortality. Patients with prior use of any anti-pseudomonal carbapenems were included in subgroup analysis. Results: 395 P. aeruginosa infection case enrolled from total 3,263 HAI events; 63were CRPA and 332 were CSPA. The prevalence of CRPA was 15.9% (63/395). A significant risk factor related to CRPA infection were longer time at risk, prior use of anti-pseudomonal carbapenems, and prior use of aminoglycoside ( p < 0.05, 0.01, and 0.05). Besides, anti-pseudomonal carbapenem monotherapy did not significantly increase risk for CRPA infection. Conclusion: CRPA prevalence tended to be elevated worldwide as well as in Taiwan. Antimicrobials use should be monitored carefully, especially carbapenems and aminoglycoside. Further study on bigger population with carbapenems use would be needed to confirm the association between using carbapenems in monotherapy and the risk to get CRPA infections.

The implementation of care bundle to prevent catheterassociated urinary tract infections (CAUTIs) in medical wards of a Taiwan regional hospital Hsiao Yung-Tsung, Jian Shu-Min, Lin Yi-Chih, Lin Shi-Hua, Hsueh Kuei-Hsia, Hsu Jiin-Chyr. Taipei Hospital, Ministry of Health and Welfare

Background: Catheter-associated urinary tract infections (CAUTI) are one of the most common health care-associated infections; thus, CAUTI reduction metrics have been included in the Health and Human Services. According to a survey in 2016, the incidence of CAUTI was 1.38‰, accounting for 85.71% of all urinary tract infections, and the catheter utilization rate was 25.29% in a Taiwan regional hospital, Ministry of Health and Welfare. The aim of this study was to reduce the incidence of CAUTI by the introduction of bundle care. Methods: A total of 49 beds when placed with indwelling catheter were study from a general medical ward of Taiwan regional hospital. From January to May 2017, we recruited and analyzed 69 cases who received CAUTI bundle care. The CAUTI bundle care included 4 components. First, avoid unnecessary urinary catheter in situ, and check for clinical indications. Second, prior to every catheter care procedure, perform hand hygiene and then wear gloves and apron. Third, maintain the quality of urinary catheter, including regularly empty urinary drainage bags into a clean container, and urinary drainage bags should be placed lower than the bladder level but not on the floor. Lastly, the physician checked the necessity of using catheter every day, and the catheter was removed as soon as possible if not required. In addition, an internal audit team led by the chief physician of the ward conducted internal audits monthly using a table specifically designed for CAUTI bundle bare, and provided feedback to the ward. Our project team had regular meetings to check the compliance of the intervention plan and important index of the results of the review to improve. Results: The compliance rate of CAUTI bundle care increased from 66.7% in the beginning to 80%. Based on 20.44% of urinary catheter utilization rate, the incidence of CAUTI improved from 1.38‰ in 2016 to zero infection during the 5 months in 2017. Conclusion: Our study demonstrated that the catheter-associated urinary tract infections can be effectively reduced through the implementation of CAUTI bundle bare coupled with the establishment of the management organizational system.

A project to improve Ventilator-associated pneumonia bundle care in a respiratory care center Zin-Min Huang, Yi-Sin Lu, Shih-Han Tung. Department of Nursing, Buddhist Tzu Chi General Hospital and Tzu Chi University Taipei, Taiwan

Background: Ventilator-associated pneumonia (VAP) is a common nosocomial infection in respiratory care center (RCC), our respiratory care center began implementing ventilator-associated pneumonia bundle care in March 2016, but the compliance rate of bundle care intervention was poor and the ventilator associatedpneumonia infection rate increased by 3.5‰ in December, led to high mortality and prolonged hospital stays. So the aim of this project was to upgrade knowledge, improve the compliance rate and the correctness of bundle care, and reduce the infection rate for the patients in our unit. Methods: The project was established on January 2017. A previous evaluation on our unit found that the compliance rate of VAP bundle care intervention of nursing care was 70.6% and care correctness was only 78.0%. The causes of the low compliance rate for bundle care and low care correctness were: lack of recognition by the nurses, lack of remind system on daily assessment for patient, lack of standard care guidelines for Mouth care, the lack of standard to make sure the correct elevation of the head of the bed. For these shortcomings, we (1). conducted education and elearning programs about VAP bundle care, (2) . developed a remind system on medical operating system for VAP bundle daily assessment, (3) . established the standard care guidelines for mouth care and for checking the correct elevation of the head of the bed. Results: After implementing our intervention, we found the project effectively improved the compliance rate of VAP bundle care intervention of nursing care(from 70.6% to 96.0%), increased the care correctness (from 78.0% to 97.4%), upgraded the knowledge about VAP bundle care in our nursing staff (from 71.6 to 96.6 points). The ventilator associated-pneumonia infection rate decreased and reached zero tolerance from February to April in 2017. Conclusion: With effective implementation, the ventilator associated-pneumonia infection risks can be lowered, mortality and days of hospitalization reduced, protecting patients from disease and ensuring patient safety.

Systemic analysis of the bacteria of urinary tract infections and its risking factor I-min Che 1 , Ning-jen Tsai 2 , Pei-wen Tsai 2 . 1 En Chu Kong hospital Medical Quality Division specialist, 2 En Chu Kong hospital Infection Control, Department Infection control practitioner Background: Urinary Tract Infections (UTI) is a common infection, in Taiwan Nosocomial Infections Surveillance System database, UTI was the first infection in Hospital association infection. In this study, we wanted to analysis the bacteria of UTI and its risking factor According to previous studies, the bacteria caused UTI were Escherichia coli, Klebsiella pneumoniae and Candida albicans, and the most risking factor is the day that foley used. Methods: In this study, we collected the UTI data from 2014 to 2016. Then we analyzed the strain of bacteria and its risking factor. Otherwise, we used the SPSS tool to analyzed the UTI ratio of the day that foley used or not. Results: Through the analyzed of statistical, we found the top 3 bacteria were: E.coli, candida spp, Enterococcus spp, and the 51% (106/206) UTI cases were associated with foley. The average of foley used day was 8.8 days.

Conclusion: According to our analyze, the foley used or not would not arise the ratio of HAI-UTI. That's to say, if we can execute the UTI bundle, we still can avoid the UTI. Otherwise, according to our analyze, the E.coli was the popular bacteria in UTI, so if we can prove the AST data for doctor, the doctor can quickly treat and avoid bacteremia. Finally, the average of foley used day was higher than other, so we had to try to reduce it.

Risk factors of healthcare-associated Elizabethkingia meningoseptica infections in Taiwan medical center Chia-Ping Chen*, Yin-Yin Chen, Fu-Der Wang. Infection Control, Taipei Veterans General Hospital, Taiwan

Background: Elizabethkingia meningoseptica (Chryseobacterium meningosepticum) is a non-glucose-fermenting, Gram-negative aerobic bacillus. The incidence of E. meningoseptica healthcareassociated infection (HAI) has increased in a major teaching hospital in the northern part of Taiwan over the past 5 years. Methods: Between Jan 1, 2012 and Dec 31, 2016, a total of 250 patients with Chryseobacterium spp. associated HAI at a major medical center and teaching hospital in northern Taiwan were enrolled. Host factors, hospital services, surgical and medical interventions, infection sites, and the treatment and complications of HAI were analyzed retrospectively. Results: During 2012-2016, the incidence of Chryseobacterium spp. associated HAI has increased from 0.05 episode per 1000 inpatientdays to 0.08 episode ( p = .001). the major infection sites were bloodstream infections (79.2%), urinary tract infection (12.8%) were secondly. Overall, 67.8% were E. Meningoseptica and 31.3% were C. indologenes. The mean age was 74.9 ± 18.5 years, and 174 patients (69.6%) were male. APACHE II (Acute Physiology and Chronic Health Evaluation II) scores were 21 ± 9. Admission days prior to HAI was 29 (3-227) days, and 191 patients (76.4%) was ever exposed to antibiotics. 55.6% has urinary catheter and the median duration of catheterization were 13 (1-62) days. 36.4% received central venous line and the median placement durations were 10 (1-74) days. Conclusion: There was a secular trend of an increasing number of E. meningoseptica HAIs in our hospital over the past 5 years. In addition to reinforcement of standard infection measures and hand hygiene, Antibiotic stewardship is a key to reduce healthcareassociated E. meningoseptica infections.

Elimination of catheter-associated urinary tract infection: the CAUTI Bundle Ya-Ling Shih, Hsiang-Ju Chiu, Chia-Wei Yu. Department of Nursing, En Chu Kong Hospital, New Taipei City, Taiwan

Background: The density of catheter associated urinary tract infection (CAUTI) in the surgical intensive care unit was 4.14‰ in the 3 quarters of 2016 which was higher then Taiwan clinical performance medicating from 30 regional hospitals (2.70‰). Infection control unit, medical quality management center, department of medical, purchasing department were involved to be a team in order to reduce CAUTI. Methods: There were 5 implementation strategies: (1) Revising a clear standard for the insertion of a urine catheter; (2) Interdisciplinary education for CAUTI prevention and evaluation;

(3) Embracing bundle concepts-A.B.C.D.E., A: aseptic insertion, B: bag empty regularly with an individual collection container, C: close the drainage system, D: do not use the indwelling catheter, E: early remove. (4) Using reminder system. (5) unscheduled audits to assess the members' three abilities, including: cognition, affection and technical skills. Results: Regarding these strategies, we gained three effective outcomes as follows: (1) the catheter associated urinary tract infection density reduced from 4.14‰ in the 3 quarters to 2.81‰ in the fourth quarter of 2016 (implementation period); (2) compliance and accuracy rates of proper hand-hygiene techniques increased to 90%. Conclusion: CAUTI density was still maintain low in SICU: 0‰ in the first quarter of 2017. An important lesson learned is that interdisciplinary team leads to improved collaboration and outcomes for critically patients. Furthermore, even though we each possessed the knowledge to execute our individual professional responsibilities, realizing the exceptional outcomes achieved through genuine teamwork.

Outbreak investigation of Citrobacter koseri related urinary tract infection after bed bath Yi-Ru Hsiao 1 , Jen-Hsien Wang 2 , Ya-Wen Chang 3 , Shu-Yen Hsu 4 , Shiao-Yin Hung 5 , Hsiao-Hui Chen 6 , Chia-Yu Tsai 7 , Jui-Hsing Wang 8 *. Background: Enterobacteriae is the most important pathogen of urinary tract infection. It is an international serious problem as regarding the antimicrobial resistant strains in the healthcare system. For these, the medical personnel keep going to reduce healthcare associated urinary tract infection in medical institutions. Once the cluster infection develops, it is difficult to estimate the further harm and impact. In March 2016, Infection Control Center in our hospital found several events of urinary tract infection with Citrobacter koseri in respiratory care ward through the microbiology laboratory daily report. For the case number was above the mean threshold, we initiated the outbreak investigation. Methods: We collected the information according to the clinic observation, laboratory result and medical recording in the past half year retrospectively. Furthermore, we performed 10 items environmental test, such as ward ground, drainage hole, shifting plate,bathing bed, shared shampoo and bath gel. Results: Three urinary specimens and one blood culture in three patients with Foley catheter revealed Citrobacter koseri with similar antimicrobial susceptibility testing in Feb and March, 2016. Six patients with Citrobacter koseri infection from Sep, 2015 to Jan, 2016 developed sporadically. Besides, environmental sample culture reported Citrobacter koseri in bathing bed, shared shampoo and subpacking bath gel, also. Intervention of infection control measure and education for caregivers were performed immediately. Shampoo and bath gel without re-using of empty bottles were applied individually. With bathing, the drainage system of Foley catheter must be shut down temporarily to avoid the countercurrent urine related infection. Besides, we emphasized principle and infection control policy of respiratory care ward about equipment, environment and daily care in personnel training. No more Citrobacter koseri related infection was reported and followed environmental tests revealed negative result till now. Conclusion: Enterobacteriae induced outbreak of urinary tract infection in the healthcare system is a notified issue. In this presentation, our experience about outbreak of Citrobacter koseri related urinary tract infection after bed bath was rare to be mentioned before. Transmission of enterobacteriae via equipment and environment should be reminded.

Effectiveness of bundle care intervention in reducing catheterrelated urinary tract infection: sharing in a reginal hospital in Taiwan Jeng-Hua Ling 1,3 , Chia-Hung Liao 1,2 *, Pei-Chen Chen 3 , Hong-Chang Hong 2 , Li-Yue Ye 3 , Ya-Chin Lee 1 , Yu-Shiue Chen 3 . 1 Infection Control Center, 2 Department of Internal Medicine, 3 

Background: In the hospital, the urinary tract infection accounted for 58.82% of the healthcare-associated infection in the intensive care unit, which catheter-related urinary tract infection (CAUTI) accounted for 90%, infection density were 4.05‰. Literature reviews that hospitalized patients CAUTI high medical-related infections the highest, accounting for about 36%. According to the empirical medical literature, it can be seen that the combination of care measures can effectively reduce CAUTI, so the unit to promote the prevention of urinary tract infection combined care measures, hoping to reduce the rate, to provide high-quality medical care. Methods: Started implementation of CAUTI Care Bundle in January 2015 for all patients with catheters. According to the literature, five nursing measures were established: (1) Daily assessment of whether the catheter can be removed; (2) Hand hygiene should be performed when care of the catheter; (3) Daily check the catheter for fixation; (4) Confirm the catheter drainage system to maintain a closed, sterile and smooth; (5) Clean the urethra with soap or water every day. Use checklist daily to check whether or not all care is performed. Through the establishment of consensus, operational standards revision, improvement of care tools and education and training and other intervention measures to implement. Through the audit mechanism and CAUTI detection, the results of feedback units to discuss and improve. Results: Between January 2015 and December 2015, urinary tract infection is still the most common infection site in the intensive care unit, accounting for 57.14% (4/7), slightly lower than the intervention before the 1.68%. In the case of no significant difference between the date of hospitalization and the previous year, the number of indwelling catheters decreased from 2264 days to 2060 days. CAUTI decreased from 4.05‰ to 1.46‰. Conclusion: The Care bundle is effective in reducing CAUTI while improving health care-related infections and improving patient safety. Enhance the intensive care unit to prevent health-associated infection.

Tigecycline susceptibility of coagulase-negative staphylococci isolates from central Taiwan Background: Tigecycline has a wide range of anti-bacterial spectrum, including methicillin-resistant coagulase-negative staphylococci (CNS). Not much is known about tigecycline resistance in CNS. This study is aimed to exploring tigecycline resistance in CNS from central Taiwan. Methods: Totally 311 isolates of CNS were collected from Central Laboratory of the Central Region Hospital Alliance. The speciation was performed by 16S rRNA gene sequencing. Tigecycline susceptibility was determined using both Vitek 2 and broth microdilution methods. Interpretation was done according to the EUCAST standard. Results: By gene sequencing, the most prevalent three species were found to be Staphylococcus epidermidis (108 isolates), Staphylococcus capitis (68 isolates), and Staphylococcus haemolyticus (38 isolates). The tigecycline resistance rate using Vitek 2 and broth microdilution methods was 6.4% and 1.6%, respectively. The MIC consistency (within one doubling dilution) rate among the 311 isolates was 91.0%. The consistency rates of S. epidermidis, S. capitis, and S. haemolyticus were 90.7%, 88.2%, and 84.2%, respectively. In S. haemolyticus, 9 isolates (23.7%) were interpreted as susceptible using broth microdilution method, however, they were regarded as resistant by Vitek 2 method. Conclusion: Tigecycline resistance toward CNS from central Taiwan was first revealed. The inconsistency MIC using commercial Vitek 2 method and standard broth microdilution method, especially in S. haemolyticus, deserves continuous study. Background: The goal of this performance improvement study was to evaluate the effect of a targeted intervention to decrease catheter-associated urinary tract infections (CAUTIs) and urinary catheter utilization by implementing quality improvement initiatives. Methods: CAUTI rates and urinary catheter utilization between 2013 and 2016 were calculated from 4 adult intensive care units. The intervention study was conducted over a period of 12 months. During a pre-intervention phase, the base line catheter associated CAUTI incidence rates were determined and compared with the incidence rates during the post-intervention phase. The compliance of health care staff with urinary catheter bundle elements was also measured. The implemented CAUTI prevention bundle that included: setting clear expectations for indications for indwelling urinary catheter use (an indwelling catheter policy which included criteria for catheter use); catheter insertion using aseptic technique and sterile equipment; acatheter maintenance; hand hygiene; documentation of catheter placement); daily assessment of the continued need for indwelling urinary catheter by physician; audit of checklists, checking insertion procedures and reviewing with supervisors were done by a multidisciplinary team once a fortnight. Results: During the study period, 108 of the 6650 patients were diagnosed with a CAUTI. There was a statistically significant reduction in the rate of CAUTI in ICUs, from 3.3 per 1,000 catheter-days (95% confidence interval [CI], 1.1-3.1) before the intervention to 2.6 per 1,000 catheter-days (95% CI, 1.1-1.6; P <.05) after the intervention. The urinary catheter use rate decreased from 64.6% (95% CI, 60.9%-73.4%) at baseline to 52.2% (95% CI, 46.3%-58.1%; P = 0.09). The compliance rate of the ICU nurses to the bundle elements was raised to 100% during the last 6 months of the post intervention phase. Conclusion: Increased compliance to recommended catheter associated urinary tract infections preventive practices reduced the incidence of CAUTI in ICU units. It is simple and effective and is recommended as a part of patient safety culture.

Screening and analysis of multi-drug resistant organisms (MDRO) in patients from long-term care facility in Taiwan: experience in a community hospital in southern Taiwan Hsaio-jung Huang 1 , You-yu Shen 1 , Ying-ling Chen 2 , Tzu-Chieh Weng 2 , Han-siong Toh 2 , Ping-Yu Chen 1 . 1 Chi Mei Medical Center, Chiali, 2 Chi Mei Medical Center Background: More than 40% of patients in our hospital are from long-term care facility. These patients have higher possibilities of multi-drug resistant organisms (MDRO) infection. It is commonly detected during admisssion period, and its culture periond is 3∼5 days. In order to early detect these special organism and strain early intervention including contact isolation, we started routine screening of MDRO in our hospital from Jan. 1st to Dec. 31th, 2016. Methods: We assessed all our inpatients on the admission day for the risk of multiple drug-resistant bacteria. For those who from long-term care facility and hospitalized history in six month, we did routine rectal swab with vancomycin-resistant Enterococcus (VRE) and carbapenem-resistant Enterobacteriaceae (CRE) medium and urine culture. We also performed preventive contact isolation on the first day of admission. Laboratory staff received the sample and repored according to 2015 CLSI norms. Background: Both measles and rubella are vaccine-preventable diseases. According to the guideline of Taiwan CDC for healthcare workers (HCWs), we aimed to evaluate the boost efficacy of 1-dose or 2-doses of MMR vaccine among young HCWs. Methods: From Jan. 2016 to Dec. 2016, the elegible individuals included new junior staffs and interns working in the hospital for at least one month. For anti-measles virus-IgG, VIDAS ® Measles IgG test kit (bioMérieux) was used, and the results were read by mini VIDAS ® immunoanalyzer (bioMérieux) and were expressed as negative, equivocal, and positive. For anti-rubella virus-IgG, Elecsys Rubella IgG test kits (Roche) was used on Cobas ® 6000 analyzer (Roche). The results were expressed as non-reactive and reactive. Second or third measurement of serum antibodies were performed at one month after MMR vaccination. At their discretion of the staffs, one or two doses of MMR vaccines at one month interval were freely provided only for the staffs with negative/equivocal or non-reactive results of serum IgG antibodies.

Results: A total of 1,291 and 1,430 individuals were tested for measles and rubella, respectively. At baseline, the positive rate of anti-measles or -rubella IgG was 69.5% or 83.9% in interns, and 57.9% or 94.3% in the staffs, respectively. The number of junior staffs tested requiring vaccination was 418/993 (42.1%) for measles and 65/1,132 (5.7%) for rubella. Among the vaccinated staffs, 225/269 (83.6%) had seroconversion for measles after the first dose vaccine. Among 28 staffs receiving the second vaccine, there were additional seroconversion in 15 (53.6%). Among 19 seronegative staffs vaccined for rubella, 18 (94.7%) had seroconversion after the first vaccine, and among the latter one remained seronegative after the second vaccine. Conclusion: Our results suggested 42% and 6% of the junior staffs need MMR vaccination for inadequate serum IgG titers to measles and rubella virus, and after one dose of MMR vaccine, 84% and 95% of the vaccinees exhibited seroconversion to measles and rubella. To prevent the occurrence of measles or rubella among young hospital staffs during patient care, MMR vaccination should be advised for the seronegative staffs.

The improvement project of scabies infection in a regional teaching hospital Meng-Chuan Lu, Hui-ju Hu, Hsiu-Wen Yu. Infection Control Department, Cheng Ching Hospital

Background: There are three nurses and one caretaker suffered from itchy skin continually between 12 January and 16 January, 2016 in our 3F medical ward. Our infection control department investigated it and found that one nursing staff, one caretaker and one patient were diagnosed as scabies infection, we assumed that they were infected by a patient who was transfered from nursing home. The patient had thick black crust in his two fingers and two lower limbs when he was admitted to our hospital since 1 January, 2016. We consulted the dermatologist, he was diagnosed as subacute dermatitis and contact isolation was implemented thereafter. He was referred to other hospital and then scabies was diagnosed. Methods: The spread of scabies clusters were due to delayed diagnosis and contact isolation (No wearing a protective clothing when contact with the patient, not using of personal medical equipment). The infection control department implemented the following strategies: (1) Organizing the education and training of scabies. (2) Formulate the rules of those patients who were transferred from nursing home should implement contact isolation, and the skin should be checked such as: fingers, palms, wrist, armpit, hip, around the navel, genitourinary parts. If any abnormality are found we should consult the dermatologist. (3) Set up the computer information system to order the contact isolation according to the diagnostic code and medication of scabies. (4) Use personal sphygmomanometer and stethoscope. (5) Make sure cleaning and disinfection of the equipment and environment. Results: Long-term care institutions patients are at the high risk of scabies infection, and the symptoms are often atypical, and caused medical staff easy to ignore the isolation measures and resulted in outbreak, and also increases the risk of infection in medical staff. In order to reduce the spread of scabies, the implementation of longterm care institutions patients with contact isolation on his own initiative and skin assessment is very important, and the setting of information reminding system can also help. Following the implementation of the above measures, tracking the incidence of scabies in the medical ward of the third floor from February 2016 to April 2017, and we found no spread of scabies, showing that the strategy was effective.

Experience report on surveillance of vancomycin-resistant enterococci Lin-Hui Chao 1 *, Ya-Wen Chang 1 , I-Ling Chen 2 , Ing-Kit Lee 3 . Background: Hospitals use many strategies to control nosocomial transmission of vancomycin-resistant enterococci (VRE). Strategies include ""passive surveillance,"" with isolation of patients with known previous or current VRE colonization or infection, and ""active surveillance,"" which uses admission cultures, with subsequent isolation of patients who are found to be colonized with VRE.

To study epidemiologic features of and risk factors for intestinal colonization and infection by vancomycin-resistant enterococci (VRE) in hospitalized patients. Methods: We analyzed surveillance data from 1 January 2016 to 31 December 2016 in a center where VRE is endemic. All clinically significant enterococci were tested for susceptibility to vancomycin. All intensive care unit admissions were screened on admission and weekly thereafter. Interventions included isolating/ cohorting VRE patients, monthly prevalence surveys of VRE patients, the introduction of an electronic alert system, programmes to improve hand and environmental hygiene, and the appointment of an antibiotic Physician. Results: In the sample of 264 patients investigated, 11.7% of patients had positive VRE while in the performance of periodic surveillance directed at patients at risk (193 cases), 8.08% showed positive result for the VRE. 11.0% out of 161 VRE colonized patients developed BSI by VRE. 43.9% had central venous catheter inserted as well. Most patients had one or more of the known risk factors associated with colonization with vancomycin-resistant enterococci. 94.7% of the patients received antibiotics during the study, and 15.2% received vancomycin. Conclusions: Colonization by VRE was mostly endemic and the colonized patients were not often infected. However, when clustered cases of colonization occurred, they were then associated with an increased rate of infection. Routine surveillance for VRE together with other measures can control VRE infection and colonization, even where VRE is endemic, and where facilities are constrained. Evidence of colonization of patient's means it can be empirical precaution and thus provide patient safety by minimizing the risk of nosocomial infection.

Influenza A outbreak investigation and control measures of regional teaching hospital Chun-Hua Chung 1 , Ya-Chi Li 1 , Ya-Fang Wang 1 , Yung-Ching Liu 2 , Wen-Chuan Lin 1,3 . 1 Infection Control office, 2 Division of infectious Diseases, Department of Internal Medicine, 3 Division of Pediatric Infectious Diseases, Department of Pediatrics, Taipei Medical University Background: In November year 2016, an outbreak caused by influenza A occurred in the nursing home of regional teaching hospital. Ninety residents of a total of Twenty four residents of respiratory symptoms, with fever and cough. Clinical attack rate was 26.7%. Methods: Among 10 residents with flu symptoms, eight were laboratory-confirmed influenza A with rapid antigen testing. Influenza virus chain reaction with polymerase nucleic acid detection (Reverse transcription polymerase chain reaction, RT-PCR) and virus isolation and culture. At the same time the application of epidemiological methods: to investigate, to collect suspected cases, to determine the clinical epidemiological data for analysis and monitoring. Results: Influenza A H3 was confirmed among 4 patients, only one was confirmed with swine H1N1. Confirmed that the pathogenesis of respiratory tract infection incident is Influenza A virus. Immediately take care of patients with strict infection control measures, the epidemic soon to be controlled. Conclusion: Strict infection control measures were applied including isolation of infective resident, early rapid test for those with flu symptoms, using mask for all our staff and family members and reinforce hand washing and environmental cleansing. Effective chemoprophylaxis was given for all staff. To lessen the impact of this infection, very strict infection control measures should be done. No more new case reported by 9th November.

Intervention programs for healthcare-associated infections control in a teaching hospital in Taiwan (from 2011 to 2016) Frank Lien 1 , Yu-Hong Tsai 1 , Wan-Ling Tai 2 , Lin-Lin Pan 1 *. Background: According to WHO's report, healthcare associated infections (HAIs) is the most frequent adverse event in healthcare. Previous studies also showed HAIs results in prolonged hospital stays, increased medical cost, and leads to significant mortality. Methods: Cases meeting the CDC's criteria of HAIs are categorized and included into surveillance system. Interventions for infection control was implemented including HAI related staff education, internal auditing and antibiotic stewardship program. Of all infection control methods, hand hygiene remains one of the most fundamental and critical part, thus hand hygiene compliance rate was set as threshold and monitored for improvement. Data of HAIs and internal auditing of hand hygiene at Chiayi Chang-Gung Memorial Hospital, a regional teaching hospital of southern Taiwan, between 2011 and 2016 was collected and analyzed. Pearson correlation analysis was used to investigate the correlation between hand hygiene compliance rate, and overall and ICUs' HAIs incidence per thousand inpatient-days. Linear regression method was adopted to reveal the trend of the HAIs' rate. Trend analysis is based on the slope of regression trend lines. Results: Slopes for overall inpatient and ICU inpatient HAIs are −0.018 (R 2 = 0.65) and −0.201(R 2 = 0.72), respectively. Results show over the period of time (from 2011 to 2016), ICUs HAIs has declined markedly. During 2016, the overall and ICU HAIs per thousand patients-day are 1.38‰ and 4.52‰, respectively. Using our linear regression model, the average of first 4 months of 2017, overall and ICU HAI rates will be approximately 1.37‰ and 4.45‰, showing large portion of HAI reduction will be contributed by ICUs. Preliminary data of first 4 months of 2017 showed, overall and ICU HAI rates are 1.43‰ and 3.43‰, respectively. Such result showed HAI rate has an overall slight increase (0.05‰) and a markedly decrease in ICU (1.02‰) in the first four month of 2017, which is compatible to our assumption based on prediction model. Pearson correlation showed hand hygiene compliance rate has significantly strong correlation with ICU HAI rate (r = −0.843, p = 0.035), but not with overall HAI rate (r = −0.532, p = 0.277). Background: Blood culture contamination (BCC) results in not only increased workload on lab staffs but overuse and abuse of antibiotics, which causes a raised cost, prolong hospital stay, and invalid treatment. Therefore, decreasing BCC rate is very important. During 2015-2016, ICU in a regional teaching hospital in midland of Taiwan showed average BCC rate was 14.9-18.1%, much higher than threshold value (3%). The infection control team had inspected the procedure of blood sample collection and found 3 improper operation procedures as the followings: (1) hand hygiene, to (4.17∼4.76%). Conclusion: Although the BCC improvement program did not reduce the BCC rate to be below 3%, yet the use of 2% CHG to execute skin disinfection did decrease the BCC rate, shorten the waiting time of skin disinfection, increase working efficiency of medical staffs, and elevate quality of healthcare.

Improve the quality of Ventilator associated pneumonia bundle unit by means of team cooperation Hsiao-Shan Chen 1 , Hung-Jen Tang 1,2,3 , Iig-Ling Chen 1 *. Background: With the innovation and progress of medical technology, invasive devices are also important factors related to infection. In the intensive care unit, respirator-associated pneumonia is one of the leading causes of death. Reference to the Taiwan Clinical Performance Indicator (TCPI), The incidence of respiratorassociated pneumonia in the intensive care unit was slightly higher than that of peers. Combined with the prevention of respiratoryrelated pneumonia infection combined care. Improve the quality of combined care and reduce the rate of respiratory-related pneumonia infection in a team-based mode of cooperation.

Methods: The hospital in June 2015 to December 2016, In the intensive care unit to use the prevention of breathing apparatusrelated pneumonia combined care -checklist a total of five projects:

(1) Daily out of respirator assessment, (2) Daily suspension of sedatives, (3) At least 2 times a day with 0.2% chlorhexidine gluconate gel for oral care, (4) Bed head raised 30-45°, (5) Drain the ventilator pipe apply to patients who use respirators for clinical care.

Results: The correct rate of combination of respirator-associated pneumonia infections between June 2015 and March 2016 is increasing, from 75% to 100%, infection rate decreased by about 43%. After the implementation of the respiratory tract although the number of days is not reduced, But the number of cases of infection was reduced from an average of 2.25 cases per quarter to an average of 1 per quarter (22% reduction), after the implementation of VAP Bundle, about 8 cases of VAP were reduced. It can be seen that the correct rate of combination of respirator-associated pneumonia infection is associated with a decrease in respiratory rate.

Conclusion: ""VAP Bundle implementation"" of the team to establish the need for cross-departmental staff common participation and cooperation, to educate people to correct the correct training, establishment of staff attention and identity, and then willing to put into action. Regularly provide the implementation of respirator-associated pneumonia infection combination of correct rate and respiratory-related pneumonia infection rate and other relevant information, feedback to the implementation unit for continuity review.

Effect of the implemention of multidimentional infection control measures on healthcare associated infections in an intensive care unit Ya-Ling Lee, Chiung-Hui Ho, Tzu-Ting Kuo. Infection Control Office, Da Chien General Hospital

Background: There was an outbreak of carbapenem resistant Klebsiella pneumoinae (CRKP) in our intensive care unit (ICU) for that multi-faceted interventions were implemented in two stages since 2015. We hypothesized that the infection control response may have had a secondary benefit of reducing incidence of healthcare associated infections (HAIs). Methods: We implemented first stage of infection control measures since October 2015 including complete terminal disinfection and cohort isolation of patients with CRKP or CRPA or VRE. Second stage of interventions started since April 2016 and comprised applying chlorhexidine (CHG) bathing to patients in ICU as well as finding and disinfection or eradication of niduses from which MDRO were cultured. The items of niduses discovered were feeding cups and plastic baskets belonging to individual bed but not individual patient, working tables of nurses, and two communal bedpans. We analyzed data retrospectively from a prospective infection control database since May 2015 at the ICU of 41 beds and calculated the monthly trends of HAIs before and after two stages of infection control measures in ICU after adjusting the effects of hand hygiene and antibiotics usage. In order to assess the changes of HAIs in 3 periods, we used the generalized additive model (GAM). The fixed effects were period, time ( per 1 month), DID, the compliance and the accuracy of hand washing, and the random effect was period*time. The random effect showed the change of HAIs after interventions. Results: After implementation, the trend of UTI showed reversely downward although the trend of HAP was as same as before ( Figure) . There was a significant reduction trend in UTI after second stage of interventions (RR 0.48, 95%CI 0.23-0.99) (Table) . Results: There were 53 patients infected with CRE, whose specimens including sputum (n = 18), urine (n = 13), wound (n = 9), ascites (n = 2), line (n = 2), bile (n = 1), and blood (n = 8 The group had 19% non-compliance in hand microbial counts. 15% of the acceptable group had microbial load close to the upper limit (>10 6 ). Observation revealed they all did not have thorough scrubbing during HH but most of them achieved 90% or above reduction of bacterial load after using sanitizer. All participants used <40 seconds in hand washing (WHO recommends 40-60 seconds). Over 50% reported washing their hands only when observable dirt was found. Cohort at the most crowded location had the highest microbial count per individual but lowest incidence and counts of Staphylococcus aureus. MRSA isolates were found in 4 participants before HH but they were all removed after use of sanitizer. Coliform counts were compliant. Yeast and mould count was relatively low. 80% of them claimed that their employment agencies provided hygiene training. Discussion revealed that they all welcome further enhancement in hygiene knowledge and on the job training. Conclusion: Domestic helpers require enhanced supports on hygiene training. Special attention should be given on infection control. Carriers could have the risk of spreading MRSA/resistant species in the community. On-the-job training for Domestic helpers is desirable.

Improve the quality of self-made ice consumption of the number of bacteria to improve the quality of the project Yi-Chia Chen 1 *, Yu-Hsiu Lin 1 , Yee-Huang Ku 1 , Jyh-Jou Chen 2 .

1 Infection Control Center, 2 Intensive Care Department of Medicine

Background: The hospital has a Western restaurant and Chi Mei coffee food sales, all use self-made edible ice. October 2014 to January 2016 Chi Mei coffee homemade edible ice number of bacteria were more than the standard test, in addition to the temporary suspension of the use of the number of additional raw bacteria to meet the ice, but also continue to explore the reasons, but the improvement measures are invalid. Methods: March 2016 by the nutrition department, infection control center, safety and health room, public affairs room, jade meal company to improve the project team, through the ""characteristics of the map"" and ""true verification"" and other quality management practices and tools, establish five issues and set the target pass rate of 100%, and to develop improvement measures (1) the development of equipment, equipment, cleaning and disinfection norms, check the mechanism and training methods, (2) formulate the standard of inspection and operation standards, check the mechanism and training methods, (3) formulate inspection standard operating rules, check the mechanism and training methods, 4. update of water quality filtration equipment, through the above four measures to improve. Results: Since March 2016 intervention to improve, until September 2016 pass rate of 0%. But completed the development of three standard operating manual, and continued in accordance with the standard implementation of the implementation process, October 2016 to November 2016 to achieve self-made consumption of ice 100% pass rate target. But, From December 2016 to January 2017, the number of bacteria and more than the standard(100 cfu/ mL), may be due to lack of ice machine pipeline sterilization equipment and ice machine surrounding environment pollution. So the development of countermeasures: (1) Regular cleaning of the ice machine's surroundings (once a month) and (2) ice machine lines use ozone (once a month) and disinfectant (once a quarter) sterilization, Finally, in February 2017 to April 2017 monitoring results pass rate of 100%.

The hospital provides safe food for the staff and the public, use teamwork to develop a standard operating process and check for the extent to which the problem and the staff can be matched. Therefore, we will continue to monitor the number of bacteria, and to assess the increase in E. coli flora test, to ensure food safety and hygiene.

To explore the practical utility of chlorhexidine gel in clinical care unit Shu-Yuan Kuo 1 *, Yu-Hsiu Lin 1 , Yee Huangku 1 , Pak-On Leung 2 , Mei-Fe Sie 3 , Jyh-Jou Chen 4 Conclusion: We concluded that in addition to early ART, safe sex education should be enhanced in HIV prevention campaign.

Evidence-based assessment of mortality of patients infected with carbapenem-resistant Klebsiella pneumoniae (CRKP)

Background: Carbapenem resistant K. pneumoniae (CRKP) has aroused widespread attention due to its very limited therapeutic options, and in recent years this strain has increased rapidly.

Though it seems inferable that drug resistance is associated with higher mortality in general, but some other studies found no such relationship. We utilize EBM methods to obtain and critically appraised best evidence to estimate mortality of patients infected with CRKP in general and analyze factors for mortality of this infection, for awaking to awareness of infection control and working out preventive strategies.

Methods: The terms: Carbapenem, Klebsiella pneumoniae, CRKP, motality, and associated synonyms were combined with Boolean characters (AND, OR) to formulate as keyword to search Cochrane Library, PubMed and C.E.P.S databases. No relevant systematic review (SR) or trials were found in Cochrane and C.E.P.; In PubMed, Three SRs were filtered out of 126 searching results. The titles and abstracts of 3 SRs were subsequently reviewed and one latest 2017 SR/meta-analysis was selected for critical appraisal. We analyzed data retrospectively from a prospective infection control database since May 2015 at the ICU of 41 beds and calculated the monthly trends of MDROs before and after two stages of infection control measures in ICU after adjusting the effects of hand hygiene and antibiotics usage. In order to assess the changes of MDROs in 3 periods, we used the generalized additive model (GAM). The fixed effects were period, time ( per 1 month), DID, the compliance and the accuracy of hand washing, and the random effect was period*time. The random effect showed the change of MDROs after interventions.

Results: The changes of the four MDROs were subtle in period 1 (baseline, before interventions) and still non-significant in periods after interventions (Table) . The fixed effects of hand hygiene and DID were either non-significant. Conclusion: Although our CRKP outbreak was controlled by these infection control measures, the trends of previously stationary MDROs did not change as well.

Pulmonary tuberculosis in cancer patients at a medical center in northern Taiwan Chi-Lan Kao 1 , Yen-Ping Tsai 2 , Hsin-Yen Chen 1 , Wan-Tsui Huang 1,3 . Background: Pulmonary tuberculosis and cancer are two major public health problems. The association between pulmonary tuberculosis and cancer is interesting and distinct. Cancer is a known risk factor for the development of active tuberculosis. This retrospective study aims to investigate the characteristics of active tuberculosis in cancer at a medical center.

Methods: Data of patients with pulmonary tuberculosis were retrieved from the electronic medical records from January 2012 to December 2016. Pulmonary tuberculosis documented with microbiologically Mycobacterium tuberculosis infection was defined when sputum were found to be positive by culture or polymerase chain reaction. The study was emphasis on patient characteristics, cancer type, and the development of pulmonary tuberculosis in cancer patients.

Results: There were 29 cancer patients with pulmonary tuberculosis during the period studied. Twenty-one patients (72.4%) were men. The median age was 68 years (range, 47-92 years). Twenty patients had one solid-organ tumors. The most common site for underlying malignancy was lung (6 patients, 20.7%). Two patients had two kinds of solid-organ tumors. One patient had gastric cancer, oral and tongue cancer, and acute myeloid leukemia. Four patients had B-cell lymphoma. One patient had T-cell lymphoma. One patient had multiple myeloma. The pulmonary tuberculosis was developed in 5 patients (17.2%) within 30 days of the diagnosis of cancer. Pulmonary tuberculosis was developed during chemotherapy in only 3 patients (10.3%).

Conclusion: As a result, physicians should be aware of the risk of developing pulmonary tuberculosis in cancer patients.

investigate the influence of antimicrobial prophylaxis in surgery and effect of probiotics on the gut microbiome. Methods: The patients who underwent spine surgery between July and December 2016 were included in this study. Twenty patients were classified into two groups according to permuted block method: control (n = 10) and postoperative administration of drugresistant lactic acid bacteria for 5 days (treatment group, n = 10). The patients received surgery and antimicrobial prophylaxis as usual. The feces of the patients were sampled at preoperation, postoperative day (POD) 5 and 10. The gut microbiome were analyzed by using Ion-PGM system and CLC genomics workbench. Results: There were no significant differences between control and treatment groups in the basic characteristics, such as age, gender, underlying diseases and surgical site. The antimicrobial agents at perioperative period in the control group were cefazolin (CEZ) (n = 8), clindamycin (n = 1) and combination of CEZ and vancomycin (VAN). those in treatment group were CEZ (n = 8) and combination of CEZ and VAN (n = 2). There were no significant differences in antimicrobial agents and administration period between two groups. In the control group, two patients showed soft stool at POD5. The mean number of (operational taxonomic units) OTUs at preoperation, POD5 and 10 in the control group were 487, 370 and 418, respectively (Fig. 1) . The number of OTUs in the control group was significantly decreased in POD5, compared with preoperation (P < 0.05). The mean number of OTUs in the treatment group were 490, 423 and 439, respectively ( Figure 1 ). The changes of the gut microbiome were showed in Figure 2 . In PERMANOVA analysis (weighted UniFrac), the composition of the gut microbiome in POD10 was significantly different between both groups ( p = 0.012). Conclusion: The gut microbiome was influenced by the antimicrobial prophylaxis in surgery, and the probiotics might reduce the influence.

Evidence-based efficacy of antibiotic prophylaxis in patients undergoing elective laparoscopic cholecystectomy Yi-Ching Huang 1 , Yuan-Hsin Chu 2 . 1 Division of Infectious Disease, Jen-Ai Hospital, 2 Department of Infection Control, Jen-Ai Hospital

Background: Elective laparoscopic cholecystectomy may result in surgical site infections (SSI), which may increase length of hospital stay and healthcare costs. Antibiotic prophylaxis in the form of a single dose of cephalosporin is still administered before the skin incision in elective laparoscopic cholecystectomy in many clinical settings. There has been a controversy over the use of routine antibiotic prophylaxis in patients undergoing elective laparoscopic cholecystectomy. We apply EBM methods to search systematically analyzed evidence to evaluate the safety and efficacy of routine antibiotic prophylaxis in patients undergoing elective laparoscopic cholecystectomy.

Methods: The terms elective laparoscopic cholecystectomy, Antibiotic prophylaxis, surgical site infections and the synonyms were used to formulate a PICO and were engaged as keywords to search Cochrane Library, and PubMed to filter systematic review (SR) in priority. One relevant 2016 SR (and meta-analysis) was included out of 33 searching results in PubMed afterward for critical appraisal.

Results: The SR included 21 RCTs (5207 patients). Antibiotic prophylaxis decreased the incidence of surgical site infections [risk ratio (RR) 0.61, 95% confidence interval [CI] 0.45 to 0.82, P = 0.001) and global infections (RR 0.55, 95% CI 0.38 to 0.79, P = 0.001) during hospitalization or after discharge, and postoperative length of hospital stay (weighted mean difference −0.16 (95% CI −0.28 to −0.04, P = 0.008). No adverse events were reported. Subgroup analyses revealed that two doses of antibiotic and 3-10 doses of antibiotic significantly reduced the incidence of surgical site infections compared with placebo or no antibiotics (two doses: RR 0.16, 95% CI 0.06-0.47; 3-10 doses: RR 0.46, 95% CI 0.27-0.80), while a single dose of antibiotic administration did not.

Conclusion: Antibiotic prophylaxis is safe and effective in decreasing the incidence of surgical site infections and global infections during hospitalization or after discharge, and postoperative length of hospital stay in low-risk patients undergoing elective laparoscopic cholecystectomy. Two or more doses of antibiotic prophylaxis should be recommended for such patients (Level 1).

To reduce the incidence of upper respiratory tract infection in psychiatric patients in a regional teaching hospital experience 

(ICN) and timely reminders were given. 4. We cohort care the patients with upper respiratory tract infection. 5. We increased the frequency of inter-patient environment cleaning including bed, dining table, chair. After these interventions, this event was stop on June 24 th . Conclusion: Since the influenza is transmission by droplet and contact especially in psychiatric patients. They always share their food or can't control themselves. So the healthcare workers need more alertness to patients and inform the message to infection section immediately.

Prognostic factors of healthcare-associated bloodstream infection in adult patients older than 40 years Hsuan-Yin Ma 1 , I-Chen Hung 2 , Ya-Huei Huang 2 , Ying-Ying Chang 2 , Wang-Huei Sheng 2,3 , Jann-Tay Wang 2,3 , Wei-Chu Chie 4,5 , Jen-pei Liu 6 , Yee-Chun Chen 2,3 . 1 Derchen Women and Children Hospital, Kaoshiung, Taiwan, 2 Infection Control Center, National Taiwan University Hospital, Taipei, Taiwan, 3 Department of Internal Medicine, National Taiwan University Hospital, Taipei, Taiwan, 4 Department of Family Medicine, National Taiwan University Hospital, Taipei, Taiwan, 5 School of Public Health, National Taiwan University, Taipei, Taiwan, 6 Department of Agronomy, National Taiwan University, Taipei, Taiwan

Background: Healthcare-associated bloodstream infection (HABSI) may worsen clinical outcomes among hospitalized patients, especially in geriatric population. We aim to known the characteristic and search the prognostic factors regarding HABSI of geriatric patients. Methods: The demographic characteristics, comorbids, laboratory data and 30-day mortality after HABSI from electronic chart records between January and December 2014 at a medical center were reviewed retrospectively. Differences of demographic characteristics and outcomes between the middle aged (40-64 year) and geriatric population (≧65 years) were performed. Prognostic factors associated with 30-day mortality were identified through multiple logistic regression. 

Investigation on the needle and sharp injuries of healthcare workers at a regional teaching hospital in Taiwan Chien Chen Wang. Chiayi Chang Gung Memorial Hospital, Chiayi, Taiwan.

Background: Needle and sharp injuries is the most common occupational injury in the hospital, hepatitis B virus (HBV), hepatitis C virus (HCV), syphilis, human immunodeficiency virus (HIV) is a common cause of occupational exposure to health care workers, Infection can cause physical and psychological discomfort. This article explores the incidence of needle and sharp injuries in a hospital and the investigation and analysis after the implementation of the safety needle.

Methods: The hospital have standard for the Staff were injured by the needle and sharp. The process includes: (1) wound treatment, Conclusion: The promotion of safety needles belongs to Taiwan's legislative policy, and provides 105 years to be fully used safety needle. We investigated the behavior of the medical staff in 2016 and the types of items, obviously the use of safety needles does reduce the incidence of scraps (reduced by 19%). The only safety needle is not the only needle, It also contains special needles for inspection, and the promotion of safety tools also includes changes in the use of habits, will also affect the occurrence of acupuncture. It is still the industry for the special safety needle research and development and medical staff habits, period can reduce the incidence of needle and sharp injuries. Background: The hospital will begin in 2015 to execute ""invasive medical treatment care quality upgrade program"" from the Ministry of Health and Welfare Department of Disease Control Agency. The project is needed to complete the catheter placement checklist and daily care table, which is a big burden for the clinical staff. However, comparing of before and after the implementation of the project, arrangements for staff to fill out the cognitive questionnaire, it was found that the implementation of CAUTI Bundle for the patient is meaningful and recognized, but if the form of information can reduce the burden of paper work, can increase the implementation of combined care of health care workers to implement the wishes. Methods: In October 2016, the CAUTI Bundle form was compiled into the participating units. When nurses added pipelines to the familiar piping system, they set up the table and brought the basic information into the future. In addition, when the doctor opened the urinary catheter surgeon, that is, start the placement of the table to fill the reasons for the placement. In the future when opening of the doctor's advice, then jumping out of the doctor's account and checking the table, you can immediately know the number of days placed, and to assess the reasons for indwelling, a substantial reduction in clinical staff to fill out the time of the paper form, and missed the fill or assessment of the phenomenon. In addition, with the project to give the quarterly report, the information program can be transferred out of materials of physician and cares, significantly reduce the quarterly report of the typing time, and paper consumption.

Results: In September, 2016, the hospital started to promote in every unit of departments, from October to December to participating unit began to fill the form, the form completion rate of the doctor was about 20%, while the rate of nursing staff was about 90%. In January 2017, the hospital began to promote (except pediatric), and to the department, the units of education and training, and in the competent meeting of the propaganda, and continue to review and improve the project. As of May 2017, the physician form completion rate has been as high as 70%, caregiver form completion rate of 98% or more, a substantial increase in completion rate. Conclusion: Combination care is not difficult for health care workers, the difficulty is the time spent to fill out the form, and the time of find physician to fill the form. The effective use of information programs can indeed increase the implementation of medical care for health care workers.

Scoring systems for identifying hospitalized critical patients at risk of imipenem-resistant gram-negative bacilli septicemia at sepsis onset I -Ling Chen 1 , Cing-Ling Tai Background: The emergence of carbapenem resistance among Gram-negative bacilli (GNB) has been increasingly reported worldwide and is a matter of great concern, since it complicates both empirical and guided treatment for severe critical ill patients.

Inappropriate administration of empirical antibacterial may increase the mortality rate of septic patients. We aimed to derive a clinical prediction score to predict the risk of acquisition of imipenem-resistance (IR) GNB septicemia.

Methods: This retrospective cohort study enrolled all patients suffered from GNB bacteremia 48 hours after admission to Kaohsiung Chang Gung Memorial Hospital (KCGMH), a 2700 bed tertiary-care medical centre in southern Taiwan between January 1, 2013 and December 31, 2015. Adjusted analyses were performed by multivariate analysis and propensity score-based matching. Risk factors for IR-GNB septicemia were evaluated by multivariate logistic regression with backward selection (<0.05) and a novel prediction score (IR-GNB septicemia) was derived.

Results: Three hundred and seventy episodes were included, after controlling for age, Charlson index and sex. The following independent risk factors were obtained by a multivariable logistic regression analysis: prior extended-spectrum cephalosporins exposure days ( Youden's index statistics indicated a score of > 10 as best cutoff value that had a sensitivity of 78% and specificity of 73%.

Conclusion: This score might help clinicians to stratify the risk of IR-GNB septicemia infection when deciding empirical therapy for these patients at sepsis onset.

The effectiveness of application information technology to improve tuberculosis phase assessment Ya-Jhen Ji 1 *, Yueh-Ying Tsai 2 . 1 Yunlin Christian hospital of Tuberculosis case manager, 2 Yunlin Christian hospital of Supervise of Nursing Department

Background: The medical service followed the update progress with the development of information technology. The rate of tuberculosis assessment at all stages was 33-45% in a regional hospital in 2010. It may lead to treatment failure and drug resistance occured if the patient was not assessed by chest X-ray examination and bacteriological testing on time. The outpatient message list was used to remind the physician to take the chest Xray examination and the bacteriological test in 2013. But, the rate of each tuberculosis phase assessment was about 73-76%. Therefore, we hoped that the rate of achievement will be improved by application of information technology. Methods: We developed electronic message board in the medical information system in order to remind the physician to order chest X-ray examination and bacteriological test in the outpatient medical system before the prescription in 2014. Each stage of continuous improvement was up to 93-96% in 2015. We applied mobile phone Line to contact the public health care division to communicate in August 2015. So, the health care division can enhance the case to remind the sputum test and side effects of medication.

Results: The rate of tuberculosis assessment of phase three was 97% in 2016. Other phase was up to 100%. It can really improve the continuity of tuberculosis assessment using the electronic message board and Line information software. The income of tuberculosis stage continuous assessment of Health insurance payment was 154,000 NTD in 2010, we lose 92500 NTD because the low rate continuous assessment. The health insurance payment rose up to 138,000 NTD in 2016. The estimated amount of loss was 500 NTD and the amount of loss has been reduced significantly.

Conclusion: The application of information technology can improve the continuity of stage assessment to remind patients to perform chest X-ray examination and bacteriological testing. However, the case take medicine regular although, but there are treatment failure occurred such as three case CXR deterioration and the sputum smear turned positive in four case before the end of treatment. Medical staff should pay attention to precaution the Line leaked personal information. Results: Whole data from our HIS database, including surgical scheduling system, medical system, birth notification system, anesthesiology system, operating room management system, and nursing operation system. And booking surgical scheduling was the key point to start the SSI system. We created three table into HIS to increase the data collection integrity; we found that the manager support was the key point that provide this system useful.

According to this study result, the cover ratio of SSI data was from 0% (July 2016) to 96.6% (October 2016), 100% (January 2017). And 3 surgical data can automatically. Supervisor support and practice of medical personnel are indeed the important antecedents of system quality, information quality and service quality. Service quality of IT department has positive significant influences on system quality and information quality. Conclusion: Through this experience to prove that the integration of medical information system, can reduce the burden on clinical staff, automatically bring the information to avoid multiple input caused by inconsistencies. genomes, the T. pallidum Amoy strain owns eight strain-specific genes, however, miss ten universal genes in other genomes. For instance, the Amoy strain loss some tpr family genes, which likely answer for the increasing immune escape. Interestingly, we also identified the 23S rRNA A2059G mutation in the Amoy stain, which is the first report outside of its initial identification in Czech Republic and is different from the frequent A2058G mutation identified in China.

Conclusion: In summary, we sequenced the whole genome of T. pallidum Amoy stain in this study. This work will aid our better understanding of the different pathogenesis of syphilis and as well the geographical distribution of syphilis epidemiology. phenotypic and MALDI-TOF MS identification methods were compared for their performance in species identification. Methods: The fungal isolates were rechecked to certify their authentication by mycology methodologies and then characterized by ITS gene sequencing. Then we evaluated D1/D2 regions sequencing, SCGE and two MALDI-TOF MS systems (Vitek MS and Bruker MS) and Vitek 2 Compact system in C. haemulonii complex identification. A susceptibility assay was performed using the Sensititre YeastOne TM YO10 methodology following the manufacturer's instructions.

Results: Twelve of 13 (92.3%) isolates were identified as C. nivariensis and one as C. bracarensis using ITS sequencing as the reference method; results obtained by D1/D2 sequencing and ITS SCGE were concordant with ITS sequencing results for all (100%) isolates. SCGE was able to subtype 12 C. nivariensis into four ITS SCGE length types (LTs). All isolates failed to be identified by the Vitek MALDI-TOF MS system (bioMérieux), whilst the Bruker MS system (Bruker Daltoniks) correctly identified all C. nivariensis isolates but using a lowered (≥1.700) cut-off score for species assignment; the C. bracarensis isolate was identified but with score <1.700. The Vitek 2 Compact system could not identify 11 C. nivariensis and one C. bracarensis isolate and misidentified the remaining C. nivarensis strain as ""C. glabrata"". All isolates had low MICs for azoles, echinocandins, amphotericin B and 5-flucytosine. Conclusion: Our results reinforce the need for molecular differentiation of species of C. nivarensis and C. bracarensis. The performance of MALDI-TOF may be improved by adding mass spectral profiles (MSPs) into the current databases. The antifungal susceptibility profile of isolates should be monitored. More researches are needed to study its effectiveness and its role as first-line therapy. 

Clinical manifestations of candidemia caused by uncommon Candida species and antifungal susceptibility of the isolates in a regional hospital in Taiwan Background: This retrospective study investigated clinical manifestations of candidemia caused by uncommon Candida species and antifungal susceptibility of the isolates in a regional hospital in Taiwan.

Methods: The uncommon Candida species was initially defined as Candida species other than C. albicans, C. tropicalis, C. glabrata complex, C. parapsilosis complex and C. krusei. All uncommon Candida isolates were identified and confirmed by molecular methods. In vitro susceptibility testing of the uncommon Candida species to nine antifungal agents was conducted using the broth microdilution method with the Sensititre YeastOne (SYO) system (Trek Diagnostic Systems, Ltd., East Grimstead, UK).

Results: Twenty-one patients, comprising 11 males and 10 females with a median age of 69 years, were recruited. Cancer (n = 11) was the most common underlying disease, 19 (90.5%) cases had prior antibiotic exposure, and only two patients had prior antifungal use. The overall in-hospital mortality rate was 38.1%. C. guilliermondii (n = 11) was the most common pathogen, followed by C. curvata (n = 3). C. guilliermondii isolates exhibited relatively high rates of azole minimum inhibitory concentrations (MICs) above epidemiological cut-off values (ECVs), whereas C. pelliculosa and C. lusitaniae isolates all remained susceptible to azoles. All three C. curvata isolates had high caspofungin (>8 mg/L) and fluconazole MICs (8 mg/L) and could be defined as multidrug-resistant. Conclusion: Uncommon Candida species frequently exhibit high rates of non-susceptibility to antifungals. Identification of all Candida isolates at the species level from blood samples is of value for treatment.

The incidences and outcomes of cryptococcosis among different risk populations in Taiwan Background: Invasive fungal diseases are life-threatening infections in prolonged neutropenic patients with hematologic malignancies who are treated with intensive cytotoxic chemotherapy. Although liposomal amphotericin B (L-AmB) or an echinocandin class agent is administered as an emperical antifungal therapy in such patients, adverse events including breakthrough infections despite treatment and substantial drug toxicity have been becoming significant problems. There is therefore a critical need for antifungal treatment strategies to overcome these impediments.

We conducted a single-arm, multicenter (5 institutions), prospective study in order to evaluate the efficacy and safety of sequential administration of L-AmB after micafungin (MF) as an empirical antifungal therapy for antibiotics-refractory febrile neutropenia in patients with hematologic malignancies who received cytotoxic chemotherapy.

Results: A total of 18 adult patients were enrolled, and 12 patients who fulfilled the protocol defined criteria were evaluated. Underlying diseases consisted of acute leukemia (n = 10), malignant lymphoma (n = 1), and myelodysplastic syndrome (n = 1). Prior antifungal prophylaxis was given in 9 patients. Treatment success was achieved in 8 patients (66.7%). None of the patients showed pulmonary lesions in the radiological imaging or developed breakthrough infections. Serum galactomannan antigen increased in one patient, and plasma (1-3)-β-D glucan test was positive in 2 patients, but these tests improved in any of these cases. Treatment failure was observed in 4 patients. Among them, 3 patients discontinued the study drug due to adverse events. These events were drug-related toxicities such as hepatotoxicity, nephrotoxicity, hypokalemia, skin eruption, and muscle stiffness, but all of them were below grade 2 toxicity, and were reversible after finishing the drug. One patient revealed a persistent fever probably due to cytomegalovirus infection, which subsided by ganciclovir administration.

Conclusion: Although the number of patients studied was limited, sequential therapy with MF and L-AmB seems to be an encouraging antifungal treatment strategy for those with hematologic malignancies who experience antibiotics-refractory neutropenia after cytotoxic chemotherapy. Further investigation in large-scale studies are needed.

Background: Azole-resistant Aspergillus fumigatus (ARAF) has become a growing public health concern. International consensus suggests that empirical therapy for A. fumigatus aspergillosis should be guided by the local clinical and environmetal resistance rate. This study investiagted the current status of aozle resistance among A. fumigatus clinical isolates in Taiwan Background: Candidemia has become an emerging healthcareassociated infection with significant mortality, and Candida tropicalis is a common pathogen in Asia. The present study aimed to identify specific biomarkers to predict patient survival by metabolomics.

Methods: Patients aged >20 with candidemia caused by Candida tropicalis with blood samples available within 4 days of the onset (Day 0) of candidemia were included for analysis. They were categorized into two groups depending on their survival at Day 14.

Metabolomic profiles of the blood specimens collected at Day 0-4 of survival and death groups were compared to identify specific biomarkers to predict patient survival at Day 14. The partial least square-discriminate analysis (PLS-DA) were performed, and the scores of the first two components were plotted to visualize how well the relevant metabolites could identify the death and survival groups. Statistical differences between death and survival groups were compared by Wilcoxon rank sum test. Results: A total of 31 patients with C. tropicalis candidemia were included. The median interval from the candidemia onset to the collection of blood specimens was 3 days (interquartile range 1-3.5 days), and 11 patients (35%) received antifungal agents before blood sampling. Day 14 mortality was 29% (9/31). The PLS-DA scores plot separated the survival and death groups (Figure 1 ). The identified potential markers to predict day 14 mortality were phenylalanine, deuteroporphyrin IX, and hippurate. The area under the curve (AUC) and coefficient of logistic regression model, and results of Wilcoxon rank sum test of the three metabolites are shown in Table 1 The age-stratified uptake rates of once-weekly rifapentine plus isoniazid.

The rate for the permanent discontinuation due to 3HP intolerance was similar to 9H. However, overall, patients with 3HP experienced significantly lower rates for the permanent discontinuation of treatment compared with 9H. Further evaluation of factors facilitating treatment completion of LTBI will be crucial to achieving our End TB goal, reducing incidence to 32/ 100,000 by 2020.

Surveillance of severe community-acquired pneumonia of unknown etiologyresult from unknown pathogen detection/ investigation group (UPDIG), Taiwan Tsung-Pei Tsou 1 *, Wan-Chin Chen 2 , Min-Nan Hung 3 , Sung-Hsi Wei 4,5 , Hsin-Chun Lee 3 , Chia-Ping Su 2 , Jung-Jung Mu 6 , Ming-Tsan Liu 6 . Patients with pathogen identified by isolation or PCR from respiratory tract specimens or blood, or those with seroconversion will be classified as having a definite etiology. Patients with pathogen identified by means of antigen detection will be classified as having a possible etiology. Background: Staphylococcus aureus is an important human pathogen that can also asymptomatically colonize the nares of between 20% and 40% of the adult population. Methicillin-resistant strains (MRSA,) which may also display resistance to multiple other drugs, are a major cause of healthcare associated infections worldwide. There is increasing concern that strains of S. aureus in the clinical setting can also harbour genes (qacA/B and smr) for reduced susceptibility to commonly used antiseptics and disinfectants, such as benzalkonium chloride and chlorhexidine. It is well recognised that elderly subjects residing in long-term care facilities (LTCF) can act as reservoirs of MRSA, but little is known of its prevalence in healthy elderly living in the community. We aimed to determine prevalence of MRSA and presence of biocide resistance genes in healthy elderly subjects attending community centres for the elderly. Methods: Elderly attendees at seven community centres for the elderly were screened for MRSA nasal colonization. Swabs were cultured on chromogenic agar for MRSA and isolates confirmed by MALDI-TOF. Susceptibility testing was performed following EUCAST guidelines. qacA/B and smr were determined by PCR.

Results: A total of 450 elderly subjects were screened. 31% (140/ 450) of subjects were positive for S. aureus nasal carriage. Of these isolates, 11 (7.8%) were methicillin resistant. Overall 14.8% of isolates were positive for qacA/B and 13.5% for smr. There was a significant association between methicillin resistance and presence of qacA/B genes ( p = 0.034; CI 1.024-14.8). Four MRSA strains were resistant to ≥ 3 other drugs.

Conclusion: Overall carriage of MRSA in these subjects (2.4%) is marginally higher than the figures reported for the general population (0.5-1%,) but considerably less rates reported for elderly subjects in LTCF (15-50%). The presence of biocide resistance genes is of concern as multi-drug resistant strains with resistance to biocides may be difficult to eradicate. Tolerance to biocides may contribute to persistence of MRSA in the community as well as the healthcare setting. Continued monitoring of such strains is essential, as well as education on the proper use of biocides and antiseptics to prevent residue remaining in the environment which could act as a facilitator for induction of biocide tolerance.

Emphysematous cystitis in the geriatric patients with Escherichia Chih-Cheng Lu 1,2 , Wen-Chou Fan 1 . cytoplasmic antibody were all negative. The serum microscopic agglutination test for leptospirosis was positive. The kidney and liver biopsies confirmed leptospirosis by immunohistochemical stain (Figure 1 ). 3 weeks after admission, he remained intubated with mechanical machine support, still had intermittent fevers and hemoptysis despite transfusions of erythrocyte and platelet and antibiotics treatment. We started 5 sessions of plasma exchange with 1.5 plasma volumes on 22, 24, 26, 28, 30th hospital days. After 5 times of plasma exchange, his consciousness, pulmonary hemorrhage and icterus were improved dramatically. He was extubated on 31th admission day. He recovered well and was discharged 38 days after admission. Conclusion: Although the role of plasma exchange in management of Weil's disease is not well defined, our experience shows that combined antibiotics and plasma exchange may be helpful in severe leptospirosis with pulmonary renal syndrome.

Association of scabies and psychiatric disorders in childhood: a nationwide population-based study Chien-Yu Lin 1,2 *, Hsin Chi 3 , Shey-Chaing Su 1 , Cheng-Yin Tseng 1 , Po-Huang Liu 1 , Nan-Chang Chiu 3 , Jui-Ming Liu 4 , Ren-Jun Hsu 5 . Background: Streptococcus sanguis is a gram-positive anaerobic bacterium found in oral cavity and proven to be related with infective endocarditis. Leptospirosis and scrub typhus are zoonoses with worldwide distribution, and co-infections have been reported. We present a case of S. sanguis bacteremia in a farmer who also had scrub typhus and leptospirosis co-infection.

A 73 year-old male farmer referred for altered consciousness, sepsis related multi-organ failure, GI bleeding and pneumonia. He was admitted to intensive care unit once arrived at the Emergency Department. Blood culture showed S. sanguis bacteremia but the patient did not respond to empirical antibiotics with Tapimycin and his multi-organ failure got worse. On examination, the patient still had fevers, low blood pressure (100/ 60 mm Hg) with tachycardia (120/min), and presented more symptoms such as yellowish skin color, diffused petechiae of body surface, and oliguria (0.7 mL/kg/day). Troponin I was 2.42 ug/ L, but ECG revealed no ST elevation, consistent with NSTEMI. Echocardiogram did not reveal vegetations. Gastrointestinal endoscopy revealed bleeding gastric ulcer. Laboratory studies revealed platelet count of 9,000/uL, elevated D-dimer and fibrinogen, which could not explain DIC as the cause of bleeding diathesis. He also had liver dysfunction, jaundice, high blood level of ammonia (130 umol/L), A/G reverse and acute kidney injury with hyperkalemia (5.5 mmol/L). Abdomen sonography revealed ascites, splenomegaly and cirrhosis of the liver.

Picture above showed multiple petechiea found on gastric wall during panedoscope on 6/29. Methods: Female BALB/c mice (7 weeks old) were infected with approximately 1 × 10 9 CFU of MRSA strain MW2, intraperitoneally. Subsequently, mAbs, vancomycin hydrochloride (VCM) and daptomycin (DAP) were injected intraperitoneally (alone or in combinations). The animals were monitored and their mortality recorded. Statistical analysis was done using the log-rank test to determine differences between treatment groups. Results: There is a significant difference ( p < 0.05) in treatment groups (single and combination therapies) with the most effective lifesaving activities exhibited by the 3H + DAP combination at 65%.

The finding of this study shows higher survival rates after 8 days in the lethal sepsis model with combination mAbs (3H + 5H at 0.5 mg each) at 60%, than single mAbs used as 1 mg of 3H (45%) or 1 mg of 5H (35%) alone. Also, combination mAb and antibiotics (3H at 1 mg + VCM at 5 µg or 3H at 1 mg + DAP at 5 µg) showed higher survival rates (55% and 65% respectively) compared with VCM (20%) or DAP (25%) alone ( p < 0.05). Conclusion: Combinations including ZBIA3H and antibiotics represent an attractive alternative to antibiotic monotherapies currently used to treat S. aureus bacteremia.

Epidemiology of sepsis in children: a nationwide cohort study Conclusion: BCG-osteitis/osteomyelitis is more strongly correlated with the vaccination of newborns than with that of older infants. The question of whether postponing BCG vaccination could lead to decreased risk of BCG-osteitis/osteomyelitis without increasing TB meningitis, BCG-lymphadenitis, and injection site suppuration in young infants needs careful evaluation.

The prevalence of bone loss and short term effect of antiretroviral therapy on bone mineral density in male Japanese naïve patients with HIV Yusuke Yoshino, Keita Misu, Kazunori Seo, Ichiro Koga, Takatoshi Kitazawa, Yasuo Ota. Department of Internal Medicine, Teikyo University School of Medicine, Tokyo, Japan

Background: It is well known that low bone mineral density (BMD) is prevalent in HIV patients. Many studies in North America, Europe and Oceania revealed the relationship between HIV and low BMD, and there were still few studies about that in Asian countries. Especially, research of the anti-HIV drug early impact on BMD after treatment initiation is underrepresented in Asian countries. We studied the prevalence of bone loss and changes of BMD after initiation of continuous anti-retroviral therapy (cART) in Japanese naïve patients with HIV. Methods: Male patients with HIV who visited the outpatient clinic of Teikyo University Hospital, Tokyo, Japan between April 2010 and November 2016 were enrolled. Patients underwent BMD analyses before and 44-56 weeks after initiation of cART with the same dual energy X-ray absorptiometory scan. Both the lumbar spines BMD and the average of bilateral femoral necks BMD were analyzed and employed to calculate Z-scores. Clinical factors were collected, including age, body mass index, smoking status, corticosteroid use, cART regimens and laboratory data. We assessed changes in BMD after cART initiation by paired t-test. To determine risk factors for low bone mass in naive patients and influencing factors on BMD after the start of cART, stepwise multiple regression analysis was used.

Results: Thirty-one patients were followed up. Median lumbar spines Z-score was −0.500 and median femoral necks Z-score was −0.350 in naïve settings. Age, serum AST level and serum ALT level were the independent risk factors for low bone mass in naïve patients. The change of BMD after cART initiation was shown in Figure 1 . Use of Tenofovir Disoproxil Fumarate (TDF), use of protease inhibitors (PIs) and low CD4 cell counts were independent risk factors on BMD loss in the lumbar spines. N-terminal telopeptide/creatinine (urinary test) was the independent risk factor in femoral necks.

Conclusion: Slight decrease of BMD was observed in Japanese naïve patients with HIV. BMD decreased significantly after cART initiation. In lumbar spines, using TDF or/and PIs more significantly increased the risk of BMD loss. In femoral necks, cART more significantly affected on BMD of patients with higher bone metabolic activity.

Short-term and long-term effect of highly active antiretroviral treatment on incident tuberculosis in people living with HIV/ AIDS Yung-Feng Yen 1,2,3,4 , Pei-Hung Chuang 5,6 , Ian Jen 7 , Marcelo Chen 8,9 , Yu-Ching Lan 10 , Chun-Yuan Lee 11 , Yun Lee 4 , Yi-Ming Arthur Chen 4,12 . 1 Section of Infectious Diseases, Taipei City

Background: The short-term and long-term effect of highly active antiretroviral therapy (HAART) exposure on incident tuberculosis (TB) has not been extensively studied. This nationwide populationbased cohort study was aimed to determine the short-term and long-term effect of HAART on incident TB in people living with HIV/ AIDS (PLWHA) in Taiwan Background: In the highly active antiretroviral therapy (HAART) era, bacteremia is the one of an important cause of morbidity and mortality in HIV-infected patients. Few studies have described these patients. The study aimed to assess the etiologic pathogens, sites of infection, mortality rate and the factors associated with the mortality in HIV-infected patients who had bacteremia. Methods: Retrospective electronic database and chart review of the HIV-infected patients with bacteremia at Vajira Hospital between January 1, 2009 to November 30, 2014 was performed. Results: Of the 115 HIV-infected patients, 67 (57.4%) were male, of these 83 (59.7%) were not receiving antiretroviral drug and 65 (46.8%) had CD4 below 50 cells/mm 3 . Among 150 isolates of bacteremia, we found that the most common organism was Salmonella spp. (46, 30.67%) and Escherichia coli (E.coli) (35, 23.33%). Ceftriaxone resistance was high (91.3%) in bacteremia caused by Salmonella group C. The common sites of infection were primary bacteremia (38%) and urinary tract infection (15.1%). The mortality rate was 23% and the factor that affected the mortality were albumin level (P = 0.002), underlying disease (P < 0.005), cirrhosis (P < 0.005), chronic renal failure (P = 0.009), malignancy (P = 0.012), focal site of infection (P < 0.005), lower respiratory tract infection (P < 0.05), and intraabdominal infection (P < 0.05). Conclusion: Salmonella spp. and Escherichia coli are the leading caused of bacteremia in HIV-infected patients. The common sites of infection were primary bacteremia.

Dengue virus nonstructural protein 1 induces platelet activation Wei-Chueh Wu, Trai Ming Yeh. Medical Laboratory Science and Biotechnology, National Cheng Kung University, Tainan, Taiwan Background: Dengue virus (DENV), a mosquito-borne flavivirus, is responsible for ∼390 million infections annually, and leads to dengue fever or potentially lethal dengue hemorrhagic fever (DHF) or shock syndrome (DSS). DHF/DSS patients often show severe symptoms such as vascular leakage and thrombocytopenia. During dengue infection, DENV nonstructural protein 1 (NS1) can be secreted into blood and causes vascular leakage. However, whether NS1 is involved in the pathogenesis of thrombocytopenia remains unclear. In this study, we hypothesized that NS1 may play a critical role of DENV-induced thrombocytopenia. Methods: To verify this hypothesis, freshly isolated platelets from healthy donors were treated with DENV type 1 recombinant NS1 from drosophila and were evaluated for markers of activation, cytokine secretion and aggregation. Results: We found that NS1 could bind to toll-like receptor 4 of platelets. Furthermore, P-selectin expression and macrophage migration inhibitory factor secretion were increased in NS1stimulated platelets. Last, platelets aggregation with suboptimal dose of ADP was enhanced after NS1 stimulation. Conclusion: Taken together, our results demonstrate that NS1 could directly bind to platelet and induce platelet activation. These results suggest that NS1 may contribute to the pathogenesis of thrombocytopenia during dengue infection.

MARVAS09 acts as a potent inhibitor of human enterovirus A71 replication through suppressing viral IRES Gunaseelan Saravanan, S/O 1 , Justin Jang Hann, CHU 1,2 . 1 MARVAS, Department of Microbiology and Immunology 2 National University of Singapore 2 CTU, IMCB, A*STAR Background: Human Enterovirus A71 (HEVA71) is the main causative agent for hand, foot and mouth disease (HFMD) in young children in many parts of the world especially in Asia. The neurotropic nature of HEVA71 can cause severe infection and complications including encephalitis and even death. Due to the lack of effective antivirals to curb HEVA71 infections, this study adopted an approach of identifying potential therapeutic agents through reverse genetics and high throughput screen of a flavonoid compound library. Methods: A 500-compound flavonoid derivatives library was screened with the HEVA71 Bi-cistronic IRES construct to identify compounds with anti-HEVA71 IRES activity. Chosen hits were further verified with cell viability and viral plaque assays. Results: The high throughput screen with the flavonoid compounds has identified MARVAS09 as a potent inhibitor of HEV71. Downstream secondary assays reaffirmed that MARVAS09 can disrupt viral protein and viral RNA synthesis through reducing HEV71 IRES activity. Further studies deduced that MARVAS09 was a narrow-spectrum antiviral, which is only effective against human enteroviruses but not chikungunya virus or herpes simplex type 1 virus. The development of HEVA71 resistant mutants against MARVAS09 demonstrated the effectiveness of MARVAS09 against a critical target of HEVA71, namely the viral IRES. More importantly, we are able to show the strong in-vivo efficacy of MARVAS09 with the murine model challenged with HEVA71. Conclusion: In conclusion, MARVAS09 is a potent antiviral against HEVA71 and possibly other human enteroviruses which can be further developed as a potential clinical antiviral for HFMD.

Seroprevalence of human herpesvirus type 8 in patients with chronic obstructive pulmonary disease Cheng-Chuan Su 1,2 *, Shih-Ming Tsao 3,4 , Chun-Liang Lai 1,5 , Ming-Nan Lin 1,6 , Jen-Pi Tsai 1, 3, 5 Background: Seroprevalence of human herpesvirus type 8 (HHV-8) is high in patients with various immunologic abnormalities. Patients with chronic obstructive pulmonary disease (COPD) are immunocompromised, but HHV-8 seroprevalence in COPD patients has not been documented. Methods: Blood samples collected from 64 COPD patients and from 64 age-and sex-matched healthy controls were used to determine lymphocyte and monocyte counts and HHV-8 antibody and DNA. Results: COPD patients had a lower lymphocyte count and a higher monocyte count than healthy controls (P = 0.0006 and 0.0003, respectively). The seroprevalence of HHV-8 antibodies was moderately higher in COPD patients than in controls (37.5% vs. 26.6%, P = 0.18). Three patients who were positive for HHV-8 antibodies were also positive for HHV-8 DNA. Lymphocyte counts were higher in COPD patients positive for HHV-8 antibodies than in negatives (P = 0.014) and in COPD patients positive for HHV-8 DNA than in negatives (P = 0.048). Lymphocyte count increased (from 1,688 to 2,856 and 3,184 cells/μL) with increasing HHV-8 viral load (from 133 to 1,450 and 2,090 copies/mL). Conclusion: COPD patients have a modely higher HHV-8 seroprevalence than healthy controls. Lymphocyte count is significantly higher in COPD patients positive for HHV-8 antibodies or DNA than in negatives. Lymphocyte count increases with increase in HHV-8 viral load in patients positive for HHV-8 DNA. Methods: Multiple and sequential specimens of serum, nasopharynx swab and sputum/tracheal aspirate were collected. The virus (A/Taiwan/1/2017) was isolated from the respiratory specimen. The important and different molecular signatures of the A/Taiwan/1/ 2017, compared with previous H7N9 viruses were investigated. Results: The important and different molecular signatures of the A/Taiwan/1/2017 virus included an insertion of three basic amino acid residues (RKR) at the cleavage site connecting the HA1 (hemagglutinin) and HA2 peptide regions. For antiviral resistance, R292K substitution in the NA (neuraminidase) protein was found in the A/Taiwan/1/2017 virus, suggesting that this virus had developed resistance to oseltamivir. The NS1 of A/Taiwan/1/2017 is 237 amino acids in length and the previous isolates were 217 amino acids. The extended C-terminal region of 20 amino acids contains PDZ (PSD-95/Discs-large/ZO-1) binding domain, which also contributes to pathogenicity. Conclusion: The signatures of high avian pathogenicity and oseltamivir resistance is of a concern and may have implication on clinical management and antiviral therapy. Current guideline that specifies neuraminidase inhibitors (NAIs) as drug of choice to treat patients infected with avian influenza A (H7N9) virus, should be revised based on geographic distribution of resistance, especially in Asia-Pacific region.

Trends in the molecular epidemiology of HCV: an individualpatient-and sequence-level analysis Siao-Ru Lin, Ming-Huei Gu. Department of Laboratory Medicine, Hualien Tzu Chi Hospital, Buddhist Tzu Chi Medical Foundation, Taiwan Background: Regional and genotype-specific mutational patterns of HCV are essential for informing direct-acting antiviral agents (DAAs) and designing diagnostic assays for use in regions where standard genotypic resistance testing is not affordable. We try to understand the molecular epidemiology of drugresistance associate variants (RAVs) in treatment-naïve HCVinfected patients. Methods: First, we set up a workflow for specimen collection, viral nucleic acid extraction, performing PCR assays, and directedsequence analysis. Then, we performed 2-step PCR assays on several specimens to find HCV genotype 1b-infected patients. To investigate the prevalence of RAVs to DAAs in treatment-naive HCV genotype 1b-infected patients, we amplify HCV viral regions of NS5A, and 5′ untranslated-region. Using Bioedit ® and Genotype tool on NCBI website analyzed the sequencing data. Results: We collected 34 reference genome of HCV from NCBI genebank as templates for HCV-specific primer set. The amplified target in silico was submitted in Genotyping tool. It showed 100% agreement. From November 2016 to May 2017, a total of 159 patients were enrolled in HCV genotype analysis. The genotype or subtype 1a, 1b, 2, 2a, 2b, 3, 3a, 6, 6a and 6w were identified in 6.9%, 56.0%, 5.0%, 20.8%, 4.4%, 0.6%, 0.6%, 3.1%, 1.3% and 1.3% of the successfully amplified cases, respectively. At same period, we performed HCV NS5A mutation analysis on 54 patients, mutation of codon 31, 93, and both of them were detected in 3.7%, 18.5% and 1.9% of cases, respectively. Conclusion: This study is still in progress, the conclution remaining unclear. However, naturally occurring RAVs to DAAs pre-exist in treatment-naive HCV genotype 1b-infected patients. Clinicians should consider RAVs before start DAA-based antiviral therapy. He suffered from upper airway infections (cough, running nose and sorethroat) 4-5 days prior to admission. He also fell onto the ground but brain CT showed no obvious intra-cranial hemorrhage. He suffered from rapid respiration and chest CT revealed subpleural consolidations at LUL and bilateral lower lobes, more at RLL, which favoured bilateral pneumonia. He had to be admitted to our intensive care unit after ET tube intubation and mechanical ventilation use. Influenza rapid test showed influenza A. Oral Oseltamivir (Tamiflu) was initiated and 600mg Peramivir (Rapiacta) was added in use as acute respiratory distress syndrome (ARDS) developed. FiO2 100% had to be used. However, his ARDS improved soon after Oseltamivir (Tamiflu) and intra-venous Peramivir (Rapiacta) were continued in use. RT-PCR of Taiwan CDC confirmed influenza A (H3) infection. (Oseltamivir and Peramivir (Rapiacta) had been used for 5 days and 7 days respectively.) His ET tube could be removed 1 week later. He became stable and could be discharged on day 14. Discussion: (1) This patient suffered from severe respiratory complications induced by influenza but could get well soon after proper prompt use (within 24 hours) of the neuraminidase inhibitor. (2) There was not any guideline or studies about concurrent use of neuraminidase inhibitors Oseltamivir and Peramivir together. In our patient, his fever subsided after Peramivir use and no need for ECMO use. It seemed that Oseltamivir and Peramivir could be used together and had no antagonism to each other. After Peramivir binds to NA enzyme, they are not easily dissociated, and Peramivir can inhibit the NA activity for longer period.

Reactivation of cytomegalovirus during the hospital course of sepsis and its association with outcome Hsin-Pai Chen 1 , Ping-Liang Hung 2 , Wan-Huai Teo 3 , Yuan-Ming Lee 4 , Yu-Jiun Chan 5 , Cheng-Yu Chen 2,6 *. Accumulating evidence suggests that CMV reactivates in critically ill patients such as those with sepsis. Controversy exists with respect to the association between CMV reactivation and outcome of sepsis, and how the virus reactivates during the course of sepsis were not fully investigated. In this study, we systemically investigated the reactivation pattern of CMV during the course of sepsis and correlated the results to the outcome of sepsis. Methods: For sepsis patients admitted to the intensive care unit, blood samples were collected on day 1, 4, 8, 14 , and 28 during the hospital stay. CMV was detected by PCR and the viral loads were measured by quantitative PCR. Serum cytokine and CMV IgG were determined by ELISA. Patients' clinical characteristics and survival within 28 days were recorded and correlated to the results of CMV detection. Results: All the 262 included patients were seropositive for CMV IgG. During the 28-day in-hospital follow-up course, CMV was detected by PCR at least once in 25.2% (66/262) of the patients. The positive rate of CMV PCR increased from the initial 7.7% on day 1 to 36.7% on day 28. The median viral load was lower during the early (day 1−8) than the late (day 14−28) follow-up phase (relative median viral load 1.42 vs 4.29, p = 0.020). Detection of CMV on day 1 was associated with a lower 28-day survival rate compared with those whose blood was negative for CMV on day 1 ( p = 0.0057). A higher serum interleukin (IL)-2 level, which denoted monocyte activation, was observed in patients with positive CMV PCR, especially on day 1. Conclusion: The frequency of CMV reactivation increased during the course of sepsis. Early reactivation of CMV during the initial phase of sepsis was associated with a poorer outcome. Interaction between CMV and host monocytes may play a role in the pathophysiology of sepsis.

Methanoculleus taiwanensis CYW4 T Wen-Hsin Wei, Chieh-Yin Weng, Sheng-Chung Chen, Mei-Chin Lai*. Department of Life Sciences, National Chung Hsing University, Taichung, Taiwan, R.O.C.

Background: Viruses infect all Three Domains of life, however the knowledge of archaeal viruses is limited. Lately, the diverse virus morphology and the independent virus development outside the extreme thermophilic/acidophilic archaeal host attracted the attention on archaea viral investigations. Currently, over 100 archaeal viruses have been discovered, most related to thermophilic Crenarchaea and extreme halophilic Euryarchaea. However, almost none methanoarchaeal virus was reported for the past two decades. From the core sediment on the top of gas hydrate habitat in the deformation front site offshore south-western Taiwan, Methanoculleus taiwanensis CYW4 T was enriched, isolated and characterized. Cell lysis was observed at stationary phase and a lytic virus (designed as Mta) which caused the lysis of strain CYW4 T was observed under TEM. Methods: The stationary phase grown M. taiwanensis CYW4 T culture was harvested. And virus was purified by PEG precipitation and gel filtration through a Sephacryl S-500HR column. The purified viral DNA was obtained after phenol/chloroform extraction and ethanol precipitation. Whole genome sequencing of virus Mta was performed using Illumina MiSeq. Functions of predicted ORFs and putative conserved domain were analyzed by BLASTP and BLASTX. The distant homologs searches were performed by HHpred (Söding et al., 2005) . Background: Chronic active EBV disease (CAEBV) is a rare and fatal complication of EBV infection. Unlike the cases in western countries, where EBV infected B cells, the majority cases in Asia and South American are found in T cell or natural killer cell. CAEBV is characterized by (1) markedly elevated EBV antibody or EBV DNA in blood (2) histologic evidence of organ infiltrated with virusinfected cells (3) detection of EBV protein or nucleic acid in tissue. We report a fatal case of EBV positive clonal T-cell lymphoproliferative disorder presenting as pulmonary cryptococcosis. Methods: This 69 year-old farmer, was admitted in mid July, 2014, because of intermittent fever up to 39^C, cough with whitish sputum for two weeks. Cryptococcal pneumonia was confirmed by sono-guided lung biopsy in March, 2014, with blood cryptococcal antigen at titer of 1:64. Ineffective initial fluconazole treatment was replaced with combining liposomal amphotericin B and 5flurocytosine. Later, however, even adding pip/tazo treatment for possible nosocomial infection, the patient was still febrile. Results: On admission day 11, diffuse skin rash and multiple lymph nodes in level I∼IV were found. Chest and abdomen CT revealed multiple lymphadenopathy in neck, mediastinum and retroperitoneum. Skin rash biopsy showed dermis infiltrated by T cells. Neck lymph node showed necrotizing lymphadenitis with eosinophilic necrotic debris and lymphoid hyperplasia. Immunohistochemistry by staining CD3 was performed on fixed paraffin-embedded tissues, compared with in situ hybridization for EBV-encoded RNA (EBER). Both skin biopsy and lymph node were positive for EBER and CD3, confirming EBV infected T cells ( Figure A) . Also, EBV viral load was markedly elevated of 72,888 copies/mL. Other study met the characteristics of CAEBV, including positive EBV-VCA-IgG, EBV-NA-IgG and negative EBV-VCA-IgM, EBV-EA-IgG. CD4 was 104/uL, and trimethoprim-sulfamethoxazole was initiated for prophylaxis of Pneumocystis Jiroveci pneumonia, although HIV negative. Anti-IFN-gamma antibodies were negative. The patient became afebrile with tigecycline, and was discharged in late August, 2014. He completed 1 year oral itraconzole treatment, but died of sepsis at other hospital in June, 2017. Conclusion: This case presented with pulmonary cryptococcosis was diagnosed with fatal T-cell lymphoproliferative disorder with CAEBV which is characterized by generalized lymphadenopathy, fever, and rashes. EBV is detected in T cells by immunohistochemistry. Studies for T-cell clonality confirm the diagnosis.

Topical use of antimicrobials for surgical prophylaxis in Australian hospitals Rodney James*, Kirsty Buising, Karin Thursky. National Centre for Antimicrobial Stewardship, The Peter Doherty Institute for Infection and Immunity, Melbourne, VIC 3000

Background: There have been many studies supportive of the use of appropriately administered surgical antimicrobial prophylaxis in reducing surgical site and other postoperative infections. Guidance for the appropriate use of surgical antimicrobial prophylaxis in Australia is available via national prescribing guidelines and suggested principles of antimicrobial prescribing in regard to surgical procedures are:

• only prescribe prophylaxis if there is a clear need • the use of topical antimicrobials is not recommended for surgical prophylaxis

The use of topical antimicrobials for prophylaxis during surgical procedures has been a practice commonly utilised by surgeons with limited evidence of benefit. There has currently been no major study into their general use in Australian hospitals.

Methods: A Pilot Surgical National Antimicrobial Prescribing Survey was conducted from the 18th April 2016 to 3rd November 2016. Using data from the 67 hospitals that participated in this pilot, the procedure groups for which topical antimicrobials were prescribed and appropriateness of these will be described. Conclusion: Although topical antimicrobial surgical prophylaxis accounts for only a small percentage of intra-operative prescribing, more than three-quarters of these prescriptions were deemed inappropriate. Due to reported poor documentation of the use of topical antimicrobials in surgical procedures, this would represent an underestimate of the true proportion of their use. There were several procedures for which prophylaxis was prescribed with little to no evidence for their use highlighting areas where practices should be reviewed.

Experiences share of antibiotics stewardship in four years and effectively reduce the multidrug-resistant organisms Hui-Yun Liang 1 , Iig-Ling Chen 1 , Hung-Jen Tang 1,2,3 *. Background: Drug resistance issues are worrisome and concerned all over the world. The appropriate use of antibiotics has been mentioned all the times. However, facing up to the constantly detection of the various multi drug resistance microorganisms, brings out that the rigor examinations and the control execution effectiveness of the antibiotic needs to be reviewed and improved all the time. Coordination from the antibiotics stewardship and the database system can make the control more effectively. Our hosptial since the beginning of the implementation of antibiotics stewardship has been four years. Both of the detection of drugresistant strains and the use of antibiotics have been declined. Methods: Our hospital has utilizing the database systems to assist executing in the antibiotic medication control, the infectious disease specialist would follow the ""laboratory reports"" and ""current medical orders"" at the analysis systems to exam then provide the professional commends especially before the report of the blood culture, it could only approved three days of antibiotics for the upgrade or downgrade. The quick identification of the equipments and methods can shorten the reporting time, so that clinicians can choose the appropriate anti-auxiliaries. Drugresistant bacteria were checked out immediately then proceed to isolate immediately. When the patient moves out then implement the disinfection of environmental cleaning and proceed the joint auditing. Results: Overview of the antibiotics stewardship in a 4-year study period (2013-2016) The DID Usage density of antibiotics (per 1,000 bed-day-occupancy) have been declined 30%, the Penicillins have been declined 35%, and Carbapenems declined 40%, Glycopeptide and Fluoroquinolone use to show stability. Carbapenem resistant Acinetobacter baumannii. and Methicillin-resistant Staphylococcus aureus reported a 50% drop in new cases. Conclusion: The original use of antibiotics is for the treatment of bacterial infection, but must pay attention to the use of the period and follow the rules and the recommendations. To avoid the occurrence of bacteria cause by the filter pressure of orthodontic, need to pay attention to monitoring or induced bacterial resistance to follow up, Surely it should be monitored.

Clinical impact of MALDI-TOF mass spectrometry in combination with antimicrobial stewardship intervention in adults with candidemia Nan-Yao Lee 1 , Ching-Lung Lo 2 , Wen-Liang Lin 3 , Jen-Chieh Lee 1 , Ming-Chi Li 1 , Chin-Shiang Tsai 1 , Wen-Chien Ko 1 *. Background: Candidemia is one of the severe infections associated with a poor outcome. Integration of rapid species identification via matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF-MS) with antimicrobial stewardship intervention (ASI) has the potential to early customization of appropriate antifungal therapy and improvement in patient outcomes. The objective of this study was to assess the impact of such an approach on the outcomes of adults with candidemia.

Methods: A pre-post quasi-experimental study was conducted in a medical center in southern Taiwan from January 2015 to December 2016. The ASI team, mainly infection disease specialists, provided evidence-based antimicrobial recommendations after the receipt of the causative Candida species for the intervention group. Clinical outcomes were compared to that of a historical control group.

Results: A total of 303 patients with candidemia were included in the final analysis: 151 patients in the intervention group and 152 patients in the control group. The intervention group had shorter time to Candida species identification (73.6 ± 31.7 vs. 105.2 ± 63.5 hours, P < 0.001) and to optimal antifungal therapy (1.2 ± 1.2 vs.

2.0 ± 1.6 days, P < 0.001). Of note, the sepsis-related mortality rate (17.2% vs. 27.6%; P = 0.038), 30-day crude mortality rate (27.2% vs. 39.5%; P = 0.028), in-hospital crude mortality (39.1% vs. 46.1%; P = 0.246) were lower in the intervention group. Conclusion: MALDI-TOF-MS in adjunction with ASI can result in rapid species identification, early initiation of optimal antifungal therapy, and most importantly a better clinical outcome for adults with candidemia.

Klebsiella pneumoniae perirenal abscess complicated by endogenous endophthalmitis Shu Hua Lin, Che-Kim Tan. Chimei Medical Center Intensive Care Unit

Background: Endogenous endophthalmitis is an uncommon but severe ocular infection caused by hematogenous bacterial seeding in bacteremia and septicemia. In East Asia, there is an increasing incidence of disease caused by gram-negative organisms, with Klebsiella pneumonia (KP) now as the most common cause of endogenous endophthalmitis. KP can produce infection at a variety of sites, with the risk being increased in patients with impaired host defenses (e.g., diabetes mellitus, liver abscess). KP is the main cause of Taiwan induced liver abscess, accounting for about 80% of all liver abscess cases. KP perirenal abscess complicated by endogenous endophthalmitis is few. Endogenous KP Endophthalmitis has an extremely poor prognosis, is exacerbated by delayed diagnosis.

A 75 year-old female has been past histories of type II diabetes mellitus poor controlled and recent HbA1c revealed 9.9%. She was complained of left lower quadrant pain, sleepy and septic shock was also noted. The abdomen computed tomography revealed right perirenal abscess, that pigtail drainage catheter was inserted. The blood culture and pig tail drainage abscess culture showed KP, emperical antibiotic of flomoxef use. However, she started to have redness of left eye within 24 hr then she had sudden pain, eyelid swelling and conjunctival chemosis was found and endogenous KP endophthalmitis was suspected. A orbital computed tomography revealed left orbital intraconal/extraconal space and left periorbital abscess and postseptal cellulitis, consistent with left endophthalmitis. Results: Thus, the ophthalmology was given intravitreal injection of antibiotic of ceftazidime 2 mg/0.1 cc and eye drops use. Visual acuity in the left eye deteriorated to no light perception over a 48-hour period, the intraocular pressure was 47-49 mmHg. We adjusted antibiotic to meropenem and prescuded mannitol to reduced intraocular pressure. The left eye received intravitreal injection one dose, mannitol intravenous injection three dose and meropenem for two weeks therapy. Although, the left eye no bacteria could be cultured and blindness, but stable hemodynamic status and transferred to ordinary ward. Conclusion: Acute endophthalmitis is a medical emergency. Endophthalmitis is a clinical diagnosis that is confirmed by positive aqueous or vitreous culture. However, a negative culture does not exclude the diagnosis. The primary part of the specimen culture and blood culture has a high diagnostic value, but because the eye is not easy to obtain a large number of specimens. Eye culture of the positive rate is low, even less than 30%, so usually as long as the clinical symptoms can be diagnosed. Decrease in ambulatory consumption of cephalexin and cotrimoxasole in the period of five years (2001) (2002) (2003) (2004) (2005) (2006) was observed, followed by significant decline in antimicrobial resistance of E. coli to these two drugs (to 10.2% and 23.3% in 2010, and to 6.7% and 24.5% in 2015, respectively), especially in females aged <65 years ( p < 0,01). The highest resistance rates for any isolate-antimicrobial combination ( particularly to fluoroquinolones) were detected in older males. The total proportion of ESBL producing E. coli strains rose from 1.6% in 2005 to 4.0% in 2010, and to 5% in 2015, with especially high resistance rate in older age groups ( p < 0.001). Conclusion: Decrease in consumption of certain antibiotics in the community leads to improvement in bacterial susceptibility. Due to the updates in Croatian guidelines on empirical treatment of CA-UTIs, changes in ambulatory use of antibiotics occurred. Withdrawal of amoxicillin, cephalexin and co-trimoxasole from the list of first line agents for empirical treatment of uncomplicated cystitis, elicited significant improvement in susceptibility of uropathogenic E. coli to these antibiotics. Since no obvious rise in fluoroquinolones use occurred, the increasing trend in resistance which might be caused by the spread of highly epidemic fluoroquinolone-resistant uropathogenic E. coli clonal groups, particularly in older men, requires further investigation.

Treatment of extended-spectrum-β-lactamase-producing Escherichia coli bacteremia at a medical center in northern Taiwan Tsai-Wei Wang 1,2 , Ting-Ching Wu 1 , Chi-Lan Kao 1 , Wan-Tsui Huang 1 . Clinical outcome: In the total duration of antibiotics treatment, the ETC-flomoxef was 14.5 ± 6.28 days, the ETC-quinolone was 11.5 ± 3.35 days; the DTC-flomoxef was 11 ± 3.6 days, the DTC-quinolone was 12.3 ± 5.2 days, the DTC-carbapenem was 15.7 ± 6.3 days. In the 30-day mortality rate, all groups of ETC were 0%; the DTCcarbapenem was 8.6%, whereas the other 2 groups were 0%. In the 30-day relapse rate, the ETC-flomoxef was 35%, the ETCquinolone was 0%; the DTC-flomoxef was 33.3%, the DTC-quinolone was 16.7%, the DTC-carbapenem was 31.4%. In DTC-carbapenem, its higher rate of mortality and relapse may relate to the patients' characteristics of more percentage of DM (45.7%), malignancy (45.7%), immunosuppression (42.9%), and nosocomial bacteremia (25.7%). All the data of clinical outcomes showed no statistically significant difference. Conclusion: Compared to ETC-quinolone, ETC-flomoxef revealed a tendency of higher relapse rate. In the definitive therapy, the flomoxef use in our hospital was restricted due to lack of the MIC.

Next-step, we should take comorbid conditions and disease severity into consideration. Recruiting more patients validated by Charlson score should be done in the future study.

Invasive pulmonary aspergillus with lung fibrosis, probably the sequelae of influenza infection Chia-Chen Hsu. Chi Mei Medical Center, Intensive Care Medicine

Background: Fatal invasive pulmonary aspergillosis (IPA) may follow viral influenza infections. Superinfection or coinfections are major causes of morbidity and mortality in patients with influenza.

We conducted a patient with IPA following influenza infection with refractory hypoxemia by Veno-venous mode extracorporeal membrane oxygenation (V-V ECMO) therapy. Case report: A 60-year-old man with hypertension history presented to ED. According to his family, he visited a friend about 7 days ago, and his friend had the symptoms of a common cold. Few days later, fever, frequent cough during spiking fever, scanty sputum and sore throat developed. Shortness of breath was also noticed. In emergency department, acute respiratory failure developed, so endotracheal intubation was performed and the patient was admitted to intensive care unit. Oseltamivir, intravenous (iv) with broad-spectrum antibiotic, vasopressor and iv albumin supplement were initiated. However, refractory hypoxemia was noticed so lung recruitment therapy was performed. After the discussion with cardiovascular surgeon and family members, the VV-ECMO was applied. The serum Aspergillosis antigen was positive; therefore voriconazole was added. Owing to the development of adult respiratory distress syndrome (ARDS), unstable hemodynamic status and oxygenation did not improve.

Another 5-day course of oseltamivir was then administrated. Repeated Real Time PCR for FluA showed positive result, so another 5-day course of oseltamivir was given. Antibiotics were shifted to Imipenem, voriconazole and Sulbactam sodium because of intermittent fever. However, desaturation and profound shock developed despite of vasopressor and aggressive fluid resuscitation. Progressive hepatic failure developed later. We discussed with the family about the grave prognosis and they requested for hospice care. We removed ECMO and the patient expired three weeks later. Conclusion: In summary, Aspergillus is a potential complication of severe influenza infections. Progressive pulmonary infection after the initial diagnosis of influenza should raise the suspicion of superinfections including Aspergillus. It is important to take heed of positive sputum microscopy and cultures for Aspergillus in this setting and to follow up with appropriate diagnostic procedures. Early diagnosis and treatment is the key to this fatal complication of influenza disease.

Empirical caspofungin for severe influenza suspected with invasive pulmonary aspergillosis Hui-Chun Chao 1 , Wen-Liang Yu 1,2 . During recent years, a rising incidence of invasive pulmonary aspergillosis (IPA) in critically ill patients has been reported. IPA was diagnosed in critically ill patients with H1N1 virus infection after 3 days of intensive care unit (ICU) admission, which render them more vulnerable for fungal infections. The mortality of IPA remains as high as 30-50%. Backbone of management are prophylaxis, early diagnosis and early initiation of antifungals for reduction of invasive aspergillosis related mortality. Case report: A 53 years old man has a major past history of hyperuricemia with gout, dyslipidemia and chronic kidney disease (CKD), stage 5. He suffered from progressive dyspnea in recent 1∼2 weeks. Then, he was brought to our emergency room and the laboratory data revealed anemia, hyperkalemia, acute on CKD with uremia, elevated cardiac enzyme, severe metobolic acidosis. Rapid Influenza virus antigen revealed positive and chest X-ray showed acute respiratory distress syndrome (ARDS). Due to respiratory failure, intubation was done and emergent hemodialysis was performed. Empiric antibiotics with tazobactam, caspofungin and oseltamivir. All culture revealed negative finding. Meanwhile, the result of a serum aspergilus galactomannan (GM) antigen assay was 0.88 index. So we kept antifungus use. 

Clinical significance of regional multicenter surveillance on antimicrobial consumption and resistance in Japanese acute care hospitals Arufumi Shiota. Aichi Prefectural Infection Control Network; Department of Infection Control and Prevention Aichi Medical University Hospital Background: Regional continuous monitoring of antimicrobial consumption is very important to prevent the development of drug resistance in the region. Aichi Prefectural Infection Control Network, which is led by the regional administration in Japan, has performed a surveillance of antimicrobial consumption and resistance after 2010. Herewith, we present the results of the regional multicenter evaluation of the antimicrobial consumption and resistance within seven years. Conclusion: We had clarified the regional transition of antimicrobial consumption in seven years. Notably, although analysis of carbapenems consumption showed an increase based on the values measured by AUD and DOT after 2013, no increase was noted in the carbapenems resistance for P. aeruginosa. The AUD/DOT ratio of carbapenems might contribute decreased carbapenems resistance in P. aeruginosa. Previous studies also showed that high plasma concentration of linezolid may induce thrombocytopenia. Only few studies demonstrated the relationship between plasma concentration and clinical outcomes, and to our knowledge, no research was conducted in the Han Chinese population. This study aimed to investigate the association between concentration of linezolid, AEs and clinical outcome.

Methods: A prospective observational study was performed in a tertiary teaching hospital between December 1, 2016 and May 15, 2017. Adult patients with therapeutic drug monitoring (TDM) of linezolid were included. Linezolid peak and trough concentrations were measured by HPLC and then used to calculate the 24 hour areaunder-the-curve (AUC). All patients' data were collected via electronic medical charts. The safety and clinical outcomes were assessed throughout the treatment course until 7 days after the completeness of linezolid and 30-day mortality was also recorded. All AEs were evaluated by Naranjo score, and the definite/probable AEs were defined as linezolid-related adverse events in the analysis. Results: Among 49 patients enrolled in the study, mean age were 61.2 years old and 49% were male. There were 66 linezolid TDMs collected, and most (80.4%) were performed within 14 days. The median trough, peak and AUC of standard dose (600 mg twice daily) and reduced dose (600 mg daily) were 9.19 mg/L, 24.33 mg/L, 360.98 mg/L-hr and 3.48 mg/L, 21.59 mg/L, 199.97 mg/L-hr, respectively. Plasma concentrations were 1.5-to 2 folds higher than which in the prescribing information but similar to the results in Japanese studies. Thrombocytopenia was the most common adverse events (62%) and peripheral neuropathy appeared infrequent (6%). Median trough concentration was significantly higher in patients with thrombocytopenia (total AEs: 13.06 vs. 6.97 mg/L, p = 0.0292; related AEs: 17.55 vs. 6.97 mg/L, p = 0.0007). Median trough level was also significantly higher in related leukopenia AE (22.54 vs. 9.62 mg/L, p = 0.0291). No association was identified between plasma concentration and anemia, lactic acidosis, peripheral neuropathy or clinical outcomes including mortality. Conclusion: Linezolid plasma concentrations of standard dose were 1.5-to 2 folds higher than which in the prescribing information, but similar to the results in Japanese studies. Higher trough concentration was associated with thrombocytopenia and leukopenia; however, no significant difference was observed between linezolid concentration and other AEs or clinical outcome. Dose adjustment tailored by TDM may be considered to prevent linezolid associated thrombocytopenia and leukopenia.

Investigation of the outcome of patients infected with MDR (multi-drug resistant) pathogens in one regional hospital at Southern Taiwan 2. The culture sites of clinical specimens were more on sputum (110), urine (51), than blood (24) and wound (13). 3. The distribution of age was more above 60 year-old (80%) and male predominant (54%) was noted. 4. The outcome of our cohort was more on refer to medical center (59%), still admission (22%) than died (13%), discharge (6%). Conclusion: 1. More and more MDR pathogens were isolated from clinical specimens and morbidity and mortality were increasing in these elderly patients. 2. The curde mortality rate of our cohort was 13% and MRSA, CRAB were predominant (69%) between MDR pathogens. 3. Limitation of staff and equipment available, there were many patients refer to medical center (59%) and transfer back to long term care facility later. 4. Respiratory and urinary tract infection were two major sites of infection at our hospital, especial at these elderly patients. 5. Targeted group (elderly and long term care) was our focus of infection control and well antibiotic stewardship was needed.

Effectiveness of antimicrobial stewardship programme in reducing incidence of ESBL cases Lau Hui Shan 1 *, Wong Ke Juin 2 , Fong Siew Moy 3 . 1 Department of Pharmacy, 2 Infection control Unit, 3 Infection control Unit, Hospital Wanita dan Kanak kanak, Sabah

Background: Since the first reported case of Extended spectrum beta lactamase (ESBL) in 1983, these resistant beta-lactamases has been reported worldwide and its presence is ubiquitous. Cefuroxime is one of the cephalosporin that has been implicated in the emergence of ESBL gram negative organism. The ESBL rate in paediatric surgical ward in Hospital Wanita and Kanak-kanak Sabah (HWKKS) was alarmingly high from our surveillance study and it could be linked with the common usage of cefuroxime. The objective of this study is to describe the effectiveness of antimicrobial stewardship (AMS) programme in reducing the incidence of ESBL producing Escherichia coli (E.coli) and Klebsiella pneumoniae (Kleb Pneumoniae).

Methods: This study was conducted from July 2015 to December 2016 in a peadiatric surgical ward in HWKKS. For first six months, usage of Intraveneous (IV) cefuroxime 750 mg, described in number of vials and total number of ESBL cases were calculated. All positive ESBL cultures from any specimens were included. Starting January 2016 AMS team suggested IV ampicilin/sulbactam 1.5 g as an alternative to cefuroxime in peadiatric surgical ward. After the intervention, a 6 monthly post interventional audit was done. Occurence rate of ESBL case pre and post intervention were compared with mean score of iv cefuroxime usage. Results: Two hundred and forty-seven patients were included in this study. There were no significant differences in the variation of Clcr with vancomycin treatment in daily dose (≧30 mg/kg /day vs <30 mg/kg/day) and duration of treatment (≧7 days vs <7 days). However, the creatinine clearance decreased after vancomycin treatment, comparing with baseline (before treatment), were a significant difference in the total dose of vancomycin treatment (≧14 g or <14 g) and the vancomycin trough level (≧15 mg/L or <15 m/L).

Conclusion: This study demonstrates the higher vancomycin cumulative dosage and trough level were associated with higher incidence of kidney injury. Further studies should be performed to evaluate the correlation between dosage and trough level to treatment response, and assess the applicability of high dose regimen of vancomycin in Taiwanese population based on IDSA guideline.

Impact of antibiotic stewardship and rapid diagnostic testing on coagulase-negative staphylococcus blood cultures Wen-Liang Lin 1 *, Nan-Yao Lee 2 , Hui-Hung Yu 1 , Wen-Chien Ko 2 , Pheng-Ying Yeh 1 . Background: A part of Escherichia coli most frequently isolated as a causative agent on urinary tract infection is a strain called UPEC (Uropathogenic Escherichia coli). UPEC is involved in chronicity and refractory urinary tract infections due to adhesion and invasion to the urothelial cell surface by type I pili and formation of Biofilm. It is elucidated in our previous study that proanthocyanidin A2(PAC), a cranberry-specific component, influences the adhesion and invasiveness of UPEC to urothelial cells. In addition, there are reports that PACs affect the quorum-sensing mechanism contributing to the formation of Biofilm and decrease pathogenic factors. Methods: As strains to be used, 3 strains (BK1, BK2, BK3) of uropathogenic Escherichia coli (UPEC), standard strain E. coli K12, Pseudomonas aeruginosa standard strain PAO1 and blood derived Pseudomonas aeruginosa 1 strain (568). First, the Microtiter Plate Assay was performed to evaluate biofilm formation and compared with non-addition of PAC and addition of PAC. Second, expression of csgA, csgD, crl, fimH which are biofilm related genes of Escherichia coli was examined by Polymerase Chain Reaction assay. Moreover, the inhibitory effect of PAC on the expression of biofilm associated genes was also investigated. Results: Biofilm formation was more decreased in case of addition of PAC than in case of non-addition of PAC. K12, BK1, BK2 and BK3 all expressed the genes of csgA, csgD, crl, fimH. However, even if PAC is added, it did not affect to expression of biofilm genes of E. coli. Conclusion: Our study showed that PAC suppresses biofilm formation and proved that its cause is not inhibiting the expression of biofilm associated gene. In addition, it was inferred that the cause of inhibition of biofilm formation by PAC is not by the gene mutation but the PAC itself influences intermolecular relationship such as quorum-sensing mechanism. Therefore, we need to further investigate how PAC affects quorum-sensing intermolecular.

Chih-Hao Hung 1 *, Tien-Yuan Wu 1,2 , Chi-Hua Chen 1 . 1 Department of Pharmacy, Taichung Tzu Chi Hospital, Buddhist Tzu Chi Medical Foundation, Taichung, Taiwan, 2 Department of Pharmacology, School of Medicine, Tzu Chi University, Hualien, Taiwan Background: Colistin is a polymyxin class of antibiotics with bactericidal properties. It has excellent bactericidal ability for many nosocomial infections, including Acinetobacter species, Enterobacter species, Escherichia coli, Klebsiella species, Pseudomonas aeruginosa, Stenotrophomonas maltophilia. Due to the limitations of antibiotics selection for multidrug resistance gram-negative bacteria (MDR-G(-)) infected pneumonia, the risk of infection mortality is very high. In this case, colsitn is the most common used antibiotic for MDR-G(-) infections. Except conventional intravenous(iv) injection of colistin, inhaled colistin increasing the concentration in lung tissue is a new administration route as an off-label use. However, the uncertainty effectiveness and safety need more investigation. Therefore, the effectiveness of inhalation colistin was compare with injection colistin in this study. Methods: Patients from January to December in 2016 using iv injection and inhaled colistin for more than 72 hours were enrolled into this study. The assessment criteria included bacterial culture results, down-grade or withdrawal colistin after treatment, the average length of hospital stay, and mortality rate. Results: The total number of patients treated with colistin iv vs. inhalation were 106 vs. 56 respectively. The mean age was 74.01 ± 14.51 years old vs. 75.82 ± 13.19 years old. The days of drug use were 10.09 ± 4.56 days vs. 11.06 ± 6.40 days, respectively. Down-grade or withdrawal colistin after treatment was 55 cases (51.89%) vs. 24 cases (42.86%), respectively. The average hospital stay was 28.50 ± 17.17 days vs. 46.00 ± 25.41 days after treatment, the mortality rates were 50.94% vs. 53.57%. Conclusion: In the study, iv injection of colistin in patients with complex infection was still better than inhalation. At present, iv injection of colstin treating hospital acquired pneumonia (HAP) or ventilator acquired pneumonia (VAP) was still recommended as the primary route. Since the patients used colistin with very complicated infections, the effectiveness and safety may need the largescale clinical studies to approve in the future.

Hypersensitivity to betalactam antibiotics: results of a 10-year study based on clinical history, skin and challenge tests Adnene Toumi 1 *, Karim Aouam 2 , Mohamed Chakroun 1 , Amel Chaabane 2 . 1 Infectious Diseases Department, UR 12SP41, Monastir University Hospital, Tunisia, 2 Pharmacology Department, Monastir University Hospital, Tunisia Background: Betalactams hypersensitivity remains overestimated because of the lack of an objective diagnosis tool which leading to unjustified therapeutic alternatives. This study has been performed in order to analyze the epidemiological, clinical and chronological features of betalactams hypersensitivity, to evaluate the skin tests value, and to establish a practical approach exploring such drug hypersensitivity. Methods: We included all adverse effects suspected to be induced by betalactams and notified to the pharmacovigilance unit of Monastir during 10 years. The drug imputability was established according to Begaud et al. Method. Skin tests were performed as recommended by ENDA. Results: Betalactams hypersensitivity was retained for 168 cases (25% of all notifications). Almost all of reactions were cutaneous mainly macula-papular rashes. The severity was estimated at 11.3%. The delayed reactions occurred 60.7% of cases. All reactions resolved after drug withdrawal. We identified 19 positive rechallengers. Skin tests were performed for 386 cases and were positive in 26.2% of them. Penicillins were implicated in 62.5% of cases. Almost all of immediate reactions were induced by penicillins. The hypersensitivity was selective to one betalactam in nearly half of cases. Cross-reactivity was objectified among penicillins and cephalosporins in 19% of cases, involving piperacillin-tazobactam in two cases and imipenem in one case.

Conclusion: Through the present study, we determined the betalactams hypersensitivity reactions feature and developed an exploration practical approach. The diagnosis was confirmed using skin tests and a possible dreg rechallenge. To improve these findings, drug provocation test would be a reliable tool.

PS 296 ATP has antimicrobial activity based on the chelating action of the ferric ions and shows its combined effect with anti-Mycobacterium avium complex drugs Yutaka Tatano 1 *, Yuichi Kanehiro 2 , Seiko Yamabe 2 , Chiaki Sano 2 , Toshiaki Shimizu 4 , Hideki Yagi 1 , Haruaki Tomioka 2,5 . 1 Department of Pharmaceutical Sciences, International University of Health and Welfare, 2 Faculty of Medicine, Shimane University, 3 Department of Nutritional Sciences, Yasuda Women's University, 4 Department of Basic Medical Sciences for Nursing, Department of Primary Education, Yasuda Women's University Background: ATP up-regulates macrophage antimycobacterial activity in a P2X 7 -dependent manner. However, whether ATP directly exhibits antimicrobial effects against intracellular mycobacteria is still unknown. Since we recognized direct antibacterial activity in ATP against bacteria including mycobacteria, we studied the mechanism of its antimicrobial effects. Additionally, we examined the detailed profile of the anti-Mycobacterium avium complex (MAC) activity of some antimicrobial agents in combination with ATP. Methods: We used mycobacteria including MAC, Mycobacterium tuberculosis, and common bacteria like Staphylococcus aureus. To confirm the mediation of antibacterial activity of ATP by its ironchelating effect, we established an enterobactin-deficient mutant strain of Klebsiella pneumoniae by deleting the entB gene. Bacterial siderophore production was measured by CAS (Chrome Azurol S) assay. We used clarithromycin, ethambutol, rifampin and rifabutin to evaluate the combined effects of ATP with anti-MAC drugs against MAC organisms. Results: The species and strain dependency of common bacteria were examined for their ATP susceptibility. ATP markedly inhibited the growth of M. intracellulare. Meanwhile, M. avium was moderately susceptible or resistant to ATP. S. aureus was moderately to highly susceptible. ATP's anti-MAC activity was significantly reduced by MgCl 2 and FeCl 3 . ATP-resistant Escherichia coli and K. pneumoniae produced significant levels of siderophores when cultivated in the presence of ATP. ATP exhibited moderate levels of antimicrobial effects on the mutant K. pneumoniae strain. Moreover, it was found that the anti-MAC activity of the selected drugs was markedly potentiated in a strain-dependent manner. In this case, the onset of the regrowth of antimicrobial agent-treated mycobacteria during cultivation was significantly delayed in the presence of ATP. Conclusion: In this study, ATP's antibacterial activity was found to be attributable to its iron-chelating ability. We also found that ATP inhibited the growth of various bacteria, including mycobacteria, without damaging the bacterial surface structures. Thus, ATP is promising as an adjunctive agent for chemotherapy against mycobacterial infections using anti-MAC drugs. We are currently examining the change in anti-MAC activity on increasing the intramacrophage ATP concentration.

Evaluation of synergistic antibacterial mechanism between SC5005 and a natural product against methicillin-resistant Staphylococcus aureus Chui-Hian Lim 1 , Chia-Min Yuan 1 , Chung-Wai Shaiu 2 , Hao-Chieh Chiu 1 *. Background: The emergence of antibiotic resistance has imposed a serious challenge to public health worldwide. In the United States, methicillin-resistant Staphylococcus aureus (MRSA) infections claim nearly 11,300 lives each year. As conventional antibiotics are not sufficient to treat MRSA infections, a new antibiotic with novel action mechanism is highly needed. Previously, our group showed a small-molecule compound, SC5005, exhibiting synergistic antibacterial activity with a natural product against MRSA. Here, we aim to uncover the synergistic antibacterial mechanism of this combination. Methods: The antibacterial activity of SC5005 combined with HRCA7 against MRSA was assessed by MIC assay, MBC assay and Time-Kill assay. The cell toxicity of this combination was examined in different epithelial cell lines by using MTT cell viability assay. Furthermore, in vivo efficacy of SC5005 in combination with HRCA7 is evaluated in a mouse model of MRSA skin infection. Results: The screening of nature products identified HRCA7, which is capable of potentiating SC5005's antibacterial activity for more than 8,000 times. This combination also displayed a fast-killing activity against MRSA as demonstrated by an over 97% eradication of bacteria in broth within 5 minutes. In contrast, no effect of HRCA7 on SC5005's cytotoxicity toward human epithelial cells was observed. Subsequent experimental result indicated that short exposure of MRSA to SC5005 increased bacterial susceptibility to HRCA7. While the combinatory effect of SC5005 and HRCA7 can be reduced by antioxidants with long acyl-chain, including astaxanthin and alpha-tocopherol, in a dose-dependent manner, the addition of phenolic antioxidant butylated hydroxytoluene (BHT), by contrast, stablized HRCA7 and increased the antibacterial activity of this combination. Currently, studies to evaluate the efficacy of SC5005 combined with HRCA7 on MRSA skin infection are underway. Conclusion: The findings provide strong evidences that HRCA7's prooxidant activity plays an important role in the synergistic bactericidal effect with SC5005. This combination could be further developed as a topical medication for MRSA skin infections.

In vitro activity of ceftolozane-tazobactam against Pseudomonas aeruginosa and Enterobacteriaceae isolates from medical center in Taiwan Background: Pseudomonas aeruginosa and Enterobacteriaceae are leading nosocomial gram-negative pathogens which are often multi-drug resistant. Ceftolozane-tazobactam (TOL/TAZ), an antipseudomonal cephalosporin with a β-lactamase inhibitor, is approved for the treatment of complicated intra-abdominal infections (in combination with metronidazole) and complicated urinary tract infections by FDA in the United States and by European Medicines Agency. Methods: Antimicrobial susceptibility of TOL/TAZ and ten comparators (cefepime, cefoperazone-sulbactam, ceftazidime, ciprofloxacin, colistin, imipenem, levofloxacin, meropenem, piperacillin-tazobactam (PIP/TAZ) and tigecycline) was evaluated against isolates collected from medical centers in Taiwan. All testing of minimum inhibitory concentrations (MICs) followed 2016 CLSI guidelines at a central laboratory (IHMA, Inc.). Results were interpreted using CLSI guidelines for all drugs except colistin (EUCAST) and tigecycline (FDA). Quality control strains were used in each day of testing. Background: Antibiotic therapy is the recommended choice for nontuberculous mycobacteria (NTM) infections currently. Multidrug therapy is a standard practice, but few treatment options exist. Solution of intrinsic and acquired antibiotic resistance is a major difficulty of treatment. Several previous studies suggest that single extracts or formulations of traditional Chinese medicine (TCM) alone or in combination with antibiotics may treat common bacterial infections. Therefore, synergistic combination therapy with antibiotics plus TCM could be an effective solution. This study is to provide an evidence for combination of TCM and antibiotics which may yield synergy in treatment of NTM infections. Methods: Two standard mycobacterial strains, Mycobacterium abscessus (College of American Pathologists, CAP) (RGM) and Mycobacterium avium subsp. avium (serotype 2) ATCC25291 (SGM), were chosen in this study. Three Scientific TCM formulations in extract subtly granule including Scute & Cimicifuga Combination (SCC), Dandelion & Wild Chrysanthemum Combination (DWCC) and Gleditsia Combination (GC) were applied to our experiment. Three steps of minimal inhibitory concentration (MIC) measurements for a preset concentration of both mycobacteria were performed: monotherapy of CLSI recommended antibiotics, monotherapy of TCM formulations with serial dilution concentrations and combination with antibiotic plus SCC. All of the above steps were repeatedly performed to estimate the average of MICs. Results: Bacteriostatic effects of 3 TCM formulations were observed on both M. abscessus and M. avium. The inhibitory concentrations for SCC, DWCC and GC were above 60, 100 and 70 mg/mL, and above 60, 80 and 100 mg/mL, respectively. Significant reductions of MIC for M. abscessus and M. avium were also measured when SCC was added with a concentration of more than 25 mg/ml onto the commercial antibiotic wells. The MICs of M. abscessus for clarithromycin, tigecycline, ciprofloxacin, moxifloxacin, trimethoprim/sulfamethoxazole, doxycycline, and cefoxitin, M. avium for clarithromycin and ethambutol were significantly decreased when they were combined with 12.5 mg/mL of SCC. Conclusion: TCM formulation alone yielded a similar bacteriostatic effect as antibiotics on NTM was observed. Combination treatment with the TCM formulation plus antibiotics could have a synergistic antibacterial effect on NTM. The machanism of TCM formulation on the antibiotics resistance in NTM will be explored.

Antimicrobial resistance to various antibiotics against Escherichia coli in Western part of Japan -9 years study Tetsuro Muratani 1,2,3 , Haruhisa Saku 1,2,4 , Tomoko Kobayashi 1,2,3 , Jun-ichi Ohno 1 . 1 IHibiki AMR Laboratory, Kitakyushu, Japan, 2 Hibiki Research Group for Clinical Microbiology, 3 Kyurin Medical Laboratory, 4 

Background: Although Escherichia coli is one of bacteria consisted of human intestinal normal flora, E. coli is a bacteria that has been able to cause infectious diseases at various site, such as urinary tract, intestinal tract, respiratory tract, wound, biliary tract, blood stream, etc. E. coli had been susceptible to various antibiotics. It is major problem that multi drug resistant E. coli have been emergence and increasing. We investigated the susceptibility to various antibiotics against E. coli isolated in Western part of Japan. Methods: A total of 8,079 non-duplicate E. coli isolated from urinein the period from 2008 to 2016 in Japan were used in this study. We divided the isolates to isolation period every 3 years, duration A is 2008 to 2010, duration B is 2011 to 2013, and duration C is 2014 to 2016. The MICs of various antimicrobials against the isolates were determined by the two-fold serial agar dilution method as described by the CLSI. The resistant isolates to antimicrobials were basically interpreted according to CLSI breakpoint. Results: E. coli isolates from urinary tract, respiratory tract, wound, and blood stream accounted for 72.6% (58.7/81.9 inpatients/outpatients), 12.4% (25.1/4.1), 4.0% (6.0/2.6), and 3.0% (4.6/2.0), respectively. Table shows the ratio of ESBL producers and quinolone-resistants. ESBL producers have been increasing in both inpatients and outpatients. The ratio of quinolone-resistants from inpatients have no change between 2008 and 2016, however, the ratio of them from outpatients have been increasing, oviously. Background: Increased with resistance of Neisseria gonorrhoeae to ceftriaxone (CTRX) is a global problem. We conducted a molecular epidemiological analysis of N. gonorrhoeae isolated in Japan for the purpose of examining CTRX resistance mechanism. Methods: Antimicrobial susceptibility to cefixime (CFIX) and CTRX was measured using N. gonorrhoeae isolated in Japan in 2014. Thirty two isolates were analyzed by amino acid sequence of penicillin-binding protein 2 (PBP2). Furthermore, the molecular epidemiological similarity was analyzed by Neisseria gonorrhoeae multiantigen sequence typing (NG-MAST).

Results: Thirty two isolates were classified into three groups (group A: CFIX susceptible/CTRX susceptible, group B: CFIX resistance/CTRX susceptible, group C: CFIX resistance/CTRX resistance) based on susceptibility records. The common mutations of PBP2 were group A: pattern V (5/15, 33.3%), group B: pattern X (9/ 15, 60.0%), and group C: pattern XXXIV (1/2, 50.0%). Patterns X and XXXIV were mosaic-like mutations in amino acid sequence. The common sequence type (ST) by MG-MAST were group A: ST5887 and ST6771 (2/15, 13.3%), group B: ST6778 (3/15, 20.0%), group C: ST1407 (1/2, 50.0%). In each group, characteristic ST by NG-MAST was not confirmed. In addition, 1 isolates with the same PBP2 (XXXIV-A501P) as the high-level CTRX resistant strain in France were confirmed. However, we detected the strain was sensitive to CTRX. Conclusion: We indicated the mutation of PBP2 was significantly associated with decreased susceptibiliies to CFIX. But the mutation of PBP2 did not confer a clear influence on the decreased susceptibiliies to CTRX. This study suggested that the resistance mechanism of CTRX is not only due to the mutation of PBP2 but also by a complicated mechanism.

The plasmid carrying the bla OXA-72 carbapenem resistance gene was transferred between Acinetobacter baumannii and Acinetobacter nosocomialis Hsing-Yu Chen 1 , Hung-Sheng Shang 2 , Jun-Ren Sun 2 . Background: Acinetobacter nosocomialis has been found to serve as a reservoir of resistance determinants in the health care settings. In our hospital, the carbapenem non-susceptible rate in Acinetobacter nosocomialis is approximately 20%. However, the carbapenem resistant mechanisms of Acinetobacter nosocomialis were rarely studied. The main objective of this work was to investigate the resistant mechanism and molecular epidemiology of imipenem non-susceptible Acinetobacter nosocomialis in Taiwan.

Methods: Forty-one imipenem non-susceptible Acinetobacter nosocomialis isolates were collected from bloodstream infection from 2012 to 2016. All of them were identified using the automated VITEK 2 system and confirmed by 16S-23S rRNA intergenic spacer sequencing. Antibiotic susceptibility testing was performed by the broth microdilution method. All 41 imipenem non-susceptible isolates were examined by PCR assays for the presence of the carbapenemase-encoding genes. The plasmid was sequenced and transformed into Acinetobacter baumannii to assess the transfer ability.

Results: The carbapenemase gene was detected in forty-one isolates. The most common carbapenemase gene was blaOXA-24/ 40-like (n = 29; 41%). Furthermore, six blaOXA-23-like, five blaOXA-58-like, and one blaOXA-51-like were detected. The blaOXA-24/40-like gene products were all sequencing as the blaOXA-72 gene and was conformed at the plasmid. The plasmid was sequenced as pAB-NCGN253 with one copy of the blaOXA-72 gene. In previous studies, the plasmid, pAB-NCGN253, was transmitted in Acinetobacter baumannii rather than in Acinetobacter nosocomialis. The plasmid was extracted from the clinical Acinetobacter nosocomialis and transformed into Acinetobacter baumannii. The minimum inhibitory concentrations of imipenem, meropenem, doripenem and ertapenem were increased from 0.25 mg/L, 0.38 mg/L, 0.125 mg/L and 6 mg/L to ≧16 mg/L, ≧32 mg/L, ≧8 mg/L and ≧32 mg/L, respectively. Conclusion: In conclusion, it was found that the plasmid pAB-NCGN253 carrying blaOXA-72 was dominant in the imipenem non-susceptible Acinetobacter nosocomialis isolates. The plasmid contributes to carbapenem resistance and may spread between Acinetobacter baumannii and Acinetobacter nosocomialis. Background: Carbapenem resistance is a serious global concern and efforts to find novel resistance mechanisms is on-going. The well-known mechanisms include loss of both the porins (OmpK35 or OmpK36), derepression of AmpC-β-lactamase, ESBL production along with loss of either one of the porins. In this study, we isolated two Klebsiella pneumoniae strains from a single patient, before (K1) and after (K2) meropenem treatment. The two strains showed identical PFGE pattern, but differed only in their carbapenem susceptibility.

Methods: Hodge test, DDS, SDS-PAGE, WGS along RNASeq were performed to get the holistic views on the changes of all alleles and their expression. We developed a K. pneumoniae network system (KlebsiellaNet) covering 88% of 5316 coding genes of Klebsiella pneumoniae HS11286 strain with 161,507 co-functional links. The putative candidate genes were shortlisted comparing the WGS, transcriptome analysis and the use of KlebsiellaNet. pUC-19 plasmid carrying zeocin marker was used for complementation of these genes into K2. Disk diffusion using meropenem and ertapenem was performed to check the diffusion pattern.

Results: Both the strains lacked OmpK35, while OmpK36 was present only in the K1 isolate. While, K1 was susceptible to meropenem, K2 were highly resistant. 14 putative candidate genes were short-listed for further analysis ( p < 0.05) among the 78 putative candidates. A transport protein exhibited a tendency towards meropenem susceptibility when complemented into K2 (Table 1) . Conclusion: 14 putative candidate genes were selected from >200 genes by using KlebsiellaNet. A protein belonging to MFS family transporter protein was obtained after gene complementation. Interestingly, the transporter was specific to meropenem alone. This is the first report on the contribution of this protein to meropenem resistance profile. While the transport protein mostly acts as efflux pump, the above results are contrary.

Identification of a new plasmid-mediated AmpC β-lactamase from a clinical isolate of Citrobacter freundii Chih-Ming Chen 1,2 , Hwa-Jene Wu 3 , Lii-Tzu Wu 4 *. Background: Citrobacter freundii in recent years have demonstrated resistance not only to traditional agents such as ampicillin, carbenicillin and cephalothin, but also to newer β-lactam agents such as third-generation cephalosporins, and monobactams; and some of them even produce carbapenemases. This study was performed to characterize a C. freundii isolates JA3 collected in Taiwanese hospital, carrying a novel AmpC-type β-lactamaseencoding gene. Methods: Using antibiotic susceptibility testing, plasmid analysis, transconjugation, clonimg and sequences analysis AmpC-type βlactamase-encoding gene. β-Lactamase gene-containing recombinant plasmids transformed into Escherichia coli were selected using ampicillin. Results: The resistance phenotype of JA3, was found to produce a β-lactamase that conferred resistance to oxyimino-cephalosporins and cephamycins. Resistance arising from production of a betalactamase could be transferred by conjugation with an IncW plasmid pJA3 into E. coli J53. The substrate and inhibition profiles of this enzyme resembled those of the AmpC beta-lactamase. The resistance gene of pJA3 proved to contain an open reading frame showing 96% amino acid identity with the plasmid-mediated enzyme CEF-1 of C. freundii KU6400. DNA sequence analysis also identified a gene upstream of ampC whose sequence was 91.0% identical to the ampR gene from C. freundii KU6400. In addition, a fumarate operon (frdABCD) and an outer membrane lipoprotein (blc) surrounding the ampR-ampC genes in C. freundii were identified, and insertion sequence (IS26) elements were observed on one sides of the sequences identified. Conclusion: These results clearly show that a new plasmidencoded AmpC gene with an ampR gene from C. freundii. The presence of the resistance region in this isolates, with diverse genetic determinants of resistance and mobile element, may further contribute to the emergence and spread of the gene, both at a chromosomal or/and plasmid level.

Hiroshi Hasegawa 1 , Keita Noguchi 1 , Yoko Mano 2 , Katsumi Fujitani 1,2 , Nobuhiko Furuya 1,2 *.

Background: Uropathogenic Escherichia coli (UPEC) are responsible for 70-90% of urinary tract infections (UTIs). Additionally, 20-40% of women who have been treated for UTIs experience recurrence at least once within 6 months of the initial infection. Despite proper antibacterial treatment, recurrent UTIs are secondary infections caused by the same strain of E. coli that caused the primary infection. The first-line agents for the treatment of UTIs are fluoroquinolones (FQs); however, in recent years, FQresistant E. coli have been increasingly reported worldwide. Proanthocyanidins (PACs), which are abundant in cranberries, have been used for the prevention of UTIs for over a century; however, the mechanism underlying this effect is unknown. In this study, A-type PAC was used to prevent recurrent UPEC infection via a different mechanism from that of conventional antibacterial therapy. Methods: The E. coli strain K-12 and the UPEC strains BK1, BK2, and BK3 were used in this study. The checkerboard method was used to evaluate the effect of the combination of an antibacterial and PAC on E. coli. Swarming and twitching motility was assessed using a plate-based assay. PAC was added at a concentration of 500 μM and was tested for its ability to inhibit the adhesion of E. coli to human bladder epithelial cells (HTB-9). Cell invasion inhibition was also tested in a similar manner. Results: The combination of PAC and CAZ showed a synergistic effect on BK1. PAC treatment significantly reduced the mean spreading range for swarming and twitching motility in K-12 and UPEC strains. The results for inhibition of the attachment of the UPEC strain to HTB-9 cells showed a statistically significant difference ( p < 0.05) with PAC (500 μM). Conversely, PAC did not inhibit the adherence of K-12 strain to HTB-9 cells. Mean values for HTB-9 cell invasion in PAC-treated UPEC strains were significantly smaller than those in untreated UPEC. Furthermore, K-12 did not show any invasion of HTB-9 cells. Conclusion: The combination of PAC and CAZ has an inhibitory effect on BK1. Because flagella-mediated motility has been suggested to be essential for UPEC to disseminate into the upper urinary tract, we propose that inhibition of flagellamediated motility may be a key mechanism through which PAC prevents UTIs. The in vitro anti-adhesive and anti-invasive activity of cranberry-derived PAC against UPEC suggests that its presence in urine can reduce bacterial colonization and UTI advancement.

food. Clinical manifestations include gastroenteritis, cellulitis, soft tissue infections, and a series of clinical syndromes that appear in immunocompromised patients. Due to the extensive use of antibiotics in clinical practice, the isolates of multidrug-resistant Aeromonas strains were increased. The pumps of the resistancenodulation-cell division (RND) superfamily are ubiquitous in Gram-negative bacteria and have the broadest substrate ranges. Increased expression of chromosomal genes for RND-type efflux systems plays a major role in the multidrug resistant of some Gramnegative bacteria. However, the contributions of RND efflux genes in development of multidrug resistant Aeromonas strains have not been evaluated in clinical settings. Methods: Minimum Inhibitory Concentration (MIC) was used for determination of the patterns of resistance to antibiotics in the presence or absence of efflux inhibitors. Multidrug resistance (≥3 antimicrobial drug classes) in Aeromonas strains were selected for further analysis. The bactericidal effects of treatments that combined efflux inhibitors with antibiotics were evaluated by survival rate. Real-time PCR was used to investigate the expression level of RND efflux genes. The experiments were done in triplicate. All results were presented as the means±S.D. and analyzed by Student's t-test. Results: Multidrug resistant Aeromonas strains were resistant to antibiotics including β-lactam, sulfonamides, quinolones and aminoglycoside. The MICs of piperacillin/tasobactam, erythromycin and imipenem were significantly reduced in multidrug resistant clinical isolates in the presence of efflux pump inhibitors PAßN. The expression level of RND efflux genes were elevated in multidrug resistant Aeromonas strains. Conclusion: RND efflux systems contribute to the drug resistance of Aeromonas strains. Higher level of RND efflux genes are found in multidrug resistant clinical isolates. More studies are needed to elucidate whether over expression of efflux systems confer intrinsic resistance to antibiotics in Aeromonas.

Hydrogen peroxide enhances bactericidal effects of toluidine blue-mediated photodynamic therapy on Methicillin resistant Staphylococcus aureus Yi-Ching Tsai 1 , Shin-Bei Wu 2,3 , Ya-Fang Tsai 2 , Yu-Chen Hsu 2 , Chia-Lun Yu 2 , Tak-Wah Wong 2,3 *. Background: The emergence of multiple-drug resistant bacteria, including Salmonella, has been a serious threat to public health worldwide. On top of that, Salmonella is capable of avoiding antibiotic attack after invading into host cells, leading to a problem in treatment. Thus, a novel therapeutic intervention to eradicate intracellular bacteria as well as to reduce the incidence of resistance is highly needed. Previously, we identified a small-molecule anticancer agent, AR-12, showing potent activity against Salmonella Typhimurium in macrophages and in a salmonellosis mice model. To dissociate its antibacterial activity from antiproliferative activity, a focused library of AR-12 was synthesized and screened for antibacterial activity against intracellular S. Typhimurium in macrophages and cytotoxicity toward the host cells. Methods: The antibacterial activity and cytotoxicity of AR-12 derivatives were first assessed by using an image-based highcontent system. Compounds showing good selectivity of antibacterial activity over cytotoxicity were further analysed with intracellular infection assay and MTT cell viability assay. The effect of compounds on bacterial growth in broth was evaluated by using MIC assay. Results: Among the 125 compounds tested, Cpd42 and Cpd9 exhibited a better antibacterial activity than that of AR-12. Cpd22 and Cpd24 showed an antibacterial activity equal to that of AR-12, but have a much lower cytotoxicity toward host cells. The result of MIC assay indicated that these compounds have no inhibitory activity on bacteria in growth medium, suggesting that the suppression of intracellular bacteria is via a host-targeted mechanism. Currently, studies to evaluate the efficacy of selected AR-12 derivatives on Salmonella-infected mice are underway. Conclusion: The findings provide a proof-of-concept that the antibacterial activity of AR-12 can be dissociated from its antiproliferative activity via structure optimization. AR-12 represents a promising scaffold for the development of novel hosttargeted antibacterial agents for the treatment of salmonellosis. Background: Clinical antibiotic failure may result from bacterial resistance but also from persistent forms of infection. Intracellular bacteria are protected from host immune defences and partly also from antibiotic action. Here, we study the activity of moxifloxacin in a model of intracellular infection of S. aureus (localized in phagolysosomes), comparing a reference strain to clinical isolates collected from patients with persistent infection.

Methods: Reference strain: ATCC25923. Clinical isolates: collected at the Bachmai Hospital (Hanoi) from patients still infected after 5 days treatment with an active antibiotic or with recurrence of a previous infection. MICs: microdilution (CLSI recommendations) with susceptibility according to EUCAST. Intracellular infection: human THP-1 monocytes (1 h phagocytosis and elimination of non-internalized bacteria by exposure to gentamicin; intracellular activity: change in CFU from initial inoculum at 24 h using a wide range of extracellular concentrations (0.003-100 × MIC). Static concentrations (C s ) and maximal efficacy (E max ): calculated from Hill equation fitted to concentration-response data (see Antimicrob Agents Chemother 2006,50:841-51). Results: As shown in the table, moxifloxacin C s was always close (1.5-4.2×) to its MIC for all strains, whatever their susceptibility in broth, but E max was significantly lower (less negative) for clinical isolates compared to reference strain. Conclusion: Intracellular forms of the clinical isolates obtained from patients with persistent infections are significantly less eradicated than the reference strain, suggesting a state of less responsiveness to moxifloxacin in the phagolysosomal environment. A potential link with therapeutic failure in vivo remains to be established.

Hsiow-Yun Hsue 1 , Wen-Po Tseng 2 , Huei-Ying Chen 3 , Shy-Ming Lee 4 , Wy-Chan Lee 5 *.

1 Chung Shan Hospital Infection Control Committee Leader, 2 Department of Medicine, 3 Laboratory of Department, 4 Chairman of the Committee, 5 Chung Shan Hospital Superintendent

Background: Antimicrobial resistance (AMR) is a major global public health problem both in hospital and community acquired infections. The knowledge and attitude of the public toward the use of antibiotics are the key factors influencing the antibiotics prescription and the consequence of antibiotics resistance. The antibiotics educational interventional program is designed to deliver the correct knowledge regarding the appropriate use of antibiotics in order to reduce the possibility of antibiotics resistance. Methods: The intervention was conducted by the educational intervention group with self-constructed survey list and the Cronbach's alpha value of 0.873 and the results were evaluated by using t test ( paired t-test) with random participants located around Da-an district in Taipei. There were 162 participants in the period from March to June, 2016. Results suggested that most participants have the correct knowledge after the educational intervention.

Results: Participants still prefer to take their antibiotics by their ways rather than follow the doctors' prescription, mostly because they didn't realize the important consequences of antibiotics resistance. Importantly, the educational intervention improved the knowledge of antibiotics use but have very limited influence on the attitude and behaviors of the participants.

The study suggested that a suitable way to deliver the correct knowledge regarding the use of antibiotics to the public is still an issue of concern to the authority. Methods: Each of 2 strains of E. coli, K. pneumoniae and P. aeruginosa were exposed to Ciprofloxacin DC ranging from 0.01-0.1 µg/mL; 50 µL, 100 µL and 200 µL of organism suspension was plated to ciprofloxacin containing agar plates. Following incubation at 35-37°C in O 2 for 18-24 hours, colonies selected off drug containing plates were tested to determine MIC and compared to pre-drug exposure MIC values. MIC values identical or within 1 doubling dilution were taken to indicate no impact on the tested strains. For killing, 10 5 cfu/mL of each organism was exposed to 0.01, 0.5, 0.1 Ciprofloxacin DC and log 10 reduction (and % kill) measured at 5,10,15,20,25,30,60,120,180 minutes after drug exposure. Results: MIC (µg/mL) values were as follows: E. coli 0.25 and 0.25, K. pneumoniae 0.5, 4; P. aeruginosa 0.25, 2. In all instances, regardless of the ciprofloxacin DC from 0.01-0.1µg/ml, post exposure MIC values were the same or within 1 doubling dilution to the pre-exposed MIC values. For kill experiments, log 10 reduction and percent kill of viable cells was not different than the organisms tested without drug; organism growth was minimal for the first 60 minutes of drug exposure after which organism growth was observed following 120 and 180 minutes following drug exposure and was not different between drug exposed and without drug assays.

Conclusion: Ciprofloxacin DC of 0.01-0.1 µg/mL below the MSW did not select for resistant subpopulations of E. coli, K. pneumoniae and P. aeruginosa. Similarly, exposure of the organisms in kill assays to 0.01, 0.05, 0.1 µg/mL of ciprofloxacin did not show any difference between drug exposed bacteria and those without drug. Low ciprofloxacin DC as observed after Cipro DPI administration does not appear to select for resistance with the organisms tested.

Background: Fluoroquinolone is a commonly prescribed antimicrobial agent, and up to 20% of its users registers adverse gastroenterological symptoms. We aimed to evaluate the association between use of fluoroquinolone and gastrointestinal tract perforation.

Methods: We conducted a nested case-control study on a national health insurance claims database between 1998 and 2011. The use of fluoroquinolones was classified into current (<60 days), past (61-365 days prior to the index date) and any prior year use of fluoroquinolones. We used the conditional logistic regression model to estimate rate ratios (RRs), adjusting or matching by a disease risk score (DRS). Results: We identified a cohort of 17,510 individuals diagnosed with gastrointestinal perforation and matched them to 1,751,000 controls. Current use of fluoroquinolone was associated with the greatest increase in risk of gastrointestinal perforations after DRS score adjustment (RR, 1.90; 95% CI, 1.62-2.22). The risk of gastrointestinal perforation was attenuated for past (RR, 1.33; 95% CI, 1.20-1.47) and any prior year use (RR, 1.46; 95% CI, 1.34-1.59). Current use of macrolide, an active comparator, was not associated with a significant increased risk of gastrointestinal perforation (RR, 1.11, 95% CI, 0.15-7.99). To mitigate selection bias, we have also excluded people who have never used fluoroquinolone before or people with infectious colitis, enteritis or gastroenteritis. In both of the analysis, a higher risk of gastrointestinal perforation was still associated with the use of fluoroquinolone. Conclusion: We found that use of fluoroquinolones was associated with a non-negligible increased risk of gastrointestinal perforation, and physicians should be aware of this possible association. Background: Drug-resistant tuberculosis (DR-TB), especially multidrug-and extensively drug-resistant (M/XDR-) TB are the emerging menace in the world. In recent years, to shorten the time for drug susceptibility testing (DST), the molecular diagnostics become popular. The accuracy of such methods depends on the reliable DST data. Then, we compared the whole genome sequencing (WGS) data with our DST results about 108 M. tuberculosis (MTB) isolates including MDR and XDR, to detect the mutations and evaluate those using the ""all-in-one web-based WGS analysing tool"" and to identify novel resistance responsible mutations.

Methods: 108 MTB were collected basically from DR-TB patients in Osaka Habikino Medical Center (Osaka, Japan) from 1998 to 2016 and we conducted WGS analysis of the isolates using MiSeq (illumina). TGS-TB (https://gph.niid.go.jp/tgs-tb/) was used to predict the mutations in responsible genes. The phenotypic DST results were obtained using the proportion method (Welpack-S, Kyokuto) for 10 anti-tuberculosis drugs and the minimum inhibitory concentration (MIC) for 16 drugs.

Results: 108 DR-TB included 44 MDR and 34 XDR, as the results of DST, and consisted of 88% lineage 2 (East-Asian) and 12% lineage 4 (Euro-American). To date, we analysed 30 isolates for 6 drugs. The Potential AMR prediction program of TGS-TB site (as of June 28th, 2017) predicted 85% of M/XDR-TB correctly. The proportion of the discrepancy between the phenotypic and the predicted DST was 6.7% in Isoniazid and Kanamycin, 10.0% in Rifampicin and Ethambutol, and 3.3% in Fluoroquinolones.

To increase the quality of molecular DST, we need to establish high quality genome database for DR-TB strains with reliable phenotypic DST. We will show the more detailed results of the DR-TB gene analysis including MIC data.

Li-Li Liu 1 , Shao-Gang Zhu 1 , Yong-Lin 1 , Ning-Ning Zhang 1 , Man-Li Tong 1 , Hui-Lin Zhang 1 , Li-Rong Lin 1 , Jiang-Hua Yan 2 , Tian-Ci Yang 1 . Methods: To analysis lncRNAs expression profiles and identify differentially expressed lncRNAs in CD4+ T cells response to T. pallidum infection in neurosyphilis patients. CD4+ T cells were isolated from peripheral blood mononuclear cells. RNA extraction was performed to conduct further experiments. Expression profiles of mRNAs and lncRNAs in CD4+ T cells isolated from neurosyphilis patients and healthy controls were analyzed by using microarray assay. Four lncRNAs were selected to conduct validation by using real time-quantitative polymerase chain reaction and the expression levels of those above mentioned lncRNAs in neurosyphilis patients and healthy controls were compared. Results: In total, 17, 278 lncRNAs and 15, 840 mRNA were found to be differentially expressed when comparing the CD4+ T cells between neurosyphilis patients and healthy control individuals. Of these transcripts, 2258 lncRNAs and 1728 mRNAs were significantly over or under expressed, respectively. Compared with healthy controls, AC068282.3 (fold change: 21.90) and CTD-3064M3.3 (fold change: 23.93) were the most over-expressed and under-expressed lncRNAs in neurosyphilis patients. The majority of differentially expressed lncRNAs are from intergenic regions (∼57.5%), intronic antisense to protein-coding loci (∼15.0%), natural antisense to protein-coding loci (∼12.1%). It is worth noting that 59 lncRNAs were significant difference along with significantly different mRNA. Among the 59 pairs of genes, LOC7999 mRNA was positively correlated with lncRNA RP11-160E2.16, RP11-160E2.11, RP11-160E2.19, respectively; NKX1-1 mRNA was positively correlated with lncRNA RP11-1398P2.1, RP11-160E2.19, XLOC_003422, respectively. The following five mRNAs were correlated with two differential lncRNAs, namely DUSP16, AP000349.1, FAM115C, TIMM8A, SMCHD1. GO analysis revealed that the differentially expressed coding genes were mainly involved in the biological processes, and among them, the most significant association was observed in defense response to fungus, defense response to bacterium, killing cells of other organism and disruption of cells of other organism. Subsequent pathway analysis was also conducted, and T cell receptor signaling pathway, MAPK signaling pathway, TGF-beta signaling pathway, etc. were found out to be associated with differentially expressed mRNAs.

Conclusion: The present study revealed the differential expression profiles of lncRNAs in CD4+ T cells response to T. pallidum infection in neurosyphilis patients. LncRNAs are involved in the key biological process in CD4+ T cell response to T. pallidum infection.

CovS regulates the expression of rgg through differential phosphorylation level CovR Chih-Yuan Kao 1 , Chuan Chiang-Ni 1,2,3 . Spd3. In addition, the inactivation of rgg enhances the ability to resistance stress conditions, which is thought to provide survival advantage to bacteria against the host immune system. Our previous study found that the expression of rgg is downregulated in the covS mutant. In this study, we aims to further elucidate whether CovS participates in the regulation of the rgg expression through modulating the levels of CovR phosphorylation.

Methods: CovS has been shown to have both kinase and phosphatase activities. To elucidate how CovS influences on the CovR phosphorylation status and to regulate the rgg, the kinase and phosphatase activities were inactivated by the site-directed mutagenesis (H280A and T284A, respectively). Phos-tag western blot analysis was performed to detect the CovR phosphorylation level in the covS mutants. In addition, the expression of rgg in covS isoallelic strains was analyzed by the RT-qPCR.

Results: The present study found that nonphosphorylated CovR inhibits rgg expression more strongly than the phosphorylated CovR. In addition, the expression of Rgg is regulated in the isoallelic strains (CovS H280A and CovS T284A strains). The results showed that the expression of rgg is decreased significantly in the CovS H280A strain; however, the expression of rgg is increased gradually in the CovS T284A strain compared to the wild-type strain. These results suggest that CovS regulates the expression of rgg through differential phosphorylation level CovR. Conclusion: CovS regulates the expression of rgg through differential phosphorylation level CovR.

Campylobacter jejuni cytolethal distending toxin suppresses HMGB1 and mobilizes RAGE into lipid rafts in response to inflammatory response Zhi-Pei Jiang 1 , Mei-Zi Huang 1 , Yu-An Chen 2 , Chih-Ho Lai 1,3 . Background: Cytolethal distending toxin (CDT) produced by Campylobacter jejuni contains three subunits: CdtA, CdtB, and CdtC. The interaction of CdtA and CdtC with membrane lipid rafts delivers CdtB into the nucleus, follows by induction of inflammation and cell death. High-mobility group box 1 (HMGB1), one of damage-associated molecular patterns (DAMPs), is a nuclear DNAbinding protein that stabilizes nucleosome and promotes DNA repair. Activation of HMGB1 signals is mediated by the receptor for advanced glycation end-products (RAGE), which is important for defending against bacterial infections. The aim of this study was to explore how CDT regulates HMGB1-mediated autophagy and what DAMP triggers RAGE's mobilization to lipid rafts to induce inflammatory. Methods: The molecular mechanisms underlying CDT regulates autophagy process that conferred cell death and cell cycle arrest were investigated. Biochemical analyses were conducted for exploring DAMP-mediated triggering of RAGE mobilization to lipid rafts in response to CDT-induced inflammation. Results: Our study showed that CDT induces RAGE expression, but suppresses that of HMGB1, indicating that DAMPs other than HMGB1 trigger RAGE activation in the inflammatory response. As the intracellular availability of HMGB1 activates autophagy, our results revealed that CDT controls HMGB1 that is important for regulating autophagy. We also found that DAMPs mobilize RAGE to lipid rafts in response to CDT-induced inflammation.

Conclusion: Our findings demonstrate that CDT suppresses HMGB1-mediated autophagy and triggers the DAMP-RAGEinflammation axis, which are important to the pathogenesis by C. jejuni.

Comparison of intravenous anesthetics in inhibition of IL-1β secretion and ROS production in S. aureus infected RAW264.7 cells Wen-Chun Lin 1 , Ming-Shan Chen 2 , Shew-Meei Sheu 3 *. Background: One of key pathways employed by the opportunistic pathogen P. aeruginosa to establish pathogenesis is by the formation of biofilms. Apart from biofilm formation, this bacterium is able to produce different virulence factors which will help in drug resistance and nutritional uptake. To overcome this problem, there is a need to establish alternative antimicrobials. Hence, this study was taken up to investigate the antibacterial and antivirulent properties of a flavonoid, quercetin against catheter associated P. aeruginosa. Methods: Selective P. aeruginosa strains were used to evaluate the effect of quercetin at different concentrations in, Bactericidal, biofilm viability, elastase, siderophore, pyocyanin, twitching motility, biofilm. Results: Bactericidal assay showed that, except one clinical strain (YU-V28) all the isolates were completely inhibited by quercetin at a concentration of 1 mg/mL, but quercetin did not show any significant effect on siderophore, pyocyanin and elastase production. The motility assay showed that in all the concentrations the strains formed microbial colony but unable to spread on the plate. Quercetin showed significant inhibitory effect on biofilm formation. At the concentration of 0.5 mg/mL except YU-V28 (OD 580 0.25) all the strains formed weak biofilm (OD 580 0.087-0.174) and at the highest concentration two clinical isolates formed weak biofilm (OD 580 0.12) and other strains were unable to form biofilm. Conclusion: Our study revealed that quercetin has good antibacterial and anti-biofilm activity. We suggest that this compound can be used in treating P. aeruginosa either as food supplement or in developing new antibiotic combinations. Background: Colistin resistance (CR) in Gram negative bacteria is a worldwide concern that increased recently with the discovery and spread of mcr-1 and mcr-2 plasmid-mediated genes. Active efflux is a mechanism that is known in various bacteria to confer resistance to various antibiotics, including tigecyclin, fluoroquinolones and carbapenem. Recently, one efflux pump inhibitor (EPI), the carbonyl cyanide 3-chlorophenylhydrazone (CCCP) showed an ability to reverse colistin resistance in colistin resistant Gram negative bacteria (Klebsiella pneumoniae, Pseudomonas aeruginosa, Stenotrophomonas maltophilia and Acinetobacter baumannii) with an unknown mechanism of action. The objective of the study was to evaluate the effect of this EPI on a collection of well defined colistin-resistant strains isolated in our laboratory, including naturally-resistant Gram negative bacteria and acquired-resistant bacteria (mcr-1, mgrB, pmrAB…) Methods: We have compared diameters of inhibition of colistin disk or Minimum inhibitory concentrations (MICs) realized by E-test method on Mueller-Hinton medium (MH) and on MH supplemented with 10 mg/L CCCP. Statistical analysis was performed using a Student test at a risk of 0.05. We studied a collection of 103 strains including 24 mcr-1 CR strains (19 Escherichia coli, 4 K. pneumoniae, 1 Enterobacter cloacae), 6 with a known mechanism of resistance (PmrA, PmrB, mgrB) for which 4 associated with mcr-1, 46 with unknown mechanism (Enterobacter spp., E. coli, Klebsiella spp., Salmonella enterica) and 27 intrinsically resistant Enterobacteriaceae (Serratia marcescens, Proteus spp., Providencia spp. and Morganella morganii). Results: Preliminary results showed a mean increase of colistin diameter of 6.44 ± 2.42 mm in mcr-1 positive strains when CCCP was added to the medium (diameters with colistin disk alone: 14.8 ± 1.66 mm; diameters with colistin disk with CCP: 21.3 ± 2.52; p-value = 1.72.10-13). In S. marcescens, MICs of colistin decrease from >256 mg/L to a mean of 0.132 [0.064; 0.19] mg/L ( p-value = 6.92.10-59). Genomic analysis is ongoing to identify the efflux pump(s) that could be involved in this mechanism of action due to CCCP. Conclusion: Here we show that CCCP, an efflux pump inhibitor, is able to reverse resistance to colistin in all colistin-resistant Gram negative bacteria and that this phenomenon is independent of the mechanism of resistance involved. Since other genes of resistance are implicated in resistance to colistin in these bacteria (mcr-1, mgrB…), this means that efflux pumps play an additional role in resistance to colistin. Genomic analyses are ongoing to identify such pumps that could be further targeted by specific EPIs in the future. This work opens a new area of research for the development of adjunctive therapy for CR bacterial infections.

Two lysine 2,3-aminomutases genes (AblA) in Methanoarchaea: AblA1 function as compatible solute synthesis gene and AblA2 as translational modification Yi-Shan Wu, Chuan-Chuan Hung, Cheng-Yu Chen, Mei-Chin Lai*. Department of Life Sciences, National Chung Hsing University, Taichung, Taiwan, R.O.C Background: N ε -acetyl-β-lysine is a ubiquitous compatible solute (osmolyte) among all methanoarchaea to counter the osmotic stress, which is synthesized from lysine through lysine 2,3aminomutase (AblA) to form β-lysine and the acetyl group is further transferred to β-lysine by β-lysine acetyltransferase (AblB). Whole genome sequences of the halophilic methanoarchaeon Methanohalophilus portucalensis FDF1 T revealed that there are two ablA genes; ablA1 and its co-transcribed ablB are responsible for N εacetyl-β-lysine synthesis, whereas ablA2 located within translation associated proteins and without ablB. Methods: RNA-seq was performed with Illumina HiSeq 2000. Phylogenetic relationship of AblA2 and 16S rRNA gene from methanogenic archaea and bacteria were constructed with Neighbor-joining method by MEGA 6 program. Both MpAblA1 and MpAblA2 were heterologously overexpressed in E. coli BL21 (DE3)-RIL and further purified by nickel affinity chromatography. And maintained both overexpressed proteins under the reduction state in the dihydrolipoic acid and sodium dithionite buffer. The enzymatic activity assay was determined by β-lysine production through ultra performance liquid chromatography. Results: Transcriptomic anslysis showed that the transcriptional expression of ablA2 are higher than ablA1 at the optimal growth condition and also under salt stress (29-457 fold) and temperature stress (16-669 fold), which indicated that demand for β-lysine was higher than compatible N ε -acetyl-β-lysine. Phylogenetic analysis based on the amino acid of AblA2 from archaea and bacteria indicated that this halophilic methanoarchaeal ablA2 may be horizontally transferred from Deltaprotebacteria. The amino acid sequence similarity between AblA1 and AblA2 are 41%, AblA2 and lysine 2,3-aminomutase from Desulfuromonadales bacterium C00003107 are 75%. As AblA1, homology modeled structure of AblA2 contains the Fe-S cluster, SAM binding domain, PLP binding site, but the C-terminal zinc binding sites was not found in AblA2. To further investigate the enzyme activities of AblA1 and AblA2, both genes are transformed and overexpressed in E. coli BL21(DE3) RIL, respectively. Both MpAblA1 and A2 proteins were maintained at reduction state with the addition of dihydrolipoic acid and sodium dithionite. And the UPLC system used to identify the end product β-lysine was established for future AblA1 and A2 enzyme kinetic investigation. Conclusion: Through genomic, transcriptomic and phylogenetic analysis, we identified two lysine 2,3-aminomutases genes (AblA) in Methanoarchaea: which AblA1 functions as compatible solute synthesis gene and AblA2 may function as translational modification that involve in PTM-β-lysylation.

Methanoarchaea community in the cold seep area at four way closure ridge offshore southwestern Taiwan: by culturedependent method Sheng-Chung Chen, Mei-Chin Lai. Department of Life Sciences, National Chung Hsing University, Taichung, Taiwan

Background: Widespread accumulations of methane hydrate within the accretionary prism offshore SW Taiwan are postulated to occur based on the presence of the intensity of bottom simulating reflections. Four Way Closure Ridge (FWCR) is one of the sites with intensive methane seepages and the chemosynthetic community with highly dense mussel bed was found. The carbon isotopic composition of fluxed methane indicates the biogenic methane source known only produced by methanoarchaea is dominant in this area. Here we describe the methanoarchaea community in this cold seep area through culture-dependent method. Methods: We used video-guided multi-corer (V-corer) to collect sediment samples in the cold seep area, C5 station at FWCR, during the OR3-1900 cruise of R/V OR3 on 16-20 Nov, 2015. Methanoarchaea in sediment samples were enriched at room temperature using rich media (MB/W) with mixed substrates, including acetate, formate and methanol. The growth of methanoarchaea was monitored by methane production using gas chromatography. Pure or partial pure methanoarchaeal isolates were purified through roll-tube technique and were identified by using phylogenetic analysis of 16S rRNA gene sequences. Results: Total 4 sediment cores and 30 methanoarchaea enrichments were retrieved from the cold-seep region of the FWCR. After 4 months incubation, 20 bottles of methanoarchaea enrichments contained methane production (∼14 to 6900 μmole/bottle), which indicated that there were methanoarchaea grew inside. And anaerobic roll-tube technique was performed to obtain single colony from the methane produced enrichment samples. Conclusion: Through culture-dependent method, we reveal that dominant methanoarchaea are Methanoculleus strains which occur ∼73% relative abundance in the cold seep region of FWCR offshore SW Taiwan. This may indicate that Methanoculleus strains are prevailing methane producers in cold seeps. In addition, endemic and potential new species of methanoarchaea from cold seep regions were collected in this study.

Inhibitory effect of Ganoderma neojaponicum water extract on osteoclast differentiation via up-regulation of heme oxygenase-1 and down-regulation of NFATc1 expression Chin-Huei Lee 1 , Hai-Wei Cheng 2,3 , Jiin-Tsuey Chen 1 , Chi-Liang Chern 3 . 1 Department of Biological Sciences, National Sun Yat-sen University, Kaohsiung, Taiwan 2 Department of Orthopedic surgery, Fooyin university Hospital, Pingtung, Taiwan 3 Department of Medical Laboratory Sciences and Biotechnology, Fooyin University, Kaohsiung, Taiwan Background: Osteoclasts are multinucleated bone-resorbing cells that are differentiated by receptor activator of nuclear factor kappa-B ligand (RANKL) and macrophage colony-stimulating factor (M-CSF). Reactive oxygen species (ROS), generated by NADPH oxidase upon RANKL stimulation, has been demonstrated to play a key role in osteoclast differentiation. Our previous studies have shown that Ganoderma neojaponicum water extract (GNES) increased heme oxygenase-1 (HO-1) protein expression and decreased hydrogen peroxide-induced oxidative stress in murine RAW264.7 macrophages. Methods: GNES, isolated from water extract of Ganoderma neojaponicum, was used to test its osteoclastogenesis inhibition effect in RANKL-stimulated bone marrow-derived macrophages (BMMs) or murine monocytic cell line RAW264.7 in vitro.

The results showed that GNES could inhibit RANKLinduced ROS production and the formation of osteoclast. Moreover, GNES treatment decreased the expression of nuclear factor of activated T cells cytoplasmic-1 (NFATc1), a master regulator of osteoclast differentiation, and of osteoclast marker such as cathepsin K. In addition, GNES impaired the activation of extracellular signal-regulated kinase (ERK), p38 mitogen-activated protein kinase, and c-Jun N-terminal kinase (JNK). Conclusion: Our data suggested that GNES inhibited osteoclastogenesis through downregulation of RANKL-induced ROS production by HO-1 upregulation.

The amino acid residue Histidine-65 of a novel Enzyme-IIA affects 1,3-popanediol production in Klebsiella pneumoniae Novaria Sari Dewi Panjaitan 1 , Yu-Tze Horng 2 , Po-Chi Soo 1,2 .

1 Institute of Medical Sciences, College of Medicine, Tzu-Chi University, Hualien, Taiwan, 2 Department of Laboratory Medicine and Biotechnology, Tzu-Chi University, Hualien, Taiwan 1, , also known as trimethylene glycol, a multifunctional valuable organic compound, recently has been used in various industry field, such as the synthesis of biodegradable polyesters, polyurethanes and polyethers. Besides the chemical methods, 1,3-PD can be produced through fermentation processes of glycerol in certain bacteria, including Klebsiella pneumoniae. Glycerol is metabolized fermentatively to 1,3-PD in bacterial aerobic or anaerobic metabolic pathways. Aerobically, glycerol can be uptaken into bacterial cell through glycerol facilitator protein (GlpF), then will be phosphorylated by glycerol kinase (GlpK) to become glycerol-3-P, for being further metabolized. In Enterobacteriaceae, phosphotransferase system (PTS) protein Enzyme-IIA Glc (also called Crr, for catabolite repression resistance) was found to be the central regulator of carbon metabolism. Unphosphorylated Crr protein can bind to GlpK to inhibit its function. In previous study we found that KPN00353, a novel KpEIIA, could inhibit 1,3-PD production in Klebsiella pneumoniae by direct binding to GlpK. Based on the protein-protein interaction site prediction, amino acid residue Histidine-65 (His-65) is predicted to be the important residue for KpGlpK-KPN00353 binding. Several mutations at His-65 of KPN00353 were designed in order to mimic either unphosphorylation or phosphorylation of KPN00353 protein. We found that the unphosphorylated KPN00353 could bind stronger to KpGlpK than the phosphorylated form, resulting in higher nonuptaken glycerol concentration in the used medium and lower 1,3-PD production. Background: Proteus mirabilis is a common human pathogen causing urinary tract infections (UTIs), especially in patients with indwelling urinary catheters. Many regulatory mechanisms have been evolved for bacteria to adapt to changes of diverse environments. Type VI secretion systems (T6SS) are known to contribute to bacterial pathogenicity by exerting toxic effects on host cells or competing bacterial species and are involved in a broad variety of functions such as biofilm formation and stress sensing. This large array of functions is reflected by a diversity of regulatory mechanisms. The regulation of P. mirabilis T6SS remains unclear. Recently, our transcriptome analysis revealed P. mirabilis T6SS was regulated by many regulators. Methods: We first confirmed the regulation of T6SS by regulators using real-time RT-PCR and the promoter reporter assay and identified one T6SS main structure operon and four Hcp/VgrG effector operons in P. mirabilis N2 by using sequence specific RT and then PCR. We investigated whether the regulators can bind directly to T6SS promoter region using a DNaseI footprinting assy. Overexpression of the effector toxin was performed to disclose its function. The growth predominance test and killing assay were used to observe the competition between wild type and the regulator mutant. We contructed vipAB (encoding the T6SS sheath) mutant to confirm the regulator-mediated killing of T6SS. We also purified the possible toxin to assess its function in E. coli killing. Results: Many regulators positively regulated T6SS by binding directly to T6SS promoter region and also positively regulated the expression of the effector toxins. Overexpression of the effector toxin in wild-type P. mirabili resulted in increased killing of regulator mutants and E. coli. Accordingly, the growth predominance test and killing assay demonstrated that P. mirabilis T6SS conferred growth predominance on the wild-type over regulator mutants and E. coli. Conclusion: The expression of P. mirabilis T6SS was positively regulated by many regulators, which affected its killing function. T6SS contributed to growth predominance over regulator mutants and E. coli, suggesting P. mirabilis T6SS as an antibacterial weapon. Background: Rapid diagnosis of Bacteremia is critical for sepsis patients. The new generation blood culture system, VirtuO™, possesses fully automatic functions including sign-in, loading and unloading bottles which reduce hands on time and is easy to operate for hospital staffs. To improve the delay-entry into blood culture system for the specimens in the evening and night shifts for lacking manpower to run a 24-hour microbiology service, VirtuO™ was introduced into a emergency room (ER) of a Taiwan hospital that allows nursing staffs to load blood culture bottles immediately after collection in day, evening and nigh shifts. The benefit of the intervention in turnaround time (TAT) of blood culture was investigated. Methods: The study was conducted in a 1500-bed hospital. The efficacy of introducing VirtuO™ into ER was compared to an existed procedure that blood culture specimens were loaded into BacT/ Alert 3D system only in the day-shift in a microbiology Lab with regards to TAT from medical order to detection time for evening and night shifts specimens. Results: Application of VirtuO™ in ER has achieved the following results. First, The time from medical order to blood culture bottle loading into incubator shortened for 13.7 hours in average. Second, TAT from medical order to positive detection in 90% positive cultures when VirtuO™ in ER were within 43.3 hrs, while this TAT in 90% positive cultures for the previous procedure with only BacT/Alert 3D in microbiology laboratory were within 55.7 hrs. Third, 56% VirtuO™ bottles were positive in 24 hours, while 19% in previous BacT/Alert 3D procedure were positive in 24 hours. Conclusion: VirtuO™ is an automated blood culture system and easy to operate for ER staffs. Introducing VirtuO™ in ER solves the problem of delayed entry including the time for transportation of blood culture bottles from ER to microbiology room and avoids the need of manpower for transportation in evening and night shifts, while loading blood specimens into blood culture system can be performed immediately. The intervention resulted in faster blood culture results, and therefore would be helpful for treatment of sepsis patients and antibiotic stewardship. Background: Tedizolid phosphate 200mg, once-daily, 6-day treatment has been demonstrated to be non-inferior to linezolid 600 mg, twice-daily, 10-day treatment in patients with ABSSSI, which was further investigated in a multicentre phase 3 study in a predominantly Asian population. Methods: Adult patients with ABSSSI (i.e. cellulitis/erysipelas, major abscess, or wound infection), due to suspected/confirmed Gram-positive pathogen, were randomised to receive intravenous/ oral (IV/PO) once-daily tedizolid phosphate (200 mg, 6 days) or IV/ PO twice-daily linezolid (600 mg, 10 days). The primary endpoint was early clinical response rate based on ≥20% lesion size reduction at 48-72 hours after treatment initiation in the intent-to-treat (ITT [all randomised]) population. Non-inferiority was predefined as the lower limit of 95% confidence interval (CI) for the betweentreatment difference was greater than -10%. Results: Patients (N = 598) from Mainland China, Taiwan, Philippines, and the USA were randomised: N = 300 and N = 298 in tedizolid and linezolid arms, respectively. Baseline characteristics were similar between tedizolid and linezolid arms regarding mean age (45.7 years, 47.5 years), proportion of Asian patients (63.7%, 64.8%), males (69.7%, 64.4%), and cellulitis/erysipelas (64.0%, 64.1%), respectively. As for the primary endpoint, the treatment difference was -4.6% (95% CI: -11.2%; 2.2%). In the modified ITT population (mITT; excluding patients without taking any dose of study drug [tedizolid: n = 8, linezolid: n = 1]), the treatment difference was -2.7%, with the lower and upper limits of 95% CI as -9.4% and 3.9%. Secondary endpoints including clinical response rate at end-of-therapy (EOT) and investigator's assessment of clinical success at post-therapy evaluation (PTE) showed high clinical efficacy in ITT and clinically evaluable (CE) populations. Microbiological eradication rates were favourable for both treatments, including MRSA infections (tedizolid: 72.4%, linezolid: 62.5%). Treatment-emergent adverse event (TEAE) rates for tedizolid and linezolid were 49.7% and 45.8%, and drug-related TEAEs were 20.9% and 15.8%, respectively. Drug-related injection site phlebitis was slightly more frequent with tedizolid (n = 8) than linezolid (n = 0). Discontinuations due to TEAEs were ∼2% in each arm. No deaths were reported. Conclusion: Non-inferiority of 6 days of IV-to-oral, once-daily 200 mg tedizolid phosphate to 10 days of IV-to-oral, twice-daily 600mg linezolid in early clinical response was inconclusive at 48-72 hours after treatment initiation. Tedizolid showed consistent high clinical efficacy at the later timepoints (EOT and PTE) comparable with the pivotal ESTABLISH studies1. Tedizolid was highly efficacious against MRSA infections and was well tolerated.

Tigecycline treatment for severe C. difficile infection: CDI BI/NAP1/027 Tse-Hao Chen 1 , Soon-Hian Teh 1,2 , Lih-shinn Wang 1,2 , Wei-Yi Lei 1,3 , Huei-jen Chao 4 , Siao-ru Lin 5 . 1 School of Medicine, Buddhist Tzu Chi University, Hualien, Taiwan, 2 Division of Infectious Diseases, 3 Background: Clostridium difficile infection (CDI) is a common cause of iatrogenic diarrhea. Hypervirulent toxinotype III BI/NAP1/027 strain of C. difficile was easily found in the North America, but only three cases were previously described in Taiwan. Metronidazole and vancomycin are used as first-line treatment for CDI. As refractory/severe CDI becomes more frequent and the drug of choice, fidaxomicin, is not available in most hospitals in many countries, we may need to consider an alternative treatment, such as tigecycline, for NAP1/BI/027 strain CDI. Methods: We retrospectively reviewed all patients with diagnosis of CDI confirmed by stool culture, Ag testing or PCR, from January 2016 to March 2017 in a tertiary hospital in eastern Taiwan. GeneXpert C. difficile/Epi Assay was used to identify NAP1/BI/027 strain CDI in selective cases when the physicians in charge felt necessary. Results: A total of 71 cases were diagnosed with CDI. Among them, two were identified as NAP1/BI/027 strain CDI by GeneXpert, both were also confirmed by culture. Both of our cases can be classified as severe complicated CDI because they had consciousness change accompanied with severe bandemia. Both failed to conventional combination treatment with metronidazole and vancomycin. As a result, intravenous tigecycline treatment was prescribed as an adjunctive therapy. Both of our patient recovered uneventfully from this hypervirulent strain. The successful results were shown by sigmoidoscopy before and after tigecycline treatment in one patient (Figure) . Conclusion: In this study, the advantage of adding tigecycline to standard treatment of NAP1/BI/027 strain CDI was clearly manifested. Tigecycline combined with either metronidazole or vancomycin appears to be a reasonable therapy for severe and severe-complicated CDI.

Clinical characteristics of bacteremia caused by vancomycinsusceptible Enterococcus faecalis and Enterococcus feacium in a tertiary hospital; a case-control study: alert to Enterococcus faecium strains with high minimum inhibitory concentrations of daptomycin Hiroki Namikawa 1 , Hiroshi Kakeya 1 , Koichi Yamada 1 , Wataru Shibata 1 , Hiroki Fujimoto 1 , Yukihiro Kaneko 2 . 1 Infection Control Science, 2 Bacteriology, Osaka City University Graduate School of Medicine, 1-4-3 Asahi-machi, Abeno-ku, Osaka 545-8585, Japan

Background: The mortality of bacteremia due to Enterococcus faecium was high. Further, E. faecium strains with high minimum inhibitory concentrations (MIC) of daptomycin (DAP) have recently been a problem in clinical practice. This study aimed to assess the predictor factors and prognosis of patients with bacteremia due to E. faecium and the antimicrobial susceptibility focusing on DAP against E. faecium isolates. Methods: The medical records of 60 patients with E. faecalis bacteremia and 48 patients with E. faecium bacteremia who had been admitted to Osaka City University Hospital between January 2011 and March 2016 were retrospectively reviewed. Patient backgrounds, predictor factors for infection, prognosis, and antimicrobial susceptibility of the isolates from bacteremia were evaluated. Results: The mean Sequential Organ Failure Assessment (SOFA) score and the mortality did not differ between the two groups. However, independent predictors associated with E. faecium bacteremia in the multivariate analysis were the use of quinolones ( p = 0.025), malignancy ( p = 0.021), and prolonged hospitalization ( p = 0.016). Cardiovascular disease was the opposite predictor ( p = 0.015). Further, the number of E. faecium isolates with DAP MIC ≥3 was more than that of E. faecalis ( p = 0.036). Conclusion: Careful use of DAP is crucial when administering therapy for E. faecium bacteremia. Background: Fitz-Hugh-Curtis Syndrome, which was first described in 1920, is defined as perihepatitis concomitant with pelvic inflammatory disease. Chlamydia trachomatis is considered the culprit pathogen. This syndrome usually features with right upper quadrant abdominal pain mimicking other hepatobiliary diseases, posing a clinically diagnostic challenge. Case report: A 29-year-old female was admitted because of fever, chills, persistent right upper quadrant abdominal pain, and vaginal discharge. The laboratory data revealed leukocytosis with neutrophils predominant. On admission, the abdominal CT revealed mild right side pelural effusion, and a 3.8-cm-sized mass lesion below the cecum. The transvaginal ultrasound indicated fluid accumulation in the pelvic cul de sac, and bilateral adnexa mass, favouring endometrioma. The serum serology revealed Chlamydia trachomatis IgA positive. Clinical and laboratory findings were suggestive of Fitz-Hugh-Curtis Syndrome. Two weeks after treatment with intravenous Flumarin and Clinicin, the patient showed significant clinical improvement. Conclusion: Fitz-Hugh-Curtis Syndrome should be included as a differential diagnosis in women with right upper quadrant pain, especially at their reproductive ages.

Positive Aspergillus galactomannan assay in a severe influenza patient co-infected with fungal Curvularia species Wen-Liang Yu. Department of Intensive Care Medicine, Chi Mei Medical Center, Tainan, Taiwan; Department of Medicine, Taipei Medical University, Taipei, Taiwan Background: Invasive pulmonary aspergillosis (IPA) might occur in severe influenza patients. In Taiwan, cases of severe influenza with IPA are increasingly reported. Aspergillus galactomannan (GM) antigen assay greatly facilitates early diagnosis in high-risk patients. However, GM antigen could be positive in infections caused by other filamentous fungi, such as Penicillium and Curvularia species, and careful clinical correlation is advised. Methods: A 58 y/o man of COPD had fever and productive cough for 2-3 days. He denied other systemic disease. Dyspnea on exertion (like climbing stairs), wheezing sound and mild chest tightness were noted. He was brought to our ER, where his temperature was 39.7 degree Celsius and BP was 141/116 mm Hg. Emergent intubation with mechanical ventilation was performed for impending respiratory failure. Lab data revealed leukocytosis, elevated lactate level (8.8 mmole/L), and mixed respiratory and metabolic acidosis. Influenza A antigen was positive. PCR methods confirmed FluA but different to H1N1 and H3N2. CXR showed emphysematous change with mild infiltration in both lungs. Hypotension occurred at ER, so fluid resuscitation with inotropic agent infusion was used. Under the impression of septic shock, COPD with acute exacerbation, and influenza A infection, he was admitted to the intensive care unit (ICU) for further management.

Results: In ICU, piperacillin-tazobactam and oseltamivir were used. Sputum culture yielded a mold species of Curvularia. Blood Aspergillus GM antigen index was >9.24 (normal index, <0.5). Therefore, voriconazole was initiated. Another sputum culture yielded Pseudomonas aeruginosa and Elizabethkingia meningoseptica. Antibiotic was shifted to imipenem and tigecycline. However, dyspnea became air-hunger pattern. CXR showed bilateral consolidation in progression, compatible with acute respiratory distress syndrome. He developed profound shock status and was expired after 10 days of hospitalization. Conclusion: Aspergillus GM positive has been found in patients with severe influenza in Taiwan. We first report positive Aspergillus GM testing in a severe influenza patient with Curvularia infection. Early diagnosis and management for IPA is important. Treatment of Curvularia infection usually consists of amphotericin B or itraconazole. Voriconazole also appears promising. Our case highlights that positive Aspergillus GM testing could occur in other invasive fungal infection. Therefore, GM assay must be interpreted with caution due to cross reactions to other fungi, which might need different antifungal therapy. Background: Invasive pulmonary aspergillosis (IPA) might comprise up to 23-29% of severe influenza patients. In Taiwan, epidemiological association of severe influenza with IPA has been reported. Meanwhile, prior high-level ambient fine particulate matter with a diameter of 2.5 micrometres (PM 2.5 ) before influenza epidemic was also noticed. However, detailed evidence of epidemiological linkage between PM 2.5 and IPA has been limited. Therefore, we compare the correlation of PM 2.5 (as monthly average level) by different exposure periods and critical level to the development of IPA among severe influenza and non-influenza patients. Methods: We retrospectively reviewed adult patients with severe influenza in a tertiary medical center in Taiwan from January 2015 through March 2016. The definition of probable IPA required abnormal radiological findings and positive Aspergillus galactomannan (GM) antigen and/or Aspergillus isolation. Platelia Aspergillus Ag EIA (Bio-Rad Laboratories, Marnes-La-Coquette, France) was used to detect GM in blood with a positive cut-off value of ≥0.5 index. PM 2.5 as an index of ambient air pollution in Tainan city was obtained from Taiwan Air Quality Monitoring Network. Spearman's correlation was used to measure the strength of epidemiological association between two variables. Results: A total of 124 patients with severe influenza were found during the study period. Monthly average PM 2.5 level and exposure to monthly critical level of >30 μg/m 3 within previous 1-2 months has significant correlation with the development of IPA in severe influenza patients, but has no significant correlation with IPA in non-influenza patients (Table) . Influenza cases significantly increased when average PM 2.5 exceeded critical level during prior 1st month. Conclusion: PM 2.5 exposure within previous 1-2 months, particularly above critical level, may be positively correlated with the development of IPA in the setting of severe influenza. Results: Among these 7 cases, only one patient was more than 65 years old, and 5 patients were female (71.4%). Clinical manifestations include life threatening encephalitis, pneumonitis, generalized lymphadenopathy and multi-organ involvement. Two patients received PCR for Toxoplasma; one showed positive result (50%). Two patients was confirmed to have HIV infection (28.5%). Two patients had fever or sepsis during the course, one of them presented with brain lesions. Four patients (57%) presented with multiple lymphadenopathy. Three patients with CNS involvement had ICU stay for one to two months, with one mortality; alsoToxoplasma antibodies were undetectable in two of these three cases. In the two cases taking IFN-γ profile study, anti-IFN-γ auto-antibodies were not detected in plasma; however, IFN-γ levels in whole blood were both lower compared with control group, which might be indicative of immune deficiency.

Conclusion: Most patients with multiple sites of lymphadenopathy could be controlled well with medications after excision; however, CNS involvement may cause more complicated clinical course, which need intensive medical care and may even cause mortality. Lab tests for Toxoplasma antibody is usually a rapid tool for detection but has its limitation especially in patients with immunodeficiency. IFN-γ profile could provide information of patients' immune status in disseminated toxoplasmosis.

Chun-yu Chang 1 *, Lih-shinn Wang 2 , Tsung-lang Chiu 3 , Pau-yuan Chang 4 , Chih-ming Hu 5 , Chia-ching J. Conclusion: Previous publications have reported ischemic stroke complicating meningococcal meningitis. However, to our knowledge, this is the first report of subdural effusion secondary to meningococcemia and meningitis in an adult. Early detection and appropriate treatment may improve the outcome.

Streptococcus acidominimus endocarditis presenting as acute coronary syndrome Yen-Chen Lin 1 , Lih-shinn Wang 2 . Background: Filamentous basidiomycetes are emerging fungal pathogens in immunocompromised hosts. Volvariella volvacea is a non-sporulating filamentous basidiomycete that is commonly known as the straw mushroom. We describe a case of invasive fungal infection by Volvariella volvacea in a patient with T-cell acute lymphoblastic leukaemia (ALL) on chemotherapy. This infection involved the brain, lung and skin, and was eventually fatal. Methods: A 45-year-old male Chinese Singaporean resident was diagnosed with T-cell ALL in October 2016. He was treated with the first cycle of hyper-CVAD regimen and intrathecal cytarabine with fluconazole prophylaxis. His treatment was complicated by a linerelated bloodstream infection and intermittent confusion. MRI brain showed several ring-enhancing lesions. CT thorax showed nodules with cavitation.

He was started on empiric IV ceftriaxone 4 gm daily and was switched to posaconazole prophylaxis. He underwent bronchoalveolar lavage, lumbar puncture and brain biopsy but these were not diagnostic. 3 weeks later, a repeat MRI brain and CT thorax demonstrated improvement of the lesions. He was continued on the second cycle of hyper-CVAD in December 2016. Unfortunately, he deteriorated 1 day later with another line-related infection and new nodules on repeat MRI brain. Again, brain biopsy did not yield a diagnosis. The patient became progressively obtunded and was commenced on empiric pyrimethamine, clindamycin, folinic acid, meropenem and liposomal amphotericin B. He then developed new skin nodules which were biopsied. Histology revealed broad-appearing fungal hyphae. The patient succumbed to the infection a few days later.

Results: Skin biopsy culture grew colonies of a light browncoloured mould. It was identified as Volvariella volvacea by molecular sequencing. Conclusion: Our patient may have partially responded to the switch in prophylaxis from fluconazole to posaconazole, which paradoxically may have made the diagnosis more difficult. This patient's infection resembled other forms of invasive fungal infections with lung-brain syndrome and skin involvement. On histopathologic examination, it was difficult to distinguish Volvariella volvacea from other invasive moulds. The key to managing this infection is in obtaining tissue diagnosis by microbiology, assisted by molecular sequencing techniques. Volvariella volvacea is an emerging and potentially fatal cause of invasive fungal infection in patients with haematologic malignancies, for which prompt diagnosis and treatment is crucial.

Staphylococcus warneri bacteremia complicating peritonitis in a patient with hyperthermic intraperitoneal chemotherapy (HIPEC) Wen-Sen Lee 1 , Fu-Lun Chen 1 , Shio-Shin Jean 2 , Tsong-Yih Ou 1 . erythromycin (≧8 mg/L), oxacillin (≧4 mg/L), gentamicin (≧16 mg/ L) and tetracycline (≧16 mg/L), and susceptible to vancomycin (≦1 mg/L), linezolid (≦2 mg/L), moxifloxacin (≦0.5 mg/L), ciprofloxacin (≦0.5 mg/L) and clindamycin (≦0.5 mg/L). The examination of transesophageal echocardiography (TEE) showed negative finding. The pulse-field gel electrophoresis (PFGE) of Staphylococcus warneri of the strain 1 ( peripheral blood) and strain 2 (ascites) showed the same identical pattern (Figure 1 ). Figure 1 : The pulse-field gel electrophoresis (PFGE) of Staphylococcus warneri of the strain 1 (peripheral blood) and the strain 2 (ascites) showed the same identical pattern. Conclusion: Pulmonary aspergillosis was proved. However, the tissue prove of brain and liver was not performed due to patient's will and complication. However, the patient's condition and image study showed improve after the Voriconazole was prescribed. Hence, the invasive aspergillosis involve lung, liver and brain was highly suspect. Background: Gastrointestinal (GI) bleeding is more common in patients with end stage renal disease (ESRD) and is associated with higher mortality than in Cytomegalovirus (CMV) infection. CMV disease may occur in ESRD patients without the presence of overt immunodeficiency. The gastrointestinal tract is the most common site of involvement. Clinicians should be aware of this possibility in ESRD patients who have GI symptoms. We reported a case of ESRD with regular hemodialysis (HD) accompanied with severe GI bleeding by CMV colitis. Case report: This 70 years old female had a past history of ESRD on regular HD. This time, she suffered from coffee ground and vomiting tonight. His husband said she recived HD program with weight loss only one kilogram yesterday. Due to above reasons, she was sent to emergency room (ER) for help. At ER, laboratory data showed severe hyperkalemia, with atrial flutter and coffee ground. Emergency HD was done, then she was then admitted for intensive care ward due to coronary artery disease with three veneral disease (CAD with 3-V-D), and intermittent tachy-bradycardia. After ICU, blood transfusion was performed and high dose protonpump inhibitor (PPI) infusion were also performed. Upper gastrointestinal endoscopy (UGI endoscopy) was was done, which showed gastric ulcers, antrum, with bleeding, post hemoclips; duodenal ulcers. Electrocardiogram revealed T wave -inversion so echocardiography and cardiologist was consulted due to CAD with 3-V-D, with the suspicious of non STelevation myocardial infarction due to anemia. However, echocardiography showed no evidence of hypokinease so medication use after clinical condition improved. Chest X-ray showed right upper lobe consolidation, and sputum culture was Acinetobacter baumannii. So we added antibiotics with ceftazidime plus vancomycin for infection control. We informed the condition to her family and repeated UGI endoscopy due to high risk for rebleeding. Due to easy bleeding and positive CMV-polymerase chain reaction in gastric fluid was noted. GI bleeding was stopped after ganciclovir infusion for two days. General condition got much better, so we shifted iv form ganciclovir to valganciclovir from 2016/11/29 to 2016/12/05 and she was discharged on 2016/12/05. Discussion: CMV can be established as a potential infection, which can lead to reactivation with immunosuppression. It can affect almost any organ system, gastrointestinal tract involvement is the most common. Because of the rare occurrence of CMV-associated fatal multiple gastric and duodenal bleeding, high index suspicion is needed in immunocompromised patients with gastrointestinal bleeding for early diagnosis and possible beneficial outcomes.

Histoplasma blood stream infection with bone marrow involvement in an AIDS patient Da-Chuan Yeh 1 , Yu Huai Ho 1 , Lih-Shinn Wang 1 , Ming-Huei Gu 2 , Huei-jen Chao 2 , Yung-Hsiang Hsu 3 . 1 Division of Infectious Diseases, Department of Internal Medicine, 2 Department of Laboratory Medicine, 3 Department of Pathology, Buddhist Tzu Chi General Hospital, Hualien, Taiwan

Background: Histoplasma capsulatum are rarely seen in Taiwan.

Here we report a case of disseminated histoplasmosis in an AIDS patient.

Methods: A 37-year-old man was admitted with one-month history of high fever and abdominal pain. He was newly diagnosed AIDS two weeks before and transferred to our hospital for further treatment. On physical examination the patient appeared weak and thin. His temperature was 38.5°C, with a pulse 105 beats per minute, respiration rate 21 per minute. Laboratory studies revealed hemoglobin of 8.5 g/dL, leukocyte count 1,310/μL, platelet count 45,000/μL, AST 165 IU/L, ALT 96 IU/L, CD4 count 0/mm 3 , and HIV viral load 3,570 copies/mL. Abdominal to pelvic CT showed splenomegaly, enlarged retroperitoneal, mesenteric LNs and massive amount of ascites. Lung CT showed consolidation at left lower lung field with pleural effusion. Opportunistic infection surveys were negative except PCR assays for herpes simplex virus and cytomegalovirus from blood and bronchial alveolar lavage were both positive. Lymph node biopsy was not feasible due to severe thrombocytopenia. We managed to perform bone marrow aspiration and biopsy after platelet transfusion which showed hypocellular marrow with marrow content less than 5% and negative for malignancy. Patient's condition deteriorated and he was transferred for hospice care. Results: Six weeks later, the TB culture of the peripheral blood showed growth of mold. The microorganism was subsequently identified as H. capsulatum by PCR assay. Blood culture and fungal culture were reported as sterile after one and two week incubation period. Bone marrow biopsy was revised with GMS stain which showed numerous intracellular yeast-like micro-organisms, consistent with histoplasmosis ( Figure A) . Conclusion: Histoplasmosis is rarely seen in Taiwan and only a limited number of cases had been reported. It could be missed by laboratories if incubation for recovery of fungi is shorter than four to six weeks. It would be advisable for laboratories to review their culture policies and extend fungal culture incubation to at least four weeks or more. Slow growing mold would be underestimated by a short incubation time for fungus in service laboratory.

Non-cirrhotic portal hypertension in HIV patients Yu-Huai Ho 1 , Lei Wei Yi 2 , Yung-hsiang Hsu 3 , Lih-shinn Wang 1 . Based on these findings, liver biopsy was performed. Histology analysis of the biopsy (Figure 1 ) revealed focal spotty necrosis and swelling of hepatocytes with lymphocytic satellitosis, compatible with mild acute hepatitis and mild portal area fibrosis. Noncirrhotic portal hypertension was diagnosed. Background: Voriconazole is an antifungal medicine by killing the growth of the fungi that cause infections. It is usually used for the treatment of patients with invasive aspergillosis, especially for chemotherapy patients to their infectious diseases. Voriconazole has a nonlinear pharmacokinetic profile with wide inter-and intra individual variability. High total voriconazole concertration also induce the toxicity in live, and unbounded form is more poisonous in sepsis patients. And in the hypoalbuminemia patient, we use fomula to adjust total voriconazole concertration. So the Therapeutic Drug Monitoring is important for patient avoiding the toxicity in liver. In this study, we intended to establish a liquid chromatography/tandem mass spectrometry method for clinical diagnostic purpose. Methods: By using multiple reaction mornitoring (MRM) of tandem mass analysis, the mass-to-charge (m/z) of the precursor and the product ions of voriconazole was 350.3/172.4. The internal standard, a ketocozole, was used for the calibration and basis of quantitative measurement. The m/z of ketocozole was 531.1/82.1. We also use High-throughput equilibrium dialysis method to quantitative analysis of unbound voriconazole concertration.

Results: According to the preliminary results, the within-run and between-run precisions of the method were 6.2% and 9.9%, respectively. The recovery of this tandem mass assay was 80%. Background: Immunodeficiency is a known risk factor of cerebral toxoplasmosis. Causes of immunodeficiency include usage of immunosuppressants.

Methods: A 72-year-old man who had right kidney transplantation with prednisolone, tacrolimus and cellcept treatment, was brought to another hospital with ataxia and urinary incontinence for about 3 weeks. Brain computed tomography scan was arranged at that hospital and the family was told that the patient might have cerebrovascular accident. He was then transferred to Tzu Chi General Hospital. On physical examination, body temperature was 36°C, pulse rate was 78/min, respiratory rate was 20/min and blood pressure was 112/86 mmHg. Laboratory testing revealed leukocytosis and normocytic anemia. Brain magnetic resonance imaging showed at least four lobulated nodular lesions over both frontal and right temporal lobes with marked perifocal edema and rimenhancement. After admission, stereotactic brain biopsy was performed.

Results: The pathological results of the brain biopsy showed necrosis of brain tissue with a few neutrophils, macrophages and nuclear debris. The biopsy tissue showed negative for bacterial culture and acid-fast stain. Tuberculosis and fungal culture of the biopsy tissue were also done. After discussion with the infectious diseases doctor, toxoplasmosis, nocardiosis or other fungal infections were suspected because of long term immunosuppressive for previous kidney transplantation. Paired Toxoplasma immunoglobulin M and immunoglobulin G were negative. The brain magnetic resonance imaging revealed enlargement of the right frontal lesion. The whole body positron emission tomography showed multiple brain lesions, mainly right cerebrum with mass effect and a tiny nodule in the posterior segment of right upper lung. Sulfamethoxazole/trimethoprim treatment was initiated for possible cerebral toxoplasmosis. Then, the patient had hyperkalemia and was transferred to surgical intensive care unit. Sulfamethoxazole/trimethoprim treatment was immediately discontinued. The patient had craniotomy with removal of the tumor, and had hemodialysis after the operation. However, the patient had septic shock, and his condition went downhill. He was discharged due to terminal status 4 days later (on hospital day 22). Tachyzoites were found in the necrotic brain tissue. Special stain including periodic acid-Schiff stain, Grocott's methenamine silver stain and Toxoplasma immunohistochemistry staining confirmed toxoplasmosis ( Figure 1 ). Conclusion: Long term immunosuppressants after kidney transplantation led to immunodeficiency in our patient, which became the risk factor of the infection of Toxoplasma gondii. Negative results of Toxoplasma immunoglobulin M and immunoglobulin G levels could not exclude the diagnosis of Toxoplasma infection, which were correlated with previous study.

Mycobacterium celatum pulmonary infection in a patient of AIDS with cryptococcosis and multiple STD's Lih-shinn Wang 1 , Tzu-yu Tsai 4 , Jen-jyh Lee 2 , Teng-Yi Lin 3 , Huei-jen Chao 3 , Chia-ching Jackie Wang 5 . Results: This is a 41 years old man with diagnosis of HIV. He was admitted the hospital due to bilateral pneumonia with septic shock on August 6, 2016. With inotropic agent used, unstable oxygenation and rapid acute respiratory distress syndrome, he received endotracheal intubation with mechanical ventilation. Laborartoy data revealed a HIV viral load test of 302,695 copies/mL; CD4+, 5%; CD8+, 61%; CD4+/CD8+ ratio, 0.09; and a Mycoplasma pneumoniae Ab titer of 1:80. Chest X-ray showed ill-definded ground-glass opacities in bilateral lung fields, predominantly in central lung zone and upper lobes, PJ-associated pneumocystis pneumonia (PCP) was condiered. After admission, antibiotic therapy was used, including piperacillin/tazobactam and levofloxacin for possible bacterial pneumonia. TMP/SMZ was initiated for probable PCP. As intensive care was continued, the thrombocytopenia around 11∼19 × 10 3 /uL persisted, even pneumonia seemed better. Piperacillin/tazobactam was changed to cefpirome. However, persistent thrombocytopenia was still noted. We checked his blood CMV-PCR and result was positive. Sputum CMV-PCR was also positive and a CMV viral load was 1,216 IU/mL, so ganciclovir was added for CMV infection. After that treatment, the thrombocytopenia was improved to 76∼97 × 103/uL. Solumedrol was tapered to solucortef and then oral prednisolone by standard dosing regimen. Extubation was performed on August 25, and recover well. Antibiotic therapy was shifted to oral forms of levofloxacin, TMP/SMZ and valganciclovir. He was discharged uneventfully on September 3, 2016. Conclusion: PCP remains the most common disorder at the onset of acquired immunodeficiency syndrome in a HIV patient. Coinfection with CMV is a poor prognostic sign in a setting of PCP. CMV-related organ disease should be suspected in patients with CD4+ cell number lower than 50/μL. After suscessful antipneumocystis treatment with steroid therapy, our patient still presented with severe thrombocytopenia, which led to secondary diagnosis of CMV syndrome. Other cases may experience worsening pulmonary infection due to CMV pneumonitis by the use of steroids, which therapy should be careful in the patients with a high HIV viral load and low CD4 count. However, tachypnea and fever were noted on post-operative day 4. Additionally, AST/ALT, TBI/DBL were all elevated two folds. He was reintubated and Imipenem was administrated as empiric antibiotic agent. A day later, seizure attacked twice; and Imipenem was shifted to Meropenem. The following lab exam showed DIC. CT scan revealed portal vein thrombosis occuping 50% of lumen and massive pleural effusion. Sputum and blood culture sample grew candida. The initial impression was hepatic failure and candidiasis. The patient expired two weeks later due to multiple organ failure. At autopsy, liver showed central cholestasis and bile canalicular cholestasis. In the liver specimen token via surgery, there were hepatocellular carcinoma and portal area inflammation without the cholestasis, which indicates post hepatectomy liver failure. Leptospirosis was confirmed in the samples of liver, kidney, bone marrow, lung, brain and soft tissue based on strongly postive Warthin Starry stain and Leptospira antibody.

Conclusion: This case demonstrates leptospiriosis may progressed to Weil's disease and acute disseminated encephalomyelitis. According to previous publications, the incidence of leptospirosis with CNS involvement is around 10%, mostly diagnosed by lumbar puncture. Early diagnosis may be challenging. In this case, CNS symptoms may be confused with drug-induced seizure. Weil's disease is a severe form of leptospirosis characterized by jaundice, hemorrhage and renal failure. There is cholestasis with leptospires invading the intercellular junctions of hepatocytes in this patient and the diagnosis is solid. As a conclusion, in patients with multiple organ failure after major operation, Weil's disease and ADEM should be considered as a differential diagnosis.

Co-infections of Mycobacterium tuberculosis and nontuberculous Mycobacterium: difficulty in diagnosis and treatment Lih-shinn Wang 1,2 , Chia-ching Jackie Wang 8 , Xin-yi Zheng 3 , Jen-jyh Lee 1,3 , Teng-Yi Lin 4 , Yung-hsiang Hsu 1,5 , Cheng-lung Ku 6,7 . Background: Mycobacterium abscessus subspecies massiliense is one of the three subspecies of the M. abscessus complex (MABC). It is the most important cause of non-tuberculous mycobacteria (NTM) infections by rapidly growing mycobacteria (RGM). MABC is considered highly drug-resistant; it is extremely difficult to treat and often incurable. Prosthetic joint infection due to M. abscessus subsp. massiliense is rare, and optimal therapy remains unknown. Methods: A 48-year-old male with history of gout and prolonged corticosteroid treatment presented with left hip pain with instability and limited range of motion for one month in Sep., 2014. Hip X-ray and computed tomography (CT) scan at another hospital revealed findings consistent with avascular necrosis of the left femoral head, and he underwent left total hip arthroplasty. He required reduction for subluxation of the prosthesis the next day. He was admitted again two weeks later for an open revisión to repair a recurrent subluxation of the prosthesis. Within one month of the first surgery, he underwent a third surgery (trochanter advancement osteotomy) for prothesis dislocation. One year later the patient had a one month history of bulging cyst at the left hip. The left hip cyst was felt to be a hematoma, but NTM was isolated from the culture of the cyst fluid. The isolate grew in subculture within 6 days of inoculation, and was considered as a RGM. Anti-NTM treatment was started empirically with azithromycin, ciprofloxacin, sulfamethoxazole/trimethoprim and doxycycline. Results: The isolate was later identified as M. abscessus subsp. massiliense, susceptible only to clarithromycin and tigecycline. Anti-interferon-γ autoantibodies were negative. The patient stopped anti-NTM treatment by himself after a one-month therapy, and was discharged against medical advice. The patient experienced recurrent pain of the left hip a few months later, and developed a draining sinus tract. Pus grew M. abscessus subsp. massiliense. After removal of the prosthesis, anti-NTM treatment was re-started with intravenous tigecycline, clarithromycin, clofazimine and amikacin. The patient again stopped anti-NTM therapy due to nausea, malaise, anorexia, weight loss, acute kidney injury and liver dysfunction. Symptoms improved after discontinuing clofazimine and tigecyline. The same symptoms appeared after rechallenge with clofazimine and tigecycline. Finally, the patient was placed on oral suppressive anti-NTM treatment with Results: Serotype Ia was the most common in patients less than 72 hours old, whereas III the most common in patients older than 72 hours. In early-onset disease (0-6 days), Ia and III each caused 27.5% of the infection, followed by Ib (14.5%). In late-onset disease (7-89 days), serotype III predominated (75.3%), followed by Ia (10.1%) and Ib (6.8%). Thirty-one episodes (17%) were complicated with culture-confirmed meningitis. We compared serotype Ia and III patients, and found that serotype Ia patients were significantly younger (median age, 3 days), had higher male percentage, more perinatal maternal fever, and higher mortality. The survival curve of patients with serotype III infections was significantly better than those with non-type III infections. ST17 and ST19 were exclusively found in serotype III, while ST23 and ST24 comprised of 85% of serotype Ia. Conclusion: Serotype Ia caused invasive infections in younger infants with higher mortality than serotype III. A trivalent (Ia, Ib, III) GBS vaccine might prevent 69.6% of early-onset disease, 92.2% of late-onset disease, and 91.3% of GBS meningitis in Taiwan.

Sphenoid sinusitis complicated with cavernous sinus thrombosis Yun-Hu Wang 1,2 , Po-Yen Chen 1,2 , Pei-Ju Ting 1 , Fang-Liang Huang 1,3 .

1 Department of Pediatrics, Taichung Veterans General Hospital, Taichung, Taiwan, 2 Chung Shan Medical University, Taichung, Taiwan, 3 Hung Kuang University, Taichung, Taiwan

Background: Cavernous sinus thrombosis (CST) is a rare infective disease and it was associated with high mortality rates. CST can result from infection of any of the tissues drained by the cavernous sinus. The purpose of this study was to review rare case of CST secondary to sphenoid sinusitis, which is under-researched in children.

Methods: We reported a 12-year-old girl who was diagnosed with cavernous sinus thrombosis and sphenoid sinusitis presenting with fever and nuchal pain for one week. We also reviewed the literature of cavernous sinus thrombosis secondary to sphenoid sinusitis. The clinical manifestations, laboratory data, imaging findings, pathogens, medications, surgical treatment and clinical outcomes were analyzed.

Results: All eight cases (100%) had headache, five cases (62.5%) had fever. All cases had one or more ophthalmic symptoms. Four cases (50%) of computed tomography and magnetic resonance imaging findings had revealed isolated sphenoid sinusitis. Three cases had evidence of Streptococcus infection from blood culture; two cases had Staphylococcus aureus from blood and sinus cultures; seven cases had surgical intervention followed by antibiotic therapy. All cases had received antibiotic therapy, and 5 of 8 (62.5%) were administered intravenous ceftriaxone and metronidazole. Six cases had received intravenous and oral form anticoagulation therapy. Only a case had corticosteroid treatment. Most cases had good prognosis except for three cases had morbidity.

The management of CST should the use of intravenous antibiotic therapy combined with endonasal sinus surgery. The early use of anticoagulation can reduce morbidity and mortality in CST. Corticosteroid treatments for CST remain controversial. Background: Acute febrile illness (AFI) is a very common syndrome and also represents the most frequent admission diagnose in our service. Even though this syndrome has always been conventionally included as a major indicator of an infectious disease, sometimes it takes big effort to estabilish an etiologic diagnose. Therefore, many patients with AFI are empirically diagnosed, according to clinical manifestations and certain epidemiological (like sessonal strokes, proffesional exposure, traveling.) No wonder why clinicians are always focused on clinical-epidemiologicalbinom, considering the dynamic changes of infectious nosologies in Global Era. Methods: 1455 patients hospitalized to the Infectious Disesase Hospital during 2007-2015. All patients had the same inclusion patterns and also astandardized clinicaland laboratory evaluation of all patients who met the following case criteria: (1) >14 years of age and admitted to the participating institution; (2) history of fever for at least two days; (3) temperature at admission >38°C (oral or rectal); (4) no identified cause of fever ( physicians were instructed to exclude other causes of fever such as diarrhea, hepatitis, cough and/or symptoms of respiratory tract infection, urinary tract infection, cellulitis, rheumatic fever, abscess, and/or any other known cause of fever). Admitted patients with AFI provided anamnestic data, epidemiological details and a blood sample for bacterial culture and serologic analysis. Risk factors included animal contact and consumption of raw dairy products, which were significantly associated with brucellosis, exposure to vectors in endemic areas,professional exposure and travels outside the country. Results: Case definitions were based on laboratory results. On 1455 patients evaluated between 2007 and 2015, 41% were positive for Brucella, 5% had positive hemocultures for Bacterial Endocarditis (with predominance of staphyloccouccus aureus), 16% for Riketcia and other tick borne pathogens, 3% for Hemorrhagic fever, 10 for Infectious Mononucleosis, 3% for Malaria, 8.3% for Leishmaniasis and 12.5% for Leptospirosis. Conclusion: Our survey identified several important pathogens; each one of them with an important specific percentage. The correlation of these pathogens to the specific clinical manifestation and epidemiological data allowed a better recognition of these infectious nosologies features in our country, and may also lead to the development of targeted prevention strategies. Background: Acinetobacter baumannii is an important pathogen of nosocomial bloodstream infection and pneumonia, which accounts for high morbidity and mortality rates. However, studies regaring the optimal treatment for Acinetobacter baumannii bacteremic pneumonia are limited. Methods: We conducted this retrospective study from January 2012 to December 2015 at 4 medical centres in Taiwan to compare the efficacy of colistin-based and sulbactam-based therapy for A. baumannii bacteremic pneumonia. The clinical features and outcomes of patients were investigated. Results: During the study periods, 71 patients received either colistin-based or sulbactam-based treatment. Among them, 2 patients who received concomitant treatment with colistin and sulbactam were excluded. Twenty-five patients who received colistin-based therapy and 44 who received sulbactam-based therapy were included in final analysis. There was no difference of baseline characteristics, 14-day mortality, 28-day mortality, and overall in-hospital mortality between the 2 groups. Those with ventilator-associated pneumonia were more likely to receive colistin-based treatment rather than sulbactam-base treatment (odds ratio [OR], 7.935; 95% confidence interval [CI], 2.157-29.187; p = 0.002). The only risk factor independentlyassociated with higher 14-day and 28-day mortality of A. baumannii bacteremic pneumonia is a higher APACHE II score (OR, 1.105; 95% CI, 1.024-1.192; p = 0.01 and OR, 1.127; 95% CI, 1.044-1.217; p = 0.02, respectively).

Our results concluded that patients with A. baumannii bacteremic pneumonia treated with either colistin-based or sulbactam-based therapy had similar clinical outcomes. More severe diseases (higher APACHE II scores) were independently associated with higher 14-day and 28-day mortality. Further largescale prospective studies are needed to investigate the clinical efficacy of these two regimens for the treatment of A. baumannii bacteremic pneumonia.

Comparison of culture-negative and culture-positive necrotizing fasciitis: clinical characteristics and outcomes of treatment Tsung-Yu Huang 1,2 , Yao-Hung Tsai 3 , Ching-Yu Lee 3 , Wei-Hsiu Hsu 3 *. Background: Necrotizing fasciitis was rapidly progressive and lifethreatening. Surgical intervention and broad-spectrum antibiotics usage were very important. But no bacterial was proven in some patients with necrotizing fasciitis. This study was undertaken to ascertain whether the clinical presentation and outcome for patients with this disease differ for those infected with culturenegative as compared to culture-positive pathogen. Methods: One-hundred and forty-three patients with necrotizing fasciitis were examined prospectively from April 2015 to May 2017. All patients received prompt radical debridement, adequate broad-spectrum antibiotic therapy, and aggressive resuscitation. Thirty-eight patients were infected with culture-negative pathogen (Group 1) and 105 patients with culture-positive pathogen (Group 2). Results: Group 2 was male predominated. Group 1 was characterized by a lower incidence of tachypnea and local pain/tenderness, lower Sequential Organ Failure Assessment (SOFA) Score, a lower rate of bandemia, bacteremia, lactate acidosis, hypoalbuminemia and a lower prevalence of alcoholism plus diabetes mellitus. A higher rate of acute respiratory failure, acute renal failure and higher admission rate of intensive care unit (ICU) admission were more common in Group 2. In a multivariate analysis, male (odds ratio (OR): 0.443, 95% confidence interval (CI): 0.199-0.986), local pain/tenderness (OR: 0.144, 95% CI: 0.029-0.722) and tachypnea (OR: 0.275, 95% CI: 0.092-0.824) were identified as infected with culture-positive pathogen. Conclusion: Culture-positive necrotizing fasciitis mostly presented with local pain/tenderness, tachypnea, and men predominate. There were no difference about mortality rates, limbs amputation rate, hospital stay, and ICU stay. We still need more data to confirm those above findings in our hospital. cirrhosis or diabetes. However, it is a rare cause of spontaneous bacterial peritonitis (SBP) in cirrhotic patients.

Methods: A 46-year-old man with alcoholic liver cirrhosis (Child-Pugh class C) and type 2 diabetes mellitus developed abdominal pain and fever. His abdomen was distended with shifting dullness. Laboratory examination showed a white blood cell (WBC) count of 8500/µL with 86.50% neutrophils. Paracentesis of ascites revealed a WBC count of 743/μL with 79% neutrophils. Based on the patient's history, physical examination and the results of paracentesis, spontaneous bacterial peritonitis was suggested and blood cultures were positive for Campylobacter coli. Intravenous Ciprofloxacin was administered and the patient was recovery after treatment. Results: SBP caused by Campylobacter is rare. The first case report of SBP caused by Campylobacter was presented by Targan et al in 1976. Since then, five cases have been reported in the literature. Three of the five patients were associated with peritonitis and bacteremia as the present case. Main underlying conditions are related to liver disease (34%), including alcohol abuse (27%), cirrhosis (20%), cancer (33%), diabetes (21%) and chemotherapy (19%). In the present case, the patient was at a high risk for Campylobacter infection as he had severe alcoholic liver cirrhosis.

We reported a case of SBP caused by C. coli.

Campylobacter-related SBP mainly occurs in patients with severe underlying alcoholic liver cirrhosis.

Mimic fungal ball in urinary bladder caused by tissue invasive C. glabrata, complicated with acute obstructive uropathy Lin-Fang Chen, Fu-Lun Chen. Division of Infectious Diseases, Department of Internal Medicine, Wan Fang Medical Center, Taipei Medical University, Taipei, Taiwan

A 68-year-old female patient had underlying asthma and type 2 diabetes mellitus for decades and 3 months history of C-spine injury with central cord syndrome consequence left hemiparesis, neurogenic bladder status post Foley catheter insertion. She had an episode of urinary tract infection by extended spectrum betalactamase (ESBL) -E. coli and K. pneumoniae, suspected bilateral chronic nephritis with peri-renal effusion found by the computed tomography. She was admitted for sepsis, recurrent urinary tract infection. However, after 6 days of antibiotic therapy with Doripenem (500 mg every 8 hours) as an empirical therapy for ESBL-E. coli and target therapy for P. aeruginosa. she had light yellow turbid urine with lots of necrotic debris, low-grade fever, increased dose of insulin to control hyperglycemia, and aggravated leukocytosis. Foley urine culture yielded C. glabrata. After high dose Fluconazole (6 mg/kg/day), Foley catheter obstruction developed, partially relieved by manual irrigation and Foley catheter revision. Amphotericin B continuous irrigation (50 mg every day) via 3-way Foley catheter was applied as salvage therapy, but the frequency of Foley catheter obstruction caused by necrotic debris progressed from every day to every several hours. She had acute obstructive uropathy, complicated by retrograde vancomycin resistant E. faecium septicemia. After Daptomycin (10 mg/kg/day) therapy, vancomycin resistant E.faecium septicemia was resolved without infective endocarditis. Amphotericin B continuous irrigation was replaced by systemic fluconazole (4 mg/kg/day) combined with flucytosine (40 mg/kg/day, divided in 4 doses) therapy. Urologist performed urethro-bladder cystoscopic irrigation and found a large area of thickening necrotic tissue coating over anterior and rightposterior urinary bladder wall, mimic lots of separated fungal ball. Transurethral resection of bladder debritic tissue found yeast form fungus with budding phenomenon without hyphae and pseudohyphae. The tissue culture proved mixed invasive C. glabrata and ESBL-K. pneumoniae infection. After extensively surgical cauterization, she had no more Foley catheter obstruction, and there was no more necrotic debris grossly. There were rare case reports talking about candida fungal ball in urinary bladder with severe complication, mainly candidemia. This is the first case in our acknowledge discussed about invasive urinary bladder C.glabrata mixed infection with severe complication. There should be some aggressive invasive procedure performed to treat selected C. glabrata urinary bladder infection.

Role of doxycycline in a mouse model of Clostridium difficile infection Chun-Wei Chiu 1 *, Yuan-Pin Hung 1 , Pei-Jane Tsai 2 , Wen-Chien Ko 3 . Background: Clostridium difficile infection (CDI) is a major cause of nosocomial antibiotic-associated diarrhea. Metronidazole and vancomycin are therapeutic choices for CDI but their use was associated with colonization of vancomycin-resistant enterococci and a high recurrence rate. Doxycycline shows promising in vitro antibactieral activity against clinical C. difficile isolates and can achieve a high concentration in bowel tissue. However, the efficacy of doxycycline in treating CDI is not clear.

Methods: Clinical C. difficile isolates from the Tainan Hospital, Ministry of Health and Welfare were tested for the susceptibility of doxycycline by the agar dilution method. To evaluate whether doxycycline will faciliate the proliferation of C. difficile in intestines, the ex vivo assay was performed for the growth of C. difficile by coculturing with cecum content from the mice after oral doxycycline administration. Therapeutic effect of doxycycline therapy for CDI was examined in the established CDI mouse model. Results: Of 192 clinical C. difficile isoaltes, the MIC 50 of doxycycline against clinical C. difficile was 0.25 μg/mL. The growth of C. difficile by co-culturing with cecum content showed a decrease in mice after doxycycline administration, than that in the mice after clindamycin use (clindamycin vs. doxycycline: 5.04 × 10 7 ±4.33 × 10 7 CFU/mL vs. 2.49 ± 2.28 × 10 3 CFU/mL; P = 0.0003). Meanwhile mice orally challenged by C. difficle after doxycycline administration excreted less C. difficile in faces (0.67 × 10 5 ±0.54 × 10 5 CFU/mL vs. 4.33 × 10 5 ±1.16 × 10 5 CFU/mL; P = 0.029) and less body weight loss (0.52 ± 1.13 kg vs. 1.89 ± 1.19 kg; P = 0.01) than mice after clindamycin use. Mice treated by doxycycline showed less body weight loss than control mice (1.11 ± 0.56 kg vs. 2.87 ± 0.98 kg; P < 0.0001). Besides, doxycycline therapy showed non-inferior to metronidazole or vancomycin therapy, according to body weight loss in mice with CDAD (doxycycline vs. metronidazole: 1.11 ± 0.56 kg vs. Hospital Center ""Mother Theresa"", Service of Infectious Diseases, 3 University Hospital Center ""Mother Theresa"", Service of Reanimation and Anesthesia, 4 University Hospital Center ""Mother Theresa"", SID Background: Although Tetanus is currently a relatively rare infectious disease it remains a high mortality disease (20-30%) with high management costs. The reason for that are the severe forms and complications, which require the application of therapeutic protocols in the intensive care unit. The study of these protocols is important for analyzing not only the patient prognosis but also the treatment cost effectiveness. Methods: Collection of cases and grouping them by gender, age and therapeutic protocols; Group classification according to the therapeutic protocol, adapted to the gravity of cases (mild, moderate, severe, very severe); The used method consisted in 53 cases, of group age between 14 and 85, with generalized tetanus, hospitalized in the reanimation of the infectious diseases hospital, during the period 1985 -January 2015.

Results: Data by sex: 32 males, 16 females; age groups: prevalent age group 46-55; According to therapeutic protocols: First group: patients who underwent paralysis with curare and artificial ventilation 39 cases, second group: patients undergoing tracheostomy 4; Third group: patients using sedation alone (diazepam and/ or opiate) 7; Fourth group: deep sedation, anesthesia, adrenergic blockade 2 cases. Death 5 cases. Causes of death are mainly due to paralysis and artificial ventilation complications, which apply to severe forms such as severe brain damage as a consequence / side effect of treatment, cerebral thrombosis 1 case, pneumothorax 1 case, staphylococcal pneumonia 1 case, pulmonary embolism 1 case. The complications of sedative procedures, of deep sedation and of endotracheal intubation is bradycardia and cardiac arrest, 2 case, because of hypoxia and vagal stimulation. The duration of paralysis was 24 days, between 5 and 52 days. In lighter forms, the period under mechanical ventilation was shorter, while in the severe form it was longer. Conclusion: Overall tetanus mortality in adults resulted in 9.43%. of cases. Deaths were caused by cerebral thrombosis 1 case; spontaneous Pneumothorax 1 case; staphylococcal pneumonia 1 case; Pulmonary thromboembolism 1 case; Cardiac arrest 2 cases.

Application of MALDI-TOF MS in clinical diagnostic anaerobes from blood cultures Shin-Wei Wang 1 , Pei-Fang Tsai 1 , Ya Hui Liu 2 , Pei-Jane Tsai 2 *. Background: Information regarding the use of MALDI-TOF MS as an alternative to conventional laboratory methods for the rapid and reliable identification of anaerobe in blood isolates is still limited. The aim of this study was to assess the performance of MALDI-TOF MS in identification of anaerobes from blood cultures. Methods: Retrospective, a surveillance study from July 2013 to March 2014 was conducted at the National Cheng Kung University Hospital in southern Taiwan. Identification results by MALDI-TOF MS were compared to those obtained by 16S rRNA sequencing. Results: In this study, MALDI-TOF MS was evaluated on 122 anaerobic isolates previously identified by 16S rRNA gene sequencing and with biochemical tests (Rapid ID 32A system, BioMérieux). In total, 12 genus and 25 species were identified. All strains were confirmed by 16S rRNA gene sequencing. Among these, 90.9% of the isolates were identified by MALDI-TOF MS (BioMérieux) at the species level vs 92% using the Rapid ID 32A system, with an agreement of 77.8% between the methods. The 16S rRNA gene sequencing of isolates with discrepant results showed that MALDI-TOF and API ® correctly identified 86.1% and 78.7% of these isolates, respectively. This discrepancy in MALDI-TOF MS probably mainly among the non-fragilis Bacteroides spp. and GPAC, and only 11 of them could not be identified using the method.

The results show that MALDI-TOF MS is a robust and reliable tool for the identification of anaerobic blood isolates in the microbiology laboratory. The extensive development of a database, not only frequently isolated anaerobic species, but also newly recognized and taxonomically rearranged genera and species, should be well continuously established. This technique really reduce the turnaround time for a final identification and the number of isolates that require 16S rRNA sequencing.

Wei-Han Huang 1 , Ming-Huei Gu 1 , Siao-Ru Lin 1 , Chia-ching J. Wang 2 , Lih-shinn Wang 3 *. Background: Myeloma patients may present clinical signs such as hypercalcemia, anemia, renal dysfunction and bone disability when inital diagnosis. However, infection is still a significant cause of morbidity and the leading cause of death in patients with myeloma. The increased susceptibility of patients with myeloma to infection not only results from the antineoplastic therapies but also age-and disease related immunodeficiency. Methods: A 66-year-old businessperson was informed stage II chronic renal impairment possibly due to uncontrolled hypertension for two years. He suffered from bilateral flank pain and urinary urgency for one week prior to admission and treated with oral antibiotics under the impression of urinary tract infection at local clinic. He had fever, chills, and altered mental status one day prior to admission. On presentation, the patient had unclear consciousness, confusion to stupor, with E4V2M4. Brain CT revealed mild cortical brain atrophy. CSF studies showed slightly turbid appearance, with orange color, low sugar (CSF glucose: 6 mg/dL, vs normal blood sugar: 102 mg/dL) and high protein (Total protein-CSF: 431.7 mg/dL). He had pleocytosis (WBC: 10-20/HPF, WBC Count: 990/uL, with segmented neutrophil: 48%, Lymphocyte: 41%, and Monocyte: 11%). CRP was 9.57 mg/dL. There was no pathogen identified by Gram stain, AFB stain nor India ink stain or even further culture. Under the impression of bacterial meningitis, he was admitted for antibiotic treatment. Results: After admission, lab data showed hypercalcemia (corrected calcium 3.13 mmol/L), renal impairment (creatinine 3.0 mg/ dL), A/G reverse (albumin/globulin 1.5/11 g/dL). treatment. Serum protein electrophoresis and immunofixation electrophoresis disclosed IgG kappa monoclonal gammopathy. The quantitation of IgG was 6,380 mg/dL. Serum free light chain ratio was 29.66 (kappa/ lambda: 961/32.4 mg/L). Bone marrow biopsy and flow cytometry analysis confirmed multiple myeloma, with IgG by immunohistochemistry staining. Patient was treated with standard therapy with VTD regimen (Velcade, thalidomide & dexamethasone) then with strigent complete remission. Conclusion: We present a myeloma patient initially mimicking as meiningitis. Since myeloma patients have higher susceptibility for infection, we cannot ignore general evaluation and integrate the clinical information in patient with repeated infection.

Serratia marcescens lung abscess presenting as pseudo-hemoptysis Hsin Yu Lu 1 , Li Hsinn Wang 2 , Chih Pin Lin 3 , Si Shiuan Peng 4 , Pao Yaun Chang 5 , Yung Hsiang Hsu 6 . 1 School of Medicine, Tzu Chi University, Hualien, Taiwan, 2 Division of Infectious Diseases, 3 Division of Chest Medicine, Department of Internal Medicine, 4 Department of Laboratory Medicine, 5 Department of Radiology, 6 Department of Pathology, Buddhist Tzu Chi General Hospital, Hualien, Taiwan Background: Serratia marcescens, which can produce a red pigment, prodiogiosum, has been regarded as a harmless gramnegative bacillus. Recently, It was recognized as a nosocomial and opportunistic infections. Methods: Patient was a 35-year-old male with history of pulmonary tuberculosis (TB) with resolution 9 years ago, but he often had ""hemoptysis"" since then. He was admitted due to progressive ""hemoptysis"" and general soreness. Laboratory data showed influenza A&B antigen, Aspergillus antigen and acid-fast stain all negative. Mycoplasma IgM was positive. Chest computed tomography (CT) showed sporadic fibroinfiltration at both lung and ""ball in cage"" feature at right upper lung. He was treated with clarithromycin and tranesamic acid. Bronchoscopy revealed ""active bleeding"", ( Figure) and bronchial alveolar lavage showed Aspergillus antigen positive. Therefore, voriconazole treatment was added. However, 3 days after discharge, he came back due to massive ""hemoptysis"" of 150 mL. Associated symptoms were dyspnea, dizziness and right upper chest pain. Results: S. marcescens was grown in sputum culture on admission, but he was still treated as aspergillosis. Chest X ray showed right upper lung atelectasis. Right upper lobectomy was performed, and patholgy revealed chronic abscess, no granuloma, and surprizingly no evidence of fungal infection or active tuberculosis. So, voriconazole was shifted to piperacillin/tazobactam. However, he had a fever of 37.9°C, tachycardia and leukocytosis. S. marcescens was again grown in sputum culture with mixed flora. S. marcescens lung abscess was the final diagnosis, and ""hemoptysis"" was considered as ""pseudo-hemoptysis"" when repeat sputum grew the same organism which could produce prodiogiosum, mimicking fresh blood. Repeat chest CT showed severe adhesion without adequate expansion. Therefore, piperacillin/tazobactam was shifted to levofloxaxin, and the patient had right lower lung superior segmentectomy. Bronchopleural fistula was found, and there was air leakage without improvement. The patient finally cured with thoracoplasty of right 2 nd to 6 th ribs, and was discharged.

Conclusion: Hemoptysis is common in patients of TB or aspergillosis, but rare in lung abscess. S. marcescens lung abscess is rare. But if the presenting symptom is ""hemoptysis"", it could be the red pigment, prodiogiosum, produced by S. marcescens.

Splenic abscess: an uncommon entity with potentially life threatening Mei-Chun Lee 1,3 , Chien-Hung Lu 1 , Wei-Ying Lee 1 , Chun-Ming Lee 2,3,4,5,6 *.

Background: Splenic abscess is rare with potentially life threatening. Scanty published studies are available. The aim of this study is to review the clinical features, microbiologic etiologies, treatments, and outcomes of patients with splenic abscesses in recent 5 years. Methods: We reviewed admitted patients with suspected splenic abscess and made the diagnosis of splenic abscess. The clinical characteristics, underlying disease, treatment course, organism spectra, abscess number and size, therapeutic methods, and clinical outcome at a tertiary medical center in northern Taiwan over a period of 5 years (from Jan 2012 to Dec 2016) were analyzed. Results: Of 16 patients with splenic abscess, male to female ratio was 1:1, with a mean age of 49.9 years (range 1 day to 52 years). Common presentation were fever (11 patients, 68.7%), diffuse abdominal pain (6 patients, 37.5%), left upper quadrant pain or tenderness (6 patients, 37.5%), splenomegaly (4 patients, 25%), and Left-sited pleural effusions (8 patients, 50%). Leukocytosis was noted in (15patients, 93.7%). Antimicrobial therapy was administered in all patients. Fourteen (87.5%) patients recovered under medical treatment. One (6.2%) patient underwent splenectomy, and four (25%) patients performed percutaneous drainage of their splenic abscess and 11 (68.7%) patients received antimicrobial therapy alone. Conclusion: We noted mortality may be more related to patients with underlying immunodeficiency. There was a relatively high proportion in patients with splenic abscesses receiving antimicrobial therapy alone and got a good prognosis especially in patients with small and multiple abscesses. Background: Association between psoriasis and leptospirosis has been previously mentioned but seldom cases were reported. We described a case at below. Methods: A 66-year-old woman with history of hypertension under medical control and psoriasis, was admitted to Tzu Chi General Hospital for itching erythmatous plaques over whole body and four extrimities for 3 months. She has been diagnosed with psoriasis for fifteen years and flared up intermittently. She previously worked at breakfast stand, and engaged in farming when she was young. Upon admission, she had multiple psoriatic lesions over her trunk and extremities. Stiff neck and arthritis of finger and toe joints were also noted. Results: Ten years ago, she came to our hospital for controlling her psoriasis condition. Phototherapy, acitretin, calcipotriol, and methotrexate were prescribed by dermatologist. Skin biopsy was done and showed superficial perivascular lymphocyte infiltration with elongated rete ridges, which was consistent with psoriasis. Urine PCR comfirmed Leptospire infection. Penicillin G and rifampicin was added. Her skin lesion was dramatically controlled in a week. The patient was discharged home with oral penicillin G and rifampin. In this time of her admission, antibiotics for leptospirosis were changed to ceftriaxone and fosfomycin due to unavailable of penicillin G. Skin biopsy ten years ago was also taken out for immunochemical staining of leptospira which revealed positive (Figure) .

Conclusion: Psoriasis is fundamentally an inflammatory skin condition with reactive abnormal epidermal differentiation and hyperproliferation. Current research suggests that the inflammatory mechanisms are immune based and most likely initiated and maintained primarily by T cells in the dermis. Reactivation of T cells in the dermis and epidermis and the local effects of cytokines such as tumor necrosis factor lead to the inflammation, cell-mediated immune responses, and epidermal hyperproliferation observed in persons with psoriasis. Cytokines produced in response to infection have been proposed to be involved in the pathogenesis of leptospirosis.

The clinical impact of sputum isolation of extended-spectrum beta-lactamase-producing Escherichia coli and Klebsiella pneumoniae Maki Shikata 1,2 , Naoki Iwanaga 1,2 , Yosuke Nagayoshi 1,2 , Shinnosuke Takemoto 2 , Tadayoshi Oono 1 , Keiko Hisatomi 2 , Seiji Nagashima 2 . 1 Department of Infectious Diseases, National Hospital Organization Nagasaki, Medical Center, 2 Department of Respiratory Medicine, National Hospital Organization Nagasaki Medical Center

Background: The infection caused by extended-spectrum betalactamase (ESBL)-producing organism has been emerging problem recently, because broad-spectrum antibiotics are needed for the treatment of ESBL-producing organisms. To reveal the characteristics of bacteremic pneumonia caused by ESBL-producing organisms is extremely important for clinicians, yet few studies have effectively achieved. The aim of this study is to evaluate the correlation between sputum isolation of ESBL-producing Escherichia coli and Klebsiella pneumoniae and outcomes of bacteremic pneumonia. Methods: We retrospectively analyzed 83 patients of bacteremic pneumonia caused by E. coli and K. pneumonia (including of 38 ESBL-producing) objectively between January 2015 and December 2016 in Nagasaki Medical Center. Bacteremic pneumonia was defined as cases in which E. coli and K. pneumoniae were isolated from sputum culture and clinically suspected because of respiratory symptoms such as fever, cough, sputum and dyspnea, elevated inflammatory response of the laboratory data and chest abnormal shadow. Clinical information and outcome of adults with bacteremic pneumonia caused by ESBL-producing E. coli and K. pneumoniae were analyzed. Results: A total of 83 patients with bacteremic pneumonia (including of 38 ESBL-producing) caused by E. coli (45 patients, 54.9%) and K. pneumoniae (37 patients, 45.1%) were identified, and classified 72.5% as hospital-acquired pneumonia, 14.5% as healthcare-associated pneumonia, and 13.0% as community-acquired pneumonia. Their mean age of ESBL and non-ESBL were 75.5 years and 76.4 years, and male patients of it were 73.7% and 77.8% respectively. The disclosed risk factors of sputum isolation of ESBLproducing E. coli and K. pneumonia are as follows; low BMI (P = 0.0195), antibiotic use history (P = 0.0495), ESBL isolation history (P = 0.0004), high Charlson Comorbidity Index (P = 0.0262), and high quick SOFA (P = 0.0376). About the impact of sputum isolation of ESBL-producing E. coli and K. pneumoniae on clinical course, inappropriate empirical therapy was initiated more often (62.1% vs15%, P = 0.0001), and associated with 60-day mortality (37.9% vs7.5%, P = 0.0026) Conclusion: ESBL-producing bacteremic pneumonia was associated with comorbidities and severity of the disease. Appropriate empirical therapy could lead to the improvement of prognosis.

Liu Yao-Chang 1 , Wang Lih-Shinn 1,2 , Chang Huai-Ren 1,3 , Chen Yi-Ting 1,4 , Lin Teng-Yi 5 . 1 School of Medicine, Buddhist Tzu Chi University, Hualien, Taiwan, 2 Division of Infectious Diseases, 3 Division of Cardiology, Department of Internal Medicine, 4 Department of Intensive Care Medicine, 5 Department of Laboratory Medicine, Buddhist Tzu Chi General Hospital, Hualien, Taiwan Background: Myocarditis is a life-threatening disease, and leptospirosis-associated myocarditis is often underestimated. Methods: A 21-year-old previous healthy man presented with 2day history of general malaise. The patient worked in supermarket, contacting with frozen meat frequently. Since 2 days before initial presentation, he began to experience general malaise, chest tightness, and mild fever. He reported no dyspnea, no abdominal pain, no recent traveling history, nor similar symptoms among his family or friends. Due to persistent symptoms, he was brought to emergency room of another hospital. On examination, hypotension of 72/50 mmHg was noted, and laboratory examination reported bandemia of 34%, C-reactive protein of 20 mg/dL, and troponin-I of 2.12 ng/mL. He was immediately transferred to our hospital. At Emergency Department, he was afebrile, tachycardia of 104 beats/ min, and hypotension of 76/45 mmHg. Physical examination reported no prominent findings except a 1 cm knife cut wound on his left hand, which happened at work 2 weeks ago. ECG revealed normal sinus rhythm. Chest X ray showed congested status. Cardiac echography reported diffusely poor wall motion. Under tentative diagnosis of acute myocarditis with septic shock, the patient was admitted.

Results: Adequate hydration and vasopressors were carried, and he was empirically treated with vancomycin and meropenem. On hospital day 2, Mycoplasma pneumoniae IgM antibody was positive at a titer of 1:80, and antibiotics were switched to levofloxacin. Oral penicillin-V was added by Infectious Diseases doctor's suggestion, considering leptospirosis according to the patient's occupational exposure. Hydrocortisone and colchicine were prescribed for myocarditis. The patient improved dramatically in following days. Blood cultures were sterile and there was no evidence of active viral infection. He was discharged 1 week after admission uneventfully. One month after initial presentation, microscopic agglutination test for leptospirosis revealed significant rise comparing paired sera. Leptospira santarosai serovar shermani was positive at a titer of 1:400, which confirmed leptospirosis. Conclusion: Previous studies have reported that myocarditis can result from M. pneumoniae and leptospirosis. However, antigenic mimicry between Leptospire and Mycoplasma could play a role in biologic false positive reaction. Physicians should be careful when a patient presents risk factors of Leptospira infection.

Mycobacterium abscessus complex pneumonia and possible progressive multifocal leukoencephalopathy (PML) as a presentation of AIDS Yu-Hui Lin 1 , Soon-Hian Teh 2 , Lih-shinn Wang 1,2 , Chia-jung Chiang 3 , Yi-wen Wu 2 . 1 School of Medicine, Buddhist Tzu Chi University, Hualien, Taiwan, 2 Division of Infectious Diseases, Department of Internal Medicine, Buddhist Tzu Chi General Hospital, Hualien, Taiwan, 3 Ministry of Health and Welfare Chest Hospital, Tainan, Taiwan Background: M. abscessus complex (MABC) is a group of rapidly growing non-tuberculous mycobacteria (NTM) species that can cause pulmonary infections in AIDS. Methods: A 29-year-old bisexual man presented with productive cough for 6 months. He started to have a fever of 38°C accompanied with productive cough. The fever subsided spontaneously but the patient still had intermittent cough with yellowish sputum without hemoptysis nor night sweats. He had weight loss from 104 to 77 kg. The patient went to another hospital one month prior to this admission. Chest X ray (CXR) revealed multiple nodules with cavitations over bilateral lung fields ( Figure A) . Chest computed tomography (CT) showed multiple nodules and cystic lesions over bilateral lungs ( Figure B ). Three sets of sputum acid fast stain showed negative results. One of the three sets of sputum cultures showed NTM. The patient was admitted to treat possible NTM pneumonia. Results: On admission, complete blood counts showed pancytopenia with white blood cell count of 2,720/ul and lymphocytopenia (15.8%). HIV Ag/Ab was positive by ELISA and Western blot.

HIV viral loads were 186,547 copies/ul. HAART was initiated with abacavir/lamivudine and nevirapine on hospital day 5. Other tests showed that sputum Pneumocystis jiroveci, blood Aspergillus and HSV-1 were positive by PCR. Sputum M. tuberculosis complex was undetectable by PCR. NTM pulmonary disease was confirmed by a second expectorated sputum samples. Ethambutol and clarithromycin were prescribed to treat possible M avium complex (MAC), which is the most common NTM infection in HIV/AIDS. Rifabutin and levofloxacin were added on hospital day 8. Clarithromycin was shifted to azithromycin because of intolerance. Further study of the sputum culture confirmed MABC by an official mycobacterium laboratory. Progressive lower limb pain, blurred vision and difficult speaking were noted. Brain magnetic resonance imaging (MRI) showed no mass effect, and PML was suspected. The patient was discharged after 9 weeks of hospital stay. His HIV viral loads were lower than 20 copies/uL within 4 months. Seizure happened once per month and follow-up MRI showed aggravating PML. He took ethambutal, azithromycin and rifambutin as outpatient treatment for a year, and follow-up CXR showed improvement. Conclusion: MABC infections are difficult to treat because of multidrug resistance. MAC and MABC co-infection is possible, and MAC may need to treat empirically when CD4 counts are extremely low in AIDS.

Unique physiological properties of toxigenic Clostridium difficile ribotype 078 Cheng-Ju Tseng 1 , Yung-Chieh Fu 1 , Bo-Yang Tsai 2 , Ya Hui Liu 2 , Pei-Jane Tsai 1 *. Background: The spore-forming microorganism Clostridium difficile enables itself to tolerate the extreme environments, and lots of strains enclose the toxin genes and generate toxins that damage the host and lead to various diseases, from the mild diarrhea to the severe pseudomembrane and megacolon. Two C. difficle genotypes ribotyped as 027 and 078 have been found to possess three toxin genes tcdA, tcdB, and cdtA/B, and been demonstrated as the most harmful ribotypes to creatures, which implies the importance of immediate recognition for the proper treatments.

Methods: Retrospective, a surveillance study from January 2011 to January 2013 was conducted at the medical wards of a district hospital in southern Taiwan. Molecular characterization of isolates was performed using PCR ribotyping. Results: Ribotype 078/126 (25.0%) was the most frequently identified. Focusing on the zoonotic RT078, we seen its unique performances compared to the conventional C. difficile, including RT027 and other ribotypes. The colony morphology on agar and the cellular aggregation in broth revealed the dissimilar behavior of RT078 rather than other ribotypes. The highly aggregation of RT078 cells was examined and shown neither correlated with the growth rate of bacterial population nor the formation of biofilm.

The results of this study suggested the C. difficile RT078 had the distinct physiological phenotype that could be observed easily without further molecular typing.

Klebsiella pneumoniae liver abscess associated with Guillain-Barré syndrome: first case in Taiwan Background: Antecedent infections are common with Guillain-Barré syndrome (GBS), and are thought to trigger the immune response that leads to acute polyneuropathy. Campylobacter Jejuni, human immunodeficiency virus(HIV), Zika virus are the wellknown common specimen of the antecedent infection. However, there are no report about the Klebsiella pneumoniae primary liver abscess could also lead to GBS. Case report: A 75-year-old retired male patient had undeylying disease of Type II diabetes mellitus and chronic kidney disease, stage V. He just discharged 2 weeks ago due to biliary tract infection with bacteremia which is proven of K. pneumoniae infection. He was admitted because of fever with chills for few hours. Abdominal echo was arranged and showed bile duct dilatation. Abdominal MRI revealed lobulated lesion at the S2 liver, with restricted diffusion which favor liver abscess. Flomocef 2g every 8 hours was prescribed for the impression of liver abscess. Blood culture during this admission showed no growth. However, bilateral lower leg muscle power and deep tendon reflex (DTR) decreased quickly from distal to proximal, progression of difficulty in defecation and urination during the hospilitation. Cerebrospinal fluid study showed albumino-cytological dissociation and electromyogram (EMG) indicate sensorimotor polyneuropathy of axon degeneration which indirect prove our impression with GBS. Intravenous immunoglubumin was injected. He finally discharged since the symptoms did not progress anymore. Three month later, He could walk and defecate normally when he came back to out patient department follow up. Discussion: Bilateral leg weakness with DTR decreased spreading up from distal to proximal, EMG indicated sensorimotor polyneuropathy of axonal degeneration and albumino-cytological dissociation truly correspond to GBS in this patient. Besides, he did not have other typical symptoms that present with C.Jejuni, HIV or zika virus. Thus, we could boldly associate his GBS with K.pneumoniae liver abscess. This is the first reported case of Guillain-Barré syndrome in Taiwan associated with K.pneumoniae liver abscess which should be kept in mind when making a diagnosis of acute onset of bilateral leg weakness. Methods: After admission, intermittent high fever was persisted, and lab data showed leukocytosis and persisted elevated CRP level. However, there was no respiratory or urinary symptoms. Lab data showed PCT level in reference level. During 5/7∼5/13, chemotherapy was done on 5/8 and there was no major complications noted. However, watery diarrhea was noted since 5/8 night, and CRP level elevated to 45.28 mg/L. Furthermore, stool examination showed positive toxin A + B, which suspected Clostridium difficile infection. We prescribed Flagyl 2# Q6H since 5/10 for him. We also prescribed Trancolon and Smecta for symptom relieving. We had collected rectal swab culture for Clostridium difficile identification and we had ordered to keep contact isolation. Results: During 5/14-5/20, intermittent fever up to more than 39°C was still noted. Intermittent diarrhea was still noted and we kept oral Flagyl use. Furthermore, severe electrolyte imbalance persisted during this week, with hyponatremia, hypokalemia, hypomagnesemia, hypophosphatemia, and frequently electrolyte supplement was needed, especially hypokalemia. The watery diarrhea was improving to semi-fluid-solid like stool, with decreasing frequency. We therefore discontinue contact isolation since 5/19. Conclusion: Clostridium difficile is a kind of normal bacteria in human intestinal tract, the use of antibiotics will lead to other intestinal bacteria to reduce the normal therefore cause a lot of breeding and cause disease and severe electrolyte imbalance. Cancer patients, elderly, Immune function deficiency, long-term hospitalization and the use of antibiotic treatment are more susceptible to infection. Clostridium difficile infection is through the hand and body surface contact. Hand washing for the patient has chemotherapy and medical team to prevent Clostridium difficile is particularly important.

The epidemiology of urinary tract infection in a Singapore teaching hospital: results of a pilot audit Goh Yi Suan Vivian 1 *, Low Wei Ting Gabriel 1, *, Aw Liang Cheng 1 , Park Chang Joon 1 , Low Taoyang Isaac 2 , Sarah Binte Mohd Sidek 2 , Paul Anantharajah Tambyah 2 . 1 Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 2 Division of Infectious Disease, National University Hospital, Singapore *Co-authors

Background: Urinary Tract Infections (UTI) are among the most prevalent infections worldwide. However there are limited data from Southeast Asia. Methods: We conducted a clinical audit to describe the epidemiology of UTI at the National University Hospital, a 1,200-bed teaching hospital covering the western sector of Singapore, from 15/5/2017 to 13/6/2017. Patient data were extracted from patient charts and electronic medical records on demographics, clinical and microbiological features and outcomes. We defined UTI as positive urine culture >10 3 CFU of bacteria/mL. All other definitions followed international standards. Statistical analysis was performed using Fisher's exact test and Student's t-test. Results: Overall, 103 patients had 106 urine cultures of which, there were 41 positive urine cultures, and 65 negative urine cultures. Bacteriuria >10 3 CFU/mL was more common in females than males ( p = 0.008).

Of 41 positive urine cultures, 21 had fever >38.0°C, 6 had altered mental state and 7 had lower urinary tract symptoms. 5 had a combination of symptoms while 12 patients had asymptomatic bacteriuria. There was no significant difference in symptom frequency between patients with positive and negative cultures (95%CI 0.5 to 2. 9; p = 0.67) and fever (95%CI −0.4 to 0.4; p = 0.94). After excluding 41 patients with concomitant infections, the presence of fever >38.0°C 48 hours before culture was also not associated with bacteriuria >10 3 CFU/mL (95%CI 0.67 to 5.1; p = 0.3). Urine WBC counts were significantly raised in patients with positive urine cultures ( p = 0.0001) (247 ± 357/hpf ) compared to negative urine cultures (11 ± 16/hpf ), suggesting that urine microscopy should be used as a screen for urine culturing even for hospitalised patients. Hospital-onset UTI accounted for 16 (39%) UTIs while Communityonset UTI accounted for 25 (61%). The length of hospitalization in HOUTI patients (16.5 ± 10 days) was significantly higher than that of COUTI patients (6 ± 2.6 days) (95%CI 5.6 to 15.5; p = 0.0002), otherwise, there were no differences between the two groups. The commonest causative organism cultured was E. coli (n = 17, 41%) followed by Klebsiella Pneumoniae (n = 6, 15%). There was a high degree of antimicrobial resistance detected in the E. coli: 14 (82%) were ampicillin-resistant, 7 (41%) were ciprofloxacinresistant, and 6 (35%) were resistant to third-generationcephalosporins.

Conclusion: This pilot audit is limited by the small sample size and presence of concomitant infections. Larger studies are needed to confirm our findings so we can effectively target therapies for patients with UTI without exacerbating the problem of antimicrobial resistance.

Development of an improved in-house protocol for fast and reliable identification directly from positive blood cultures using MALDI-TOF MS Menglan Zhou 1,2 , Qiwen Yang 1 *, Yingchun Xu 1 *. Background: Bloodstream infection is a major cause of morbidity and mortality in hospitalized patients worldwide. The application of matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) directly to blood culture (BC) broth has a potential to identify bloodstream infection earlier and facilitate timely management. Methods: We developed an in-house (IH) protocol for direct MALDI-TOF MS based identification of positive BCs, and compared its performance with the commercial Sepsityper™ kit. The protocol was initially evaluated and improved with known selective positive BC samples. We then studied in parallel the performance of the protocol and the colony MS identifications in positive clinical BC samples. All discrepancies were investigated by ""gold standard"" of gene sequencing. Results: Of the 54 spiked BC samples, the IH method showed comparable results with Sepsityper™ after applying the modified cut-off values. Accurate species, genus level identification was achieved in 88.7%, 3.9% of all the clinical monomicrobial BCs (284/ 301, 94.4%). It showed superior performance for Gram negative bacteria than for Gram positive bacteria (92.8% versus 82.4%). For anaerobes and yeasts, 80.0% and 90.0% accuracy in species identification were achieved, respectively. For polymicrobial mixed cultures (17/301, 5.6%), MALDI-TOF MS correctly reported a single species present in all the polymicrobial BCs under the Standard mode, while using the MIXED Method, two species were correctly identified in 52.9% of the samples. Comparison between different bottle types, the results indicated that the BACTEC™ Lytic/ 10 Anaerobic/F culture vials exhibited the best performance among the three (BACTECTM Plus Aerobic/F: 87.4%; BACTECTM Lytic/10 Anaerobic/F: 92.1%; BACTECTM Myco/F Lytic: 80.0%).

In conclusion, we developed a novel, simple and costeffective sample preparation method for direct identification of pathogens from positive BCs.

Evidence-based comparison of diagnostic accuracy of xpert MTB/RIF assay, with smear microscopy for pulmonary tuberculosis and its rifampicin -resistance detection evaluation Yi-Ching Huang 1 , Yuan-Hsin Chu 2 . 1 Division of Infectious Disease, Jen-Ai Hospital, 2 Department of Infection Control, Jen-Ai Hospital Background: Accurate and earlier detection of TB, and TB drug resistance is critical for TB control. Mycobacterial culture is considered as the best reference standard but is relatively complex and slow. Thus, Taiwan CDC recommends to develop rapid technique. Xpert MTB/RIF (Cepheid) assay has been introduced for the diagnosis of TB and RIF(rifampicin)-resistance. We herein compare its diagnostic accuracy of pulmonary tuberculosis with smear microscopy and assess the assay for rifampicin resistance in adults. Methods: It's a diagnostic question, Acquiring systematic review (SR) of cross-sectional studies with reference standard and blinding is priority. P-pulmonary tuberculosis; I-Xpert MTB/RIF; C-smear microscopy; O-diagnostic accuracy The PICO terms and their respective synonyms were engaged by boolean character ""AND""→ to search CochraneLibrary and Pubmed→ 3 SR were filterd out of 6 searching results in Cochrane Library→ One most relevant SR was selected and in PubMed we included the same SR. Results: As an initial test replacing smear microscopy, Xpert ® MTB/ RIF pooled sensitivity 89%; pooled specificity 99%. As an add-on test following a negative smear microscopy result, Xpert ® MTB/RIF pooled sensitivity 67%; pooled specificity 99%. For smear-positive, culture-positive TB, Xpert ® MTB/RIF pooled sensitivity 98%. Compared with smear microscopy, Xpert ® MTB/RIF increased TB detection among culture-confirmed cases by 23%. For RIF-resistance detection, Xpert ® MTB/RIF pooled sensitivity 95% and pooled specificity 98%. Conclusion: Compared with smear microscopy, Xpert ® MTB/RIF substantially increases TB detection among culture-confirmed cases. Xpert ® MTB/RIF has higher sensitivity for TB detection in smear-positive than smear-negative patients in adults. Nonetheless, this test may be valuable as an add-on test following smear microscopy in patients previously found to be smearnegative. For rifampicin resistance detection, Xpert ® MTB/RIF provides accurate results and can allow rapid initiation of MDR-TB treatment (Level 1).

Clinical aspect of Rhodotorula fungemia in a regional teaching hospital Yi Ching Huang. Division of Infection Diseases, Jen-Ai Hospital -Dali branch Background: Rhodotorula species are common environmental pigmented basidiomycetous yeasts, and also present in skin, nails, and respiratory, gastrointestinal, and urinary tracts in humans. Rhodotorula species as infrequent yeast pathogens increasingly cause invasive infections such as fungemia, meningitis, ocular infection, peritonitis, and endocarditis. Risk factors included immunocompromised patients and the presence of a central venous catheter. The propose of the study was to review cases of Rhodotorula fungemia over a period of 6 years to determine risk factors and outcome.

Methods: The records of the microbiology laboratory at Jen-Ai Hospital -Dali branch, a 602-bed regional teaching hospital in Taiwan, were reviewed to identify patients with positive blood cultures for Rhodotorula species from 1 January 2011 to 31 December 2016. The following information was collected: age, gender, underlying disease, presence and type of central venous catheters, parenteral nutrition, use of corticosteroids and exposure to broad-spectrum antibiotics. Results: 25 cultures of blood samples obtained from 12 patients grew Rhodotorula species, mainly Rhodotorula mucilaginosa. The median age was 66 years. Female patients were predominant. Central venous catheters presents in 9 patients, including 4 patients having Hickman catheters. Broad-spectrum antibiotics were administered in 9 patients. Only 2 patients received antifungal therapy. 30-day mortality was 33%. Conclusion: The most common predisposing factors were presence of central venous catheter and use of broad-spectrum antibiotics. Results: 13 cases were collected from May 2006 to April 2017 in a single tertiary teaching hospital. All of them were men aging from 17 to 54 years old. 11 were MSM and most common acute syndrome included fever and skin rashes. 6 were co-infected with syphilis. All HIV Ag/Ab EIA /ELISA showed positive and HIV RNA viral load ranged from 3.41 to more than 7 log 10 copy/mL. 7 had CD4 counts less than 200/uL. At the same time, 5 of 13 showed negative Western Blot and the other 8 showed indeterminate results. Repeat Western Blot was done in 6 of 13 and all revealed positive results finally, with time interval from 3 to 17 months. Conclusion: Although indeterminate Western Blot may be present in acute and early HIV infections, confirmation of HIV infection still need more information such as HIV RNA viral loads, PCR or seroconversion. For most clinicians, negative Western Blot usually excludes HIV/AIDS, and may ignore the fact of window period present in acute HIV infections. In high risk population, the diagnosis of acute and early HIV infections can be made by obtaining exposure history or a presentation of a STD.

The experience sharing of using system alert mechanism for Clostridium difficile toxin detection in a metropolitan hospital Chiung-Yin Chuang, Chin-Hsiung Kao.

Background: Clostridium difficile infection (CDI) is associated with long-term or multiple use of antibiotics. It disrupts normal gut flora resulting in diarrhea, pseudomembranous colitis, or megacolon, etc. The difficulty of diagnosis and the strain diversity make early treatment and isolation a challenge. Bacterial culture and identification are time-consuming and cannot to be used for confirming the virulent strains. By introducing inspection equipment actively and setting the molecular analysis mechanism of toxin-producing strain detection, toxin carriers receive timely treatment and infection control is implemented. Methods: Gene Xpert Dx system was used to detect the toxin genes of Clostridium difficile and NAP/O27 strains which is highly pathogenic. We set an alert mechanism in our hospital information system. It provides not only the daily list of the inpatients meeting the inclusion criteria, such as the times of diarrhea more than or equal to five time or the weight of watery diarrhea is more than or equal to 500 g a day, but also advice items and codes automatically. Results: Among 155 samples, 27 were positive for toxins (17.4%) and no NAP/O27 strains were detected. The laboratory sent information to the physicians and the infection control department immediately when positive outcome released. Also, medical information system and nurse information system revealed alert signal for isolation reminding. Conclusion: Patients suspected infected with Clostridium difficile can be diagnosed and treated by rapid molecular biology analysis. At the same time, contact precaution, antibiotic evaluation, and environmental cleaning rules establishment are carried out to control nosocomial Clostridium difficile infection and improve medical care quality.

Performance assessment of the automated iCubate platform for identification of gram-positive cocci in flagged blood cultures, including drug-susceptible and resistant isolates Tai and vancomycin (vanA/vanB) resistance associated genes using multiplex PCR technology.

Methods: A total of 212 consecutive flagged blood culture bottles with Gram-positive cocci were evaluated using iCubate platform in National Taiwan University Hospital. The results of this platform were compared to the final results of combined culture-based MALDI-TOF/MS identification, minimum inhibitory concentrations (oxacillin and vancomycin), 16S rRNA (species identification) and resistance-associated genes sequencing (mecA and vanA/vanB). Results: After exclusion of 15 (7.1%) samples with processing errors, the overall sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV) of this platform for identification of Gram-positive cocci species were 87.0% ( Background: Multidrug-resistant Gram-negative bacteria, particularly E. coli producing extended spectrum beta-lactamase (ESBL) and/or carbapenemase causes global public health concern. To date, limited data was available on genetic study of multidrug resistant E. coli in Myanmar.

Methods: A total of 390 non-duplicate consecutive isolates of E. coli collected from North Okkalapa General Hospital, in Yangon, Myanmar from January to December 2016, were analyzed. The presence of bla CTX-M , bla TEM , bla SHV was confirmed by a multiplex PCR and their subtypes were assigned by sequencing. Sequence type (ST) was determined based on Achtman scheme of MLST. Carbapenemase genes, virulence factors genes, O25b allele, and aac (6′)-Ib-cr were detected by PCR. MICs against 18 antimicrobials was determined by broth microdilution test.

Results: Among all the isolates, ESBL production was confirmed in 173 isolates (44.4%), and 30 isolates (7.8%) were resistant to carbapenems. Among the ESBL-producing E. coli, dominant phylogroup was A (n = 54, 31.2%), followed by D (n = 49, 28.3%), B2 (n = 48, 27.7%), and B1 (n = 22, 12.7%). Half of the isolates (47.4%, 82/173) had aac-6′-Ib-cr gene whereas O25b allele was detected only in phylogroup B2 isolates. A total of 154 (89%) isolates carried CTX-M and/or TEM-1b. CTX-M genes were classified into group 1 (n = 117, 67.6%) and group 9 (n = 9, 5.2%), which were identified as a dminant CTX-M-15 type, and minor CTX-M-55, CTX-M-27, and CTX-M-14 types. Isolates with CTX-M-15 gene were mostly classified into phylogroup B2-ST131. NDM gene was detected in 70% (n = 21) of carbapenem-resistant isolates. Among them, NDM-5 gene was predominant, followed by NDM-4, NDM-7 and NDM-1 genes. NDM gene-positive isolates were classified into various phylogroups/genotypes including B1-ST448 and D-ST405. OXA-181, a variant of OXA-48, was detected in one isolate each from urine and wound swab which belonged to phylogroup A-ST410. ST131-B2 clone poccessed more virulence factors than NDM genepositive isolates. Conclusion: Our study revealed high rate of ESBL-producing E. coli classified into O25b-ST131-B2 and ST405-D, and emergence of carbapenem-resistant isolates with various NDM genes in Myanmar.

Analysis on distribution features and drug resistance of clinically isolated Stenotrophomonas maltophilia in a regional hospital of central Taiwan An-Chi Chen, Yao-Shen Tung, Yui-Yein Yang.

Background: S. maltophilia is naturally resistant to many broadspectrum antibiotics that is an opportunistic pathogen capable of causing respiratory, bloodstream, and urinary infections. Hospitalized patients and receiving treatment for other conditions may be susceptible to infection, especially those with severely impaired immune systems. The purpose of this study was to collect samples of Stenotrophomonas maltophilia in order to determine drug sensitivity and epidemiological features. Methods: Epidemiologic data were obtained from 158 strains of Stenotrophomonas maltophilia collected between January 2014 and December 2015 in a regional hospital in central Taiwan. Fullyautomatic microorganism identification and drug sensitivity analysis was conducted using Phoenix 100 System (BD).

Results: A total of 158 strains of S. maltophilia were obtained from 22 outpatients (13.9%), 53 (33.5%) in the general wards, 83 (52.5%) in the intensive care units. In terms of gender, 91 of the patients were male (58%), the remaining 67 patients were female (42%). The sources of samples were sputum (N = 99, 62.7%), wound (N = 32, 20.2%), blood (N = 12, 8%), urine (N = 7, 4%); bile (N = 3, 1%)and other(N = 5, 3%). The resistance of Stenotrophomonas maltophilia isolates to Ceftazidime (CAZ), Levofloxacin (LVX), Sulfamethoxazole (SXT) were resistance rate was 77.8%, 25.9%, 19.6 respectively (Fig 1) .

The results showed that S. maltophilia had a high rate of separation from hospitalization, and prevention should focus on the cleanliness of the hospital, disinfection of the equipment and the handwashing of the staff in order to reduce the infection rate of the nosocomial infection. The drug sensitivity of S. maltophilia should be monitored and analyzed to assist the clinical Physicians choose a reasonable treatment of antibiotics.

Demographic characteristics of patients infected with Carbapenem Susceptible Acinetobacter baumannii (CSAB) and Carbapenems Resistance Acinetobacter baumannii (CRAB) with or without Multiple Drug Resistance (MDR) isolates in a regional teaching hospital in Southern Taiwan Yi-Ping Chen 1 , Po-Liang Lu 2 , Chun-Chi Huang 1 , Jeng-Yih Wu 1 , Chen-Min Kuo 1 , Tung-Nan Liao 3 , Ko Background: Overuse of antibiotics in health-care institutes had led to emergence of resistant pathogens in Taiwan. While the prevalence of multidrug resistant organism were closely monitored in the hospitals in Taiwan, the prevalence of colonization with these organisms in the community, especially among the adults, is largely unknown. This study aimed to investigate the prevalence of colonization with methicillin-resistant Staphylococcus aureus (MRSA), extended-spectrum β-lactamases (ESBL)-producing or carbapenem-resistant Escherichia coli or Klebsiella pneumoniae, and vancomycin-resistant Enterococci (VRE) among healthy adults in the community in Taiwan.

Methods: Between March 2017 and May 2017, a prospective, observational study was conducted at National Taiwan University Hospital, Hsin-Chu Branch. Nasal swabs, anal swabs (or stools) were collected from healthy adult volunteers. Bacterial culture and identification were performed using selective culture medium assisted with Vitek-2 bacterial identification system. A standardized case record form was used to collect demographic data of the paricipants.

Results: A total of 44 paricipants were included. Their average age was 45 years old. The male to female ratio was 1:1. The most common chonic cormobidities of the paricipants was hepatitris B (13.6%). 54.5% of paricipants had travelled abroad, and 25% of them had exposed to antibiotics in the previous one year. Among them, the prevelance of nasal carriage of MRSA was 6.8%. The prevelance of stool carriage of cefotaxime non-susceptible and ESBL-producing Escherichia coli was 25% and 18%, respectively. No carbapenem-resistant enterobactereciae or VRE was isolated. Conclusion: A high rate of colonization with Escherichia coli carrying cefotaxime non-susceptibility or ESBL was found among Taiwanese adults in the community. On the contrary, no carbapenem-resistant enterobactereciae or VRE was detected. The prevelance rate of nasal carriage of MRSA remains stationary.

The plasmid-mediated fosfomycin resistance determinants and synergy of fosfomycin and meropenem in carbapenemresistant Klebsiella pneumoniae isolates in Taiwan Tsung-Ying Yang 1 , Ting-Yin Wang 1 , Sung-Pin Tseng 1,2 ,

Methods: 642 CRKP clinical isolates were collected from a nationwide surveillance study (16 hospitals) in Taiwan in 2012-2013. Antimicrobial susceptibilities were determined. PFGE and MLST determined the clonal relatedness. Carbapenemases and fosfomycinases genes were detected by PCR, and their flanking regions were determined by PCR and sequencing. Synergistic activity of meropenem with fosfomycin was examined by the checkerboard method.

Results: In total, 36.4% (234/642) of CRKP isolates in Taiwan Conclusion: A clone ( pulsotype XXIII, ST11) has been found to be prevailing among fosfomycin-resistant CRKP in Taiwan. According to the in vitro data, the combination of fosfomycin and meropenem is a potentially alternative choice.

Antimicrobial susceptibility to various antibiotics against Staphylococcus saprophyticus isolated from urine in Japan Koichi Takahashi 1,2 *, Tetsuro Muratani 1,3 , Jun-ichi Ohno 1 . 1 IHibiki AMR Laboratory, Kitakyushu, Japan, 2 Fukuoka Shin Mizumaki Hospital, 3 

Background: Staphylococcus saprophyticus is one of major uropathogens that causes acute uncomplicated urinary tract infections. However, few data about antimicrobial susceptibility against S. saprophyticus isolates in Japan have been reported. We investigated the susceptibility to various antibiotics against S. saprophyticus isolated in Japan.

Methods: A total of 230 non-duplicate S. saprophyticus isolated from urine in the period from 2011 to 2016 in Japan were used in this study. The MICs of various antimicrobials against the isolates were determined by the two-fold serial agar dilution method as described by the CLSI. The resistant isolates to antimicrobials were basically interpreted according to CLSI breakpoint. The mecApositive isolates were determined by PCR using specific primers. Results: The isolates from female patients accounted for 93.5% (215/230). The isolates from male patients were not isolated in 2011 to 2013, however, in 2014 to 2016 they accounted for 8.9%. 81.4% of isolates from female patients were isolated from patients of less than 50 years old. In contrast, the isolates from male patients of less than 50 years old accounted for only 33.3%. Monomicrobial infection accounted for 87.0% (female: 89.3%, male: 53.3%). Betalactamase were not detected from all isolates. The mecA-positive isolates accounted for 15.2%. In 2011 In , 2012 In , 2013 In , 2014 In , 2015 In , and 2016 , the ratio of mecA-positive isolates were 6.7, 5.3, 0, 18.3, 17.2, and 23.5%. The ratios of mecA-positive isolates from female patients and it from male patients were 13.0% and 46.7%. The MICs of oxacillin against mecA-positive and negative isolates were ranged 2 to >128 mg/L and 0.016 to 2 mg/L. The isolates with MIC 2 mg/L to oxacillin existed 2 strains, one was mecA-negative, the other was mecA-positive. There were no isolates resistant to levofloxacin (0.008-1 mg/L), minocycline (0.004-0.25 mg/L), vancomycin (0.12-4 mg/L), and teicoplanin (0.12-8 mg/L). The resistant ratios to clarithromycin, azithromycin, clindamycin, fofomycin, and sufamethoxazole/trimethoprim were 29.6, 30.0, 2.6, 48.7, and 0.9%. Conclusion: In Japan, the mecA-positive S. saprophyticus have emerged and been increasing. The ratio of it accounted for 15.2%, especially in 2016 it accounted for 23.5%. In Japan, the increase of mecA-positive S. saprophyticus isolates is not clinical problem of chemotherapy because fluoroquinoles have no resistant. However, it is considered that we have to continue survey of antimicrobial susceptibility in S. saprophyticus.

Epidemiological analysis of drug resistant bacteria from 2015 to 2017: an experience from a regional teaching hospital in Hsin-Chu Hui Min Fan 1 , Ya I. Hsiao 1 , Yu Tin Cheng 1 , Yu Shan Huang 2 , Yi Shun Chen 1 *. 1 Laboratory Medicine Department, 2 Department of Internal Medicine, National Taiwan University Hospital Hsin-Chu Branch, ROC Background: Since our hospital joined the antibiotic stewardship program in 2015, we had been focusing on reducing the contamination rate of clinical specimens, monitoring the prevalence of multi-drug resistant bacteria, and promoting the proper use of antibiotics. For these purposes, we set up quality indicators and underwent internal discussions and staff trainings. By analyzing the isolation rates of drug-resistant bacteria for the past two years, we evaluate the effectiveness of infection control in our hospital.

1. Investigation of 4 drug-resistant bacteria with clinical significance to healthcare-associated infections, including methicillin-resistant Staphylococcus aureus (MRSA), vancomycinresistant Enterococcus (VRE), carbapenem-resistant Acinetobacter baumannii (CRAB), and carbapenem-resistant Enterobactoriaceae (CRE). 2. By using the Whonet system and working with our IT (Information Technology) department, a statistical report format was constructed. We could analyze the isolation rates and trend of drug-resistant pattern of the above bacteria from January, 2015 to February, 2017. 3. The analyzed results were reported and tracked in group meetings regularly held by our infection control committee to provide timely and effective clinical information to our infection control practitioners and physicians. Results: The isolation rates of 4 multi-drug resistant bacteria are as follows respectively: MRSA, between 27.7% and 55.3% (means: 47.4%); CRAB, reduced from 71.8% to 33.3% (means: 44.0%). Both the annual isolation rates of MRSA and CRAB declined significantly. The isolation rates of CRE dropped from 5.4% to 3.3% (means: 4.0%), with flat trend. However, VRE increased from 1.9% to 23.9% (means: 13.0%), which showed an upward trend. Conclusion: The trend analysis of MRSA, CRAB and CRE indicated successful infection control. However, the increased isolation rate of VRE highlighted the need to reinforce further monitoring of VRE infection, which is the key point of infection control in our hospital. Epidemiological analysis of multi-drug resistant pathogens could assist clinicians in choosing the appropriate antibiotics. By this analysis, a decrease of multi-drug resistant microorganisms, improved patient survival, a better quality of care, as well as the reduction of medical expense would all be expected.

Antimicrobial resistance of Staphylococcus aureus and Pseudomonas aeruginosa from otorrhea of chronic suppurative otitis media patients Yee-Hyuk Kim. Department of Otorhinolaryngology-Head & Neck Surgery, School of Medicine, Catholic University of Daegu, Daegu, Korea

Background: Knowing the bacteria that cause chronic suppurative otitis media (CSOM) and the antimicrobial susceptibility of the causative bacteria is important in the use of antibiotics as a primary treatment.

Methods: This study was conducted retrospectively, from March 2011 to February 2017, at the tertiary hospital in Korea using the medical records of 488 patients with otorrhea caused by CSOM. Results: The most commonly identified bacteria was Staphylococcus aureus (164 participants, 33.6%). Of the 164 cases, 60.0% (98 cases) were Methicillin-resistant Staphylococcus aureus (MRSA). The MRSA in 1 case was resistant to Vancomycin and 97 cases were susceptible. Seven cases were resistant to Trimethoprim-Sulfamethoxazole (TMP-SMX) and 91 cases were susceptible. MRSA had the highest sensitivity to TMP-SMX except Vancomycin. The second most frequently identified bacteria was Pseudomonas aeruginosa (109 patients, 22.3%). Of these 109 cases, 54.1% (59 cases) were Ciprofloxacin-resistant Pseudomonas aeruginosa (CRPA). CRPA were highly susceptible to Ceftazidime, Amikacin, and Meropenem. In 28.1% of patients (137 cases), the remaining bacteria were detected except for the two bacteria. No growth was found in 16% of patients, which may be due to antibiotic use before arriving at the tertiary hospital. Conclusion: MRSA was the most susceptible to TMP-SMX among oral medicine. Therefore, TMP-SMX may be a preferential treatment for MRSA in outpatient settings. CRPA showed the lowest resistance to Ceftazidime. Because no suitable oral antibiotic exists for CRPA, inpatient treatment should be considered in these cases. Background: Previous studies have suggested that sequence type 59 (ST59) predominates among borderline-oxacillin resistant mecA-positive Staphylococcus aureus (BORSA) (minimum inhibitory concentrations of oxacillin, 0.5-4 μg/mL) isolates, and clinical data related to bacteremia caused by BORSA ST45 isolates are limited. Methods: Patients (age >15 years) with bacteremia due to BORSA who were admitted to the emergency department in a tertiary hospital between January 2001 and December 2015 were evaluated. Clinical characteristics of patients with bacteremia caused by BORSA, methicillin-susceptible S. aureus and MRSA bacteremia were compared. Antimicrobial susceptibility testing was determined using the agar dilution method. Genetic characteristics of all BORSA isolates were analyzed using typing of the SCCmec gene and multilocus sequence typing (MLST). Pulsed-field gel electrophoresis was performed to identify the clonality of the emerging ST type isolates. The promoter region sequence and quinolone-resistant genes were also analyzed for all BORSA isolates and the major ciprofloxacin resistant clone, respectively. Results: Among the 65 patients with bacteremia due to BORSA, the in-hospital mortality was 24.6%, significantly lower than that of bacteremic patients caused by MRSA (38.5%, P = 0.03). The underlying conditions were similar between BORSA and MRSA patients, but with less dialysis of the former (P = 0.01). Multivariate analysis revealed that septic shock (odds ratio, 15.95; 95% confidence interval [CI], 1.41-180.89) and bacteremia originating from lower respiratory tract infection (OR, 5.78; 95% CI, 1.10-30.28) were two independent risk factors for 30-day mortality. Multidrug resistant ST45 with high-level ciprofloxacin resistance (MICs range, 8-128 μg/mL) replaced ST59 as the predominant clone since 2012. No major clustering of ST45 isolates was detected. Conclusion: Clinical presentations and underlying condition of BORSA and MRSA bacteremia patients were similar, with lower inhospital mortality. The emerging multidrug resistant ST45 clone has replaced ST59 as the most common in BORSA isolates.

Repeat HIV testing and early HIV diagnosis among clients attending voluntary counselling and testing (VCT) clinics in Wuxi, China 2013-2015: a retrospective cohort study Xiaojun Meng 1 , Wenjuan Ma 1 , Xuan Zhang 1 , Hao Cheng 1 , Jing Gu 1 , Andrew Grulich 2 , Huachun Zou 2,3 *. (3.7, 2.7-4.9 ), injection drug use (9.9, 6.5-15.1)] and having ever tested for HIV (2.0, 1.6-2.4) . The overall rate of incident HIV diagnosis among all VCT clients was 1.6 per 100 person-years. Incident HIV diagnosis was associated with male gender (adjusting hazard ratio: aHR = 8.5, 95% CI: 1.9-38.1), attending hospital-based VCT clinics (7.8, 1.1-58.3) and male-male sexual behaviors (8.4, 1.5-46.7). Higher CD4+ T cell count was observed among cases diagnosed at VCT clinics based at a local center for disease control and prevention compared to those based at other clinical services (median 407 versus 326 copies/mm 3 , p = 0.003). Background: In Japan, the 7-valent pneumococcal vaccine (PCV7) was introduced in 2010, and the PCV7 was replaced with the 13valent pneumococcal vaccine (PCV13) in 2013. Our previous study on non-invasive pneumococcal disease (non-IPD) in outpatients among children demonstrated an increase of non-PCV13 serotypes from 39.7% in 2011 to 75.1% in the PCV13 period (Nov. 2013 to Nov. 2014 . The present study aimed to investigate serotype distribution and antimicrobial susceptibilities of S. pneumoniae among children in 2016.

Methods: From May to November 2016, a total of 678 pneumococcal isolates were collected from noninvasive infections in pediatric outpatient (<16 years) who visited various hospitals and clinics throughout Hokkaido, northern main island of Japan. All isolates were serotyped by sequential multiplex PCR methods. After the PCRs, additional genetic subtyping for serogroups 6 and 15 was performed as reported previously. Susceptibilities to 18 antimicrobials were measured by broth microdilution method using ""Dry Plate, Eiken"" for all isolates of dominant serotypes 15A, 35B, 15C and 23A.

Results: Among the isolates, the prevailing serotypes were non-PCV13 serotypes 15A (14.5%), 35B (11.8%), 15C (9.3%), 23A (9.0%), 10A, 15B, 11A, and 34, accounting for 74.3% of all isolates. In total, 87.9% of the isolates were non-PCV13 serotypes. The nonsusceptibility rates to penicillin among serotype 23A, 15A, 35B, and 15C were 100%, 96.9%, 81.3%, and 7.9%, respectively. Most of the isolates with serotypes 15A, 35B, 15C, and 23A (≥91.3%) were nonsusceptible to erythromycin and tetracycline. High rates of cefotaxime non-susceptibility were found in serotypes 15A (65.3%) and 35B (82.6%). All the isolates examined were susceptible to vancomycin and quinolones. Conclusion: Our present study indicated further increase of nonvaccine serotypes following the use of conjugate vaccines, associated with increase of antimicrobial resistance in the dominant serotypes. Background: Cellulitis is usually caused by indigenous flora colonzing on the skin and appendages, such as Staphylococcus or streptococcus. However, in absence of abscess or open wound, it is difficult to establish the bacterial etiology. We usually prescribe antibiotics basing on local epidemiologic data. In recent few years, community-acquired MRSA (methicillin-resistant Staphylococcus aureus) increases gradually in Taiwan, whether antibiotics against MRSA and MSSA (methicillin-susceptible Staphylococcus aureus) should be prescribed at the same time become a clinical dilemma.

Herein, we conduct a study to establish epidemiologic data and evaluates the association between treatment response and nasal carriage status in adult patients with cellulitis. Methods: Patients, aged over 20, with diagnosis of cellulitis will be enrolled. Nasal swab, clinical samples and information will be collected and sent to further microbiologic culture and susceptible test.

Results: Totally 89 patients were included in the two-year consecutive prospective observational study. The nasal swab surveillance showed 74 (83%) patients without S. aureus colonization, 11 (12.4%) patients with S. aureus colonization, and 4 (4.5%) patients colonized by MRSA. No specific background factors, such as age, underlying disease, patients' sources and so on, can be identified to predict their carriage status. The four patients with MRSA colonization, none of them was given anti-MRSA antibiotics empirically but their clinical response, including days of antibiotics and days of hospitalization, have no different from those with MSSA colonization and without S. aureus colonization. However, once the pus formation, we noted the pus culture result was correlated with their nasal carriage status. The limitation of our study was small case number in patient group of nasal MRSA colonization. Conclusion: In Taiwan, MRSA nasal carriage rate is not high (4.5%) in adult patient with cellulitis. No specific factor can be identified to predict a MRSA nasal carrier in this group of patients. However, whether a patient is a nasal MRSA carrier or not, there is no different in their treatment response to antibiotics, empiric antibiotics against MRSA is not needed.

Background: Carbapenem resistant Klebsiella pneumoniae (CRKp) have been recognized in healthcare settings and pose a serious threat to hospitalized patients. CRKp is resistant to almost all available antimicrobial agents. Mechanisms of resistance to carbapenems involves the production of carbapenemase and loss of porins. However, the types of carbapenemase involved varies from country to country. Previously, CRKp isolated in 2013 from a tertiary hospital in Malaysia have been characterized (Low et al., 2017) . In this study, we aimed to determine the changing trends of genotypic characteristics of CRKp by comparing the isolates from 2014 with the published 2013 data.

Methods: A total of 18 CRKp isolates from 2014 were collected from the diagnostic lab in the hospital. Presence of carbapenemase genes and outer membrane protein (OMP) in 2014 isolates were determined using polymerase chain reaction (PCR). The isolates were subjected to multilocus sequence typing (MLST). The results obtained for 2014 isolates were compared with the published data on 17 CRKp isolated in 2013 from the same hospital. All isolates from 2014 and the isolates published in Low et al. (2017) were the first isolate per patient. In addition, in this study, the clonal complex of related isolates was defined by eBURST based on the assigned sequence types for 2014 isolates in this study and the sequence types for 2013 isolates in previous study (Low et al., 2017) . Results: NDM was the most common carbapenemase gene detected among the 2014 isolates and majority of the NDM producing isolates were co-producing other carbapenemase. Loss of porin was detected in 5 isolates from 2014. Compared to the previous study, the number of NDM in this hospital was significantly increased from only 1 isolate in 2013 to 12 isolates in 2014. Overall, increasing numbers of carbapenemase coproducing isolates over the 2-year period had been observed. In addition, 9 sequence types with 3 new sequence types (ST2198, ST 2199, ST2200) were identified among 2014 isolates. Similar to previous findings on 2013 isolates, ST101 was predominant in 2014, followed by ST147 and ST11. Finally, based on the eBURST analysis, the isolates from 2014 have been grouped into two clonal complexes while all 8 sequence types of 2013 isolates appeared to be singleton. Conclusion: High heterogeneity among the isolated CRKp were observed. The increasing cases of CRKp in this hospital might be due to the emergence of CRKp harboring various carbapenemases coupled with the loss of porin.

Analysis of Salmonella strains in 2016single hospital experience Chia-Chun Tsaz, Li-Chiu Weng, Pei-Ling Lien, Chi-Ching Chen.

Background: Salmonella is one of the main causes of diarrheal diseases. In general, humans contracted it from consumption of contaminated food by manure of animal origin (eggs, meat, poultry, and milk) or vegetables, which have been implicated in its transmission. It is usually characterized by acute onset of fever, abdominal pain, diarrhea, nausea and vomiting. Most cases of salmonellosis are mild; however, sometimes it can be lifethreatening. Salmonella is a gram negative rods genus belonging to the Enterobactericeae family. In the genus, Salmonella, more than 2,500 serotype are currently recognized. Antimicrobial resistance is a public health concern and Salmonella is one of the microorganisms in which some resistant has been emerged. So we analyzed 284 strains of Salmonella, to understand its serum group, specimen source, antimicrobial drug resistance.

Methods: A total of 284 strains of Salmonella were isolated from the clinical specimen of the patients. The analysis was done from 2016 samples.

Salmonella grouping, analysis of 284 strains of Salmonella, type B has 85 strains (30%), type C has 66 strains (23%), type D has 83 strains (29%), E type has 47 strains (17%), unidentified (non-PolyA-I & Vi) type has 3 strains (1%). 284 strains of Salmonella is isolated from bile, blood, pus, feces and urine, which account for 88% of feces, 8% blood, and 2% urine. We analyzed a routine antibiotic susceptibility testing of ampicillin/sulbactam, ceftriaxone, ertapenem,imipenem, levofloxacin and trimethoprim/sulfame. Ampicillin/sulbactam had 55% susceptibility, ceftriaxone had 69% susceptibility, ertapenem had 100% susceptibility, Imipenem had 100% susceptibility, levofloxacin had 70% susceptibility, and trimethoprim/sulfame had 69% susceptibility. Conclusion: O antigen group of Salmonella, found to have various types, not more often type have been separated, they mainly isolated from the feces, and in the antibiotic susceptibility test part of the carbapenem class (Imipenem, Imipenem) has 100% susceptibile, fluoquinolone drugs (levofloxacin) has 70% susceptibile, other drug also has more than 50% susceptibile testing. Salmonella is one of the most common causes of food poisoning. Symptoms of salmonellosis are mild and patients recover without specific treatment in most cases. However, in children and elderly patients, the associated dehydration become severe and lifethreatening. Antimicrobial resistance of salmonella have emerged,the right antibiotic choice is very important.

Improving the condition in operation rooms Fu-Chieh Chang 1 , Hsueh-Wen Wu 2 , Li-Lin Kuo 2 , Chang-Pan Liu 1,3 . Background: The HOCl was a weak acid which can production anions then interacted to the cell wall or cell membrane. Once this acid interacted to the pathogen, it will destroy the structure of pathogen then the osmolality will change. Finally, the pathogen will die. So, in this study, we wanted to know the really effect of HOCl killed pathogen in vitro. Methods: In this study, we used the Acinetobacter baumannii as a pathogen. In the begging, we transferred Acinetobacter baumannii to normal saline, then check the MacFaland to 0.5. We used this suspension and add HOCl, then we used the TSA agar to culture. According to previous studies, these bacteria can live in the surface of hospital over 3 months, and once the staff touch the surface, it would be a chance to infect patient by any actions. That is to say, it's very easy infected by CRAB with imperfect infection control. In this report, we found some error in our hospital that occurred on the action of bronchoscopy operated. Methods: In this Investigation, we collected the five strain CRAB from 5 patients' BAL culture. Then we check all the action of staff who operated the bronchoscopy. Then we used the Pulsed Field Gel Electrophoresis to check the strain were the same or not. Results: After the investigation, we found the staff did not use the disposable gel for bronchoscopy. It wound be another risking for infection. Otherwise, we found the staff didn't wash their hand correctly. But, thanks to the PFGE, the result showed all the CRAB were not the same. That's to say, these CRABs could live in the surface, and not only one train were presented on this ICU. Conclusion: In this case, even the CRAB were not the same, but we still found the possible chance that would cause infection once the staff don't take care. Besides that, we suggested that it's better to use the disposable gel for Bronchoscopy. Table 1 .

The ward environment cleaning monitoring took 13 months, the results showed even cleaning and disinfection have been implemented, MDRO still exists in the ward environment. Thus, we provided sanitary unit these data for educational training and ask them to improve their practice on environment cleaning. Besides, Table 1 also showed non-high-risk ward sections was free from MDRO, indicating high-risk ward sections should be regularly monitored, and supervised implementation of environment cleaning and disinfection to ensure better cleanliness in hospital environment and better healthcare quality.

The trend of carbapenem-resistant Acinetobacter baumannii and Pseudomonas aeruginosa in a regional teaching hospital Among these drug-resistant species, CRAB and CRPA are in the majority. Therefore, this article aim to analyze the trend of CRAB and CRPA in recent 5 years. Methods: The statistic data of antibiotic susceptibility of the gramnegative bacteria regarding to Acinetobacter baumannii and Pseudomonas aeruginosa were collected from the Medical Information System of the hospital from 2012-2016, and Carbapenem was seen as the index, that is, the percentage of Carbapenem-resistant strain depends on the highest percentage, either of ertapenem-, or of imipenem-resistant strain.

Results: Our statistical result showed that during the year 2012-2016, Acinetobacter baumannii were 3137 strains in total; CRAB was 86.9% (81.21%∼90.29%) in average; Pseudomonas aeruginosa were 6,607 strains in total; CRPA was 18.01% (13.89%∼20.82%) in average. We found since 2012, CRAB and CRPA decreased year by year. In 2015 they suddenly raised, yet through infection control interventions, CRAB decreased in 2016 again (the results were listed in Figure 1 ). Conclusion: CRAB showed the highest percentage of antibiotic resistance MDRO in our hospital. In addition to promotion of Antibiotic Stewardship Program, more attention should be paid to MDRO monitoring. Our monitor data were counted and the results were reported in the infection control meeting quarterly. Therefore we were able to find pre-described the trend of raised CRAB in 2015, and immediately took interventions, including contact precautions, arranging areas to place and treat MDRO patients. Besides, the auto messaging system has established in 2015, reminds the healthcare staffs of implementing contact precautions, to facilitate the prevention of MDRO transmission.

Cleaning efficacy for contaminated mobile nursing carts Tun-Chieh Chen 1,3 *, Chun-Feng Chiu 1 , Jennifer Nai-Hwa Shih 2 . Background: Mobile nursing cart is a trend for modern nursing practice, which can enhance the working efficiency and reduce the processing mistakes. But the mobile devices might become the media for transmission of multi-drug resistant organisms. This study aims to investigate the adequacy and accessibility of surface disinfection process for nursing cart, which can provide the experience for establishing standard operation processes of environmental cleanliness and disinfection. Methods:

1. We performed surface culture of every mobile nursing cart from 10 highly frequent touched points. 2. We tested the disinfective efficacy of different agents (alcohol or quaternary ammonium based spray or tissue), which determined by surface ATP bioluminescence. 3. We conducted questionnaire from healthcare workers for the convenience and accessibility of different disinfective agents.

Results: We yielded 180 microorganisms from total 320 points (56.3%) and there were 62 clinically significant bacteria (19.4%), which followed by the frequency was 14 Enterococcus faecium, 14 Stenotrophomonas maltophilia, 10 Enterobacter cloacae, 7 Enterococcus faecalis and 7 Acinetobacter baumannii. The efficacy of different disinfective agents were well, which determined by significant decline of ATP value after surface cleaniness. The healthcare workers reported similar convenience and accessibility of diffecent disinfective agents. Conclusion: The mobile medical devices are widely used in modern healthcare service. We demonstrated the devices could be the media of bacterial transmission and might result in healthcare associated infection. Alcohol or quaternary ammonium based agent is a suitable agent for this purpose.

The small parts of the automatic endoscope reprocessor may cause the positive results of repeated microbiological monitoring of gastrointestinal endoscopes after high-level disinfection Che-Yi Hung 1 *, Shu-Shia Chang 2 . 1 Kuang Tien General Hospital, 2 Kuang Tien General Hospital, Dajia Branch

Background: Contaminated endoscopes have been linked to more outbreaks of healthcare-associated infections than any other medical device. Microbiological surveillance may help in monitoring the effectiveness of GI endoscope disinfection. In this study, what caused GI endoscope cleaned and disinfected with the standard procedure but still tested positive for bacterial growth was discussed.

Methods: The EW-30 and OER-AW automated endoscope repressors(AERs)(Aizu Olympus Co., Ltd, Tokyo, Japan) were used for endoscope reprocessing. One EW-30 and one OER-AW AERs were used for endoscopes and the other EW-30 was used for colonoscopes. To monitor the quality of gastrointestinal endoscope reprocessing, two endoscopes and one colonoscope were randomly selected for obtaining surveillance cultures monthly. Cultures were obtained by adding 10 ml of sterile water to the biopsy channel of an endoscope. This wash was collected in a sterile container, plated onto blood and Eosin-Methylene Blue agar, incubated at 35°C, and examined for bacterial growth. A fishbone diagram was used to identify possible causes of a problem and to sort ideas into useful categories.

Results: A total of 30 cultures were obtained from January to November in year 2016. 8/30 (26.7%) cultures were positive; 7 (87.5%) were from endoscopes. The fishbone diagram showed that the root causes of the positive cultures may be job rotation and old equipment. The reprocessing procedure was altered, with improvement. However, 6 (60.0%) positive cultures were obtained from endoscopes processed with the EW-30. The fact was found that the joints of the water/air channel and the instrument/suction channel in the AER were damaged and could not allow the circulation of the fluid under pressure through the channels. There are different methods used for monitoring environment clean in hospital, include: direct observation, bacterial culture, and ATP (adenosine triphosphate bioluminescence assay). The purpose of terminal disinfection after patient discharged is to ensure environment clean of ward to keep next patient from infections caused by polluted environment. Thus, a regional teaching hospital apply ATP to evaluate efficacy of monitoring environment cleaning after terminal disinfection.

Methods: Two medical ward, 2 surgical ward, Obstetrics ward, and 2 intense care ward sections were selected according to number of beds. Ten to 20 high contact surfaces of each ward sections were chosen for ATP test. Sanitary staffs clean the ward environment with 500 ppm hypochlorous acid after patient discharged. After they cleaned these surfaces 30 minutes to 2 hours later, these surfaces were rubbed for sampling by ATP swab for environment monitoring.

Results: There was a total of 100 samples in which RLU values of 5 samples were over 500, thus unqualified rate of ward environment cleaning was 5% (5/100). Refrigerator door handles in surgery ward 5F showed highest RLU (1684); next were EKG monitor keyboard (RLU = 769) and bedside cabinet table surface (RLU: 671) in the medical intense care ward (CCU); then desk drawer beside the bed (RLU = 550) and closet door handles (RLU = 509) in the medical ward 3F. Besides, all 20 surfaces in surgical intense care ward (ICU) showed RLU values < 100, indicating high efficacy of terminal disinfection. RLU values were showed in Table 1 .

Conclusion: Some environmental surfaces in the monitoring showed unqualified cleanliness, which means clean work have not been thoroughly implemented. We provided sanitary unit these data for educational training and ask them to improve their practice to these high-RLU environmental surfaces, which help making environment clean better. Clean environment is associated with better healthcare quality. In general, regularly monitoring environment clean is necessary, either method is used.

Correlation between first line anti-tuberculosis drugs and hepatitis Background: According to American Society for Microbiology, blood culture contamination rate should be lower than 3%. Bacteriological laboratory of a regional teaching hospital set 3% as threshold and monitors blood culture contamination rate every month. The average blood culture contamination rate from July to September, 2016 was 3.6% (3.1-4.0%) which was higher than the threshold 3%. We further analyzed all causes and made a critical examination. The possible causes were as follows: (1) Results: Blood culture contamination rates of November and December were 1.6% and 2.6%, respectively, and both were lower than 3%. Monthly monitoring and quarterly evaluation of collecting blood culture samples were being carried on.

Conclusion: Blood culture report provides an important basis for physicians to recognize pathogens result in blood infections and prescribe drugs for effective treatment. Through operating appropriate sample colleting skills helps reducing blood culture contamination rate, and providing correct result to ensure accuracy of laboratory report and increase healthcare quality.

Application of fluorescent marker with quantitative bioburden methods: for cleanliness assessment I-Chen Hung 1 , Hao-Yuan Chang 2 , An-Chi Chen 1 , Ling Ting 1 , Yeur-Hur Lai 1 , Wang-Huei Sheng 1 . 1 National Taiwan University Hospital, 2 National Taiwan University

Background: Improvement of environmental cleaning in hospitals has been shown to decrease cross-transmission of pathogens. Several objective methods, including aerobic colony counts (ACCs), adenosine triphosphate (ATP) bioluminescence assay and fluorescent marker were developed for cleanliness assessment. However, few published data are available regarding the interpretation of fluorescent marker.

Methods: This study was conducted a prospective survey of a stratified sampling in 2 hospitals. The same 10 high-touch surfaces were tested after terminal cleaning, using fluorescent marker, ATP assay and ACC. The surfaces with fluorescent marker of our evaluation defined totally cleaning, partially cleaning (1/4 to 3/4 residual gel), no cleaning. The benchmark clean criteria of ATP and ACC were <500 relative light units (RLU) and <100 colony forming units (CFU)/100 cm 2 , respectively. Results: Of the 830 high-touch surfaces, 321 (38.7%) surfaces were totally cleaning according to fluorescent marker, 84 (10.1%) surfaces were partially cleaning, 425 (51.2%) surfaces were not cleaning. Respectively, 66.7% and 91.4% of ATP and ACC assessment were considered clean according to each test standard. The surfaces with fluorescent marker totally cleaning group had significant lower ATP value and ACC value than partially cleaning group and no cleaning group (Table 1) . And totally cleaning group had significant higher ATP clean rate and ACC clean rate than other groups. Totally cleaning vs no cleaning significance at P < 0.0001.

Our evaluation provide the surfaces with fluorescent marker totally cleaning could offer objective standards for hospital cleanliness. Strengthen the reliable wipe off of high-touch surfaces can improve the cleaning quality. Further studies are necessary to establish the association of cleanliness and microbial transmission and healthcare-associated infections.

Background: Hand Hygiene by using alcohol hand rub has been extensively promoted in our hospital since 2008. We had adopted all elements of the WHO multimodal promotion strategy of Hand hygiene including preparing resources, obtained baseline compliance rate, formal kick-off ceremony and serials education program, continued monitoring and feedback. We also developed ongoing action plan for promoting hand hygiene every year. The compliance rate had risen from 21% in 2008 to 76% in 2012. However, we found the compliance rate have reached the plateau during the period from 2012 to 2014 (76%, 73% and 74%). The objectives of the program were (1) to empower, engage, educate and train up Infection Control Link Nurse (ICLN) in Hand Hygiene promotion and supervision, (2) to recruit an ICLN from each ward and unit and appoint them as observer in hand hygiene practices, (3) to reinforce hand hygiene compliance and enhance safety culture.

Methods: The observation period had been divided into two parts: part 1 in 2014 and part 2 in 2015. 20 ICLNs had been recruited as hand hygiene observers. Introduction to hand hygiene observation had been provided by Infection Control Nurse (ICN). ICN provided orientation and coaching on the day of observation to every observer till ICLNs were able to perform observation independently. ICLNs had been assigned to observe wards except their own ward in order to avoid bias. Each observation session last for 30 minutes. ICLN self-respond questionnaire was used to evaluation the programme afterward. ICT would than collected the duly completed questionnaires and performed data analysis.

Results: The total number of hand hygiene opportunities observed by ICLN were 1262 (Part 1) and 1248 (Part 2) respectively. There is no significant variation (75.9% vs 74.1% P = 0.320) between hand hygiene compliance rate in the first part of hand hygiene observation and the observation performed by ICN in previous quarter before the study. The overall hand hygiene compliance rates had been significantly improved from 75.9% to 80.3% (P = 0.007) in all staff groups. All of the 20 ICLNs had completed the evaluation questionnaires. Background: Tuberculosis (TB) is a common legal infectious disease in Taiwan. The lesions of Tuberculosis are sometimes non-classical, leading to difficulties in making clinical diagnosis. Thus, Tuberculosis easily causes nosocomial infection and the reason behind is worth exploring. The diagnosis of Tuberculosis mainly depends on chest X-ray, TB/AFB, and TB/culture so that the patients can be diagnosed and receive treatments in time. In July 2016, another hospital notified us that one of our patients had Tuberculosis. In June 2016, this patient was admitted to our hospital for 8 days. We found 3 contacts after estimating the communicable period, which was 3 months before the patient was diagnosed. The patient neither presented with classical symptoms of Tuberculosis nor received any chest x-ray inspection. As a result, we wanted to analyze the reasons and make improvements. Methods: We used Root Cause Analysis (RCA) and implemented interventions including: (1) to add inpatient's chest X-ray inspection into routine standardized operating procedures and announce it in infection control committees and visiting staff meetings, (2) to provide in-service education, and (3) to follow up the contacts' chest X-ray inspections regularly. Results: After the interventions, chest X-ray inspection is now one of the routine admission inspections for inpatients. Also, there are two regular courses relating to Tuberculosis each year and the course completion rate of all health care professionals was 95.5%. Conclusion: Some patients with Tuberculosis may spread mycobacterium tuberculosis in hospitals before their diagnosis is confirmed, thus, increasing the risk of exposure to infections for staffs and patients. It is necessary to make convincing diagnosis in time and implement infection control interventions. All in all, we provided our caring experiences as a reference for medical and nursing management.

Investigation and treatment of abnormal increase of MDROs in surgical intensive care unit of a medical center Chi-Fen Tsou 1 , Ying-Ling Chen 1 , Hung jen Tang Results: These control measures were successful and no further cases of VRE colonization were identified. VRE was isolated from the environment (surfaces of bed rails, ambu bag, on a patient bucket urinal, and curtain). All isolates were sent to a reference laboratory for typing and confirmed that the environmental samples were the same strain as the 2 patients who isolated VRE on the first occasion. Conclusions: Although VRE is known to persist in the environment, we were concerned that VRE persisted despite cleaning of the ICU. Further investigation revealed that ward cleaner is responsible for cleaning the bed and that nursing staff are responsible for cleaning the medical instruments. But as the cot sides of a bed may be moved, it was unclear whose role it was to clean the cot sides. Transmission to patients is however more likely to occur in ""near-patient"" areas such as the hospital bed and patient equipment. While cleaning of soiled linens room and floors is usually done by the ward cleaner, which cleans these ""nearpatient"" areas is greyer and may be carried out by nursing staff, ward cleaner or healthcare assistants; depending on the nature of equipment to be cleaned. Most importantly on ""near-patient"" areas, this can be an important niche for transmission of multidrug resistant organisms.

Investigation and control of outbreak of impetigo infection among newborns in maternity nursery of regional teaching hospital Yu-Lan Wang 1 , Yu-Wen Huang 2 , Ya-Fang Wang 3 , Yong-Ching Lau 4 , Wen-Chuan Lin 5 . 1 Infection Control Office, 2 Infectious Diseases, 3 Pediatric Infection Disease

Background: Five new born were reported of the pustules on skin, arms, and eyes, impetigo was suspected at 13th October year 2015 in Maternity Nursery. Implementations of infection control measures were applied in order to control the outbreak. Methods: Environmental and laboratory investigation: We started our study by interviewing healthcare workers who worked in Maternity Nursery, five neonates presented with pustules during 10 days admission. We found that workers did not change protective clothes; medical files were located at patient's room, workers failed to clean their hands, repeated use baby towel, clean stuff located at patient's room. Environmental samples, 59 specimens, from the most frequently used neonatal care equipments, such as radiant warmer, weight scale, baby incubator, nursing ward and keyboards were collected for bacterial culture. Nasal swab samples (98) Background: An accurate diagnosis of bloodstream infection relies on the result of blood culture, especially in the emergency room (ER), where emerging or critical patients were taken care of and relied heavily on rapid and correct blood culture results. In the case hospital, the contamination rate of blood culture constantly remained as high as 7.6% (CLSI <3%), which would influence a physician in making correct diagnosis and choosing suitable antimicrobial agents. Therefore, how to reduce the contamination rate of blood culture becomes a critical issue.

Methods: An interdisciplinary team was set up. Through a quality control approach, the team analyzed the causes of blood culture contamination through real-time observation on site. Certain infection control strategies were implemented, including: (1) a new trial of 2%CHG detergent and preparation procedure; (2) a newlytested soaking time of 3 secs for disinfect cotton swabs at 2%CHG; (3) a revision of disinfection procedure; and (4) a feedback of blood culture contamination for consequent re-education and training.

Results: From January to April 2017, the contamination rate of blood culture decreased from 7.6% to 5.3%. In addition, the waiting time for evaporation of detergents decreased from three minutes to less than one minutes. For an average of 665 blood cultures in ER per month, about 1330 minutes (22 hours) were saved in one month. The weight of medical waste per sample was decreased from 12 grams to 4.07 grams with a total of 5273.5 grams, indicating a cost saving in waste handling by nealy 66% per month in ER.

Conclusion: Implementation of infection policy toward blood culture procedure is beneficial for physicians in making precise therapeutic decisions, and demonstrates to be an effective approach to reducing waiting time and cost in ER. Background: Hand hygiene is the leading measure for preventing the spread of antimicrobial resistance and reducing healthcareassociated infections. However healthcare worker compliance with optimal practices remains low in most settings. We evaluated the impact of multifaceted hand hygiene (HH) intervention on the adherence of HH and isolation rate of antibiotic resistant organisms. Methods: Multifaceted HH intervention was conducted between 2011 and 2016, and the isolation of resistant Pseudomonas aeruginosa and methicillin-resistant Staphylococcus aureus (MRSA) was compared with conventional intervention period (2006) (2007) (2008) (2009) (2010) . The intervention consists of multidisciplinary HH campaign, encouragement of the 5 moments for HH, monitoring of the consumption of HH products and feedback using reported benchmark, and direct observation of HH practices in MRSA high risk wards. Antimicrobial use density and antimicrobial heterogeneity index were also evaluated (antibiotic class evaluated: tazobactam/piperacillin, carabapenems, 4 th generation cephalosporins/ceftazidime, and fluoroquinolone). Results: Participation rates in HH campaign increased from 48.7% to 90.4% ( p < 0.001). Number of HH episodes per patients day calculated by the consumption of alcohol hand rub increased from 5.6 ± 5.5 to 8.5 ± 10.3 ( p < 0.001). There was a significant difference in the isolation rate of hospital onset MRSA between the 2 intervention period (1.07 and 0.74/1,000 patient days, p < 0.001). Isolation rate of P. aeruginosa which reveal resistance to 2 or more classes of antibiotics was also significantly decreased (0.163 and 0.134/1,000 patient days, p = 0.004). In addition to direct HH observation, relocation to a new ward had a beneficial effect in the MRSA high risk wards. Rate of antimicrobial use density in carbapenems decreased from 36.4% to 25.1%, and antimicrobial heterogeneity index improved from 0.82 to 0.85. Conclusion: Paralleled introduction of multifaceted HH intervention and other strategies such as appropriate use of antimicrobials had a significant impact on the isolation of antimicrobial resistant organisms.

The infection control experience of suspected gastrointestinal tract cluster incidence in a psychiatric center rehabilitation ward Shiun-Yin Jeng 1 , Yu-Pin Liu 1 , Huei-Lan Lu 1 , Tso-Jen Wang 1 , Chung-Shin Liao 2 . 1 Jianan Psychiatric Center, 2 Chang Hua Hospital

Background: Most of psychiatric patient had poor self hygiene and self-care, while infectious diseases occurred; it is easy to lead a cluster event in a psychiatric rehabilitation ward. The implementation of gastrointestinal monitoring system and early intervention are very important in a low function and closed rehabilitation ward. Background: In order to maintain the quality of bacterial culture tracing, infection control medical staff regularly participate the infection control training course and implementation in clinical related business, such as notification the positive results of bacterial culture, and the significant abnormalities of legal infectious pathogens, so that patients can receive effective treatment immediately. Annual collection of drug sensitivity test results, statistical analysis, groups discussing regularly with infections control team member, to discuss and collaborate on meaningful cases for promoting clinical interaction. Methods: Bacterial culture notification mechanism, the pathogen of bacteria included in this monitoring system, then the pathogenic bacteria strains to monitor the infection cases,to avoid the spread of infection and to avoid cluster infection occurred, through the TNIS system to provide the reporting function, the hospital can analyze the medical care-related infection, the results of the bacterial culture and the antibiotic resistance distribution of the hospital, as a reference for the internal review and improvement, to assist the hospital in developing appropriate monitoring mechanisms and methods, to enhance the quality of nosocomial infection control. As long as the patient test report in the presence of bacterial culture positive, the laboratory will be informed to send a newsletter to the Infection Control Center, Medical, Pharmacy, Abnormal culture results will tabulation, and in the sense of the meeting to review whether the absence of negligence.

Results: From January 1 to May 31 2017, there was one abnormal information data, three set sputum culture has sent to the laboratory on March 9 to 13, the data found abnormal on March 13, and this report has registered on the March 16, and send messages to doctor, pharmaceutical, infection control center, the relevant report sent to the Infection Control Center, to notify the ward and provided a district area care immediately after received this abnormal report. Family medicine consultation, medication used followed X-ray, blood examination on March 21. Conclusion: The establishment of a mechanism for the prevention of bacterial culture can prevent the spread of infectious diseases in the hospital, to speed up the diagnosis of infectious diseases by the physician's expertise, for the understanding of the domestic epidemic, rapid and correct diagnosis of the patient's condition.

According to the severity of the disease and infectious, the patient to provide appropriate treatment and isolation measures, and require the relevant medical staff to take appropriate protective measures to avoid causing nosocomial infection.

Experience sharing on application of fluorescent agent detection in personal protective equipment put-on and take-off training En-Tse Yang 1 , Jann- Tay Conclusion: The application of fluorescent agent detection in the evaluation for skill of putting-on and taking-off PPE not only increases the learning motivation of trainees but also provides an objective indicator and real-time feedback. In addition, by analyzing the contaminated parts, and referring it to prior put-on and take-off procedures, a better procedure and instruction manual could be figured out.

Effectiveness of active screening of vancomycin resistant enterococci (VRE) in the intensive care unit of a regional hospital Wanling Tai. Chiayi Chang Gung Memorial Hospital

Background: The prevalence of vancomycin resistant enterococcus (VRE) of healthcare-associated infections was significantly increased (9.8% in 2010 and 32.5% in 2010), and most of them were in the intensive care unit (76% CHG) bathing to patients in ICU as well as finding and disinfection or eradication of niduses from which multiple drug resistant organisms (MDRO) were cultured. The items of niduses discovered were feeding cups and plastic baskets belonging to individual bed but not individual patient, working tables of nurses, and two communal bedpans. We analyzed data retrospectively from a prospective infection control database since May 2015 at the ICU of 41 beds and calculated the monthly trend of CRKP before and after two stages of infection control measures in ICU after adjusting the effects of hand hygiene and antibiotics usage. In order to assess the changes of CRKP in 3 periods, we used the generalized additive model (GAM). The fixed effects were period, time ( per 1 month), DID, the compliance and the accuracy of hand washing, and the random effect was period*time. The random effect showed the change of CRKP after interventions.

Results: The upward trend of CRKP turned downward after second stage of interventions significantly (Figure) . Second stage of interventions showed significant effect (RR 0.53, 95%CI 0.37-0.78) (Table) . The fixed effects of hand hygiene and DID were nonsignificant. The results showed that there were no significant differences in the incidence of MDROs and the occurrence of two abnormal events. The first occurrence of MDROs was the same as that of MDROs. Sudden changes in the cause of the KPC, further analysis of the cause of the abnormal events, contact isolation measures and environmental cleanliness, so infection control intervention measures in addition to the original hand hygiene and contact isolation measures and environmental clean, the other for high-risk cases and Has been identified in the case of centralized care, the second occurrence of MDROs abnormal events, suspected bursts of bacteria for the CRAB, monitoring the cause of this event may be due to the unit space can not clean the bed and equipment into the care area, in the bed Abnormalities in the process, so the infection control measures for the collection of the original collection area is not collected, to strengthen the detection of the same ROOM is to increase the non-CRAB environmental cleaning frequency. Results: The second use of MDROs is abnormal, the first use of the bundl care for hand hygiene, contact isolation protection measures, environmental clean and high-risk partition isolation; second use of the bundl care for the hand hygiene, contact isolation protection measures And the environment clean. It is found that the combination of MDROs is suspected to be in the group, and each time the MDROs are detected in the unit, the analysis of the abnormal events is discussed with the care unit, and the main reason for the occurrence of the event is possible, Combined care work, can still effectively solve the outbreake of events.

Conclusion: In Taiwan, the medical care environment and clinical care workers often fail to clarify the care area and the patient area immediately when the work is busy. As a result, the compliance rate of hand hygiene and contact isolation can not reach 100%. Therefore, to reduce the care workers due to hand hygiene or contact isolation measures are not implemented lead to migration and cause infection. In addition, for the detection of MDROs must be concentrated to receive, from the two abnormal event handling still need to consider all the staff, infection control requirements in a holistic manner to consider the most efficient way to solve this problem, by this experience is used as a reference for dealing with suspected the MDROs outbreake of events. Background: Following the pilot study of 6 alcohol-based hand gels in TTSH in 2014, 4 of them were chosen for more extensive evaluation in April-June 2015.

Methods: 4 inpatient wards and 2 outpatient occupational and physiotherapy clinics were chosen based on their alcohol-based handrub utilization in 2014. During the study period, 1 of the 4 hand gels were placed in wards and clinics at the beginning of each week, and existing handrubs removed during the trial period. All healthcare workers did handwashing or handrub according to WHO 5 Moments, as per hospital policy. After each week of trial, there was a ""wash-out"" period of a week when hand gels were replaced with existing handrubs. Healthcare workers were invited to participate in a survey on nine attributes of product using a Likert-scale of 1-5 during trial period. The mean score was computed for each attribute and student's t-test used to compare the differences. Staff were told to report any adverse reactions to the study team who monitored and recorded skin reactions during the trial. Results: Of 1056 participants, 637 were nurses, 286 were alliedhealth professionals and 133 were doctors. 82.5% of the participants thought that Product C worked better than the current product and was rated the best in overall satisfaction. Comparison of two means for Product C and current handrub on eight attributes were significant, for example-splashing (mean score of 4.23 vs. 2.46, p < 0.001).

Conclusion: Product C was the most preferred hand gel and should be considered as alternative to current hand hygiene product. Phase 3. Model sharing phase (July 2015 to September 2016): we hold experience sharing conference and regular meeting to help eight hospitals in southern Taiwan to improve CAUTI. Results: In phase 1, CAUTI rate declined from 12.6‰ to 6.9‰ in three ICUs. In phase 2, CAUTI rate reduced from 6.3‰ to 5.6‰ in six ICUs. In phase 3, total 9 hospitals overall CAUTI rate improved from 3.7‰ to 2.89‰. Conclusion: Multi-modal interventions with nursing-driven base are essential in reduced CAUTI and point extension make successful experiences to duplicate in other units and other hospitals.

The epidemiology and predictors for mortality of bloodstream infections among patients with temporary hemodialysis catheters Meng-Ching Chen 1 , Yin-Yin Chen 1,3 , Fu-Der Wang 1,2,3 . 1 Infection Control, 2 Division of Infectious Diseases, Department of Medicine, Taipei Veterans General Hospital, Taipei, Taiwan, 3 National Yang-Ming University, Taipei, Taiwan Background: Bloodstream infections (BSIs) are common in patients with temporary hemodialysis catheters, and result in prolongation of hospitalization, increasing medical expenses, and increased risk of mortality. This study aims to explore the profile, recognized pathogens and predicted prognostic factors for mortality of bloodstream infections. Methods: The retrospective study was performed in a medical center from 2010 to 2015. Epidemiologic investigations were conducted. Patients with temporary hemodialysis catheters and reported as BSIs were inclueded except individuals under the age of 20. The chi-square test was used for categorical variables, the Mann-Whitney U test was used for data that had nonnormal distribution, and the logistic regression analysis was used for assessing predicted prognostic factors for mortality. Results: A total of 309 patients with BSIs were included in the study period. These patients were predominantly male (66.2%), and had a mean age of 72 ± 16 years. The median of length of stay were 24 days. The most common isolated organism were Staphylococcus aureus, Acinetobacter baumannii, and Klebsiella pneumoniae. The most common isolated antimicrobial-resistant pathogens were methicillin-resistant Coagulase-negative staphylococcus (75%), Carbapenem-resistant Acinetobacter baumannii (64.6%) and methicillin-resistant Staphylococcus aureus (63.1%). The crude rates of in-hospital mortality rate was 55.1%, and the 30day mortality rate was 78.3% in all deaths. The logistic regression analysis identified shock (odds ratio [OR], 14.3; 95% confidence interval [CI], bedridden (OR, 2.3; CI, ), using of continuous veno-venous hemofiltration (CVVH, OR, 15.8; CI, antibiotics (OR, 2.5; CI, , and mechanism ventilator (OR, 2.2; CI, 1.04-4.61) as independent predictors for mortality. Conclusion: The most common isolated organism of BSIs was Staphylococcus aureus in patients with temporary hemodialysis catheters. Shock, bedridden, using of CVVH, antibiotics, and mechanical ventilator are independent predicted prognostic factors for mortality.

Use of VAP bundle to prevent ventilator associated pneumonia in a medical center intensive care unit Ya-Ching Huang, Chun-Hsing Liao, Mei-ling Fang, Ching-jung Tsai, Pei-chi Tu, Shu-chuan Huang. Far Eastern Memorial Hospital Background: Mechanical intubation is an essential part for critical patients. Ventilator associated pneumonia (VAP) is an important complication of intubation, which will increase hospitalization cost, prolonged hospital stay and potentially threaten patient's life. Bundle intervention is proposed to effectively decrease the incidence of VAP. In order to reduce the occurrence of VAP, a VAP care bundle care was implemented in an intensive care unit since 2015.

Methods: This intensive care unit provides care for medical cases mainly and has 30 beds. The VAP bundle includes (1) Education to enhance the VAP bundle care awareness. (2) Background: Many critically ill patients can survive by use of mechanical ventilation. However, these patients have serious clinical condition, and are immune-compromised status, many health care associated infections like ventilator-associated pneumonia (VAP) will be the major complication. VAP is defined as pneumonia after 48 hours use of ventilator mainly due to bacterial infection. According to the US nosocomial infection surveillance system data, the incidence of VAP is 5-16‰ which is also an important indicator of quality control in a critical care unit. The incidence of VAP is related to length of hospital stay, duration of mechanical ventilation. The incidence will be increased by 1% in every one day of hospital stay, peak incidence occurred on the seventh day, average 5-10 days of ventilator use. Thus, if ventilator is used for longer duration, there is increase in patient' hospital stay and mortality. In our hospital, incidence VAP in intensive care unit is 0.69-1.72‰ from 2014 to 2016 which is expected to achieve zero tolerance in coming year. However, there are many new health care personnel in these year, multiple health care measures are not fulfilled for our target. In 2017, Central Disease Control department implements the program of bundle care for invasive health care procedure. Fortunately, we can participate in this program, and strengthen to prevent VAP with application of bundle care. Methods: We have applied the health care personnel training for basic knowledge of bundle care. Since February 2017, we implement VAP bundle care which include (1) elevate the head of bed for 30-45°, (2) oral cleansing by using tooth cleaning sponge, small tooth brush, and chlorhexidine contained mouthwash, (3) setting of cleaning and sterilization of steam inhalation therapy equipment and changing the tubing regularly, (4) addition of setting bottle to facilitate the removal of fluid inside the ventilator circuits under avoiding contamination of environment. These standard infection control measures are added to intensive care unit manual and keep health care personnel to follow.

Results: Follow up from February 2017 to June 2017, there was no VAP cases during this period. We continue the health care providers education and training, and apply bundle care for VAP, thus the incidence VAP will effectively be reduced. Conclusion: Evidence based strategy for bundle care seems to be simple, but how to develop, promote, and implement the strategy and counter checking of these measures are more significant. Implementation of VAP bundle care will prevent and reduce VAP incidence, decrease the duration of ventilator use and the average length of hospital stay, and decrease the patient morbidity and mortality.

An experience of reduce the catheter-related urinary tract infection in a medical center Huei-Wen Lai 1 , I-Lun Hou 1 , Yu-Lin Lee 2 , Tsun-Jung Liu 1,2 . 1. the doctor's orders for Foley insertion, change to standing order instead stat order. 2. After catheterization for 48 hours, the system at 11:00 was reminded doctor to remove foley every day. 3. The skin disinfection for foley insertion use 2% aquae CHG. 4. Change the size of the catheterrecommended in addition to urine sediment, hematuria, after surgery, BPH, etc. recommended the use of 16Fr or other size, the rest of the situation can be considered using 14 Fr. 5. Expand the aseptic area and pre-connected catheter and urine bag to prevention comtamination. 6. Use bladder scan to avoid repeat foley insertion. 7. Training specialists check the bundle care of foley insertion and daily care.

Catheter-related urinary tract infection in 2015.01∼2016.03 for 5.95‰, after intervention in 2016.04∼ 2017.03 reduce to 3.61‰ (P-value = 0.0001). Conclusion: The Infection Control Committee has identified effective measures in the Infection Control Manual and will reduce the catheter-related urinary tract infection as a continuous monitoring quality indicator.

The survey for the effectiveness of VAP (ventilator-associated pneumonia) prevention after implementing care bundle in the ICU of a regional hospital in Taiwan Background: The ICU within 17 beds belongs to general adult intensive care unit. It started to implement care bundle for VAP prevention from June, 2015. To realize the effectiveness, we analyze the densities of infection and the use rates of ventilators during the two periods of pre-intervention (Nov. 2013 -May 2015 and postintervention (Jun. 2015 -Dec. 2016 . The result can be as a reference to continuously improve the quality of medical care. Methods: After infection control practitioners collected cases based on surveillance definition of VAP formulated by CDC, the cases have to be reconfirmed by the minister of Critical Care Medicine. Following the interventions proposed by CDC and JCT, 5 items were lifting the head of bed 30-45°higher, ceasing sedative daily, daily evaluation of extubation, draining the water in ventilators to empty, and performing oral nursing care with mouthwash containing 0.12-0.2% chlorhexidine at least 2 times per day and then keeping records in the check list. Results: Statistics show the densities of infection during the periods of pre-intervention and post-intervention respectively were 1.38‰ (6/4350) and 1.71‰ (8/4666). The use rates of ventilators were 51.08% and 54.29%. Conclusion: The density of infection increased by 0.33‰ in 19 months of post-intervention stage. The value of Chi-squared Test was p > 0.05 (=0.686). The use rate of ventilator increased by 3.21%. The rate (54.29%) was higher than the rate (51.2%) in the equivalent ICU which identified by TNIS, Taiwan. By further analyzing the cases, it can be found no more than 3 cases per season and they individually occurred in every different month. Zero infection maintained in the latest season shows that if daily evaluation of extubation can be put into effect, it is definitely able to improve the quality of medical care.

Exploring the effectiveness of the urinary catheter care bundle system Shaw-Shin Wu 1 , Hui-mei Huang 2 *, Wei-Pin Lai 3 . 1 Taichung Veterans General Hospital, Registered Nurse, Taichung, Taiwan, 2 Taichung Veterans General Hospital, Supervisor, Taichung, Taiwan, 3 Taichung Veterans General Hospital, IT Engineer, Taichung, Taiwan

Background: To establish an information system and online catheter removal assessment tool for urinary catheters bundle care. To Improve bundle care compliance, and thus reduce the urinary catheter indwelling days, usage and infection density. Methods: Patients of a comprehensive ward of a medical centre were selected as study object by purposively sampling. Study period was from January 2016 to April 2017. The program designer set up an online catheter removal assessment tool, real-time query system, and mobile APP program. The mobile APP program includes a placement check and a daily care evaluation. Results: During the study period, 913 patients had urinary catheter, with a total of 3920 days of the catheter indwelling. After the program has been set up, catheter placement check was done for 168 patients during catheter insertion, the compliance rate of utilizing the program was 100%; and a total of 2833 staff preformed urine catheter daily assessment using the program, the compliance rate increased from 29.4% to 60%. The average length of catheter indwelling decreased from 4.40 days to 3.94 days. Catheter utilization rate decreased from 25.34% to 21.63%. Catheter-related urinary tract infection density decreased from 2.76 ‰ to 1.19 ‰. And the satisfaction rate after informatization is 100%. Conclusion: When the system was first implanted, the initial compliance rate was only 29.4%. The reason was that physicians did not complete the daily assessment of catheter removal in the program. By adding an evaluation path and a reminder window to the physician's ordering system, the compliance rate increased to 60%. Through combining the implantation of informatization, online real-time inquire system, mobile devices and user-centered program design to achieve the purpose of simplifying the process, increasing the efficiency of data processing and integrating, and enhancing compliance. As a result, significantly improved the compliance rate of care bundle and user satisfaction, reduce the use of catheter, the length of indwelling and infection density.

Effectiveness of ventilator-associated pneumonia bundle intervention in the regional hospital Yu-Hsiu Lin 1 *, Siao-Pei Guo 1 , Pak-On Leung 2 , Mei-Fe Sie 3 , Yee-Huang Ku 1 , Jyh-Jou Chen 4 Background: Nebulizer devices are widely used to deliver aerosol therapy, especially in pediatric patients with respiratory disease. However, nebulizers are potential sources of microbial contamination of the respiratory tract. Small-volume nebulizers medications for administering bronchodilators, including hand-held nebulizers, can produce bacterial aerosols. Ultrasonic hand-held nebulizers have been associated with nosocomial pneumonia Methods: In order to understand the suitability and influence of the ultrasonic nebulizers used in treatment of respiratory diseases of paediatric patients, we conducted a systematic literature review of the articles published before April 2017. Keywords in both Chinese and English were entered into the search engine which comprised of 5 medical database. We then selected 5 articles which match our standard for further review. Results: After 24 hours of usage, the Burkholderia cepacia complex (Bcc) can be isolated from ultrasonic nebulizers. Bcc isolates were obtained from nebulizer draining tubes, operating water chambers, oscillator and nebulizer solutions. The Evidence-based medicine literature suggests that appropriate maintenance of ultrasonic nebulizers is crucial for preventing Bcc contamination of nebulizers and subsequent respiratory tract and blood infections. New contamination control measures were implemented as follows:

(1) Availability of ultrasonic nebulizers was strictly limited to <5 d. At that time, the devices were returned to the medical engineering section for maintenance. (2) Nebulizer draining tubes and oscillators were completely disinfected once every 24 h using 85% ethanol; and (3) After each use, nebulizers were surveyed for diaphragm breakage or pinholes using a device that measured electrical resistance. In addition to that, observation of children breathing patterns, to avoid cold and high and dense fog caused by respiratory spasm problems.

Conclusion: The literature evidence shows that the use of ultrasound nebulizers does have the risk of nosocomial respiratory infections. Moreover, additional maintenance procedures are required for instrument maintenance, so it is recommended to use a single use and disposable jet nebulizers. In regards of patient's safety, cost consideration and therapeutic effectiveness, it is recommended to implement the cleaning and disinfection procedures. In order to make the best treatment of spray therapy, the right equipment such as mouth tube or mask, should be chosen to fit children's age and cognitive ability.

The effectiveness of deploying care bundle for the precaution of catheter-associated urinary tract infection in the ward of orthopedics belonging to a regional hospital located in southern Taiwan Li-Chun Liu 1 , Li-kuan Chih 1 , Chih-Lin Liu 1 , Su-Chen Liu 2 , Chih-Hsuan Cheng 3 , Wen-Han Chang 1 , Ying-Sheng Pan 1 . 1 Infection Control Committee, Tainan Municipal Hospital, Taiwan, 2 Nursing Department, Tainan Municipal Hospital, Taiwan, 3 Orthopedics, Tainan Municipal Hospital, Taiwan

Background: The hospital is with 516 beds for acute patients while 48 beds laid in the ward of orthopedics. The beds are applied for placing and curing the patients from the Departments of Orthopedics, neurosurgery, and Urology. The density of Urinary Tract Infection in the ward in 2014 occupied the 3 rd place from the ranking of all wards in the hospital. It became the only ward which participated in the measure, Care Bundle for Catheter-Associated Urinary Tract Infection, proposed by CDC and JCT on June 1 st , 2015 and started to collect the number of patients using catheter per day. To realize the effectiveness, this sheet is discussed in depth. Methods: The cases of Catheter-Associated Urinary Tract Infection were collected based on the surveillance definition of Healthcare-Associated Infection, which is the second edition revised by CDC in 2009. Following the measure proposed by CDC and JCT, 4 items placed for inspection were executing hand hygiene before inserting catheter, using aseptic technique and sterile when inserting catheter, catheter in proper fixed ways, and executing hand hygiene immediately after inserting catheter. The 5 measures of daily care evaluation to take precaution against Catheter-Associated Urinary Tract Infection were evaluation of extubation, hand hygiene, fixed position, aseptic and fluent, and urethral orifice clean.

Results: Statistics show that the density was 3.27‰ during the first half year (Jun.-Nov. 2015) after deploying Care Bundle while those were 4.55‰ and 2.63‰ during the second half year (Dec. 2015 -May 2016 and the third half year (Jun.-Nov. 2016 ). The use rates were respectively 15.88%, 17.496%, and 18.96%. Conclusion: Comparing the measures deployed after one and a half year with those after the first half, the density of infection obviously declined by 0.64%. The value of Fisher's Exact Test, P > 0.05(=), shows deploying Care Bundle can lower down the density but there is still room for improvement. The rate of using catheter rises by 3.08%. The possible reason can be related to the specific patients being received and cured. The above information provided to the medical profession as a reference.

Investigation and treatment of abnormal increase of bloodstream infection in peripheral blood stem cell transplantation Hsiao-Shan Chen 1 , Hung-Jen Tang 1,2,3 , Yin-Hsun Feng 4 , Ya-Ling Hsien 4 , Chin-Yi Li 4 , Iig-Ling Chen 1 *. Background: Ventilator-associated pneumonia (VAP) is a common healthcare-associated infection in intensive care units, which can cause patients to have severe morbidity and mortality. In Jan. 2013, we executed the ventilator care bundle (VCB) to reduce VAP in our hospital, including daily assessment of readiness to extubation, daily sedation vacation, elevation of patient's head of bed to 30-45°, using 0.12% chlorhexidine for oral care, and exhausting water in ventilator circuit every two hours. In 2015, the VAP incidence reduced to 2.45‰ in our surgical intensive care unit (SICU), but it still higher than those in the SICUs of other teaching hospitals (1.26‰). In order to further reduce VAP incidence in our SICU, we used quality control circle to reinforce the efficacy of VCB. Methods: Our hospital is a 960-bed regional teaching hospital with a 16-bed SICU. We used quality control circle to find 3 disadvantages in our VCB program, including poor cognition of VCB in SICU members, a lack of internal auditing to carry through VCB, misarrangement of suction tubes and aerosol therapy apparatus, and inaccurate angle of elevation of bed head. So, we executed an improvement plan, including regular training programs for VCB in SICU, development of the auditing system, adequate storage of suction tubes and aerosol therapy apparatus, and adding angle scale on bedside.

Results: After this intervention, the VAP incidence in our SICU reduced to 0.36‰ from 2.45‰. The accuracy of VCB increased to 95.2% from 65.1%. Conclusion: Quality control circle is a good way to reinforce the efficacy of VCB.

To explore the effectiveness of CAUTI Bundle care in a southern regional hospital Yan Ru Chen*, Yee-Huang Ku. Infection Control Center, Chi Mei Medical Center, Liouying

Background: Urinary tract infections (UTIs) are one of the most common medical care-related infections, accounting for about 40%, of which about 70% is associated with the use of the catheter, and the hospital is involved in the Department of Health and Welfare Medical treatment care quality improvement program to promote the catheter-related urinary tract combination of care measures, through the clinical field audit, found the problem, the feedback unit to improve, to reduce the urinary tract-related urinary tract infection, improve medical quality, protection Patient safe. Methods: The internal control of the catheter is carried out by the internal care manager, the external control of the infection control nurses, the monthly audit statistics and feedback, and the ""male catheterization"" competition will be held, and the ""This technology is standardized, the actual application in clinical practice'"".

Results: Before the promotion of the program (from January to June in 2015), the general hospital CAUTI density of 0.7‰, intensive care unit of 1.9‰, in July 2015 to start the field audit, catheter placement and care compliance and CAUTI infection density Table 1 shows that although the audit compliance rate increased, but the intensive care unit CAUTI density increased by 38.7%, the general ward CAUTI density is flat before the plan to promote.

In January and March of 2016, the compliance rate of Nurse Practitioner perform male patient catheterization was only 25%. In April, the ""male catheterization"" competition was held in the course of internal medicine, surgery and The rate of compliance was 96. 88% (93/96), and the compliance rate was increased to 75% from April to August in 2016. The compliance rate was 96%.

Conclusion: Leading to catheter-associated urinary tract infection may be a large number of factors, as soon as possible to remove the catheter, aseptic technique operation and daily care is necessary to perform a ring, through the audit, regular feedback, into the unit staff in-service education and evaluation indicators, and Continuous tracking, period can effectively reduce the catheterassociated urinary tract infection density.

The application of diversified strategy to improve quality of catheter care Yu-Mei Ho 1 *, Wen-Yin Huang 2 , Wen-Jen Yau 2 , Tsung-Yen Yang 1 , Po-Wen Yang 3 , Nan-Ping Yang 4 The key elements of CAUTIs bundles consisted of hand hygiene, wearing personal protective equipment, use of disposable gloves, cleansing of urethral meatus prior to catheter insertion using sterile saline, daily assessment of catheter need, aseptic urine sampling technique, making sure that draining bag and catheter were at right position. The multidisciplinary team was led by deputy superintendent; regular educational programs and committee were held. Incidences of CAUTIs and costs were compared between pre-and post-intervention phases.

Results: There were 30 and 9 patients suffered from CAUTIs during pre-and post-intervention phases. The incidence rates of CAUTIs were 2.87 and 0.86 per 1000 catheter-days during pre-and postintervention phases, respectively ( p = 0.02 1. Establishment of the standard process of catheter care: The education and training of the standard catheter placement process and the daily care process, besides, keep monitoring the SOP, and production of catheter flow chart. 2. Remind early assessment of the possibility of removal of the catheter: remind the card of catheterization in bed-side, remind the days of catheterization in chart, and remind the days of catheterization in HIS system. 3. The implementation of the standard catheter care: To design new catheter fixed stickers,out of bed patients to use convenience pants which could hide urine bag, 3 M hooks affixed to keep urine bag smoothly. Placed ruby-red tape on the 1000 mL of the urine bag, checked the drainage system to maintain closed, and to avoid distorting or compressing after changed position.

Results: After the implementation of intervention measures, we rechecked from September 1 to 30, the uncompleted care of indwelling urinary catheters rate of 13.66%. the indwelling catheter associated urinary tract infection rate was 0‰, the average number of days of indwelling catheter dropped significantly to 7.8 days in January to August. Conclusion: In our unit, the indwelling catheter associated urinary tract infection rate was 3.35‰ last year. we checked, and did problem analysis, found out the causes of problem. With creative strategies to intervene, which those interventions included in the SOP. So that, the indwelling catheter associated urinary tract infection rate decreased to 0‰. Through transdiscipline, enhance the care of the implementation of infection control.

Interprofessional collaborative practice to reduce the incidence of ventilator associated pneumonia and the days of mechanical ventilation used in the intensive care unit Wan-Er Jang, Hsin-Yu Cheng, Yu Lin. Taipei Hospital, Ministry of Health and Welfare of Jang, Wan-Er

Background: Ventilator associated pneumonia is a common hospital infection in critically ill patients, but also an important indicator of healthcare quality. In 2017, the infection rate of ventilator associated pneumonia was increased, so the establishment of this project, Interprofessional collaborative practice to reduce the incidence of ventilator associated pneumonia and the days of mechanical ventilation used in the intensive care unit, thereby reducing mortality, hospital days and health care costs Methods: Further research with a larger cohort study could help to clarify these characteristics.

The impact of bundle care on preventing healthcare-associated infections in an intensive care unit at a local hospital in Taiwan bundle care on preventing HAIs in an intensive care unit (ICU) at a local hospital in Taiwan.

Methods: A quality-improvement intervention, including education, VAP bundle, central venous catheter (CVC) insertion bundle, CAUTI bundle, process and outcome surveillance, have been sequentially introduced since 2013. During the intervention period, we implemented a multidisciplinary care bundle consisted of (1) hand hygiene, (2) use of maximum barrier precautions, (3) use of 2% chlorhexidine scrub for the insertion site, (4) head-of-bed elevation 30°-45°, (5) education of providers, (6) a dedicated line cart, (7) checklist, (8) oral care with chlorhexidine solution twice daily, and (9) daily consideration of the need for the catheter and so many measures. The incidence rates of HAIs before and after bundle intervention were measured and compared.

Results: The incidence rates of overall HAIs were 7.57‰ and 1.82‰ before and after bundle intervention. There was no episode of VAP recorded during the study period and the rate of ventilator use in ICU significantly decline from 60.89% to 42.96%. The incidence rates were 5.71‰ and 1.27‰ for BSI and 2.24‰ and 0.00‰ for CAUTI before and after bundle intervention, respectively. Background: The purpose of this study was to compare the effectiveness of chlorhexidine gluconate (CHG) oral rinse and normal saline in reducing the number of oral bacteria and incidence rate of ventilator-associated pneumonia (VAP), in order to establish a standard procedure for clinical practice.

Methods: A double-blind randomized clinical trial was conducted in an intensive care unit(ICU) of a district hospital. The patients were randomly assigned into the experimental group (n = 22) received 0.2% CHG and the control group (n = 24) received normal saline for mouth care. Before the study initiated, education training was carried out in the ICU for the nursing staffs on oral care and oropharyngeal swab sampling. Chi-square and t-test were used to analyze the differences between the two groups. The survival time was analyzed to show whether there was a difference in the occurrence of VAP.

Results: There were no significant differences in age, sex, types of artificial airway and the days of artificial airway use in the two groups. There were significant differences between colony growth (X 2 = 12.458, p < 0.001) and abnormal body temperature (X 2 = 5.123, p = 0.024). The incidence of VAP in the experimental and control groups was 1 (4.5%) and 4 (16.7%) (X 2 = 1.741, p = 0.187), respectively. The incidence of VAP in the experimental and control groups was 1 (4.5%) and 4 (16.7%) (X 2 = 1.741, p = 0.187), respectively. The survival analysis showed that the mean duration of VAP was 11 days in the experimental group and 5 days for the control group.

Conclusion: In comparison with the effectiveness of normal saline for oral care of patients with artificial airways and ventilator, the use of 0.2% CHG can significantly reduce the oral bacteria growth, reduce abnormal rate of temperature, and delay the time of VAP occurrence. It is recommended that future studies may be necessary to increase the sample size, to determine the effectiveness of 0.2% CHG in reducing the incidence of VAP. The findings suggest that 0.2% CHG can be used for oral care to reduce patient oral bacteria and delay the time of VAP occurrence.

Coexistence of MCR-1 and NDM-9 in a clinical carbapenemresistant Escherichia coli isolate Chih-Cheng Lai 1 , Chi-Chung Chen 2,3 , Yin-Ching Chuang 2,4 *, Hung-Jen Tang 5,6 *. Background: Since New Delhi metallo-β-lactamase-1 (NDM-1) was first detected, several variants have been further identified that differ by one or two amino acid substitutions. In 2014, a novel NDM-9 metallo-β-lactamase was first identified from a sequence type 107 (ST107) Klebsiella pneumoniae strain. Like the other NDM variants, NDM-9 confers high levels of resistance and compromises the effectiveness of most antibiotics, including carbapenems. Although polymyxin B and colistin ( polymyxin E) offer limited treatment options for multidrug-resistant (MDR) organisms carrying NDM-9, the emergence of the plasmid-mediated colistin resistance gene mcr-1 (for mobilised colistin resistance gene) conferring resistance to colistin ( polymyxin E) may attenuate the utility of colistin. Methods: Antimicrobial susceptibility testing of the clinical isolate was tested by the agar dilution method. Genetic backgrounds were detected including multilocus sequence typing (MLST), drug resistant gene by PCR and sequencing. Results: The 79-year-old female patient had a history of colon cancer and chronic kidney disease and had undergone a radical left hemicolectomy 1 month prior to the current hospital admission. She was admitted to intensive care unit for peritonitis with septic shock. Her clinical condition and haemodynamic status gradually improved following antibiotic treatment with piperacillin/tazobactam (TZP) for 7 days. The results of laboratory examinations were as follows: white blood cell count, 22,100/ mm 3 ; platelet count, 191,000/mm 3 ; creatinine, 4.89 mg/dL; aspartate aminotransferase, 10 IU/L; and C-reactive protein, 43.7 mg/L (reference value, <5 mg/L). Despite TZP being resumed for UTI complicated by sepsis syndrome, her clinical condition rapidly deteriorated and the patient's family requested supportive care. Finally, the patient died on the 11th day of hospitalization. Meanwhile, the urinary catheter specimen grew carbapenem-resistant E. coli. Molecular diagnosis was identified the E coli isolate carrying MCR-1 and NDM-9. Conclusion: The present case indicates the possibility of coproduction of MCR-1 and NDM-9 among clinical isolates and indicates the importance of surveillance of colistin-and carbapenem-resistant Enterobacteriaceae isolates and associated genes in humans, with special reference to MCR-1 and NDM-9.

Background: This study was intended to investigate the impact of implementation of catheter-associated urinary tract infection (CA-UTI) bundle care on the incidence of CA-UTI in high-risk units in Taiwan. Methods: Thirteen high-risk units, including medical (n = 5), surgical (n = 3), cardiac intensive care units (n = 2), respiratory care centers (n = 2), and respiratory care ward (n = 1) were included in this quality-improvement project. The stud period was divided into the pre-intervention phase (between January 1, 2013 and July 31, 2013) and post-intervention phase (between August 1, 2013 and October 31, 2013). Results: The incidence of CA-UTI decreased from 3.86 to 2.98 per 1,000 catheter-days (95% CI, 0.65-0.82; p < 0.0001) before and after the introduction of the CA-UTI bundle. Among 66 episodes of culture-proven CA-UTIs, Candida spp. were the most common pathogens (n = 17, 25.8%). For the seven elements of the insertion bundle, the compliance was the lowest for cleaning of the perineum, followed by hand hygiene. The overall compliance rates of the insertion bundle were 93.4%, 99.5%, and 96.3% in medical centers, regional hospitals, and district hospital, respectively. For the six elements of the maintenance bundle, the compliance was the lowest for daily review of the need of a Foley catheter. The overall compliance rates of the maintenance bundle were 95.7%, 99.9%, and 99.9% in medical centers, regional hospitals, and district hospital, respectively. Conclusion: The implementation of CA-UTI bundle care reduced CA-UTI. A process surveillance checklist can be helpful for understanding which parts of the bundle care require improvements.

The role of doxycycline in the therapy of multidrug-resistant E. colian in vitro study Chih-Cheng Lai 1 , Chi-Chung Chen 2,3 , Yin-Ching Chuang 2,4 *, Hung-Jen Tang 5,6 *. Background: This study assessed the in vitro antibacterial activity of combinations of amikacin and doxycycline or tigecycline against multidrug-resistant E. coli isolates. Methods: Twenty-four different pulsotypes, including 10 extended-spectrum β-lactamase (ESBL)-, 10 carbapenem-resistant, 2 New Delhi Metallo-beta-lactamase (NDM)-and 2 Klebsiella pneumoniae carbapenemase (KPC)-E. coli isolates were collected. Combination effect was detected by two methods, including the checkerboard, time-killing. Results: All 24 isolates were susceptible to amikacin and tigecycline. Only 30% of ESBL and 50% of carbapenem-resistant E. coli were susceptible to doxycycline. Both of the NDM-E. coli had a MIC of 64 μg/ml. The checkerboard method showed that for the ESBL-and carbapenem-resistant E. coli, the synergistic effects of amikacin/doxycycline were 80% and 90%, respectively. For the two KPC-and two NDM-E. coli, the FIC index of amikacin/doxycycline were 0.5/0.375 and 0.5/0.281, respectively. For the ESBL-and carbapenem-resistant E. coli isolates, the combinations of amikacin and doxycycline exhibited synergistic activities against 80%, and 80% and 10% vs 60%, and 80% and 10% of the isolates at concentrations of 1x, 1/2x and 1/4xMIC, respectively. The synergistic effect seems to be similar for doxycycline and tigecycline based combinations with amikacin. Conclusion: The antibacterial activity of doxycycline can be enhanced by the addition of amikacin and is observed against most multidrug-resistant E. coli isolates.

The impact of 30-day mortality with central venous catheter associated bloodstream infections in a medical center Shu Mei Sun 1 , Yin Yin Chen 1,2 , Fu Der Wang 1,2,3 . 1 Infection Control, Taipei Veterans General Hospital, 2 National Yang-Ming University, 3 Medical Department, Taipei Veterans General Hospital Background: Central venous catheters are significant risk factors for healthcare associated bloodstream infection, and often increased mortality risk. Methods: A retrospectively surveillance study was conducted in a nearly 3000-bed medical center in taiwan. All hospitalized patients were older than 20-year-old and developed healthcare associated bloodstream infection admitted after 48hrs during 2013-2014 were included. We only included the first episode of bloodstream infection,if patients who had experienced more than one episode of healthcare associated bloodstream infection. Univariate analysis using the Chi-square test or t-test or Mann-Whitney U test to identify significant risk factors of CABSI. And finaly the logistic regression was used to explore the impact and prognostic factors of 30-day mortality with CABSI. Results: A total of 1538 healthcare associated bloodstream infections were enrolled in this study, and 36.2% of these were central venous catheters associated. Patients who experienced CABSI were high mortality rate than non-CABSI group (44.8% vs. 27.1%, p < 0.001) as well as 30-day mortality (34.7% vs. 20.8%, p < 0.001) in the univariate analysis result. After controlling for potentially confounding factors by use of logistic regression modeling analysis, we identified CABSI was the independent risk factors for 30-day mortality (OR, 1.57; 95% confidence interval [CI] 1.18-2.10, p = 0.02). Conclusion: Our result demonstrated central venous catheter associated bloodstream infections led a higher mortality in a medical center.

Removing catheters to reduce usage of urinary catheterization and urinary tract infections in hospitalized patients Wan Tsuei Huang 1 , Yin Yin Chen 1,2 , Fu Der Wang 1,2,3 . 1 Infection Control, Taipei Veterans General Hospital, 2 National Yang-Ming University, 3 

Background: This study applies ""removing catheters"" to evaluate the effect of evaluation measures on the rate of catheter-associated urinary tract infections, catheterization days, and urinary catheter use. Methods: A Prospective follow-up study was performed in medical center. we approach all indwelling urinary catheters presence for more than 24 hours during Jul 2014 to Jun 2015. Since July 1, 2014, intervention in catheter catheterization of urinary catheterization evaluation measures to observe the use of catheter and catheterrelated urinary tract infection rate changes. Results: A total of 4,677 patients were recruited. Utilization rate of indwelling urinary catheters from 33.91% before the intervention decreased significantly to 30% ( p = 0.032). The duration of catheterization was increased from 5.1 days to 6.9 days ( p = 0.001). The average rate of catheter-associated urinary tract infections is 4.42‰ ( p = 0.826), The chi-square trend test had no significant difference. Conclusion: Evaluation of urinary catheter extraction provided physicians or nurses to assess and remove of the catheter as soon as possible. There are many factors that affect CAUTI, It is recommended that the future continue to promote multi-faceted measures such as through teamwork to reduce unnecessary catheter indwelling; and to more attention to bladder function assessment, Continuing to reduce catheter use and resuscitation rates, and hope to be able to reduce CAUTI finally.

Regional hospital using bundle care to lower ventilatorassociated pneumonia infection in ICU Hsiao-Pei Kuo*, Yee-Huang Ku. Infection Control Center, Chi mei Medical Center, Liouying, Tainan, Taiwan Background: Respiratory-related pneumonia not only increases patient mortality, prolonged hospital days and medical expenses, more seriously affect the patient's prognosis. Intervention of respirator-associated pneumonia combined with compliance with the rate of compliance audits, can effectively reduce the respiratory rate of pneumonia associated. Methods: There are five adult intensive care units in a district hospital in the south, and a combination of respirator-associated pneumonia is based on the 2015-2016 Intrusive Care Disposal Quality Improvement Program. July-December 2015 for the first phase after implementation, January-June 2016 for the second phase after implementation, July to December 2016 for the third stage after implementation. Results: Statistics of the first phase of compliance: daily extubation assessment 100%, 100% stabilization agent, oral care 98%, 87% bedside, 100% of the emptying. Due to bedside elevation compliance is low, by the respiratory therapist and unit care chief to enhance the reminder and feedback. The second stage of the audit effectiveness of bedside elevation compliance rate of 86%, the third stage audit results bedside compliance compliance rate of 92%. Oral care from the first stage compliance rate of 98% to upgrade to the third stage of 99% compliance. Respiratory associated pneumonia infection rate decreased from 2.64‰ before intervention to the third stage 2.12‰. Conclusion: The establishment of regular feedback and audit mechanism is very important, and then through teamwork, can effectively improve the compliance rate, thereby reducing the respiration rate of respiratory-related pneumonia. So the third phase of the hospital compliance rate of up to 91.4%, can reduce the respiration of 24.5% of pneumonia.

Improvement of ventilator-associated pneumonia bundle care through teamwork in an intensive care unit Yu-Hsia Wang*, Li-Hsueh Chen, Nai-Zhen Hou. Department of Nursing, Tainan Municipal Hospital Background: With a mortality rate approaching 35-90%, ventilator-associated pneumonia (VAP) accounts for 13-18% of health care-associated infections (HCAIs). Patients receiving mechanically assisted ventilation have more risks for acquiring pneumonia compared with patients not receiving ventilatory support and the risk for acquiring pneumonia increased 1% per day. Prevention is better than cure. Therefore, caring for patients on mechanical ventilators is especially important. It's expected that implementation care bundles improved HCAIs rates and ensure high care quality. Methods: Care bundles including care of ventilator circuit, daily assessment of sedation, oral hygiene, head of the bed elevation to 30-45°, intermittent subglottic secretion drainage, and drain from ventilator circuit was implemented from January, 2015. The incidence of VAP was higher than other hospitals during initial implementation stage and the causes were lack of knowledge of care bundle strategies, low implementation rate, lack of unified mouthwash ingredients, lack of standards and auditing practice, low correction rate (50%) in care practice. For strengthening implementation of infection control measures, the changes in institutions, management, and human behavior were needed, hence the measures included education programs, evidence-based research, innovative approach, VAP guideline and audits through multidisciplinary team work. Results: Care quality improved. In addition, staff's knowledge about VAP increased from 70% to 100% and accuracy and implementation of care achieving 100% via audit. The analysis of control chart for monitoring rate of VAP indicated that the process was under control with dramatic improvement by comparison with other hospitals. The other benefits included higher rate of ventilator weaning and lowered infection rate. Conclusion: The use of teamwork builds safety culture and contributes to further promote compliance with care bundles. The changes in institute, management and staff's behavior not only improve quality of health care but also assure patient safety, and benefits of hospitals, health workers and government.

Reduced the ventilatorassociated pneumonia in the surgical intensive care unit: upgrade the integrated care Pei-Yu Taeng, Fang-Yu Shih. Nursing, Buddhist Tzu Chi Medical Foundation, Taipei Tzu Chi Hospital Background: The incidence of ventilator -associated pneumonia in the surgical intensive care ward in 2015 was 2.22‰, which was higher than that of others(1.26‰). In 2016, integrated care was carried out. However, it was found that was poorly practiced. The program is not familiar with the lack of relevant education and training, in April 2016, the incidence of respiratory-related pneumonia was as high as 6.33‰. Resulting in the nursing care quality has a great challenge. Methods: The intensive care unit physician, nurse and respiratory therapist composed of special committee in July 2016. Using of innovative methods to produce VAP Bundle Care posters. To standardize elevating the bed 30 degrees, modify the oral care and daily Teamwork hand over document. Sustained training on the job to enhance colleagues' realizes the integrated care. Results: After 10-month later, the unit-integrated care increased from 58% to 98%. The improved cognitive test increased from 62 points to 95 points. The ventilator-associated pneumonia rate decreased from 4.16‰ to 2.87‰. Conclusion: The implementation of integrated care in the respiratory cause of the relevant pneumonia, can reduce the infection caused by the ventilator. Also reduce the number of days the patient to use respirators,and shorten the length of patient stay and mortality in the intensive care unit.

The efficacy of care bundle to improve catheter-associated urinary tract infection Si-Yin Lin 1 , Chih-Jen Liu 2 , Chen-Szu Chin 3 *. In addition to formal program, we started a protocol that recruiting patient himself and the family members to participate the care bundle of CAUTI in order to improve the care quality and reduce the rate of CAUTI. Methods: The hospital task group started the program of intervention from July 2105. After primary surveying on 20 staffs on the performance of ""urinary catheterization,"" it revealed that, although the knowledge of CAUTI was 90% correct, the performance was only 55% correct. Since then, we introduced the care bundle programs for urinary catheterization in our medical /surgical wards. The programs are: (1) in-service education for the medical team members, including hand-on teaching and on-line learning; (2) were the independent risk factors for 28-day mortality. We further stratified patients into critically ill and non-critically ill group, and found appropriate antimicrobial therapy was not associated with survival benefit in both groups. Conclusion: Host factors were the predictors for mortality in patients with CRKP bacteriuria. Antimicrobial treatment was not associated with survival benefit in either critically ill or noncritically ill patients. The findings may provide some insight on future antibiotic stewardship interventions.

The influence of changing frequency of patient's urinary drainage bag on the cleanliness and urinary tract infection Chu-Chun Lin 1 , Yin-Yin Chen 2 , Fu-Der Wang 3 *. Background: Indwelled urinary catheter is one of the commonest clinical treatment. There is no evidence based on replacing timing of foley catheter or urinary drainage bag, the range from a week, two weeks to per month in cilinical standard operation procedure. The research motivation comes from our operation is different from the guideline of the US Centers for Disease Control and Prevention. Methods: Observational study between June 1st, 2013 and July 31st, 2014, 107 of patients were allocated to our study. Cases were inpatients with foley over 14 days that were in infection ward of central hospital in Taipei city. The risk factor of the contamination quality of the urinary drainage bag with binary outcomes was analyzed by logistic regression. And we observe two groups with changing urinary drainage bag in 14 days or not. Results: It is found that there are 63 (58.9%) patient's foley were clean and 44 (41.1%) were contaminate in not changing bag group. In changing bag group 29 (78.4%) were clean and 8 (21.6%) were dirty ( p = 0.033). The two groups in the urinary drainage bag of cleanliness there was no significant difference ( p = 0.302).

In silicone material than latex catheter coated silicone coating material has trained more microorganisms ( p = 0.013). Compared to the two groups, the not changing bag group microorganism culture data was over 105cfu, and with 14 patients (13.1%), changing group is 4 patient (10.8%), there is no significant in statistic analysis ( p = 0.486). Logistic regression were use analyzing, we adjusted other variables, not changing group has sheltering effect. However, whethere changing urinary drainage bag or not would be not significant in bacteria culture ( p = 0.717). Conclusion: Patients indwelling catheter for more than 14 days in the group did not change the urinary drainage bag every 14 days the catheter is significantly dirty, but there is no significant difference in the urinary drainage bag and microbial culture with the replacement urinary drainage bag group. Background: Most of the residents in the LTCFs were survivors from acute medical conditions and frequently harbored multi-drug resistant organisms resulting from their previous treatment course. Therefore, infection or colonization with Methicillin-resistant Staphylococcus aureus (MRSA) have become an important issue. This study planned to investigate the antimicrobial agent susceptibility of and the molecular typing of methicillin-resistant Staphylococcus aureus (MRSA) in the residents and environments in six long-term care facilities (LTCFs) in Taiwan. Methods: We prospectively investigated the existence of MDROs, including MRSA, in residents of six LTCFs and their environments from January to December 2016. Active surveillance of colonization of MDROs was performed by culturing nasal and rectal swab samples every three months for the residents. If MSA was identified, then swab specimens from environmental sources were collected and cultured. The minimal inhibitory concentration (MICs) of common anti-microbial agents were tested. The SCCmec genotype and Multilocus sequence typing (MLST) of selected MRSA isolates were further analyzed. Results: A total of 339 isolates of MRSA was identified during the study period. Among them, 135 isolates (39.8%) were identified from LTCF residents. Generally, the isolate numbers from LTCF residents in each surveillance culture correlated with the number of isolates from the environment. The predominant SCCmec genotypes were type IV and V. The predominant MLST types belonged to ST59 (42.3%, 60/152) and ST45 (19.7%, 28/142). The predominant MLST strains varied from facilities to facilities. However, the MLST typing from the environment cultures could correlate with the culture from the residents. The minimal inhibitory concentration (MICs) of 339 isolates of MRSA to various commonly used antibiotics were determined. All isolates were susceptible to vancomycin, nitrofurantoin, quinopristin-dalfoprestine, linezolid, mupirocin, and teicoplanin. Daptomycin (99.7%), rifampicin (97.9%), and fucidic acid (96.8%) also exerted excellent in vitro activities against these 339 isolates of MRSA. However, the MIC values of tetracycline, trimethoprimsulfamethoxazole, and ciprofloxacin were significantly lower in ST59 as compared to ST45. Conclusion: The MRSA widely distributed in some but not all the long-term care facilities in Taiwan. No single clonal expansion was found in different facilities. However, the MLST typing of the environmental stains highly correlated with the colonized strains in LTCF residents. An individualized environmental cleansing policy should be adopted to facilitate the LTCF to reduce the environmental colonization with MRSA. Commonly used antimicrobial agents against hospital-acquired strain of MRSA remained highly active against these isolates from LTCFs.

Fang-Chen Hung, Chun-Hsing Liao. Far Eastern Memorial Hospital Infection control center Background: Oncology patients with central artificial venous catheters have increased risk of bloodstream infection. Up to 70% of the patients in our oncology ward use central venous catheter for chemotherapy and the incidence of bloodstream infection was 5.92‰ in 2014. Due to the high incidence of bloodstream infection at this unit, a series of improvement program was carried out since 2015.

Methods: This unit has 55 beds and an average hospital stay of 11.3 days. A task force was formed by oncologists, nurses, and infection control staffs. A serial of interventions were enforced including (1) use of pre-filled heparin syringe (2) reduction of non-essential use of artificial blood vessels (3) Background: Ventilator-associated pneumonia (VAP) is a frequent hospital acquired infections among intensive care unit patients. The goal of this study was to apply a bundle for ventilatorassociated pneumonia prevention as a process for quality improvement in a single surgical intensive care unit. Methods: It consisted of the implementation of an evidence-based ventilator-associated pneumonia prevention bundle adapted to surgical ICU and using the plan-do-study-act cycle as a strategy for quality improvement. The 7-element care bundle was implemented, including head-of-bed elevation 30°-45°, daily sedation vacation and assessment for extubation, peptic ulcer disease prophylaxis, oral care with chlorhexidine, and prevent tube condensate from entering the airways and maintenance of endotracheal tube cuff pressure at 20-30 mmHg. The bundle compliance and VAP rates were then followed. This study analyses the SICU utilization, ventilator utilization, and VAP incidence between January 2014 and June 2014 to assess the impact of VAP bundle in a clinical setting. Results: A total of 202 SICU patients were analyzed in this study. The mean age was older than control group (63.83 vs. 48.71 years old). The ventilator days were longer than control group (9 vs. 4.8 days). Compared with control group, the patients with VAP has higher incidence of underline with COPD(16.7 vs. 11.2%), higher rate of H2 blocker or PPI use (100% vs. 70%), higher rate of using inotropic agent immediately after ventilator support (33.3% and 18.4%), and higher rate of TPN use (66.7% vs. 13.3%). The mean interval between intubation and diagnosis of VAP was 14.3 days (1-56). The mean interval between intubation and mortality was 36.6 days (9-92). In response to VAP bundle interventions, no difference in SICU utilization ( p = 0.32) between the pre-VAP and post-VAP bundle phases was noted, whereas the ventilator utilization was decreased, from 359 ventilator days to 295 ventilator days ( p = 0.19) monthly; the VAP density had remarkably decreased from 1. 4 Background: There is a worldwide increase in the incidence of Carbapenemase-producing enterobacteriaceae (CPE). CPE contain plasmid-borne genetic materials that are capable of spreading rapidly between bacteria and between individuals, rendering Carbapenems, an important group of broad spectrum antibiotics useless. We detected a significant increase in CPE, including three outbreaks, in a 285-bed convalescence hospital in 2016. Although standard infection control measures like hand hygiene audit and regular environmental decontamination were in place, the CPE incidence increased and rose to 0.34/1,000-patient-bed-days in 2Q2016. Hence, a CPE reduction program was formulated. Methods: The CPE reduction program started in July 2016 and focused on three areas: (1) active surveillance screening: Patients who complained of diarrheal symptoms or stayed more than 7 days with antimicrobial therapy were screened for CPE. This enabled early identification and isolation of CPE carriers; (2) Focused hand hygiene compliance improvement program during napkin care and enteral feeding. Concurrent feedbacks were given to frontlines, and results were reported to senior management on monthly basis. (3) Patient environmental care improvement: environmental cleanliness was further maximized by regular ward decontamination by the central cleansing team and ad hoc cleaning once CRE was detected. Staff cleansing technique was assessed by regular procedural and hand hygiene audit. UV fluorescent marker was also utilized and applied to high touch areas including the pantry sinks to monitor their performance. Further environmental measures were initiated including sink decontamination, revised body fluid disposal procedures, revised handling enteral feeding milk and equipment, and patient utensils.

Results: The incidence of nosocomial CPE dropped by 67% to 0.11/ 1,000-patient-bed-days in 1Q2017 and there were no CPE outbreaks after the launch for our CPE reduction program. Hand hygiene compliance improved from 90.9% to 95.4%: in particular, hand hygiene before enteral feeding was increased from 95.6% to 100%; Hand hygiene for napkin care were >95%. UV fluorescent marker assessment showed percentage of marked environment being effectively cleaned increased from 71% to 83%. Conclusion: Our CPE reduction program controlled the nosocomial spread of CPE in our hospital. Early identification and isolation of CPE carriers, enhanced environmental hygiene and focused hand hygiene are essential elements of our success. Finally, good communication and cooperation between frontlines and infection control team are indispensable for any MDRO reduction programs. Methods: We conducted a retrospective study on 286 patients treated with mechanical ventilation for more than or equal to 2 calendar days from April 2015 to October 2016. We compared mechanical ventilated patients defined as VAE with patients defined as no VAE. We also analyzed risk factors and clinical outcomes of VAE. Statistical analysis was performed with Chisquare test and significant difference was determined by p < 0.05. Results: Among 286 episodes of mechanical ventilation, 18 episodes were complicated with VAE; 6 of ventilator-associated conditions (VAC), 9 of infection-related VAC (IVAC) and 3 of possible VAP (PVAP). Chest Xp findings were as follows; 11 of pleural effusion, 5 of pulmonary edema, 1 of pneumothrax and 1 of no findings. VAE occurred more frequently in medical ICU than in surgical ICU ( p < 0.0001). The length of ICU stay became longer in VAE ( p < 0.0001). The mortality rate in VAE group was significantly higher than no VAE group ( p < 0.0001). There were no significant differences between VAE group and no VAE group on intravenous volume overload and nutritional management. All patients defined as VAE showed hypoalbuminemia (<3.0 g/dL). Conclusion: Since VAE affects patient's QOL and prognosis, It might be also important to improve malnutrition in VAE patients with nutrition support team for preventing VAE.

The excellent infection control policy of zero infection in Staphylococcus aureus bacteremia in hemodialysis roomone teaching hospital experience Han-Chuan Chuang 1 , Jen-Zon Chen 2 , Yuarn-Jang Lee 1 . 1 Division of Infectious Diseases, Department of Internal Medicine, Taipei Medical University Hospital, 2 Department of Infection Control, Taipei Medical University Hospital

Background: The patients under regular hemodialysis has risk of staphylococcus aureus bacteremia. The risk factors included vascular access and co-morbidity. The mortality rate of staphylococcus aureus was high up to 15-50%. S. aureus can cause infective endocarditis, septic arthritis and osteomyelitis after bacteremia and in-hospital mortality of infective endocarditis was 22-66%. This important issue is how to prevent the episode of staphylococcus aureus bacteremia. According to previous studies, staphylococcus aureus nasal carrier would follow staphylococcus aureus bacteremia. Therefore, we wonder to know if decreasing nasal carrier rate of staphylococcus aureus in hemodialysis room would reduce the incidence of staphylococcus aureus bacteremia. Methods: One infection control policy was implemented since May 2016 to Dec 2016 in the hemodialysis room of one 736-beds teaching hospital in Taiwan. We included 319 patients. The policy included two steps: (1) active surveillance of staphylococcus aureus nasal swabs every 3 months and (2) decolonization as staphylococcus aureus carrier. Results: After this policy, the rate of staphylococcus aureus nasal carrier diminished as three sequential screening. The incidence of staphylococcus aureus bacteremia was near zero except one patient with methicillin-sensitive staphylococcus aureus bacteremia. Conclusion: As this study supported that the active surveillance give the good infection control in the patients under regular hemodialysis.

An outbreak of influenza in the psychiatric hospital in Taiwan Yu-Chen Lin*, Kuan-Sheng Chung, Hsiu-Chu Liu, Jui-Sui Wang, Chih-Lung Hsieh, I-Yen Lee, Kuei-Ching Lin. Kaohsiung Municipal Kai-Syuan Psychiatric Hospital Background: The study was investigated the process of influenza transmission and infection control intervention was applied in the psychiatric hospital. In addition, the weakness point of infection control intervention, fomite transmission, the effect and timing of Tamiflu ® (oseltamivir phosphate) used was discussed in this study. Methods: The action research method was used in order to offer the suitable intervention of infection control on time in the psychiatric hospital. The information about the influenza vaccination rate, the way of Tamiflu ® (oseltamivir phosphate) used, symptoms of influenza, result of influenza rapid screening, epidemic curve of disease transmission was collected. Results: The first patient with influenza-like illness (ILI) was reported by the critical intensive care unit on 22nd of Jan, 2016. Six cases with influenza-like illness were reported after one night. An acute ward, a pediatric and adolescent ward was reported an outbreak event within 7 days. 25 patients and 6 health care professionals were suffered from seasonal influenza in the three psychiatric wards. The influenza vaccination rates among health care professional were 97.65% in average. Unfortunately, the seasonal influenza vaccination rates among psychiatric patients were 44.33% in average. Beside of this, morbidity of seasonal influenza was 15.20% in average. Unfortunately, positive influenza rapid screening result was not higher than expectation (36.36%), even though type A influenza virus (H1N1) was detected and quantitated by real-time RTPCR. Tamiflu ® (oseltamivir phosphate) was applied not only for psychiatric patients, but also for health care professionals. As a result, dosing rate of Tamiflu ® (oseltamivir phosphate) was 82.35% in average. The average days of isolation were 8.7 days in three psychiatric wards. Conclusion: In conclusion, the infection control intervention in the psychiatric ward is not as easy as in the medical or surgical ward due to the movement ability among psychiatric patient. Seasonal influenza vaccination could see as the health prevention intervention to protection patient before influenza season coming. Tamiflu ® (oseltamivir phosphate) dosing could suggest if an outbreak event was occurred in the ward. Environment clean and disinfection was highly suggested in order to stop drop infection be transformed into contact infection by psychiatric patients' hands.

Evaluation of quality of hand hygiene skills using fluorescent lotion and ultraviolet test Wan-Ching Wu 1 , Po-Lin Chen 1,2 , Chia-Wen Li 1,2 , Wen-Chien Ko 1,2 , Nai-Ying Ko 1,3 . 1 Center for Infection Control, National Cheng Kung University Hospital, Tainan, Taiwan, 2 Division of Infectious Diseases, Department of Internal Medicine, National Cheng Kung University Hospital, Tainan, Taiwan, 3 Nursing Department, National Cheng Kung University Hospital, Tainan, Taiwan Background: To promote the quality of hand hygiene skill, several innovative teaching programs has been initiated since 2012. In order to evaluate the quality of hand hygiene skills of employees in the study hospital, an investigation with the ultraviolet light (UV light) test was underwent in March, 2017. Methods: A total of 706 staff members, including administrative workers and healthcare providers of different specialties were enrolled. The quality of hand hygiene skills was evaluated using fluorescent lotion and UV light test. Briefly, fluorescent lotion was smeared evenly on both hands before doing handwashing, and then UV light was used to check the lighting spots on both hands, indicating fluorescent lotion not removed by handwashing. The cleaness of seven main areas (including palms, back of hand, gaps between fingers, knuckles, thumbs, finger nails, wrists of both hands) were measured (+1 for each section) and scored 0 to 7 for each hand. Results: A total of 706 employees (321 nurses, 180 members of hospital administration, 93 medical technicians, 76 doctors, 26 maintenance workers and janitors, and 8 other staff ) in a medical center participated in this evaluation. The results showed 394 (55.8%) participants passed the test perfectly, and 225 (31.9) with missing no more than 2 areas on both hands. The most missing points were the finger gaps of left hands (122, 39.1%), left hand fingers (116, 37.2%) and gaps between right hands (120, 38.5%). The scores for 323 nurses, 180 administrative members, 93 medical technicians, 76 doctors, 26 maintenance workers and janitors, and 8 other staff were 12.96 ± 1.57, 13.02 ± 1.55, 13.18 ± 1.28, 12.59 ± 1.75, 13.23 ± 0.99, and 12.25 ± 1.67( p = 0.106). Staff from examination rooms/departments got the highest score of 13.48 ± 0.81, followed by the intensive care units (13.46 ± 0.97), administrative stations (13.10 ± 1.58), ( p < 0.001). Conclusion: The ability of performing good hand hygiene skills varied in different stations and departments, while no significant difference was observed among employees of different specialties. The fluorescent lotion and UV light test can reveal the missing points of hand hygiene skills individually, and help the staff to improve their skills accordingly.

Using quality control circle to reduce the positive surveillance culture rate of intestinal endoscopes Mei-Yu Wu 1 , Pei-chin Hung 2 , Chen-Shuan Chung 2,4 , Hsin-Yen Tsai 3 , Chun-Hsing Liao 1,3 . 1 Infection control center, 2 Ultrasound and Endoscopy center, 3 Infectious Disease Division, 4 Gastroenterology and Hepatology Division, Far Eastern Memorial Hospital, New Taipei City, Taiwan Background: Positive surveillance culture from endoscope after disinfection is not uncommon. In 2015, the overall positive rate of surveillance bacterial culture was 12%, and the rate was 39% for endoscopes not disinfected in the endoscopic center. There are several problems including incorrect manual cleaning, inappropriate storage, or lack of automated washing device. A quality control circle is formed by staffs from endoscopic center and departments with endoscopes to improve the quality of endoscope disinfection.

Methods: The quality control circle had implemented meausres including (1) Cleaning performed by qualified personnel after and education and training. (2) Annual educational workshop and trainging. (3) Poster of standard operating process in the cleaning room. (4) Insepct the correctness of manual cleaning and use of disinfection agent by circle leaders. (5) Procure the washing machines and storage cabinets. The circle member meet quarterly and inspect all participating units regularly. Results: In 2016, a total of 53 people had received education and training. On-site audit for 8 units was performed every six months, including use of automated washing machine, process of manual cleaning, storage of endoscopes, regular check for concentration of OPA, etc. The most common defects were incorrect manual cleaning process, inappriate endoscope storage, and expiration of OPA test stripe. After the effort of quality control circle, the number of defects decreased from the 29 (total) (March 2015) to 9 (December 2015). The positive surveillance culture rate reduced from 12% in 2015 to 0% in the first quarter of 2017. The top three isolated organisms were Pseudomonas aeruginosa, Pseudomonas putida, Enterobacter cloacae. Conclusion: With quality control circle to perform regular checks and interventions, the quality of endoscope disinfection could be improved significantly.

An outbreak of severe respiratory tract infection due to human metapneumovirus infection in one regional hospital in Southern Taiwan Wan-Ling Chen 1,2 , Bao-Chen Chen 3 , Ye-Hsuan Jao 2,4 , Miao-Ci Cai 2 , Ying-Chen Hsu 2 , Yeuh-Ju Chen 5 , Wen-Hsin Tsai 5 , Hung-Chin Tsai 5,6 .

August 2016. The mean±standard deviation(range) age for staff members and patients, was 45.3 ± 15.5 (23-63) and 67.6 ± 19.2 (21-92) years old. The most common presentations were upper respiratory tract infection, but 14 (27%) of 52 patients had pneumonia. Besides, 10 (19%) of 52 patients had hypoxia. The fatality rate was 7% (1 of 15 patients) among confirmed cases and 5% (2 of 37 patients) among patients with possible cases. In this outbreak. there were 15 confirmed cases by viral cultures or polymerase chain reactions and 37 possible cases by clinical diagnosis. The phylogenetic tree analysis showed the high degree strain similarities, indicating they originated from the same source. Conclusion: HMPV is an important pathogen, invading elderly persons and health care workers in this outbreak at the regional hospital.

The survey and disposal of CRE-KPC in a regional hospital in Taiwan Chih-Lin Liu, Li-Kuan Chih, Li-Chun Liu, Wen-Han Chang, Shu-Chen Wang, Chao-Tai Li, Pao-Chen Chen, Ying-Sheng Pan.

Background: The hospital owns 516 beds. From March to July 21 st , 2016, 10 cases (28.6%) were proved to be positive CRE-KPC results which were reconfirmed by CDC, Taiwan. Comparing with one case (1.1%) occurred from 2010 to 2015, the results were obviously higher. In order to prevent the spread of disease, the related interventions were implemented immediately. The experience would be shared with the medical profession as a reference. Methods: Bacteria culture laboratory isolated the specimens which fit the CRE drug resistant condition defined by CDC. Then active notification was proceeded and infection control interventions were activated. On June 21, 2016, after discussing with the staffs in the Division of Laboratory Medicine, related interventions were adopted. To clarify whether each individual case is relavant to one another, interventions adopted were investigation in the ward, proceeding the sampling inspection of environment in the ward where the patients have lived as well as the hands of staffs who worked there, and revising the standard operation procedure of multiple-drug resistant strains. Results: By analyzing those 10 cases, one was the patient of healthcare-associated infection while the other 9 weren't. On July 26, 2016, 119 specimens, including those collected from the ward as well as from the hands of staffs, and 1 rectalswab specimen from CRKP case, were sent for inspection. 3 specimens (3.36%) verified to be CRE-KP drug resistance were the same as KPC drug resistant case identified by CDC. They are specimens collected from bedside bar, bedside table, and rectalswab of CRKP case. Conclusion: Colonization often occurs before infection. In this event, 3 specimens from one inpatient and his surroundings were verified to be CRKP through environmental collection and inspection. Although the source of infection can't be confirmed, the investigation was continously proceeded and finally found it resulted from the insufficient concentration of bleach formulated by the staffs. After improvement, we kept following up from August 29th and found no cases in the following two months. Environmental clean and disinfection are the key points of epidemic prevention. This study would be provided to the medical profession as a reference of how to implement the interventions to deal with multiple-drug resistant strains. The key words were as follows: ""citrate"" [tiab] AND ""catheter lock"" AND (""cancer"" OR ""malignancy""). Results: In our synthesis of 8 papers, 2 papers met the inclusion criteria distinguished by PICO. Chaftari AM 2017 (level 2) enrolled 60 patients with hematologic malignancies who had peripherally inserted central catheters (PICC),use lock consisted of 15 or 30 μg/ mL of nitroglycerin in combination with 4% sodium citrate and 22% ethanol. Each lumen was locked for at least 2 h once daily prior to being flushed. It can reduce central line-associated bloodstream infections (CLABSI) rate from 1.9/1,000 catheter days to 1.6/1,000 catheter days. The adverse events were one headache and one rash but mild and transient. Rosenblatt J 2013 (level 5) reports on the synergy of glyceryl trinitrate (GTN) with citrate and ethanol in rapidly eradicating methicillin-resistant Staphylococcus aureus, methicillin-resistant Staphylococcus epidermidis, Pseudomonas aeruginosa, and Candida albicans biofilms in an in vitro model for catheter biofilm colonization. Conclusion: Sodium citrate may effective central venous catheter lock solution in hematologic malignancies because of it's antimicrobial and antithrombotic effect. But the evidence was not sufficient,we need to more prospective trails confirmly.

Effectiveness of preventing needlestick injuries and body fluid exposure by implementing safety-engineered devices at a medical center in Taiwan Shu-Fen Chien 1 , Ju-Chun Lo 1 , Yu-Ling Lai 2 , Jann-Tay Wang 1,3 , Wang-Huei Sheng 1,3 , Yee-Chun Chen 3 , Shan-Chwen Chang 3 .

1 Center for Infection Control, 2 Occupational Safety and Health Office, 3 Department of Internal Medicine, National Taiwan University Hospital, Taipei, Taiwan Background: Since the increasingly complicated medical environment and the threats of emerging infectious diseases, issues of workplace safety and health promotion are getting more important. The needlestick injuries (NSIs) and body fluid exposures are common occupational hazards at medical institutions. Besides continuously enhance the training and education of personnel protection as well as continuous improvement of tools and equipment in the workplace are also expected to avoid the occurrence of accidental NSIs.

Using safety-engineered devices (SEDs) is an important reform and step for the prevention of NSIs. In 2011, Taiwan has allowed laws for the usage and promotion of SEDs in the daily practice at hospital settings. It is necessary to investigate the effect of using SEDs in term of reducing the incidence of NSIs. Methods: The study was conducted at a medical center with 2300 beds. Since 2011, it has started to promote the usage of SEDs over the hospital phase by phase. Training and education program to prevent healthcare personnel from NSIs were also conducted. Closely monitoring on the incidence and analysis on the underlying mechanisms of NSIs were implented simutaneously. Interventions to improve the medical and practice understanding the factors result in NSIs were performed. Results: The overall incidence of NSIs decreased from 3.90% in 2011 to 2.82% in 2016. The decrease is especially obvious for interns and nurses. In addition, the rate of NSIs caused by conventional devices (including syringe, intravenous catheter, winged needle, insulin syringe, etc.) decreased from 57% in 2011 to 39% in 2016, which resulted from conventional syringe declined from 41.7% in 2011 to 26.9% in 2016. However, the occurrence of NSIs made by the incorrect operating of safety devices and safety devices being not activated had increased from 1.94% in 2011 to 8.97% in 2016. Conclusion: The use of SEDs can effectively prevent NSIs but must work correctly with standard operating procedures. Training and education on the usage of SEDs are necessary before promoting use of SEDs. Keeping and making a more safety workplace environment with sustained efforts should be ongoing to promote occupational health.

Diffuse rectum ulcer bleeding with severe cytomegalovirus infection patient: a case report Ching I Ting, Wen lian Yu. Chimei Medical center Intensive care unit Background: Cytomegalovirus (CMV) usually causes an asymptomatic infection or produces mild flulikesymptoms; afterward, it remains latent throughout life and may reactivate. In immunocompromised individuals, symptomatic disease usually manifests as a mononucleosis syndrome. Symptomatic CMV disease can affect almost every organ of the body. Rare manifestations of CMV infections in immunocompetent individuals include thrombocytopenia, and hemolytic anemia. We present a case of colon cancer and recived chemotherapy with UFUR. However, massive bloody stool passage was found and pandoscopy revealed esophageal ulcer and the pathology confirm CMV infection. Ganciclovir therapy was performed and stopped bleeding after surgical intervention. Case Report: This 81 y/o male patient was admitted due to left femoral intertrochanter comminuted fracture. After admission, surgical treatment was performed on 11/03. Aspiration pneumonia was developed with fever and dyspnea condition after the operation. Due to unstable vital sign and dyspnea, he was transferred to ICU on 11/11 and intubation due to respiratory failure. Broad spectrum antibiotic, septic work-up and fluid resuscitation were perscribed for the patient. Much bloody stool passage was noted on 11/13, GS was consulted and anal fissure bleeding was suspected. However, recurrence of bloody stool was noted, colonoscopy on 11/21 revealed multiple rectal ulcer s/p biopsy. The pathology of rectal ulcer showed favor CMV colitis, thus, ganciclovir was added (11/26-12/2) and combined with antibiotic (Tazobactam/Piperacilin) for pneumonia treatment. We had repeat colonoscopy due to persistent intermittent bloody stool passage, repeat CMV-PCR and antigenemia. The repeat colonscopy showed rectal ulcer with oozing. Antibiotic was adjusted as infection men suggestion, diflucan was added for funguria. The patient's condition became getting stable. Due to negative of antigenemia and negative of stool CMV-PCR, oral form ganciclovir was discontinued (12/2-12/23) after discussing with ID man. Due to improved of clinical symptoms, the patient was discharged on 12/28 in relative stable condition. Discussion: Most gastrointestinal CMV infections respond well to ganciclovir treatment, Independently from the cause of the underlying immunosuppression. Therefore, the patient should be offered an antiviral treatment as soon as possible. Early diagnosis of suspected CMV infection in immunosuppressed patients with gastrointestinal symptoms is of the utmost importance. Repeat endoscopy may be considered if previous study did not meet a conclu gastrointestinal CMV disease should be kept in mind with immunocompromised patient compatible with clinical symptoms.

Evaluation on the effectiveness of cleaning products for action care work car in a region hospital Chun-Feng Chiu 1,2 , Min-Han Hsieh 1,3 , Tun-Chieh Chen 1,3 . 1 Infection Control Office Kaohsiung Municipal Ta-Tung Hospital, 2 Department of Nursing Kaohsiung Municipal Ta-Tung Hospital, 3 Department of Internal Medicine, Kaohsiung Medical University Background: In a regional hospital in south Taiwan, there were e-nursing carts for nursing staff for their daily medical work. In order to evaluate if the mobile e-nursing carts became the media of multi-drug resistant organism (MDRO) transmission, ATP-bioluminescence and culture were performed to assess the efficacy of surface cleaning in 10 high-touch surfaces of mobile e-nursing cart. The mean value of ATP-bioluminescence of the surfaces was 1112.6 RLU. The culture grew 180 isolates of microorganism (56.3%). The result revealed that the cleaning is insufficient. There's no a universal regulation. After investigation the will of cleaning by questionnaire, it correlated with the convenience of cleaning tools. Therefore, this study tried to assess efficacy of cleaning by four different cleaning tools. This study also evaluated the will of medical staff for cleaning the mobile e-nursing cart by different cleaning tools. By the analysis, we try to find out how to increase the will of medical staff for cleaning the mobile e-nursing cart, to make sure the cleaning and to avoid healthcare-associated infection. Methods: 1. The mobile e-nursing carts were cleaned by four different tools. To assess the outcome of cleaning, ten high-touch surfaces of each cart was checked by ATP-bioluminescence after cleaning. 2. Questionnaire was designed to medical staff to evaluate the satisfaction the medical staff for different cleaning tools. Results: 1. The use of four different cleaning tools in the e-nursing carts of the vehicle clean, found that different cleaning products can make the surface after cleaning. The mean value of ATPbioluminescence of the surfaces was 484.5 RLU. 2. Total 128 questionnaire were filled in by medical staff. The questionnaire included the items of convenience, operation, cleaning effect, odor and stimulation to skin. The result showed that the medical staff satisfied with antiseptic wet tissue B. Conclusion: A cleaning tool with convenience to operation, effective cleaning, lower stimulation to skin and smell will increase the will and frequency of cleaning for medical staff. If medical staff implement the cleaning is more important for the cleaning of mobile e-nursing cart. Therefore, the establishment of standard procedure for the mobile e-nursing cart cleaning and system of audit is essential.

Improve the integrity of oral care for nurses in the prevention of ventilator-associated pneumonia Shwu-Hwa Leu 1 *, Mei-Ju Su 1 , Li-Yao You 1 , Shu-Chun Chang 1 , Yueh-Chen Fu 1 , Hsia-Ling Wu 1 , Li-Chu Li 1 , Chin-Yu Chen 2 , Tzu-Ping Shih 2 , Wen-Chih Wu 3 , Cho-ching Sun 4 Background: The use of ventilator-associated pneumonia (VAP) for the intensive care unit quality indicators to monitor important items,not only will extend the hospital stay will increase the cost of medical care (Ashraf & Ostrosky-Zeichner, 2012), so there is a systematic introduction to prevent the combination of care, remodel the correct concept of staff care, the implementation of unified and more effective care measures in the combined care, but also to respirator patients with oral care is more important, Expect effective prevention of VAP, improve patient cleaning and comfort, in order to enhance the professional care and service quality. In April 2016, The team used technical audit and observation methods. The average correct rate of nursing staff was 73.3%, and the integrity of oral care was only 68.7%. First, the newly transferred personnel to protect the lack of awareness (19.7%); Second, the lack of appropriate care products can be used (21.8%);Third, there is no relevant care operation check mechanism and standard operating practices (16.2%), so the purpose of this project is to improve the integrity of oral care to 95%. Methods: In order to improve the integrity of oral care for nursing staff, improvement measures are made every month from June to September, 2016. 1. Participate in TRM competition and external expert clinical guidance. 2. Improved care products and equipment, use multi-functional aspiration toothbrush,oral denture and 0.2% Chlorhexidine oral cleaning gel to mouthwash. 3. System to prevent the prevention of VAP care standards: revised special oral care technology, and develop the use of breathing apparatus operating instructions. 4. Implementation of VAP care check and reward: internal check, outside the hospital inspection counseling, Bundle protection angel recognition and implementation of the incentive system. Results: June 2016-May 2017prevention of VAP care integrity check results care integrity 99.4%, Nurses' knowledge of oral care increased from 81.2% to 100%; And maintain VAP zero case. Conclusion: Since the introduction of modular care, the unit has been effective in preventing infection and significantly improving peer awareness and care skills. Effective teamwork can increase the efficiency of the work, in the process of care, while buy and use the new improved care products, not only more convenient and effective; the current hospital has been promoted to the implementation of the respiratory care ward, To share the unit to promote the implementation of experience, in order to achieve the purpose of maintaining patient safety and improve the quality of medical services.

The role of cytomegalovirus in post-H1N1 pneumonia: contributor or bystander? Hui-Ling Huang 1 *, Wen-Liang Yu 1,2 . 1 Department of Intensive Care Medicine, Chi Mei Medical Center, Tainan, Taiwan, 2 Department of Medicine, Taipei Medical University, Taipei, Taiwan Background: Influenza may cause severe disease that needs care in the intensive care unit (ICU). Viral infections in the ICU often involve the respiratory system and can cause significant morbidity and mortality especially in immunocompromised patients. Meanwhile, critically ill patients admitted in ICU are predisposed to many nosocomial infections due to underlying illnesses, various invasive procedures and prolonged hospital stay. We report detection of cytomegalovirus (CMV) and Morganella morganii in a patient after prolonged ICU stay. Both pathogens are often ignored but might be of clinical significance. Methods: Case study. Results: This 78 y/o man had history of hypertension, chronic kidney disease and brain stroke under medication control. He was in acute respiratory distress and required immediate intubation. He was admitted to the ICU. Hemodialysis was used because of increasing metabolic acidosis and progressive pulmonary edema. Polymerase chain reaction (PCR) testing of nasopharyngeal sample was positive for influenza A (H1N1). He completed a 10-day course of oseltamivir therapy. Then piperacillin was used for pneumonia caused by Klebsiella pneumoniae. After 32 days of ICU stay, he was transfer to the Respiratory Care Center (RCC) because of difficulty in weaning ventilator. However, septic shock was noted during stay in RCC. The cultures of blood and sputum yielded M. morganii. Blood Aspergillus galactomannan antigen index was 0.11 (normal, <0.5). In addition, CMV-PCR results of sputum and blood samples were positive. CMV viral load was 9,550 IU/mL. Imipenem had been used for 14 days without anti-CMV treatment. His condition improved thereby and he was successfully extubated and weaned from the ventilator. After 24 days stay in RCC, he was transferred to the ward. He was discharged uneventfully after 71 days of hospital stay. Conclusion: CMV antigenemia is not an uncommon finding in ICU patients with immunocompromised status. Although associated with a higher morbidity and mortality, the clinical significance of CMV in ICU patients is unknown. Meanwhile, the organism M. morganii with proven pathogenicity could be potentially dangerous and should not be overlooked. Thus M. morganii infection was vigorously treated, whereas CMV reactivation was ignored in the current case. It is difficult to discern whether CMV plays a role in post-H1N1 pneumonia status. CMV might be a bystander as no anti-CMV therapy was given, but it might be a contributor associated with prolonged hospital stay.

Clinical characteristics of the patients infected with vancomycin-resistant enterococci Hsiu-Lun Su 2 , Ya-Lan Chen 1,2 , Chen-Chi Tsai 1,3 *, Shu-Juan Tsai 1,2 , Jia-Lin Lin 1,2 , Hui-Ying Chuang 1,2 , Mei-Lan Shen 4 , Chorng-Jang Lay 1,3 . 1 Infection Control Committee, 2 Nursing department, 3 Division of infectious disease, department of medicine, Methods: Our hospital is a 960-bed regional teaching hospital. This study is a retrospective case-analysis study. We reviewed the medical records of the patients infected with VRE from Jan. 2016 to Dec. 2016. Results: There were 114 patients infected with VRE, whose specimens including urine (n = 68), wound (n = 14), blood (n = 7), stool (n = 7), Ascites (n = 5), Bile (n = 4), CVP line (n = 2), Drain tube (n = 1), other (n = 2), Pleural effusio (n = 2), pus (n = 2). Most patients are 70-80 years old (61.4%). The most common comorbid disorders included hypertension (50%), diabetes mellitus (46.5%), and bedridden status (48.2%). There were 6 patients re-admitted in half year. One of them had 4 times of re-admission, and another had 5 times of re-admission. VRE was isolated from these two patients in every re-admission. Conclusion: This study showed that most of the patients infected VRE were elderly people with comorbid disorders. About one half of the patients were bedridden. We found that VRE could be found in re-admission of the patients infected with VRE.

Using bundle cares to reduce ventilator associated pnumonia density-case by a regional teaching hospital in Taiwan Chiachi Liu 1 , Chunchih Yeh 1 , Siangru Syu 2 . 1 Infection Control Committee, I.C.U, 2 Ten Chan General Hospital, Chung-Li, Taoyuan, Taiwan Introduction: Prevention of infection counts in intensive care critical ill patients. Ventilator associated pneumonia (VAP), for one, not only prolong the time of ventilatior usage, but also increase the durations of hospitalization and the medical costs, and that even counted one of the important cause for mortality of patients. Methods: Ventilator associated pneumonia reached 1.61‰ in 2014 for I.C.U. for Ten Chan General Hospital, Chung-Li, Taoyuan, Taiwan, of which seemed on the rise compared with the following two years. Reviewing the bundle cares, the causes for ineffective execution were: care professionals short of understanding of care methods, lack of standing operation procedure, didn't lift the bedhead measuring instruments and short of the evaluation charts. Therefore, the following infection control and contain measures were introduced, of which include: Consclusion: The most effective way to reduce VAP is to remove the tubes earlier, and that chime in with the medical qulity demands and the object N.O. 6 -Implementation the idea of bundle cares and lower the medical care associated infection. It will shorten the hospitalized days for the patients, to stay away from deseases, and make sure the safety for medical care.

PS 492 I-CARE intervention to reduce catheter-associated urinary tract infections in an adult intensive care of medical center Yueh-Ju Chen 1 , Cheng-Ho Chang 2 , Jui-Kuang Chen 3 . 1 Infection Control Committee, 2 Nursing Department, 3 

Background: In our hospital, as early as 2001, we implemented nurse-generated reminders to physicians to remove catheters 5 days after insertion. We were able to effectively reduce catheter duration from a mean of 7 to 4.6 days, lowering the rate of CA-UTI by about 25%. Unfortunately, reminder interventions can also be easily ignored and catheters may remain in place without action. Since then, the rate of CA-UTIs began to increase in recent years, reaching 7.2 cases per 1000 urinary catheter days in 2015.

Methods: I-CARE intervention was performed in August 2015 with coordinated strategy comprised of (1) I-CARE slogan poster: Indication Clean Aseptic Remove-early Easy and smooth (2) team works with focus on nurse driven catheter removal potoca (3) Background: Ambient air may be contaminated with or carry significant levels of a variety of potentially detrimental microorganisms. Exogenous flora are primarily aerobes, especially grampositive organisms. The microbial concentration in an indoor operating room (OR) is one of the extrinsically influencing factors for surgical site infections. The aim of this study were to assess microbial colony counts in working ORs in a tertiary referral medical center.

Methods: This prospective observation study was conducted at the 28 ORs of a medical center in Taiwan. Each OR was equipped with a ventilation system with a high efficiency particulate air filter. The ventilation systems of the ORs produced a minimum of approximately 20 air changes of filtered air per hour. The vertical laminar airflow systems were used to move particle-free air over the aseptic operating field at a uniform velocity. The microbiologic air counts were measured using an impactor air sampler for 4 months.

Results: The overall mean number of bacterial colonies in the ORs was 78 cfu/m 3 . The mean number of colonies was the highest for transplant surgery, followed by pediatric surgery. No fungi were cultured from the air samples. A total of 25 (10%) contained pathogens isolated from air in ORs; Coagulase-negative staphylococcus was the most common pathogen, followed by Micrococcus spp., and Staphylococcus. Conclusion: Our study demonstrated that free fungi and the number of mean bacterial colonies were far from the suggested levels during surgical activity despite a well-controlled OR ventilation system with a HEPA filter. Air flow conditions in hospital ORs are controlled and regulated to remove airborne microorganisms to help control infections. Methods: The IC team visited bacteriology lab for more information about cotton batting-like material as received abnormal results. Next we visited OR in the company of OR head nurse to investigate and to see whether the number of surgical site infections cases show abnormally increase in the past 6 months. Results: We found green cotton fibers on the top surface of the operation lampshade when visited OR. These green cotton fibers should be from green cloth used to wrap surgical instruments. We speculated cotton fibers scattered when surgical instruments parcels were opened. We also found in sanitary staff's operation procedure that they routinely clean the floor, operation tables, walls and so on, but didn't include operation lampshade and therefore, resulted in fibers accumulation. Environmental cleaning and disinfection were implemented. Air quality monitoring was followed again subsequently and the results were all normal. Also, there were no abnormal increase of surgical site infections [SSI rate was 0.20% (6/2952), there was no significant difference ( p = 0.632) compared with the mean value of pre-infective stage (0.13%)]. Conclusion: Operation room is an important environment where surgery was performed and saving lives. To ensure the OR environment clean and free from pollution is very important. In this case we rechecked and revised sanitary staff's SOP and added works cleaning the top of the lockers and the operation lampshade to avoid accumulation of cotton fibers and dust to cause environmental pollution.

Evidence-based assessment of the effects of antibiotic prophylaxis for preventing burn wound infection Fang-Ching Liu 1 , Yuan-Hsin Chu 2 . 1 Division of Infectious Disease, Jen-Ai Hospital, 2 Department of Infection Control, Jen-Ai Hospital

Background: Burn injuries are associated with significant death and disability, multiple surgical procedures, prolonged hospitalization, and high costs. Taiwan ICST and French Burn Association suggested that antibiotic prophylaxis should be given as soon on severe burns rather on local infection burns patients or infections not assured. We herein perform a search to provide evidence-based recommendations for prophylactic treatment in burns patients. Methods: We used the relevant terms as keywords (Prophylaxis AND Antibiotic AND burn) to search Cochrane Library and PubMed electronic databases and acquired a 2013 Cochrane systematic review for critical appraisal.

Results: Thirty-six RCTs were included and 11 trials were metaanalyzed about a particular antibiotic, silver sulfadiazine, showed a significant increase in burn wound infection and longer hospital stay, compared with dressings/skin substitute: (OR = 1.87; 95% CI: 1.09 to 3.19, I2 = 0%) and (MD = 2.11 days; 1.93 to 2.28), respectively. There was no evidence of an effect of systemic antibiotic prophylaxis in non-surgical patients on burn infection rate (3 trials) . Systemic antibiotics (Trimethoprim-sulfamethoxazole) were related to a significant reduction in pneumonia (only one trial) (RR = 0.18; 0.05 to 0.72).

The limited evidence about the effects of antibiotics was unable to draw reliable conclusions. Therefore, antibiotic prophylaxis is not suggested routinely administrated for all burns patients, which is consistent with Taiwan ICST suggestions and French Burn Association guidelines. Further, 48-72 hours after antibiotic prophylaxis interventions, appropriateness of drug usage should be reassessed, preferably by the results of bacteria cultures, to adjust the applications of antibiotics for burns patients.

Experience sharing of amblyopia caused by EMB drugs in regional hospitals Ya-Jhen Ji 1 *, Shu-fan Lin 2 , Yueh-Ying Tsai 3 . 1 Yunlin Christian hospital of Tuberculosis case manager, 2 Yunlin Christian hospital of Thoracic surgeon, 3 

Background: Tuberculosis is the oldest disease. The standard treatment includes four kinds of medication combination treatment as INH, RMP, PZA and EMB. The treatment process will vary from person to person with different side effects. EMB related blurred vision is most likely to be confused with degenerative diseases in the elderly. Methods: Mr. A is a 70-year-old person who has the history of Type 2 DM and cataract. Tuberculosis was found on September 24, 2015 and the patient began to use a four-in-one standard treatment regimen. Before treatment, left eye blurred vision was mentioned and his right eye had 20/70 vision and normal color discrimination (from Public Health Center). The patient complained that blurred vision deteriorated on January 5, 2016. He was diagnosed denegerative glaucoma at local clinic on January 7, 2016. But he did not informe to the doctor at chest OPD on the next day. The patient only told the information to the DOTS observers. The patient received operation of cataract of right eye on local clinic on Jan 18, 2016. Then, abnormal color discrimination was found on January 26. 2016. Therefore, we stopped anti-TB treatment on January 30, 2016. His family questioned the deterioration of vision due to EMB. So, (1) we referred the patient to Ophthalmology authority. (2) The case was applied for phytosanitary relief. (3) The patient was transferred to tuberculosis hospital (PCH). Methods: 1. A visual checklist was printed automatically for the patient who take EMB by computer settings.

2. Check simple vision test including color discrimination by case manager before treatment and every month during treatment with EMB 3. Record the result of simple vision test 4. Use the questionnaires for tuberculosis diagnosis, treatment in the case of the occurrence of abnormal to keep the record by case manager and the public health department. Results: We have implemented the program including 36 cases since Sep, 2016 and there is no new onset of occurrence due to taking EMB caused vision-related medical disputes. Conclusion: Every medical dispute is the process of learning and improvement. We hope to provide better quality service after this event and want early prevention of drugs induced side effects. Therefore, we could put the end to medical dispute and support a win-win strategy between the patient and medical caregiver.

Through the combination of care to reduce the respiratory care center catheter-related urinary tract infection effect I Ping Yang. Chiayi Chang Gung Memorial Hospital Infection control commissions Background: Respiratory care center in January 2016 catheterrelated urinary tract infection density of 15.3‰, compared with the previous 6 months average duct-related urinary tract infection density 5.1‰ high; by the chi-square test statistical value of P = 0.0083, field audit found that the implementation of hand Health is not implemented, catheterization process in the urethral disinfection process and disinfection time is not enough, set the urine bag at the exit of the bed along the situation, in recent years found in the import and active combination of infection control measures to help reduce Catheter-related infection, the project will be further explored. Methods: From the beginning of 20160201 to 20161231 only, after the collection of information and the status quo analysis, the solution according to the planning period, the implementation period, the evaluation period of the implementation of the planning period: planning education training courses to investigate the catheter placement and care process, Revised the placement checklist, care assessment form and audit plan; implementation period: the revised operation process, including the time of hand washing, the newly revised indwelling catheter checklist and care assessment table published in the unit bulletin board and publicity know, Conduct education and training, perform technical audits and use catheterization, care checklist; evaluation period: through the unified teaching of the correct catheterization and care process, reply teaching and planning audit. Catheter-related urinary tract infection density increased from 6.1‰ before the reduction to 5.5‰, catheter placement compliance rate increased from 0% to 100% Results: 1. Catheter -related urinary tract infection density: from the planning period 6.1‰, the implementation of 5.8‰ to the evaluation period has dropped to 5.5‰. 2. Catheter compliance compliance: increased from 0% to 100% before improvement. Conclusion: The UTI bundle is listed as a catheterization and care standard, and educational training and auditing system is organized to make the catheter placement and care concept more effective. According to the literature, it is pointed out that the removal of the catheter is a preventive urinary tract The main cause of infection, but because of the relationship between patients with respiratory care center, the implementation of the catheter in the early removal of the more difficult, but also the resistance of the project. Due to the January 2016 catheter-related urinary tract infection density rise, and then timely analysis and improvement, to achieve the desired results.

Impact analysis of hemodialysis-related bloodstream infection after bundle care intervention Chia-Hung Liao, Jeng-Hua Ling, Zong-Fang Jhung, Hung-Ni Lee.

Background: In Taiwan, most end-stage renal failure patient received hemodialysis replacement treatment. Health-care associated bloodstream infection may result in elevated morbidity, mortality, prolonged admission, and higher cost. We conducted previous evidence-based procedures identified as having the great effect on the rate of bloodstream infection as ""bundles care"" to the hemodialysis units. Finally, we compared hemodialysis associated bloodstream infection rate before and after the interventions. Methods: This project was conducted step-by-step. Staff education was performed before June of 2016. And we used double-lumen hemodialysis catheter insertion checking list, double-lumen/ hickman hemodialysis catheter care checking list, and arteriovenous fistula care checking list to standardize the procedures since 2016, June, 1st. We calculated and analyzed compliance and correct rate, and gave feedback to the hemodialysis units every month. Finally, we analyzed hemodialysis associated bloodstream infection rate before and after the interventions. Results: After education and coarse, we conducted the evidencedbased procedures identified as lowering the rate of bloodstream infections as checking lists. After intervention, compliance and correct rate improved gradually. And hemodialysis associated bloodstream infection also improved (January ∼ May of 2016: 0.692‰; June ∼ October of 2016: 0.32‰). Conclusion: An evidence-based bundle care intervention of hemodialysis units may resulted in improved compliance and correct rates of medical staff, and reduced hemodialysis associated bloodstream infection rate.

Use of an education programme to encourage hand hygiene with alcohol-based handrub and reduce near-patient environmental contamination in elderly residents in long-term care O'Donoghue Margaret 1 , Yang Lin 1 , Siu Gilman Kit-Hang 2 , Lee Paul Hong 1 , Boost Maureen Valerie 1 , Ho Jacqueline Mei Chi 1 , Suen Lorna Kwai Ping 1 . 1 School of Nursing, The Hong Kong Polytechnic University, 2 Department of Health Technology and Informatics, The Hong Kong Polytechnic University Background: Elderly residents of long-term care facilities (LTCFs) colonized or infected with multidrug resistant organisms act as important reservoirs leading to cross-contamination within the LTCF as well as transfer to the hospital upon admission. Colonization with MRSA is well recognized as a major risk factor for subsequent infection. The role of patients' hands has been neglected both in hospitals and LTCFs even though it has been suggested that they may represent an under-recognized reservoir of pathogens. We aimed to determine if education, regular reminders and improved availability of alcohol based hand rub (ABHR) would increase frequency of hand hygiene by elderly residents in long term care capable of performing hand hygiene without assistance. Methods: Nasal swabs were collected from 321 elderly subjects attending community centres for social activities. Swabs were cultured on chromogenic agar. Species identification was performed using MALDI-TOF. Antimicrobial susceptibility testing was performed and results interpreted according to EUCAST guidelines. Presence of AR genes was detected by PCR. Figure 1 . Hand and near-patient environmental contamination before-and after a one-month hand hygiene intervention involving use of ABHR in a LTCF for elderly residents (data for test subjects).

Results: All participants complied during which time we were able to demonstrate a significant reduction in both hand and nearpatient environmental contamination for those patients who previously had highest levels of organisms present (Figure 1 ). Frequency of hand hygiene events per day also increased significantly over the period with significant change in frequencies in each quartile ( p = < 0.001). Conclusion: By adopting directly observed hand hygiene for subjects capable of using ABHR without assistance, it should be possible for all residents of a LTCF to achieve improved hand hygiene on three occasions by introducing this methodology before each of the main mealtimes. For elderly unable to perform hand hygiene by themselves, we propose that HCWs physically assist these patients by rubbing the ABHR on their hands for them. This preliminary study shows promise as an approach to reduce the MDRO problem in the LTCF setting. Background: In order to monitor infectious diseases and avoid potential epidemic outbreaks in Taiwan, our hospital participated in the Epidemic Prevention Cloud Project driven by Center for Disease Control (CDC) in 2016. We developed and established automatic laboratory data delivered platform connected with CDC server to provide prompt and sustained data for disease surveillance.

Methods: Several strategies were launched in our hospital to achieve the assignment. First, the medical technologists and information technicians participated in the training courses on LOINC (Logical Observation Identifies Names and Codes) and RELMA (Regenstrief LOINC Mapping Assistant). Then, we mapped and organized all laboratory raw data from laboratory information system database. The information technicians developed the automatic data exchange programs based on LOINC to standardize laboratory report information. The data were analysed and audited by medical technologists before uploading daily. In addition, the uploading data were sampling investigated by CDC during testing period to confirm the accuracy. Results: We could successfully transfer twenty kinds of regulated organisms such as Mycobacterium tuberculosis, Hepatitis C virus, and Streptococcus agalactiae and so on. We totally submitted 2892 designated items from August to November to CDC. More than 10% increases in overall uploading pathogens were observed compared with baseline in 2015.

Conclusion: In summary, we finished the automatic data exchange programs and transfer platform to provide data for public health surveillance. The goverment could immediately monitor all hospitals report data in Taiwan to effectively analyze recent trends in all kinds of infectious diseases by Epidemic Prevention Cloud Project. The continued surveillance of infectious diseases should be performed and popularized in the future.

Adopting the gatekeeper model to enhance quality of port-acath Hsin-Hsu Chen, Hui-Ping Sun. Tainan Municipal Hospital

Background: Port-a-cath is most often used for chemotherapy regimen and nurses have been playing a central role in patient's catheter care. The quality of catheter care directly influences patients' safety so patients and family members should be incorporated into the gatekeeper model of port-a-cath care to enhance communication between healthcare professionals and patients and quality of care through shared decision-making (SDM) strategy.

Methods: The team used Maternal Health booklet for reference to formulate Port-A-Cath Care booklet which contained the information of self-assessment, postoperative wound care, infusion notes, principle of care after discharge to home and infusion record after discussions and allowed patients to understand the use and management of port-a-cath. Results: Patients' condition at home and present condition at admission would be understood through self-assessment of brochure after their admission to hospital; therefore, nurses could provide patients with timely access to relevant information and nursing intervention. Since the implementation of the study at a regional hospital in February, 2017, we found that completion rate of port-a-cath care guidance reached 100%; in addition, patient satisfaction scores increased from 3.31 to 4.1, with a significant improvement in suitability of nursing procedure and timeliness of nursing care. Conclusion: Although we worry that giving more information regarding port-a-cath to the patients and family members will increase probability of disputes occurrence between patients and hospital staff if the staff members don't follow the standard of care, it is necessary to incorporate patients and family numbers into the gatekeeper mode to have a better understanding of clinical quality and thereby enhance education training of practice staff to improve patient safety. Background: Effective control of tuberculosis (TB) requires a close partnership between private and public healthcare providers. An integrated web-based information system is vital and crucial for engaging all relevant healthcare providers in prevention, diagnosis and treatment of TB. Methods: A ""MONITOR"" information system was established in 2016 by Taiwan Centers for Disease Control (TCDC) with single entryway and enhanced capacity and flexibility. Main purpose of the system is to promote more rapid response and provide decision support. Information such as TB epidemiology, case management, treatment, DOTS rate, contact tracing coverage, TB clusters, TB statistics and laboratory data are included in the system. Information without personal data can be viewed online with access permission. Daily laboratory data including AFB-smear, culture, identification, drug susceptibility testing and the interferon-gamma release assay (IGRA) are automatically entered from different interface of diverse information systems in clinical settings and public health sectors. The system provides timely warnings of drug-resistant cases, IGRA-positive cases and high-risk populations to management staff according to their responsibilities. In addition, daily scheduled analyses of laboratory quality indicators, turnaround time and visualized profiles are updated automatically.

Results: Currently, the information system covers 100% of TB patients under management and 100% high-risk populations which need to be rapidly diagnosed of drug-resistant TB. Automated data transfer reduces workload and prevents entry errors caused by first-line staff. We observed increase of timeliness and accuracy of data. Automated real-time analysis of indicators can enhance laboratory quality management and facilitate prompt response to occurrences. Moreover, centralized monitoring of effectiveness of the laboratory services and case management are conducted by TCDC periodically. The system improves mobilization of TB control activities and outbreak responses. Conclusion: Our findings indicate that effective, timely and comprehensive information sharing through the web-based ""MONITOR"" system can improve TB care and control.

Assessing the effects of different processing procedures on the yield of Treponema pallidum DNA from blood Li-Rong Lin 1 , Jin-Yi Fan 1 , Xiao-Zhen Zhu 1 , Zheng-Xiang Gao 1 , Xu Shen 3 , Kun Gao 1 , Dan Liu 1 , Jian-Jun Niu 1,2 , Li-Li Liu 1,2 *, Tian-Ci Yang 1,2,3 *. Background: Blood is convenient for the extraction of Treponema pallidum DNA for PCR, but the sensitivity of T. pallidum DNA detection is unsatisfactory. Here, we investigated the effects of different processing procedures on the yield of T. pallidum DNA from blood to improve the sensitivity. Methods: We employed a real-time PCR with the target of tp0574 to evaluate the T. pallidum DNA yield in a series of simulated experiments and from infected rabbit blood, testing different centrifugal forces, processing times, and storage temperatures. Red cell lysis pretreatment was also conducted. Results: The T. pallidum DNA yields following red cell lysis pretreatment were 40.4 times greater from whole blood and 32.4 times greater from residual hematocytes than without pretreatment. For the whole-blood simulated experiments, the T. pallidum DNA yields in the lower layer were 2.8, 4.6, 7.3, 12.6, 15.24, 16.7, 65.1 and 73 .1 times that of the upper layer at the centrifugal forces of 500, 1000, 2000, 4000, 5000, 7000, 10,000 and 20,000 g, respectively. However, the T. pallidum DNA yields from clots were only 1% at different centrifugal forces. The infected rabbit blood experiment had similar results to those mentioned above. In addition, sample processing time (within 48 h) and storage temperature (between 4°C and 25°C) did not affect T. pallidum DNA extraction efficiency. Conclusion: The T. pallidum DNA yield can be significantly improved by red cell lysis pretreatment and appropriate centrifugation. Furthermore, the DNA extraction yield of T. pallidum is greater from whole blood or residual hematocytes compared with plasma, serum or clots.

Comparison of three preparation methods for direct identification of microorganisms from positive blood cultures by MALDI-TOF-MS Jeong Su Park 1,2 *, Da Yeong Song 2 , Taek Soo Kim 2,3 , Kyoung Un Park 1,2 , Eui-Chong Kim 2,3 . 1 Department of Laboratory Medicine, Seoul National University Bundang Hospital, Seongnam-si, Korea, 2 Department of Laboratory Medicine, Seoul National University Hospital, Seoul, Korea, 3 Department of Laboratory Medicine, Seoul National University College of Medicine, Seoul, Korea

Background: Direct identification of microorganisms from positive blood culture bottles using matrix-assisted laser desorption/ ionization time of-flight mass spectrometry (MALDI-TOF-MS) has been reported effective to treatment of septic patient. However, additional cost for commercialized MALDI Sepsityper kit (Bruker Daltonics) may be burden to clinical laboratories. This study evaluated three preparation methods (simple centrifugation, ethanol/formic acid preparation, Sepsityper method) from positive blood culture. Methods: This study was performed between September 2013 and March 2014 at the Seoul National University Bundang Hospital in Seongnam, Republic of Korea. Bottles showing positive blood cultures by BacT/ALERT 3D (bioMérieux) were tested. Cultures were subjected to further MALDI-TOF-MS, VITEK II (bioMérieux), and Microscan (Siemens Healthcare) analysis. The pellets from three direct preparation methods were identified by MALDI-TOF-MS and the cut-off score was 1.7. Fisher's exact test and McNemar Chi-square test was used; a p-value of less than 0.05 was considered statistically significant. For simple centrifugation method, 1.5 mL of broth in tube was centrifugated at 13,500 rpm for one minute and the supernatant was discarded. For ethanol/ formic acid preparation, after simple preparation, non-Sepsityper pellet from aerobic bottle was suspended in 1 mL of 0.26% saponin and washed. The pellet was treated with 75% ethanol, and then in 70% formic acid and acetonitrile. The Sepsityper preparation was performed according to the manufacturer's instructions. Results: Total number of identified patient was 83 and 26 were cultured both aerobic and anaerobic bottles. 5 fungi, 7 mixed bacteria, 1 polymicrobial samples were excluded. The samples from Sepsityper method were identified better than other methods ( p < 0.001). Especially in simple centrifugation, Gram positive bacteria were significantly less identified than Gram negative bacteria. echinocandin agents is preferred for C. glabrata infection. We suggest that echinocandin should be started as an empiric antifungal therapy if the blood culture turns positive after 50 h of aerobic incubation.

Efficacy and safety of echinocandins for the prevention of invasive fungal infections in patients with hematologic malignancies Mei-Chuan Lee 1 , Han-Siong Toh 2 . 1 Department of Pharmacy, Chi-Mei Medical Center, Tainan, Taiwan, 2 Department of intensive care medicine, Chi-Mei Medical Center, Tainan, Taiwan Background: Patients with hematologic malignancies are at high risk of invasive fungal infections (IFIs), which has been associated with high mortality rate. In the past, azoles are the only available antifungal prophylactic agents. The novel antifungal agents, echinocandins, had been found to have strong antifungal activity against most isolates of Candida spp. and Aspergillus spp. The aim of this study was to evaluate the effect of echinocandins for the prevention of invasive fungal infections in patients with hematologic malignancies. Methods: The following databases were searched: Medline, Pubmed (1966 to May 2017 and Cochrane databases (The Cochrane Library 2017) without language restrictions. We also handsearched the related references. Only randomized control trials (RCTs) comparing azoles and echinocandins were included for further analyses, and all echinocandins (micafungin, caspofungin and anidulafungin) were compared in their effectiveness. Statistical analysis was performed with Review Manager Version 5.3 and meta-analysis was synthesized using the random-effects model. The results was expressed as relative risks (RR) with 95% confidence intervals (CIs) and heterogeneity was tested using the Cochran Q statistics generated from the χ 2 test. Results: After quality assessment, we reviewed evidence from 5 RCTs that enrolled 1632 patients with hematologic malignancies who received antifungal agents as prophylaxis for IFIs. There were 3 RCTs comparing micafungin and azoles, and 2 other RCTs comparing caspofungin and azoles. There has no RCT comparing anidulafungin with other antifungal agents. The overall incidence of IFIs has no significant difference between the two echinocandins and azoles (RR:0.84, 95% CI 0.71-1.01). However, subgroup analysis showed that micafungin is more effective than azoles for prevention of IFIs (RR:0.78, 95% CI 0.61-0.98), and caspofungin has similar effects with azoles (RR:0.94, 95% CI 0.72-1.23). (Figure 1 ).

Conclusion: This meta-analysis shows that micafungin might be a better choice for IFI prophylaxis in patients with hematologic malignancies. However, the effectiveness of caspofungin might be mitigated due to the smaller sample size in the studies. The data of safety is still insufficient and further RCTs are required. Consider the cost and efficacy, micafungin could be suggested as the first choice of IFI prophylaxis.

The distribution and drug susceptibility of Candida species in cancer patients with candidemia in a cancer center Yi-Hsuan Chen, Ting-Mei Liu. Department of Pathology and Laboratory Medicine Koo Foundation Sun Yat-Sen Cancer Center, Taiwan

Background: Candida is the most common cause of invasive fungal infections in immunocompromised cancer patients. The knowledge of antibiotics susceptibility of different Candida species may help clinicians in treatment planning. Methods: We retrieved the data of the Candida species in patients with candidemia during the period from 2014 to 2016. The yeast identification methods included window test, CHROMagar Candida, Urease, Pellicle, API-20 C AUX, and VITEK 2 YST. We also performed yeast drug susceptibility test by ATB FUNGUS 3. The Interpretive guidelines for in vitro susceptibility testing of Candida species were applied according to the CLSI M27-S3. Results: There were 50 colonies of Candida species in candidemia. C. albicans [n = 21 (42%)] was the most commen species, followed by C. parapsilosis [n = 10 (20%)], C. glabrata [n = 7 (14%)], C. tropicalis [n = 6 (12%)], C. guiliermondii complex [n = 2 (4%)], C. krusei [n = 1 (2%)], C. inconspicua [n = 1 (2%)], C. haemilonii [n = 1 (2%)], C. lusitaniae [n = 1 (2%)]. We also performed anti-fungal drug susceptibility tests including Flucytosine, Amphotericin B, Voriconazole, Fluconazole, and Intraconazole for 82 Candida colonies isolated from aseptic specimens. C. albicans (n = 42) and C. tropicalis (n = 9) was susceptible to Intraconazole with sensitivity of 98% and 67%, respectively. Both species were susceptible to other antifungal agents. However, C. glabrata (n = 12) was indiscernibly susceptible to Fluconazole and Intraconazole but had susceptible rates to Amphotericin B, Flucytosine, Voriconazole with 100%, 92% and 83%, respectively. All of C. parapsilosis (n = 12) and C. lusitaniae (n = 1) were susceptible to all antifungal agents. C. krusei (n = 3) was a species with intrinsic resistance to Fluconazole. The susceptibility of C. krusei to Flucytosine was intermediate, and had the susceptible rates of 100% and 67% to Voriconazole and Amphotericin B, respectively. C. guiliermondii complex (n = 2) were susceptible to all anti-fungal agents except intermediately susceptible to Intraconazole. C. Inconspicua (n = 1) was intermediately susceptible to Fluconazole and Intraconazole and susceptible to other antifugal agents. Conclusion: C. albicans remains the most common species found in patients with Candidemia, which is similar to reports in other countries. Nevertheless, the incidence is decreasing to be less than 50%. C. tropicalis is not the second common species, but C. parapsilosis or C. glabrata. The knowledge of the distribution of anti-fungal drug susceptibility of different Candida species in the hospital may help clinicians in prescription planning.

Ocular candidiasis in patients with candidemia Takashi Ueda 1 , Yoshio Takesue 1 , Kazuhiko Nakajima 1 , Kaoru Ichiki 1 , Kaori Ishikawa 1 , Yasunao Wada 1 , Toshie Tsuchida 1 , Yoshiko Takahashi 2 , Mika Ishihara 2 , Takeshi Kimura 2 . 1 Department of Infection Control and Prevention, Hyogo College of Medicine, Nishinomiya, Japan, 2 Department of Pharmacy, Hyogo College of Medicine, Nishinomiya, Japan Background: Ocular candidiasis is a major complication of candidemia. At our institution, ophthalmological examinations are routinely performed in patients with candidemia based on the bundle approach (Takesue Y et al., JAC, 2015; 70: 587-93.) . Methods: The incidence of ocular candidiasis among patients with candidemia, the timing of diagnosis, the vitreous body involvement rate, antifungal use, and clinical outcomes were reviewed in non-neutropenic adult patients with ocular candidiasis that were reated between April 2007 and December 2016. Results: Among 120 candidemia patients, 116 underwent ophthalmological examinations, and ocular candidiasis was detected in 28 patients (24.1%). Twenty-one patients were diagnosed at the initial examination (median 4 days from the culture of the blood sample positive), and 7 further patients were diagnosed at follow-up examinations (median 11 days). Vitreous body involvement was detected in 9 of 28 patients (32.1%). Among the 24 conscious patients, 9 complained of visual abnormalities (37.5%). Candida albicans was the most commonly detected causative agent in the patients' blood specimens, and the C. albicans isolation rates of the patients with and without ocular candidiasis differed significantly (92.9% vs. 31.1%, P < 0.001). The patients with ocular candidiasis had significantly higher β-D-glucan values (278.4 ± 212.6 pg/mL vs. 132.6 ± 197.6 pg/mL, P = 0.001). Treatment success (an improvement in the patient's clinical data and the resolution of ocular disease confirmed by an ophthalmological examination) was achieved in 20 of 28 patients (71.4%) and visual recovery was obtained in 7 of 8 patients (88.9%). The 28-day mortality rate was 7/28 patients (25%). Fluconazole, voriconazole, and liposomal amphotericin B were administered as initial antifungals to 10, 4, and 14 patients, respectively. As alternative selection, voriconazole was used in 4 and liposomal amphotericin B in 6 patients.

Stepdown oral therapy was introduced in 9 of 28 patients (32.1%). None of the patients received intravitreal injections of antifungals or underwent vitrectomy. The mean duration of intravenous antifungal therapy was significantly longer in patients with vitreous body involvement compared with patients with Chorioretinitis (42.8 ± 12.4 days vs. 31.2 ± 13.7 days, P = 0.047). Conclusion: The incidence of candidiasis (24.1%) was concomitant with previous reports. However, a higher frequency of vitreous body involvement was experienced. In addition, a considerable number of patients who were not diagnosed with ocular candidiasis at the initial examination exhibited the condition during follow-up ophthalmological examinations.

Outcome of fluconazole dosing on candidemia caused by different Candida species Ching-Yao Shih*, Hui-Shan Tsai, Wen-Liang Lin, Feng-Ying Yeh. Department of Pharmacy, National Cheng Kung University Hospital, Tainan, Taiwan Background: Fluconazole was amongst the first line of antifungal agent in invasive candidiasis in Taiwan. Different Candida species showed various susceptibilities to fluconazole which dosedependent sensitive of fluconazole posed concern on its efficacy. Methods: This retrospective study included 194 adult patients with candidemia who has treated with antifungal agents at least 48 hours from July 1, 2012 to June 30, 2014. We recorded patient's medical history and laboratory data during treatment. Mortality 14 days after the end of treatment and pathogen eradication rate were primary and secondary endpoint respectively. Logistic regression and Mantel-Haenszel test were utilized to analysis the variant that might influence clinical outcome. Results: Mortality rate and pathogen distribution were Candida albicans (30.7%, 45.4%), C. tropicalis (37.2%, 22.2%), C. parapsilosis (25.9%, 13.9%), C. glabrata (28.0%, 12.9%) and other Candida species (27.3%, 5.7%) respectively. Overall mortality rate was 30.9%. C. tropicalis infection has higher mortality rate, but no statistical significant. Overall eradication rate was 79.4%. C. albicans infection was higher proportion to clear (86.4%, median: 8 days) then C. glabrata (68.0%, median: 9 days) ( p = 0.56). Fluconazole was used as initial treatment in almost all case (96.4%) and 80.9% of fluconazole dose adjusted by renal function and weight accordingly, but only 37.1% of them gave loading dose. Logistic regression found that continue treatment with another 2 weeks after documented clearance was significant associated with survival rate (Odds ratio: 5.6, 95% CI: 2.18-14.38). However, to give fluconazole loading dose and to keep maintenance dose of fluconazole more then 6 mg/kg were showed no significant associated to mortality rate and pathogen eradication. Conclusion: In this study, adequate treatment course of fluconazole is associated with patient survival. Even if different Candida species do not reveal poor clinical outcome, but a new treatment strategy of candidemia for increasing fluconazole-resistance Candida pathogen is necessary.

Pulmonary fungal infection due to a Basidiomycete species (Ceriporia lacerata) in a HIV patient Lih-shinn Wang 1 , Chia-ching Jackie Wang 5 , Huei-jen Chao 3 , Siao-ru Lin 3 , Chih-bin Lin 2 , Yung-hsiang Hsu 4 Background: Basidiomycetes are rarely reported as a cause for invasive respiratory infection, despite airborne spores being abundant in the environment. Immunocompromised individuals are more susceptible. Methods: A 27-year-old MSM was admitted for 2-week history of FUO in Aug., 2013. He complained of fever, dizziness, sore throat, purulent sputum, cough, and myalgia. He denied abdominal pain or diarrhea. He received antipyretics, which did not resolve the fevers. He had been treated with entecavir for HBV infection for 4 years. He was found to be HIV-infected on admission. His CD4 was 42 cells/uL, and he was started on zidovudine, lamivudine, and nevirapine. Chest X-ray (CXR) revealed bilateral pulmonary infiltrates. He also had reactive RPR and positive TP-PA in both the serum and CSF. Intravenous penicillin treatment was administered for neurosyphilis, and the patient partially improved with sulfamethoxazole/trimethoprim treatment. Then the patient developed generalized erythematous patches and plaques, which was attributed to sulfamethoxazole/trimethoprim or nevirapine. Nevirapine was changed to Kaletra, and sulfamethoxazole/trimethoprim to clindamycin. He was diagnosed with PCP by sputum PCR, but CXR did not improve. Toxoplasma serology and IHA for ameba were negative. Results: The patient was again febrile on hospital day 12 and blood PCR confirmed CMV infection. Whole body Gallium scan showed increased uptake of bilateral lungs without other focus of infections. He became afebrile after initiating intravenous ganciclovir treatment, but still CXR did not improve. Chest HRCT revealed mixed alveolar and interstitial lesions of bilateral lungs, and patchy consolidation of hilar regions. Pathology of bronchoscopic biopsy revealed fungal infection, and amphotericin B (AmB) was added. The patient could not tolerate conventional AmB due to rigors and fevers, and we replaced it with the liposomal formulation. After 2 weeks of liposomal AmB treatment, his antifunal therapy was changed to oral itraconazole treatment. Culture of bronchoalveolar lavage grew a filamentous fungus ( Figure) , and PCR confirmed the isolate as a Basidiomycota sp., Ceriporia lacerata. The patient was treated with oral itraconazole for 6 months. No recurrence was found after a 3 year follow-up. Conclusion: Fungal infections account for a large number of AIDSindex diagnoses and complicate the course of many patients with HIV infection. Clinical suspicion and broader recognition of diseases caused by filamentous fungi among clinicians and microbiologists would improve therapeutic experience and selection of better treatment strategies. Background: Candida glabrata species complex includes Candida glabrata sensu stricto and two major cryptic species, Candida nivariensis and Candida bracarensis. Distinction of these species is relevant for epidemiological purposes and for antifungal management. Phenotypic identification methods frequently misidentify these species as ""C. glabrata."" Hence their identification depends on molecular methods, or by MALDI-TOF MS. Although DNA sequencing of the fungal rDNA internal transcribed spacer (ITS) region or the D1/D2 region of the 28S rRNA gene is accurate, molecular analysis is time-consuming and expensive. In the present study, we sought to examine the ability of the Vitek MS and Burker MS systems to differentiate between the species within the C. glabrata complex, using a variety of software including ClinProTools. Methods: Two commercial MALDI-TOF MS systems (Vitek MS system [bioMérieux] and the Bruker MS system [Bruker Daltoniks]) employing the Vitek MS RUO and Bruker ClinProTools programs, respectively were evaluated for the identification of 33 isolates of C. glabrata complex (17 C. glabrata, 14 C. nivarensis, 2 C. bracarensis). Results: C. glabrata sensu stricto was identified correctly by both systems with distinct principle components compared with the two cryptic species. All C. nivariensis and C. bracarensis could not be identified to species level by the Vitek MS v2.0 IVD and Bruker Biotyper MS v3.1, but were all correctly identified by the Vitek MS RUO. The generic algorithm model from ClinProTools software showed 100% recognition capability and cross validation for the discrimination of C. nivariensis and C. bracarensis. Spectra peak statistics revealed that five markers (3292.47Da, 4152.64Da, 6245.75Da, 6585.15Da and 7304.68Da) can reliably distinguish between C. nivariensis and C. bracarensis, with area under the curve (AUC) values of 0.928, 0.9998, 1, 1 and 1, respectively. A small ""inhouse"" Bruker spectral database was established incorporating spectra of two clinical isolates representing C. nivariensis and C. bracarensis identified in this study. After complementation with the ""in-house"" database, all the remaining 14 C. nivariensis and C. bracarensis isolates were correctly identified to species level (score >2.00). Conclusion: MALDI-TOF MS enabled rapid and reliable identification of C. glabrata complex isolates. The use of the Vitek MS RUO system, Bruker ClinProTools software and the addition of MSP to Bruker MS databases representing the local diversity of isolates assisted with achieving differentiation of cryptic species within C. glabrata complex. Background: The Gram-positive bacterium Enterococcus faecalis and the fungus Candida albicans are both common opportunistic pathogens in human body, such as the oral cavity and gastrointestinal tract. An estimated 27-56% of nosocomial C. albicans bloodstream infections are polymicrobial. The top three most commonly co-isolated bacterial species were Staphylococcus epidermidis, Enterococcus spp., and Staphylococcus aureus. Previous in vitro studies showed that E. faecalis could inhibit the C. albicans hyphal morphogenesis and reduce its biofilm formation, furthermore co-infection of E. faecalis and C. albicans in mice resulted in less mortality than infection with either species alone. However, in clinical mixed bloodstream infection associated with Candida and Enterococcus is rare but it causes with worse clinical outcomes in immune compromise cancer patient. The interaction between these two microbes in mixed-type infection in bloodstream, i.e. the planktonic form is investigated in this study. Methods: The epidemiology of bloodstream infection with E. faecalis, C. albicans, and mixed infection cases collected from Chi-Mei hospital were analyzed. RPMI-1640 medium was used to co-culture the two microbes for testing their interaction in growth. An in vitro biofilm assay was used to study the influence of yeast form C. albicans mixed infection with E. faecalis. The drug susceptibilities against antibiotics, ampicillin and gentamicin, and antifungals, caspofungin and amphotericin B, of these microbes will be analyzed to check their phenotypic changes that associate with virulence. Results: In Candida-associated mixed bloodstream infection, the incidence of infection cases with E. faecalis is significantly higher than any other gut bacteria. Similar to previous reports, the decrease in biofilm formation in C. albicans was proved by an in vitro biofilm assay. The resistance to antibiotic was enhanced in E. faecalis by co-cultured with C. albicans. Conclusion: The interaction of E. faecalis and C. albicans might alter the antibiotic susceptibility of E. faecalis in patients, no matter presenting in biofilm or planktonic forms. A special attention must her. Generally, acute hepatitis caused by LTBI treatment with isoniazid is quite rare. At present, the exact mechanism of Isoniazid related hepatotoxicity is still unclear. It might be related to age, slow acetylation to toxic Hydrazine metabolites, or CYP450 2E1 homozygous wild type (CYP2E1c1/c1 genotype). Conclusion: For adverse events of acute hepatitis induced by LTBI treatment with isoniazid, we recommended to trace liver function 1-2 times every week closely, especially those patients whose liver function fail to recover after quitting medication. In conclusion, during the treatment, paying highly attention to the patient's clinical symptoms with closely tracking the biochemical tests of blood, treating the adverse events in time, are all important to ensure the completion of treatment and achieve the purpose of cure.

Spatio-temporal analyses on mild and severe influenza illness and their association with air pollution in Taiwan Pin-wei Shih 1 , Ta-Chien Chan 2 , Chwan-Chuen King 3 *.

Institute of Epidemiology and Preventive Medicine, National Taiwan University (NTU), 2 Research Center for Humanities and Social Science, Academia Sinica, 3 Institute of Epidemiology and Preventive Medicine, College of Public Health, NTU Background: The epidemic of influenza in Taiwan usually varies in different years. For example, in early 2016, the epidemic was so severe that it led to massive crowd in emergency rooms of many teaching hospitals, shortage of human resource in intensive care unit (ICU) and insufficient availability of ECMO. On the other hand, since the exchanges of cross-strait between Taiwan and mainland China becomes more frequent, the diversity of influenza virus has increased. Therefore, we should understand the spatio-temporal distributions of mild and severe cases of influenza and their associated factors. Methods: We use the software, Satscan, to discover the areas that suffer from high incidences of influenza-like illness (ILI). Then we went further to understand the epidemiology of the illness in these areas to figure out the risk factors, such as weather conditions, population density, air pollution, or poultry farms density. Finally, we confirmed the strength of the association between the risk factors and the disease by using the regression model. Results: Air Quality Index (AQI) refers to the impact of daily concentrations of six substances including O 3 , PM2.5, PM10, CO, SO 2 and NO 2 . AQI can imply the degree of effect of the air pollution on our health. It is also well-documented that the respiratory system becomes more sensitive during the period of poor air quality and in addition to the condition of local weather. Such an interaction of weather and air pollution may help us observe that cases of respiratory illness increase after certain time lags. Therefore, the higher incidences of influenza-like illness would occur more frequently in those time periods with poor air quality, and those areas with severe problems of air pollution. Conclusion: We hope that the results from this study involving AQI, local weather, and more implications from epidemiology of mild and severe influenza cases altogether can provide an early warning for harm reduction of influenza-like illness and implementing the most appropriate preventive measures.

Comparative assessment of guidelines for individuals with a risk of infective endocarditis Takahiko Fukuchi. Division of General Medicine, Jichi Medical University Saitama Medical Center Background: Infective endocarditis is a severe disease with high morbidity and mortality rates. Health care providers worldwide should consider means to prevent this disease; however, guidelines related this topic differ among industrialized countries. The author makes close examination of these guidelines. Methods: The guidelines of the United Kingdom's National Institute for Health and Care Excellence (NICE) are the most notable, but do not contain prescription recommendations for prophylactic antibiotics, while the European Society of Cardiology (ESC) recommends antibiotics for only the highest-risk individuals, and only during dental procedures. The American Heart Association (AHA) recommends antibiotics for the highest-risk individuals during dental, urological, gastroenterological, dermatological, and other invasive procedures. In contrast, the Japanese Society of Cardiology recommends antibiotics for a broader range of individuals including those with lower risk such as during procedures for almost all congenital and valvular heart diseases. In addition, the recommendation in the NICE guideline has been changed slightly because an English population-based study revealed higher incidence of endocarditis after the introduction of an aggressive new guideline in 2008. One reason for the many differences among guidelines might be that no randomized control trial has been conducted on the prophylactic effect of antibiotics for individual at risk of endocarditis; therefore, guideline formulating committees do not do not have access to concrete evidence. In terms of causative pathogens of endocarditis, staphylococcal endocarditis, which is unpreventable by oral amoxicilline, has increased worldwide. Findings from studies in East Asia, including Japan, show that up to one third of endocarditis cases are caused by Streptococcus spp. Moreover, as states in all major guidelines, oral hygiene is more important than dental procedures for preventing Streptococcus infection. Results: A historical cohort study reveals indirect evidence for prevention of endocarditis with amoxicilline, which indicates that the UK guidelines might be insufficient. The Japanese guidelines implemented standard practices driven more by the local political climate than scientific evidence. On the basis of current evidence, neither the AHA nor the ESC guidelines can be considered superior to the other. Conclusion: Primary care physicians and dentists should consider prophylaxis against endocarditis during dental procedures as recommended AHA or ESC.

Establishing assessment of chemotherapy order templates side effects Hui-Ping Sun, Huang-Pin Liang. Tainan Municipal Hospital Background: The side effect of chemotherapy got worse with doses accumulation over time and length of chemotherapy. Nurse staff played a vital role in maintaining patients' quality of life and effectiveness of treating, preventing or reducing distressing side effects due to chemotherapy and reinforcing knowledge related to chemotherapy and self-care behavior. Currently, staffs often focused on possible side effects of separate chemotherapy drug. However, chemotherapy templates varied from the types of cancer and the side effects depended on the type of chemotherapy being offered. The purpose of the study was to establish assessment form of chemotherapy order templates side effects.

Methods: Retrospective chart review was performed to collect six chemotherapy order templates for breast cancer, colorectal cancer, and hepatocellular carcinoma which were the top three most common cancers at a regional hospital. The side effects regarding every chemotherapy order template were assessed and identified as the common and specific side effects based on literature review of reaction to chemotherapy.

Results: Offering information about potential side effects to the treatment before starting chemotherapy could reduce the discomfort and deterioration in quality of life. Furthermore, the process of information-giving not only improved nursing care quality but reinforced strong nurse-patient relationship. Conclusion: The chemotherapy side effects in patients are sometimes different from those recorded in literatures so staff can record the chemotherapy side effects on patients through the assessment process. After analyzing the assessment form, the data can be used to establish guideline for education of the common side effects of chemotherapy order templates.

Reduce the incidence of needlestick injuries in clinical staff Yi Fang Hsieh 1 , Ya Fang Wang 1 , Wen Chuan Lin 2 , Yong Ching Lau 3 . Conclusion: Needlestick injury as long as a second, that is able to change the life of a medical worker, through education to strengthen awareness, reduce the staff due to lack of habit or lack of cognitive use of safety needle, Expect to effectively prevent the injury and blood, body fluids exposure events, training every clinical medical staff to develop a safe culture.

Prevalence and risk factors for extended spectrum betalactamase producing Escherichia coli in patients with urinary tract infection Minori Matsumoto, Kazushi Tanaka, Yoshizumi Takechi.

Background: The aim of this study was to determine the prevalence and risk factors of extended spectrum beta-lactamase (ESBL)-producing Escherichia coli (E. coli) in patients with urinary tract infection. Methods: One hundred and four of ESBL producing E. coli were isolated from urine between 2016 April and 2017 March. The prevalence of ESBL-producing E. coli in patients with urinary tract infection was examined. Univariate analyses were performed with gender, age, inpatient status, previous hospitalization, recent history of urinary catheterization, recent exposure to specific antibiotics, and past history of urogenital organ operation as risk factors for the emergence of ESBL-producing E coli. Then, multivariate analysis was performed with all significant variables. Results: In patient with urinary tract infection, the antimicrobial susceptibility of E. coli to each of the oral antibiotics, Cefcapene Pivoxil, levofloxacin and Sulfamethoxazole/Trimethoprim was 100%, 84%, and 91%, respectively, and the prevalence of ESBLproducing E. coli was 38.1%. Conclusion: The overall prevalence of ESBL-producing E. coli was 38.1% and the risk appeared to be increased in cases with a previous hospitalization, inpatient status.

Incidence and mortality trends of infections among sepsis patients: a nationwide study on 23 million residents Meng-tse Gabriel Lee 1 , Shih-Hao Lee 1 , Yueh-Sheng Chen 2 , Wan-Chien Lee 1 , Shy-Shin Chang 3 , Szu-Ta Chen 4,5,6 , Shyr-Chyr Chen 1 , Chien-Chang Lee 1 . Background: Estimation of sepsis source of infections and mortality based on a nationwide database with linkage to death certificate is lacking. We aimed to determine the trends of the source of infections and outcome of sepsis for the entire 23 million Taiwanese population. Methods: Using the entire health insurance claims data from the National Health Informatics Project, we identified patients hospitalized with sepsis and their sources of infection. Sepsis cases were identified using validated ICD-9CM codes conforming to the sepsis-3 definition, and the 30-day all-cause mortality was obtained by linkage to a death certificate. enzyme-containing microcapsules (MC) were tested to remove three the SAs in sediments. Methods: The remaining SAs were analyzed by high-performance liquid chromatography (HPLC) with a photodiode-array detector. The extracellular enzymes were extracted from the spent mushroom compost (SMC) of Pleurotus eryngii from a mushroom cultivation farm. The bacterial community were analyzed by next-generation sequencing.

Results: Degradation of three SAs was enhanced by adding MC to the sediments. The order of SA degradation was SMX > SDM > SMZ. Different bacterial communities from different sediments exhibited different SA degrading rates. SA-degrading bacterial consortiums increased and adapted during four time re-additions of SAs and could efficiently degrade SAs for 75 days. 13 bacterial genera (Achromobacter, Acinetobacter, Alcaligenes, Aquamicrobium, Arthrobacter, Brevundimonas, Flavobacterium, Methylobacterium, Microbacterium, Oligotropha, Paracoccus, Pseudomonas and Rhodococcus) were major bacterial communities involved in SA degradation in the sediment. Conclusion: Microbial degradation is a promising process for SA removal in the sediments. The addition of MC in the sediments enhanced SA degradation. MC could be used for SA removal in the sediments.

Chin-Te Lu. Section of Infectious Disease, Department of Internal Medicine, Lo-Hsu Medical Foundation, Lo-Tung Poh-Ai Hospital

Background: Scrub typhus, a vector-borne zoonosis caused by Orientia tsutsugamushi, is an important endemic disease in Taiwan. The seasonal occurrence of scrub typhus varies according to the climates in different countries, and the disease occurs more commonly during rainy seasons. Forest clearings, riverbanks, and grassy regions provide optimal conditions for mites. I-lan country is in the northeastern of Taiwan with unique northeast monsoon climate region. By the record of Notifiable Infectious Diseases in one community hospital of I-Lan during the year 2012-2016, 19 confirmed cases, 16 cases (84%) occurs in winter (12 cases in January, 2 case in February, 2 case in December). For the finding of winter type in our hospital, we collect and analysis the cases data of Taiwan area including I-Lan country. Methods: For this study, monthly data of cases number were obtained from the Notifiable Infectious Diseases Statistics System and Infectious Diseases Database at the Taiwanese Center for Disease Control. The scrub typhus is a 4 th grade notifiable disease, blood samples from patients with suspected scrub typhus were collected and sent to the CDC for laboratory confirmation. Confirmed diagnosis for scrub typhus are based on a positive real-time polymerase chain reaction test, or a 4-fold increase in OTspecific immunoglobulin M or immunoglobulin G antibody of paired sera by using an indirect immunofluorescence assay technique. Epidemiological data were obtained from 2001 through 2016 to analysis the difference epidemiology of I-lan and Taiwan.

Results: During the years between 2001 and 2016, Confirmed cases totaled to 6,424 cases in Taiwan area. Focus in I-Lan County, 121 cases during past 16 years, and 41 cases were found in In aboriginal area (Datong and Suao Township) and 80 cases in plain townships. The average number of cases was about 400-500 per year in recent 5 years of Taiwan area. The data of incident months of the confirmed cases show, scrub typhus cases occurs throughout the entire year with a rapid increase in numbers in April and May and reaches its peak in July in the data of Taiwan area except I-Lan County which reach its peak in December to January of next year. In Taiwan area, most cases occurs in summmer (41.7%) and fall (27.5%), but in I-Lan County most cases occurs in Winter (62.0%) with the peak in January(38.8%). The seasonal variation of scrub typhus cases in I-lan, different from those reported in other area of Taiwan may due to the unique northeast monsoon climate which needed further studies.

In comparison with the summer type of other area of Taiwan, our study revealed the unique epidemiologic characteristics winter type scrub typhus in I-Lan country. These findings may provide valuable information in the diagnosis, treatment, and prevention of the disease in the local area. Background: To determine whether intestinal Angiostrongylus cantonensis antigens can induce protective immunity in rats, gut antigens prepared from female adults (FAGP) and somatic antigens prepared from both male (MA) and female (FA) adult worms were used to immunize rats. Methods: Rats were immunized twice with MA, FA, or FAGP antigens and then challenged with 50 third-stage A. cantonensis larvae, and different readouts were used to monitor protective immunity. Additionally, protein profiles of MA, FA, and FAGP extracts were analyzed and characterized by immunodetection methods. Results: A 15% reduction in fifth-stage larvae from brains and a 14% reduction in adult worms from pulmonary arteries were observed in rats immunized with FAGP compared to controls. However, there was a >50% reduction in rats immunized with MA or FA. The lengths of larvae and adults recovered from FAGP-immunized rats were shorter than those recovered from other groups. The number of first-stage larvae recovered from fecal material in FAGP-immunized rats was significantly reduced. Additionally, FAGP induced the highest splenocyte proliferation. Serum IgG titers were not directly correlated with protective immunity. An 84 kDa gut membrane protein was strongly recognized by anti-FAGP antibodies, which may be important in anti-A. cantonensis immune responses. Conclusion: We demonstrated that immune responses induced by FAGP reduced the growth, development, and reproduction of A. cantonensis in subsequent infections. While the possibility of using FAGP combining with MA or FA antigens as a multi-function vaccine in immune protection against A. cantonensis needs to be further elucidated, we hope that it provides a novel strategy for this parasite vaccine development.

2). Then, we checked whether they received at least twice measlescontaining vaccine from the record of the infection control office. Results: The number of subjects was 1,084. In test 2, 296 people (27%) were LMI (A). Of them, 165 (15%) fulfilled the criteria in test 1, so it was regarded as secondary vaccine failure. 54 (5%) had two vaccination records (B). There were 26 persons who had both (A) and (B), and the number of vaccine subjects was 270. When the vaccination records were kept in the infection control office, assuming the examination cost was $15 per person and the vaccination cost was $70, the expenses of at least $2,210 could be reduced. Conclusion: It is speculated that the measles antibody decreases markedly year by year. In order to take measures against nosocomial infection, HCWs need to accurately manage the vaccination history twice, not once. Managing the vaccination record in the infection control office is more reliable and more economical than the procedure that repeats the test.

Neuraminidases NanA, NanB and NanC based vaccine to prevent pneumococcal infection Rajendra-Prasad Janapatla 1 , Mei-Hua Hsu 1 , Cheng-Hsun Chiu 1,2 *. Background: Routine immunization with the pneumococcal conjugate vaccines (PCV7/PCV10/PCV13) increased the incidence of non-vaccine serotypes in community and clincal isoaltes, this increased the need for developing pneumococcal protein vaccines that have broader serotype coverage. Since neuraminidases are major virulence proteins they are potential targets for both vaccines and small molecule inhibitors. In this study, we explored the possibility of combining the three neuraminidases together as vaccine antigens to devise a universal protein vaccine to prevent systemic pneumococcal infection. Methods: To evaluate the degree of protection of NanA, NanB and NanC protein antigens against S. pneumoniae intravenous challenge, individual or combination of neuraminidases NanA, NanB and NanC (10 μg each) were injected subcutaneously. To confirm the presence of anti-neuraminidase antibodies in the antisera, we performed enzyme-linked immunosorbent assay (ELISA). S. pneumoniae cell suspension was used to infect immunized ice by tail vein injection. The antisera against individual neuraminidase (NanA, NanB or NanC) were also tested for their inhibitory effect on the activity of neuraminidases. If a specific antineuraminidase serum neutralizes neuraminidase activity, the exposure of Thomsen-Friedenrich antigen (TA) antigen on RBC cells, and the value of fluorescence intensity will be reduced. Results: We observed high (4-6 logarithm fold) IgG class antibody titres against NanA, NanB, and NanC; this indicates that all the three neuraminidases are immunogenic and ideal as antigens for immunization. We found that the antisera against only full length NanA and NanA-Lectin efficiently inhibited (>90%) neuraminidase activity. However, antisera against lectin domains of NanB and NanC were sufficient to neutralize their enzyme activity. Immunization with combination of NanA, NanB and NanC caused significant increase in survival rates relative to PPV23 vaccine group and negative control, in serotype 3, 6B, 14, 15B, 19A and 23F survival was 60%, 60%, 20%, 60%, 40% and 40%, respectively. Conclusion: In future formulations of pneumococcal virulence proteins and adjuvants will be needed to develop serotype independent protection against pneumococcal infections. We recommend the inclusion of three pneumococcal neuraminidases in future protein vaccine formulations to prevent IPD.

PS 532 PCV13 serotypes are still important causes of pediatric invasive pneumococcal disease in Portugal after more than a decade of conjugate vaccine Catarina Silva-Costa 1 , Andreia Horácio 1 , Sandra I Aguiar 1 , Joana P Lopes 1 , Maria J Brito 2 , Mario Ramirez 1 , Jose Melo-Cristino 1 . Results: The number of isolates recovered in each epidemiological year was constant and similar to that of 2011-12, except 2015-16 but this may be due to the fact that we are still receiving isolates from participating laboratories. Overall, 37 different capsular types, as well as non-typable isolates were detected. Although most isolates expressed capsular types included in PCVs, serotypes not included in any PCV formulation accounted for a significant fraction of the isolates (50%, n = 111). Among these, serotypes 10A, 24F, 8, 15B, 12B were the most frequent, represented by at least 10 isolates each. Regarding PCV serotypes, isolates expressing serotype 14 (n = 25), serotype 1 (n = 18), serotypes 3 (n = 14) serotype 7F (n = 13), 6B (n = 12) and 19A (n = 10) were the most frequently found. Overall, 23% of the isolates were penicillin non-susceptible. Resistance to erythromycin was expressed by 22% of the isolates and simultaneous expression of erythromycin resistance and penicillin nonsusceptibility was found in 14% of the isolates. Conclusion: PCV13 serotypes are still expressed by a considerable proportion of isolates responsible for IPD. The data presented here emphasizes the potential role of PCV13 in the NIP in further diminishing pediatric IPD.

vaccination programme. We have previously described our methodology 1 and aim to present results of surveillance data from 2016. Methods: All paediatric cases of febrile convulsions, Encephalitis/ Meningitis/Encephalopathy, Kawasaki Disease, thrombocytopenia, lymphadenitis, Guillain-Barré Syndrome, intussusception admitted to KKH from January-December 2016 were prospectively screened and evaluated for any possible associations with vaccination. Clinical charts were reviewed and vaccination histories extracted from the National Immunisation Registry. A standardised criteria, in accordance with the World Health Organization Uppsala Monitoring Centre (WHO-UMC) definitions was used to identify cases with higher probability of being associated with vaccination. The criteria included duration between disease onset and date of prior vaccination as well as excluded cases with alternative diagnosis based on laboratory infective screens. All event rates were generated based on per 1000 patients admitted.

Results: In 2016, there were 6.38 possible vaccine-related monitored events per 1,000 admitted patients, compared with 6.803 per 1,000 patients in 2015. There was an increase in the rate of Meningitis from 0.07 per 1000 patients in 2015 to 0.50 per 1000 patients in 2016. The rates for febrile convulsions, Kawasaki Disease, thrombocytopenia and lymphadenitis were lower in 2016 compared to 2015. There were no cases of GBS detected that were associated with recent vaccination. Two cases of intussusception following Rotavirus vaccination (Rotarix) were identified. One case occurred 12 days after receipt of dose 2 and the second, 10 days after receipt of dose 1.

The findings of the HK-INSPIRE active childhood vaccine safety surveillance in KKH, 2016 provides reassurance on the safety of the childhood vaccination programme in Singapore. No significant safety concerns were detected for most events of interest. An increase in risk of intussusception after receipt of rotavirus vaccination is known to be about 1 in 40,000. Preliminary analysis of the meningitis safety signal suggested that it may be attributable to a wider total increase in aseptic meningitis admissions and unlikely related to vaccines.

Prevalence of potential drug-drug interactions in HIV-infected adult on antiretroviral drugs at Faculty of medicine Vajira hospital, Navamindhradhiraj university Kittisak Pholtawornkulchai 1 *, Sirote Luengsupabul 2 . Background: Presentation with multiple tumor is rare and differential diagnosis is important. A young man presented with multiple tumors was admitted and the tumor biopsy revealed smooth muscle tumor. Methods: A case report and review of literature was performed.

Results: EBV infection is associated with many tumors and regarded as a tumor virus. With increasing number of patients with HIV infection, the incidence of smooth muscle tumors is increasing. Such patients may present with variable aggressiveness and clinical outcomes and may exhibit a more favorable prognosis compared to conventional leiomyosarcoma. Conclusion: As medicine advances, the presence of multiple smooth muscle tumors in HIV-infected patients should be alert. Vigilance and familiarity with distinctive tumor presentations can contribute to early diagnosis.

Aminoacid polymorphism in Gag p6 domain alters Galectin-3 promoting effects on viral replication and budding of HIV-1 CRF07_BC isolates from injecting drug users (IDUs) in Taiwan Background: Bell's plasy is an acute peripheral facial nerve affection.Severe pain suggests herpes simplex or zoster infection and may precede a vesicular eruption or/and progression to Ramsey -Hunt syndrome.In half of the herpes zoster infections,vesiculationnot necessarily appears or may be delayed (zoster sine herpete). New evidence (detected viruses with PCR within the facial nerve) implies that Bell's palsy is caused by latent herpes viruses. Methods: We clinically evaluated 10 patient, 17-65 years old, withpheripheral facial palsy. The patients are clinically assessed for the severity of facial nerve damage and scored with the House-Brackmann facial nerve grading system. They were sreen with routine blood test,thyroid hormone evaluation,serology for herpes Zoster and Borreliaburgdoferi; MRI of the head with gandolinium; was requested to have nerve conduction studies. Results: 6 patient were female, 4 male, 5 patient are scored with grade II, 3 with grade III, 1with grade I and 1 with grade IV at House -Brackmannscale. Six patients were positive for herpes Zoster. None of them had vesicular eruption. In eight patient was normal MRI, in two of them,internal acoustic meatalsegment of the facial nerve was enhanced only on the affected side. At the conduction studies four had demyelination of the nerve, two severe axonal damage(with Zoster positive antibodies and have eanhancment at the internal acoustic meatal segment of the facial nerve in the affected side) Patients with Yosterpositive and Zoster negative were treated with corticosteroid and antiviral therapy. In five patient with facial nerve damage was totally recovered, in two significant improvement, three of them remainwith severe sequelae. The recuperation was greaterat the patient that was used corticosteroids and antiviral therapy (acyclovir, valacyclovir)even in them with Zoster negative antibodies. The patient with severe sequelae were from the group with Zoster ntibodies,with enhancement in the MRI and with axonal damage at the conduction studies. Conclusion: New evidences ipliestha Bell's palsy isa caused by latent herpes viruses. Considering zoster even without eruption and start antiviral therapy, is important sinceit is thought to be the cause of Bell's palsy in quite a number of cases. Although the number of cases at this study are small to have significant statistically conclusion. Background: Several new direct antiviral agents (DAAs) against the HCV virus are approved and marketed. The combination of DAA with SOC ( pegylated interferon and ribavirin) is only recommended to patients with tolerance to interferon. Therefore several IFN-free treatments are now considered, including NS3/4A protease inhibitors, RNA-dependent RNA polymerase inhibitors, and NS5A inhibitors. TG-2349 is a novel protease inhibitor of the acylsulfonamide class, and is orally active with significant plasma and liver exposures from an once-a-day dosing regimen for CHC. DAG181 is a NS5A inhibitor. It has superior potency in cell-based assays. This in vitro study examined the potential of combining TG-2349 with DAG181 to reduce HCV replicon colonies. Methods: Cells containing a genotype 1a or 1b sub-genomic replicon carrying the neo gene were plated in 6-well plates and treated with either one or two compounds at 1, 10 or 20 x EC 50 . Culture medium was changed twice a week and cells were split at 95% confluence. Results: On HCV 1a H/SG-neo replicon, the number of colonies reduced dramatically to below 110 when TG-2349 was combined with DAG181. Numbers of 109 ± 1, 6 ± 5, and 2 ± 3 were found for TG-2349 at 1.5, 15, and 30 nM, respectively, when combined with DAG181 at 74 pM. With 150 pM of DAG181 included, the number of colonies further reduced to 65 ± 43, 1 ± 1, and 1 ± 1 for TG-2349 levels of 1.5, 15, and 30 nM, respectively. Another replicon, HCV 1b PI-Luc/neo-ET, with a more dramatic reduction of colony count was observed when the combination of TG-2349 and DAG181 were applied. Numbers of 2 ± 2, 0, and 0 were found for TG-2349 at 1.5, 15, and 30 nM, respectively, with 74 pM of DAG181. Addition of DAG181 at 150 pM resulted in cell colony numbers of 1 ± 1, 1 ± 1, and 0 for TG-2349 at 1.5, 15, and 30 nM, respectively. Conclusion: No antagonistic effect on the number of resistance colonies was observed when combinations of TG-2349 and DAG181 were investigated. Significant reduction of the number of resistance colonies were observed when TG-2349 was combined with DAG181. These studies indicated that the combination of TG-2349 and DAG181 has at least an additive, if not synergistic antiviral effect.

Novel lytic bacteriophage YMC16/01/N133_KPN_BP targeting Klebsiella pneumoniae carbapenemase-producing Klebsiella pneumoniae Jongsoo Jeon 1 *, Yong-Joon Cho 2 , Dongeun Yong 1 , Kyungwon Lee 1 . Background: Klebsiella pneumoniae carbapenemases (KPC) are an important mechanism of carbapenem resistance of Gram-negative bacteria. Lately, KPC-producing bacteria are rapidly increasing in clinical settings and their high drug-resistance is associated with significant morbidity and mortality. Especially, KPC-producing K. pneumoniae has been spread rapidly worldwide and it becomes a major hospital pathogen. Herein, we characterized the novel Klebsiella phage Βφ-N133 (YMC16/01/N133_KPN_BP), which can lyse KPC-producing K. pneumoniae clinical strains. Methods: K. pneumoniae lytic phages were isolated by using the agar double-layered method from sewage samples from a university hospital in South Korea. Their host ranges were determined by the spot test with 58 KPC-K. pneumoniae clinical strains. Whole genome sequencing was performed using the Illumina Miseq sequencer. The isolated phage was characterized by transmission electron microscopy, one-step growth curve, adsorption rate, thermal/pH stability analysis, and bacterial lysis assay.

Results: A novel Klebsiella bacteriophage Βφ-N133 belonging to the family Siphoviridae with an isometric head and a long noncontractile tail. It showed a very broad host range lysing clinical KPC-K. pneumoniae strains of 42 out of 58 (72%). Whole genome analysis of Βφ-N133 exhibited double-stranded DNA with 53,443 bp, a G + C content of 59.3% and 75 putative open reading frames, and known toxin-related genes were not identified in silico. Moreover, bioinformatics analysis results revealed that the whole genome of phage Βφ-N133 is a novel genome that has no BlastN matchs with any other Klebsiella phage genomes in the NCBI database. One-step growth and adsorption curve showed the latent period of 60 min, the burst size of 17 plaque forming units/infected cell and adsorption rate of over 89% within 10 min. The strain showed the broad pH stability in the pH range 5-10 from 1 day up to 4 days, with the high thermal stability observed at 40 and 50°C. Bacterial growth was strongly inhibited by Βφ-N133 at multiplicities of infection (MOIs) of 10. Conclusion: A novel Klebsiella phage Βφ-N133 was isolated and characterized using clinical KPC-K. pneumoniae strains. These results showed that Βφ-N133 can facilitate development of alternative antibacterial agents to treat infectious diseases caused by KPC-K. pneumonia. In vivo safety and efficacy of Βφ-N133 will be examined in further studies. Background: Most people infected with Human T-cell Leukemia Virus Type 1 (HTLV-1) remain asymptomatic carriers. Its prevalence in Taiwan was only 0.5%. We present a case of HTLV-1 related adult T-cell leukemia/lymphoma (ATLL). Methods: Patient was a 58-year-old retired nurse. She suffered needlestick injuries over 20 years, and her blood was rejected for donation at a blood bank 4 years ago. She received a report from blood bank, and someone interpreted it as having HIV infection. But HIV infection was never confirmed. Three weeks ago she had intermittent fevers, night sweats, and general malaise. She then developed general lymphadenopathy over bilateral neck, axillary, and inguinal regions three days prior to admission. Acute retroviral syndrome was suspected but HIV EIA and Western blot were negative. Results: Laboratory testing revealed mild leukocytosis, 9% atypical lymphocytes, CD4: 824/μL. Chlamydia IgM was positive, strategies was applied as follows: to use QR code to convenient online education course, regular monitoring of the standard procedures, to use red alarm marker at the level of 1000ml on the Foley bag, regular audit for correct Foley catheter insertion, in-hospital broadcasting of proper hand washing, to setup procedure to remind the attending doctor by nurses orally, to show the days of Foley catheter insertion on computer, and to use special trolley for Foley catheterization. Evaluation of the effectiveness includes monthly audit of the correct rate of the Foley catheter insertion and daily care by nursing staffs, and the test for the knowledge about CAUTI bundle care. Results: Before conducting the CAUTI bundle care in our hospital, CAUTI rate was 1.4‰ (2/1426 person-day) in March to June 2016. After this plan, CAUTI rate during the same period in 2017 was reduced to 0.73‰ (1/1364 person-day). Knowledge measurement of CAUTI bundle care increased from 87.4% to 96.4% after the strategies (Figure 1 ). Correct rate of the Foley catheterization and daily care were changed from 84.5% and 86% to 99.4% and 100%, respectively ( Figure 2 ). Conclusion: During the initial period of CAUTI-Bundle Care plan, our nursing staffs thought that it was not necessary to perform this plan due to the manpower shortage and the limited resources at our hospital. In addition, there was lower CAUTI rate of our hospital compared to other local ones. After understanding the upgraded evidence of practice guidelines of prevention and maintenance of Foley catheter, they later realized and accepted the promotion of the plan. After the systemic strategies, correct execution rate by nursing staff can achieve 100% and CAUTI rate can further reduce by 0.67‰. Our study showed that CAUTI Bundle Care plan can be executed correctly by a series of promoting strategies and further reduce CAUTI rate. In future, we expect this CAUTI Bundle Care to be a part of our regular medical care. By computer assist, the workload could be reduced. Finally the CAUTI Bundle Care will be promoted to other medical branches in our health system to promote nursing care quality and to reduce CAUTI rate.

Promotion of ventilator-associated pneumonia bundle care implementation rate Yu-Ling Huang 1,2 , Pei-Jung Chuang 3 , Hsiao-Wei Wang 1,2 , Ming-Tzer Lin 1,2 , Chuang-Cheng Hsiao 4 . 1 Respiratory care ward, 2 Respiratory care center, 3 Chief president, Hsiao Chung-Cheng Hospital, New Taipei City, Taiwan

Background: Ventilator-associated pneumonia (VAP) prolongs duration of ventilator usage and length of hospital stay and may even result in mortality of critically ill patients. So prevention of VAP is up most important. Our hospital as a local community hospital began to take part in VAP bundle care plan since 22 nd March 2017, including total 32 beds (Intensive Care Unit (ICU) 2 beds, Respiratory Care Centre (RCC) 10 beds and general ward 20 beds). By the bundle care team setup and cross-functional teamwork, the purpose of this plan is to promote implementation rate of bundle care in health care workers, which would further reduce VAP rate to protect the patient safety, improves the care quality, and lowers the healthcare cost.

Methods: This plan has been initiated in a local community hospital since March to June 2017 according to practical steps. Poster propaganda and education training with pre-and post-test about ability was done to promote perception and implementation of VAP bundle care in health care staffs. Implementation rate was evaluated by the weekly field audit after the plan began. Audit was done from randomly selected samples in each unit (2 patients from ICU and 5 patients from RCC and general ward).

Results: Before execution of VAP bundle care, VAP rate during March to June 2016 was 0.86‰ (3/3480 person day). After the plan, the VAP rate was 1.49‰ (5/3365 person day) during the same period of 2017 ( Figure 1 ). Implementation rate can achieve to 100% in each unit within seven weeks after application of the plan ( Figure 2 ) (six weeks, five weeks and four weeks for general ward, RCC, and ICU, respectively). Conclusion: Based on the psychological theory, a habitual behavior would be acquired through repetition of the action or the experience. At the beginning of the implementation, it took much time to remind and educate the staffs because they did not form a habit to carry out the bundle care in daily practice yet. After the establishment of the habit, the implementation rate can achieve to 100% within five to seven weeks. Different duration to achieve maximized implementation rate could be related to the different bed numbers of each unit. The less the bed numbers, the shorter the duration to achieve 100% implementation rate. VAP rate did not change much after the application of this plan .The possible explanation for this is that the VAP rate was already low in our hospital before, and that there is not enough time to observe evident change of the VAP rate.

Our study showed that, the implementation rate can achieved 100% within seven weeks as a result of systematic propaganda, staff education and training. The bed numbers was the possible issue in the achievement of 100% implementation rate. Future research with longer duration is needed to observe the change of the VAP rate.

Impact of the bundle care model on ventilator associated pneumonia in medical intensive care unit Shu-Chien Liang 1 *, Yu-Mei Ho 2 , Wen-Yin Huang 3 , Wen-Chen Yao 3 .

Intensive care unit, 2 Infection Control, 3 

Background: Due to the progress of thoracic medicine and medical instrument technology, improve the survival rate of patients with respiratory failure, but increase the ventilator-associated pneumonia (VAP), VAP is the most frequent nosocomial infection in the intensive care unit (ICU). Some studies have even found mortality attributable to VAP. Statistics of the ICU of a regional teaching hospital in 2015, the average VAP rates were 0.94 ‰. In order to avoid infection and death of patients, it is recommended that early weaning ventilator and involve the reduction of VAP in hospitalization and the proper management of the patients safety.

Methods: Through the database literature verification and team decision-making, in 2016 from January to December intervention prevent VAP bundle care. Intervention projects include education and training, DOPS technical learning activities and improvement of bed equipment, and enhanced 5 combined care modes, including twice daily 0.12% CHG oral care, daily discussion of early removal of endo, patients continuous intravenous sedation with a daily wake-up trial, in which the infusion of sedatives was discontinued, and Level of arousal will be assessed using the Richmond Agitation and Sedation Scale (RASS), head of bed elevation to 30 degrees, and water in the ventilator tubing can be emptied. In addition, designed a bundle care nursing information system to enhanceis learning and provide prevention tips.

The study results revealed that the patients in the experimental group significantly reduced the VAP. The average rates of VAP were 0.94 ‰ (n = 1061) in 2015, post VAP bundle care, the average VAP rate was zero tolerance (0 ‰, n = 1191) in 2016. The rate of staff confidence in VAP care knowledge and compliance with ventilator care bundle practices were significantly increased, from 25% and 33.3% in the pre-intervention stage to 83.3% and 91.6% in the post-intervention stage. The accuracy rate of RASS sedation evaluation was increased from 0% to 82.6%. Moreover, the sedation patients reached the RASS clinical recommendation level ratio increased from 9% to 87%. Not only reduce iatrogenic injury, but also enhance the use of sedative safety care. Continued tracking to July 2017, VAP density is still zero tolerance. Conclusion: Through the team resource management, the establishment of VAP bundle care, and design of nursing information system, include slogans tips, so that be able to provide the prompts and operation to ensure that the correctness of the clinical care process and monitoring mechanisms to enhance care accuracy rate, achieving the goal of ""zero tolerance"" of VAP, to enhance the patient safety.

In vitro analysis of activities of antimicrobial agents against clinical common organisms causing bloodstream infections, hospital-acquired pneumonia and intra-abdominal infections from twelve teaching hospitals in China: results from the Chinese antimicrobial resistance surveillance of nosocomial infections (CARES) program Henan Li 1 , Chunjiang Zhao 1 , Qi Wang 1 , Ji Zeng 2 # , Yan Jin 3 # , Zhidong Hu 4 # , Kang Liao 5 # , Yanping Luo 6 # , Chao Zhuo 7 # , Rong Zhang 8 # , Xiuli Xu 9 # , Wenen Liu 10 # , Yingmei Liu 11 # , Yunzhuo Chu 12 # , Hui Wang 1 *. 1 Department of Clinical Laboratory,

",0.6438734578852052
Are We Ready? Evidence of Support Mechanisms for Canadian Health Care Workers in Multi-jurisdictional Emergency Planning,"Background: Federal, provincial and municipal leaders in Canada have adopted a culture of preparedness with the development and update of emergency plans in anticipation of different types of disasters. As evident during the 2003 global outbreak of Severe Acute Respiratory Syndrome (SARS), it is important to provide support for health care workers (HCWs) who are vulnerable during infectious outbreak scenarios. Here we focus on the identification and evaluation of existing support mechanisms incorporated within emergency plans across various jurisdictional levels.","T he vulnerability of Canada's public health system was exposed during the 2003 global outbreak of Severe Acute Respiratory Syndrome (SARS), with 251 confirmed cases in Canada, and 44 deaths including three health care workers (HCWs). 1 Human resources were stretched as many HCWs were quarantined and/or asked to work extra shifts to manage patient loads. 2 It is now widely recognized that mitigation and preparedness are necessary for effective disaster management. 3 National, provincial, municipal, and institutional emergency plans are being developed or updated in anticipation of different types of disasters; many specifically focus on pandemic influenza. [3] [4] [5] The Public Health Agency of Canada published the Canadian Pandemic Influenza Plan (CPIP, 2004) 5 based on, and in accordance with, the World Health Organization (WHO, 2005) Global Influenza Preparedness Plan, 6 which defines the responsibilities of national authorities in the event of an influenza pandemic, and provides guidelines for pandemic planning. CPIP requires that all provinces and territories develop their respective pandemic influenza contingency plans based on CPIP and WHO guidelines. Furthermore, in Canada, six provinces and three territories now have legislation requiring municipalities to file official emergency plans, although few plans were complete or available at the time of this study. [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] In Canada, the provision of health services is a priority in pandemic planning. 5, 20 Emergency plans include strategies to enhance human resource pools by drawing on employee rosters, retirees, students and professionals from relevant health disciplines. However, to ensure effective implementation and adequate response capacity, these reserve HCWs must have the necessary skills and training, and they must be available and willing to work. Support mechanisms for HCWs are essential to enable them to stay healthy and perform their roles as critical personnel during disasters, while simultaneously coping with personal loss and trauma. 21 Emotional, informational and instrumental supports have a protective health effect by buffering the negative effects of stress. 22 They can be categorized according to the type of emotional, informational or instrumental aid for the recipient. 23 Many studies have explored best practices for infection control and workplace supports to protect against physical exposure to contagions. 2, 24 However, few studies have focused on formalized informational and emotional support for HCWs. The purpose of this study was to examine current emergency plans from national, provincial and municipal/regional jurisdictional levels to identify which support mechanisms have visibly been incorporated into official emergency plans.

Twelve emergency plans from national, provincial and municipal levels were selected for content analysis. The Canadian Pandemic Influenza Plan (2004); 5 29 pandemic influenza plans were selected for content analysis and obtained from jurisdictional websites. Plans were obtained from multiple regions for national representation. The Ontario and British Columbia pandemic influenza plans were selected because of the provinces' experience with SARS.

A small sample of capital region (or municipal) plans were selected for review based on availability at the time of the study. It was beyond the scope of this study to review a large sample of municipal pandemic plans, and many pandemic plans were not available. The Vancouver Island Health Authority Pandemic Influenza Plan and the City of Edmonton all-hazards plan were accessed online. The Calgary Health Region Pandemic Influenza Response Plan was obtained electronically through contacting the Calgary Health Link. The City of Toronto pandemic plans were obtained from jurisdictional websites, while the City of Ottawa Interagency Pandemic Plan was obtained by contacting the Emergency Management department. These plans were accessed online, based on the assumption that, as public citizens, many HCWs would turn to the Internet for this type of information. A member of our research team confirmed the plans were the most current version by phoning the departments responsible for each plan.

The emergency plans were coded using NVIVO TM software through the classifica-tion of sentences/paragraphs as meaning units. The plans were coded based on the following categories: 1) informational, 2) instrumental, and/or 3) emotional support mechanisms for HCWs and other first responders. Subsequent theme categories were developed inductively.

Informational supports were defined as any activity designed for adequate and appropriate dissemination of information to HCWs in pre-disaster planning, response and/or post-disaster follow-up (e.g., communications, resources, education). Instrumental support mechanisms were defined as tangible aid to protect workers from physical exposure to infectious disease, and policies/procedures which could enable HCWs to perform their roles during a disaster (e.g., personal protective equipment, training, infection control protocols). Training activities were double-coded as informational/instrumental supports with the rationale that training includes education and relates to disseminating information. It also included skill development, which facilitates performance and effective use of infection control procedures. Segments of text were labeled as emotional support if they focused on reducing anxiety, exhaustion, 

Attend bi-annual preparedness practical sessions for health care workers working in hospitals and public health units/agencies. (BCPIPP, p. 146) Conduct a simulation of antiviral distribution based on the priority groups and based on available quantities. (QBPIP, p. 70)

Communications has produced a pandemic influenza video, with accompanying supportive material. The purpose of this video is to assist medical professionals… to better understand influenza pandemics. (BCPIPP, p. 184)

For people who are infected, sick and staying at home, particularly those who live alone, fast, easy access to Info-santé and Info-social are certain to play a major role during the pandemic, [however], they could quickly become overwhelmed. (QBPIP, p. 61)

Federal communications on influenza currently focus on… surveillance, through FluWatch bulletins, which are directed to the public health professionals but available to the public through the Health Canada website. (CPIP, p. 43) The province, in collaboration with local public health units and other stakeholders, will educate both health care providers and the public about their role in identifying signs of influenza and preventing its spread. (OHPIP, p. 20)

Communication with health care professionals and the public on the appropriate use of antivirals is needed during the pre-pandemic and pandemic periods. Clinical guidelines on the use of antivirals in the hospital and the community will be developed as part of the clinical care guidelines. (CPIP, p. 108)

During a pandemic, it will be essential to inform both the pubic and health professionals about the symptoms and treatment of influenza, as well as when to seek advice and referrals. Fact sheets … have been developed to assist health care providers with diagnosis, and the general public with self-treatment. (CPIP, p. 38)

Ensure that all audiences, including media, key opinion leaders, stakeholders, employees are satisfied with the level of communication… (CPIP, p. 425)

Because staff in the health care sector will be overburdened, communications tactics must be effective and simple, and require minimal staff to implement. (OHPIP, p. 22)

Professionals may need training or refresher courses in tasks they don't normally perform, including supervision and management. Due to the limited number of health care professionals that will be available… volunteers and other non-medically trained staff will likely be needed to perform direct patient care. (CPIP, p. 412)

There is serious concern about the practicality of training community volunteers as alternate care providers. In most cases, volunteers will not [have] that background necessary to assume responsibility for administering medications, applying medical equipment, or assessing the severity of cases -even with just-in-time training. (BCPIP, p. 55) Enhancing compliance to directives through informational support … the MOHLTC will achieve good compliance with the recommendations… if : staff have the information they need, understand the scientific basis for the standards, understand their roles, and know what protection/precautions they should use. (OHPIP, Appendix A3, p. 18) and work-family role conflict, or enhancing feelings of self-worth.

Coding reports and emergent themes were reviewed and discussed by four researchers until consensus was reached. The research team includes several researchers with extensive expertise in occupational health, psychosocial aspects of disasters, and gender-based analysis, as well as a former senior public health official with extensive experience in public health policy formulation and management of public health services at multiple jurisdictional levels.

Content analysis of these emergency plans revealed 28 types of identified supports for HCWs across the plans, which have been divided into three categories (informational, instrumental, and emotional supports). Evidence of these supports is presented as quotations from the emergency plans in Tables I, II and III. The themes for each support category are summarized here.

The following informational supports were identified in one or more plans: 1) clarification of worker roles and responsibilities; 2) training exercises to enhance readiness; 3) timeliness, adequacy and clarity of communication; 4) infectious disease surveillance; 5) policies, priorities, and contraindications related to antivirals/vaccines; 6) widespread dissemination of educational materials; 7) stakeholder satisfaction with level of communication; 8) staff and volunteer training for unfamiliar tasks; and 9) enhancing compliance to directives through informational support.

Identified instrumental supports include: 1) Provision of adequate and effective supplies/equipment; 2) concierge assistance for quarantined workers; 3) mobilization of additional human resources; 4) instructions of what to do in the event of insufficient resources (plan B); 5) special provisions for HCWs working in rural and remote areas; 6) plans for nontraditional sites for medical services; 7) regulatory support for additional human resources; 8) adequate screening and security to protect HCWs; 9) job protection for HCWs, spouses and volunteers; 10) national stockpiling, priority grouping, and strategic options for vaccines/antivirals; 11) governmental guidelines for infection control; 12) training and equipment for personal protection; and 13) mechanisms to identify and report cases.

The last category of support focuses on alleviating the emotional impacts of the crisis. The identified emotional supports include: 1) counseling for staff; 2) provision of family care; 3) fostering trust with the public and workers; 4) provision of information and/or instrumental supports; 5) offering a choice to use higher levels of precautionary measures than directed; and 6) mechanisms to identify vulnerable HCWs. 

During an influenza pandemic the health system must have access to adequate equipment and supplies, and a system for purchasing, storing and distributing those supplies. (OHPIP, p. 57) Concierge assistance for quarantined workers …activate plans to provide food, medical and other essential life-support needs for persons confined to their homes by choice or by direction from P/T/L health officials. (CPIP, p. 67)* * This quotation represents acknowledgement for the need for this support, not the actual provision of this assistance.

During an influenza pandemic, vaccination will be provided at mass vaccination centres under the responsibility of health and social services centres. …The model involves a minimal number of nurses, since several tasks usually performed by nurses will be handled by other resources trained to do so…(QBPIP, p. 45)

A pandemic will likely result in shortages of medications, medical supplies and potentially operational supplies. Since other jurisdictions… will potentially be affected by these shortages, the response plan should NOT rely heavily on outside assistance in terms of the provision of supplies and equipment. (BCPIPP, p. 171)

Additional staff must be trained and dedicated to these designated areas in advance of the pandemic, because one or two nurses constitute all the health-care personnel available in these communities. (CPIP, p. 323)

Plans to extend support programs for health care workers (including trainees, volunteers and retirees)… at NT sites should also be included in overall plan for the management of human resources. Support should include: provision of food and drink, grief counseling, support for families and job protection. (CPIP, p. 415)

Review liability protection/malpractice insurance coverage to see how it will extend to cover workers in NT sites, professionals, [and] those taking on tasks not usually part of their scope of practice. (CPIP, p. 416)

Security will be needed to maintain public order …crowd control, traffic control, and ensure protection of site. (CPIP, p. 408)

Job protection for spouses who do family care to allow workers to work in health care. (CPIP, p. 419) Protection of the jobs of workers who take leave to assist during the crisis. (CPIP, p. 409)

Antivirals…are anticipated to be in limited supply during a pandemic. Consequently, a national stockpile has been established and guidelines regarding their use and priority groups have been developed and are currently under review. Depending on the… influenza virus strain, the priority groups may change to target the most affected groups. (NBPIP, p. 7) If deemed necessary, offer antivirals to close contacts of initial cases. (QCPIP, p. 43)

Governmental guidelines for infection control … additional barrier precautions to prevent health care worker contact with a patient's blood and body fluids, non intact skin or mucous membranes… The full description of routine practices to prevent transmission of nosocomial pathogens can be found on the Health Canada website (http://www.hc-sc.ge.ca/pphb-dgspsp/dpge.html#infection). (OHPIP, p. vi)

Agencies and practices have a responsibility to ensure workers have the knowledge, skills, equipment and support to protect themselves in the workplace. Agencies who are sending one or more workers to provide services to a client with respiratory symptoms should note the need for appropriate precautions on the file and ensure all workers are informed and have the appropriate protective equipment. (OHPIP, Appendix A2, p. 11)

Provincial medical officers of health, healthcare practitioners and regional health authorities report significant influenza developments to Manitoba Health, such as outbreaks …in hospitals, nursing homes and schools. (PPIM, p. 21)

This study shows evidence of supports for HCWs, however several gaps are apparent and worth consideration. It was encouraging to see informational supports in several plans, as communication was cited as a key problem during the SARS outbreak, 1 and feeling informed is associated with higher confidence in infection control measures. 30 Effective communication can help alleviate fear, 31 and broader public awareness of preventive measures enables HCWs to concentrate on patient care and occupational safety. Trust, a contentious issue among HCWs, management and government authorities, 1,32 is fostered largely through careful communication. 1,2 Inadequate or inappropriate communication may foster irrational behaviour. A pertinent example occurred in the United States during the anthrax attacks in 2001, with public use of potentially ineffective or even harmful preventive antibiotics in response to inconsistent information from authorities and the media. 33 Informational supports within the plans are comprehensive, although crisis communication should begin in advance of a disaster and include long-term follow up. 34 Instrumental supports dominated the content of the plans. They encompass diverse policies/procedures, such as ensuring availability of physical resources and mobilizing human resources for response, including mechanisms to ensure employees are appropriately compensated for their risks and contribution.

Instrumental supports also include offering physical protection for staff and volunteers through vaccination and/or antiviral therapy, the provision of personal protective equipment (PPE), and extend to functional isolation rooms, job protection, departmental security, and sufficient case identification and reporting. While the need for stockpiling adequate equipment and supplies was acknowledged in federal and provincial plans, responsibility was largely deferred to the institutions. It was unclear if adequate resources to implement these recommendations followed.

The plans did not include recommendations for structured opportunities to practice routine tasks while donning full PPE or recommendations related to fatigue from PPE and additional shifts. Viable supports include shorter work shifts for HCWs, or frequent, enforced breaks to reduce discomfort and fatigue, and enhance safety and work performance. 24, 30, 31, 35 Another noted gap was the unanimous decision not to prioritize the families of HCWs for vaccination, with rationale focused on logistics and ethical justification of defining family members. 5, 20 Notwithstanding the difficulty of making this policy decision, it remains that HCWs cannot be assured their families will be protected, given the HCW's occupational role. It prompts the question of how willingly they will respond to the next largescale outbreak.

Few emotional support mechanisms were formally acknowledged in the plans, with the exception of the Québec Pandemic Influenza Plan. 29 This plan recognizes HCWs to be particularly vulnerable to anxiety, depression, exhaustion, and problems related to post-traumatic stress resulting from their direct involvement in pandemic response, and emphasizes providing post-pandemic psychosocial services during the process of returning to normal daily life.

A visible support at local levels was the provision of grief counseling and pastoral services for hospital staff to manage posttraumatic stress. 36 This support is valuable for staff who feel vulnerable and those who are angry about perceived or actual levels of available protection. 35 Another emotional support recommendation was that child/elder care be provided by local jurisdictions; although substantial resources are required for implementation. The provision of these services fosters a sense of security from the knowledge that loved ones are cared for by trained individuals.

The necessity of addressing the physical needs of quarantined persons was acknowledged, however supports to mitigate the emotional impacts of quarantine were absent. Quarantine is a lonely, psychologically intense circumstance, 37 and emotional support by phone or other means has potential to alleviate distress and enhance compliance with infection control measures. 38 Moreover, as experienced during SARS, regardless of whether they are quarantined, HCWs can expect to encounter stigma, 38 which may persist long after the outbreak has ended. [38] [39] [40] Finally, our analyses revealed an absence of gender-specific supports in the plans. Unfortunately this finding aligns with the During a pandemic, the availability of health care workers… could be reduced by up to onethird due to illness, [and] concern about disease transmission in the workplace, and caregiving responsibilities. (OHPIP, p. 21)

Decision-makers will enter into a covenant with the people of Ontario to be transparent and to provide the best available information. (OHPIP, p. 20)

Ontario will make use of various communications channels… to provide health care workers with information that can be useful for their own protection and for their patients/clients. (OHPIP, p. 22)

Offering a choice to use higher levels of precautionary measures than directed Because some recommended infection control standards represent a change… it may take time for staff to understand the basis for the change, become comfortable with the new surveillance methods and infection control precautions, and adopt new practices. … they should have the option of continuing to use higher levels of precautions. However, the setting should discourage the use of any precautions …that, if misused, increase the risk of exposure to FRI [febrile respiratory illness]. (OHPIP, Appendix A, p. 9)

[During] influenza pandemic, clinical supervision will pay particular attention to the phenomenon of compassion fatigue or stress, the perception of personal performance, the feeling of competence and the means of providing services. (QBPIP, p. 80) disaster management literature. 41 Further research is required to understand how disasters are experienced differently by men and women, 42 and to ensure adequate supports are in place to strengthen resiliency and address respective vulnerabilities. 43 Several limitations of this study are acknowledged. Municipal and regional plans are at various stages of development, and many were not complete at the time of this study. Furthermore, supports available through employee assistance programs were not reviewed. The broader plans outline some responsibilities for smaller jurisdictions, so future studies could explore how those responsibilities are addressed within communities.

The support mechanisms identified serve to enhance personal and collective resiliency among health professionals. Several support gaps exist in the emergency plans analyzed. The lack of structured training opportunities and drills, priority vaccination to protect the families of HCWs, and emotional supports such as child/elder care are notable. There is also a need for understanding the importance of gender-based analysis and policies at the institutional level to address the varying needs of female and male responders. Résultats : Les plans d'urgence comprenaient surtout des mécanismes de soutien informationnel et instrumental, mais peu de mécanismes de soutien affectif ou social. Aucun plan ne comportait d'analyse sexospécifique de l'incidence des flambées de maladies infectieuses sur le personnel masculin et féminin. Le besoin de mesures de soutien affectif était pris en compte par les administrations fédérale et provinciales, mais on en recommandait l'instauration à l'échelle locale.

Conclusion : Des mécanismes de soutien des travailleurs de la santé figurent dans l'échantillon de plans d'urgence étudié, mais l'analyse de leur contenu montre que l'on ne prévoit pas assez de mesures de soutien affectif ou social pour le personnel essentiel, particulièrement les employés qui devront travailler dans des situations extrêmement tendues et prendre des risques considérables pour leur propre santé. Nous présentons aussi les conséquences d'un transfert des responsabilités de soutien aux administrations municipales et à celles des établissements.

",0.6438278021773793
Impact on health care workers employed in high-risk areas during the Toronto SARS outbreak,"Background: A number of publications focusing on health care workers (HCWs) during a severe acute respiratory syndrome (SARS) outbreak have suggested that HCWs experienced psychological distress, particularly increased levels of posttraumatic stress symptomatology (PTSS). Factors contributing to increased distress in HCWs working in high-risk areas treating patients with SARS have not been fully elucidated. The goal of this study was to quantify the psychological effects of working in a high-risk unit during the SARS outbreak. Methods: HCWs in a Toronto hospital who worked in high-risk areas completed a questionnaire regarding their attitude toward the SARS crisis along with the Impact of Event Scale-Revised, which screens for PTSS. The comparison group consisted of clinical units that had no contact with patients infected with SARS. Results: Factors that were identified to cause distress in the 248 respondent HCWs were the following: (a) perception of risk to themselves, (b) impact of the SARS crisis on their work life, (c) depressive affect, and (d) working in a high-risk unit. In addition, HCWs who cared for only one SARS patient in comparison to those caring for multiple SARS patients experienced more PTSS. Conclusions: As expected, HCWs who were working in highrisk units experienced greater distress. Contrary to expectations, HCWs who experienced greater contact with SARS patients while working in the high-risk units were less distressed. This suggests that HCW experience in treating patients infected with SARS may be a mediating factor that could be amenable to intervention in future outbreaks.","In early March 2003, the first case of severe acute respiratory syndrome (SARS) was reported in Toronto, Canada [1, 2] . This major metropolitan city subsequently suffered the largest outbreak of SARS in North America. At the end of this outbreak, SARS had claimed 44 Canadian lives [3, 4] and infected 251 people, of whom 108 (43%) were health care workers (HCWs) [4] . The fact that hospitalacquired infections accounted for the majority of infections worldwide during the SARS outbreak [5, 6] was especially significant for HCWs working in the health care system during this period.

The SARS outbreak and the public health response substantially changed working conditions for HCWs in Toronto by requiring the implementation of extensive surveillance and infection control (IC) measures [7, 8] . The new precautions physically distanced HCWs from their patients and colleagues [9] . HCWs were instructed to minimize their contact with each other at work and outside of the work environment, and socialization with other HCWs was discouraged. These changes extended outside of the work environment, where HCWs reported instances of stigmatization, which often included their family members [9] .

In the early stages of the SARS outbreak, little was known about the etiology, modes of transmission, methods of containment, or natural history of this emerging infectious disease [1, 10] . In the face of a number of unknowns, HCWs were required to care for their colleagues and other patients with the fear of contracting SARS themselves. HCWs became infected with SARS despite the institution of IC precautions [11] [12] [13] .

The impact of working in this type of environment has been identified by a number of studies of HCWs from around the world as being stressful, if not traumatic [14] [15] [16] [17] [18] [19] [20] . Several variables [14] [15] [16] [17] [18] [19] [20] that could increase the likelihood that an HCW would experience distress while working during an outbreak were identified. These variables included (a) being a nurse [15] , (b) working part-time [15] , (c) experiencing health fear and social isolation [16] , (d) being placed in quarantine [17] , (e) lower self-efficacy [18] , (f) lifestyle affected by the SARS outbreak [15] , (g) one's ability to do one's job being affected by precautionary measures [15] , and (h) personal vulnerability [19, 20] . It is essential to try to identify factors in the health care environment that may be precipitants or modifiers of HCW distress so that HCWs can be provided with the most favorable working conditions possible in times of extreme stress. In order to address these stressors, several studies [15, 16, [20] [21] [22] postulated similar methods to decrease the concerns of HCWs: clear directives regarding IC measures, use of active coping and positive framing, obtaining support from colleagues and family, and the ability to provide feedback to and obtain support from management.

These studies [14, 16, 23] have assisted in identifying distress in the general HCW population during an outbreak; however, the experience of those working in high-risk units has not been previously reported. The goal of this study was to extract factors that determined the psychological effects of the Toronto SARS outbreak on those HCWs working in units, which were considered to be high risk for contracting SARS. It is relevant for future planning to be aware of a possible differential risk for HCW distress related to various levels of exposure to infected patients.

HCWs in selected clinical units at a Toronto tertiary care health care institution, which treated patients with SARS, were asked to participate in the study. The assessment and care of patients who were suspected of having SARS or those diagnosed with SARS were limited to three specific hospital areas. These were ""high-risk"" units consisting of a special SARS unit, the intensive care unit (ICU), and the emergency department (ED). HCWs from these high-risk units constituted the study group. Eight selected units that were not involved in either the assessment or management of patients with SARS agreed to participate as a comparison group. The eight comparison units included two oncology units, three general medicine units, one cardiology unit, one general surgery unit, and one multiorgan transplant surgery unit within the same hospital system. An e-mail was sent to each unit manager, informing them about the study and asking for their permission to distribute the questionnaires to employees on their units. All contacted nursing units agreed to participate. Blank questionnaires with sealed boxes for collection of responses were placed at various designated work areas in each participating unit between June 16, 2003, and July 9, 2003.

The self-report questionnaire consisted of a demographics section to identify respondents by age, gender, marital status, number of children, and the setting in which they lived. The questionnaire developed for this study contained 91 items in total, which were grouped into seven domains: (a) HCWs' perception of personal risk, (b) HCWs' perception of their risk to others (e.g., spouse/partner, family members, and friends), (c) confidence in IC measures, (d) confidence in information received regarding SARS, (e) impact on personal life, (f) impact on work life, and (g) depressive affect. These domains were chosen based on the available literature on stress in the workplace and the opinion of experts working and caring for SARS patients. A copy of the survey is available on request.

The responses to the multiple-choice questions were scored on a 6-point Likert scale from strongly disagree to strongly agree. These responses were dichotomized into positive and negative responses. The Impact of Event Scale-Revised (IES-R) was imbedded in the questionnaire. The IES-R [24] is a self-report scale, which assesses the prevalence of posttraumatic stress disorder (PTSD) symptoms based on DSM-IV diagnostic criteria. The IES-R has been found to be more sensitive to a more general construct of traumatic stress in those with lower symptom levels [25] , such as those who might be experiencing posttraumatic stress symptomatology (PTSS) [26] [27] [28] [29] [30] . The response format of IES-R consists of a 5-point Likert scale from 0 to 4 and provides three subscores for PTSD criteria: avoidance, intrusion, and hyperarousal. In this study, for the purpose of detecting the presence of PTSS rather than diagnostic PTSD, we used a total score of 20 or greater as a cutoff of significant stress in the HCW population based on other published studies [31, 32] . Ethics approval for the study was obtained from the University Health Networks Ethics Board.

Statistical analysis SAS version 6.12 (SAS, Cary, NC, USA) was used to perform the statistical analysis for this study. The statistical analysis determined means, standard deviations, and frequencies. Frequency distributions were computed for responses to individual survey questions. Differences between means from study groups were analyzed using t test and ANOVA. Univariate and multivariate logistic and linear regression analyses were conducted. A P value ≤.05 was considered significant.

A total of 248 HCWs (41.1%) from a possible 604 HCW respondents completed the questionnaire. The predominant group of respondents was nursing staff: 173 out of 248 respondents. There were 88 respondents (32%) out of a possible 275 respondents from the comparison ""low-risk"" units and 160 (48.6%) out of a possible 329 respondents from high-risk units. In the high-risk units, 55% (n=120) of HCWs in the ICU and 48% (n=24) of HCWs in the SARS unit responded, while the ED had the lowest response rate of 26.2% (n=16). The respondents were mainly female (86%) and young (mean age=36.9 years, S.D.=9.2), which reflects the general makeup of HCW (Table 1 ). The comparison and study groups were similar with respect to demographics. No differences in age, gender, marital status, number of children, living arrangements, or changes in living arrangements were seen among the four groups (SARS unit, ED, ICU, and comparison group).

Overall, the high-risk groups had a longer work experience (13.8±8.8 years; P= .04), as compared to the comparison group (11.3±9.3 years). Differences were observed between groups in their length of work experience (P= .03), where the post hoc analyses using Tukey's HSD for multiple comparisons showed differences between the ED group with 18.5±9.7 years of working experience and the comparison group who had less experience with 11.3±9.3 years. Fifty-six percent of HCWs from the high-risk unit had had daily contact with SARS patients. HCWs from the highrisk areas were more likely to have been in quarantine because of unprotected contact with SARS patients than those working in the comparison units (P= .03). Forty-nine percent of respondents felt underappreciated by their hospital coworkers, and 42% did not feel appreciated by the society at large for the nature of their work. Sixty percent of the respondents indicated that friends and neighbors avoided them, while 36% reported that people avoided their family members because of concerns of contracting SARS.

The results of the univariate logistical regression presented in Table 2 indicate that IES-R scores (PTSS) are independent of gender, age, years of work experience, marital status, and number of children (0 vs. ≥1). Multivariate logistical regression shows that perception of risk to others, confidence in IC measures, confidence in the information provided, and impact on personal life were not significant predictors of IES-R scores. Similarly, multivariate linear regression of IES-R avoidance, hyperarousal, and intrusion subscores was not impacted by HCW confidence in IC measures, confidence in the information provided, perception of risk to others, or perception of personal risk. The intrusion subscores were not affected further by the impact on personal life.

The clinical unit of work was a factor in HCW psychological distress. ANOVA of continuous IES-R scores by unit was significant at Pb.001 (SARS unit: 22 The dependent variable is the IES-R b20 and ≥20. Independent variables for the IES-R were determined using modified backward stepwise logistic regression. In this procedure, all variables significant in the univariate analysis were initially included in the model. Variables that did not remain significant were not included in subsequent analysis.

none or taking care of two or more patients with SARS (OR=2.6; 95% CI=1.4-5.0). Multivariate logistical regression (Table 3) indicates that working in a high-risk unit, attending only one SARS patient, perception of personal risk, impact on work life, and depressive affect contributed to the presence of PTSS. Multivariate linear regression calculations were performed on the continuous subscale scores of the IES-R for avoidance, intrusion, and hyperarousal. 

This study quantifies the perception that caring for patients with SARS or those suspected of having SARS increased the incidence of traumatic stress in HCWs. We also identified that the level of contact (number of patients with SARS treated) has an important mediating effect on the degree of PTSS experienced. Data showed that caring for only one patient with SARS is significantly more stressful than caring for none or caring for two or more patients with SARS. In order to cope successfully when exposed to a stressful situation, an individual is required to assess his or her self-efficacy, which entails self-appraisal of his or her individual ability to cope with the stressful event [33] [34] [35] . The experience of HCWs who cared for several patients with SARS and were exposed on repeated occasions without being infected may have bolstered their confidence in their own ability to successfully use IC measures to manage their own risk and the risk to others. Effective coping, in this case, caring for patients with SARS, would not only have reduced the level of personal distress but may also have promoted a sense of personal efficacy to meet the ongoing requirements of a challenging task on a daily basis [36] . Other factors identified to moderate the level of PTSS in the HCW population in this study were (a) impact on work life, (b) perception of personal risk, and (c) depressive affect, which are similar to those identified by other studies of HCWs dealing with the SARS outbreak [14] [15] [16] 18] .

The Ontario provincial government SARS commission report [37] identifies that the swiftness of the outbreak, the risk to HCWs, the associated mortality within a short time frame, and the initial uncertainty of the exact causative agent all added to the fear and perception of personal risk to the HCW. Personal risk to the HCW was an identified stressor in our study population, but the stress may have been moderated by the confidence expressed by HCWs in the IC measures and the IC information they received. This confidence may have helped to mitigate the powerlessness and vulnerability originally experienced by HCWs in the early days of the SARS outbreak. Confidence in IC measures and information appeared to be a common finding among HCWs in other studies [14, 38] and may have been a determining factor in assisting HCWs from various countries in dealing with the perceived personal risk while continuing to work in health care institutions.

The impact on work life became noticeable because of the mandated restrictions in HCW socialization in the hospital and requirements to utilize protective gear while in the hospital setting. Socialization with friends/colleagues, which is an important element of support in traumatic situations [39] , was no longer available to the HCW staff. This isolation would have been intensified by the public fear of this new emerging infectious disease, which resulted in HCWs feeling stigmatized. Although the media portrayed HCWs positively and held them in great regard, our data support research [9, 15] that found that HCWs felt stigmatized.

Since no visitors to the hospital were allowed early in the SARS outbreak, HCWs took on a greater role in providing support and comfort to their patients. ICU HCWs are used to finding themselves caring for the dying patient. However, during the SARS outbreak, they found themselves to be the only ones at the bedside of patients who were dying of SARS. The patient's families were often quarantined or ill themselves and were unable or were not allowed to visit. These types of events would have placed a psychological burden on the HCW who would not have the opportunity, as in non-SARS times, to grieve with the patient's family and coworkers. The effect of a depressive affect on the level of distress experienced by HCWs is not surprising as it is known from previous epidemiological studies [39] that depressive disorders and PTSD are among the major psychiatric problems associated with disaster and traumatic events and will often co-occur. It has been postulated [40] that the combined effect of loss and threat may explain the frequent co-occurrence of PTSD and depression. In this case, the illness of coworkers with SARS was a loss to their colleagues, and the threat that they may also contract the SARS virus placed a heavy psychological burden on the individual HCW. Other studies [14] [15] [16] 23] of HCWs during the SARS outbreak also found that large percentages experienced difficulties with depression, poor family relations, sleep problems, and somatic symptoms associated with depression.

A methodological limitation of the study is the response rate of the various units. The method of distribution of questionnaires based on ethical considerations did not allow for an accurate estimate of the number of HCWs who were possible respondents (the number of staff who actually saw the questionnaire and decided not to participate). Response rates were based on using the full complement of staff who were assigned to the unit. This may have resulted in higher calculations of nonrespondent rates since the number of staff who would have come in contact with the survey would have been variable over the 3-week period that the study was carried out. Response rates of this study are comparable to other Canadian studies of SARS and HCW-47% in the study of Nickell et al. [15] and 23.3% in the study of Maunder et al. [16] .

Survey responses from the high-risk units were higher than those from the low-risk comparison units. It is likely that HCWs who chose to participate differed in some respects from those who did not. HCWs in the high-risk units would be expected to be more acutely aware of the SARS experience and, thus, were likely to be more motivated to participate. Another determining factor may also have been the availability of staff time. Comparison units, which were low-risk units, had a smaller patient-to-staff ratio than the high-risk units, which usually had patient-to-staff ratios of 1:1 in the ICU and SARS unit. The number of HCWs available to be surveyed in the various high-risk units was variable, but it was scientifically necessary to include all designated high-risk units so that we would have a representative sample of this category of HCW. The ratio of possible respondents in the ICU versus the ED and SARS unit was approximately 5:1. The results need to be interpreted conservatively given the limitations of the number of respondents in the various categories.

The characteristics of the study population are considered to be similar to those of the population of the health care facility surveyed during the SARS outbreak. For example, nursing staff comprises 75% of frontline HCWs in this health care facility and 70% of the study population. Also, two studies [15, 16] that were carried out in Toronto would suggest that our study sample is representative of other HCWs in Toronto since their findings are similar to those of our study, for example, having found increased distress to be associated with health fear and work stress. In contrast, our study also explored the exposure to SARS patients not only by self-report but also based on the information of each unit in which the HCW was working. The fact that HCWs in high-risk areas experienced more distress than those working in low-risk areas was not unexpected, but the finding that the greater number of patients cared for was associated with reduced PTSS needs to be further explored.

The seven-domain questionnaire, which was specifically designed for this study for assessing risk factors for psychological distress, was not standardized but has been used by several other groups [14, 16] . No diagnostic interpretations of the IES-R or depressive affect data are made since results are self-report and no diagnostic interview was carried out. It may also be that the psychological distress experienced by the staff may be underestimated since the study was carried when the city of Toronto was at the tail end of the SARS outbreak.

This study suggests that the level of distress related to working in high-risk areas may be different from those in other hospital areas. Acknowledgment of this differential would be instrumental in implementing supportive workforce resources during an infectious disease outbreak. Efforts to address the level of distress in these areas may need to be centered on providing HCWs with ways to enhance their self-efficacy in dealing with high-risk situations such as SARS. A greater experience with SARS patients appeared to mitigate distress level. Using a buddy system (pairing an experienced HCW with a less experienced HCW) may help to transfer skills and address the social isolation brought about by changes in the work environment. This may be further enhanced by providing opportunities for HCWs to exchange and address concerns, share strategies that have been helpful, normalize feelings related to stressful situations, and discuss maladaptive responses. Involvement of occupational health experts [37] would help minimize the negative effects on work life and address the sense of helplessness that leads to a depressive affect [41] . Implementing systems for communication between HCWs and administration, health care facilities, and government [37] , as well as preparation and planning for an outbreak, may help HCWs to reframe disease outbreaks so that they have a better sense of self-efficacy and control of the situation.

Feedback from HCWs as to the factors that play a role in determining the level of psychological stress is the first step in identifying potential areas for intervention in the future. Further follow-up of HCW resilience and personal/work requirements needs to be done to empower HCWs to deal with any future outbreaks.

",0.6429940021950984
Plants as Factories for Human Pharmaceuticals: Applications and Challenges,"Plant molecular farming (PMF), defined as the practice of using plants to produce human therapeutic proteins, has received worldwide interest. PMF has grown and advanced considerably over the past two decades. A number of therapeutic proteins have been produced in plants, some of which have been through pre-clinical or clinical trials and are close to commercialization. Plants have the potential to mass-produce pharmaceutical products with less cost than traditional methods. Tobacco-derived antibodies have been tested and used to combat the Ebola outbreak in Africa. Genetically engineered immunoadhesin (DPP4-Fc) produced in green plants has been shown to be able to bind to MERS-CoV (Middle East Respiratory Syndrome), preventing the virus from infecting lung cells. Biosafety concerns (such as pollen contamination and immunogenicity of plant-specific glycans) and costly downstream extraction and purification requirements, however, have hampered PMF production from moving from the laboratory to industrial application. In this review, the challenges and opportunities of PMF are discussed. Topics addressed include; transformation and expression systems, plant bioreactors, safety concerns, and various opportunities to produce topical applications and health supplements.","The production of plant-derived pharmaceuticals has attracted great interest. Mapp Biopharmaceutical Inc., a company located in San Diego, CA, USA has produced a drug in tobacco leaves called ZMapp, which has been used to combat the 2014 Ebola virus outbreak in Africa [1] . As of October 2014, seven infected patients received an early treatment with ZMapp and fully recovered. Another patient, receiving a late treatment with ZMapp in November 2014, however, succumbed to the disease and died. Additional Ebola patients were unable to receive the treatment due to an insufficient supply of ZMapp. This is unfortunate since it is the only drug to date that has been effectively used to treat patients infected with the Ebola virus, even though it has not been approved by the U.S. Food and Drug Administration (FDA). ZMapp has been subjected to clinical Phase I and 2 trials in 2015, sponsored by the National Institute of Allergy and Infectious Diseases (NIAID) (see Table 1 ). On 15 September 2015, ZMapp was granted a fast track status by the FDA [2] . Middle East respiratory syndrome coronavirus (MERS-CoV) is an emerging disease. Due to the high mortality rate of MERS (above 35%), it caused a public panic in South Korea during May 2015. As of 27 November 2015, MERS-CoV has infected 1618 patients and caused a total of 579 deaths worldwide. Over 26 countries have reported MERS-CoV cases [3] . Currently, no effective drug is available to treat the MERS-CoV virus. Plant Biotechnology Inc. (Hayward, CA, USA) produced an immunoadhesin (DPP4-Fc) in transgenic tobacco. Purified DPP4-Fc exhibits strong binding to MERS-CoV and prevents the virus from infecting lung cells. In June 2015, Plant Biotechnology Inc. received funding from NIAID to support further development and testing of this drug [4] .

The concept of using plants to produce recombinant pharmaceutical proteins, referred to as plant molecular farming (PMF) or pharming (PMP), is not new. Human growth hormone, initially produced in tobacco and sunflower in 1986, was the first-plant-derived recombinant therapeutic protein [5] . Mason et al. [6] later expressed the hepatitis B surface antigen (HBsAg) in transgenic tobacco. This plant-derived antigen was physically and antigenically similar to the HBsAg obtained from human serum and recombinant yeast. The yeast-derived HBsAg is clinically used for HBV vaccination. Since 1994, more than 100 pharmaceutical proteins have been expressed and characterized in plants. By 2011, more than twenty PMF pharmaceuticals were placed in preclinical or clinical trials [7] . Several PMF products have completed Phase 2 trials and one product has been approved by the FDA (Table 1) . Although several plant-derived drugs have been commercialized as research and diagnostic reagents (such as tobacco derived aprotinin and rice derived lysozyme from Sigma-Aldrich Company (St. Louis, MO, USA) or received USDA approval as a vaccine additive for use in poultry (Dow Agro Sciences, Indianapolis, IN, USA) [1] , the current review mainly focuses on PMF in relation to human pharmaceutical applications. Plants represent a promising system for the production of human pharmaceutical proteins on a large scale, and at a low cost. Many production challenges, however, such as low yield [7] [8] [9] [10] , plant glycosylation [11] [12] [13] , purification and downstream processing hurdles [14] [15] [16] , have limited the development of PMF-based human pharmaceuticals on a clinical scale.

In May 2012, the first PMF-derived enzyme, ELELYSO™ (taliglucerase alfa) (Protalix BioTherapeutics, Karmiel, Israel), was approved for human use by the FDA [17] . ELELYSO™ is based on the use of carrot cells to produce recombinant taliglucerase alfa, which is used in an enzyme replacement therapy to treat adult patients with Gaucher disease. The production and application of ELELYSO™, however, is not representative of other PMF-derived pharmaceuticals for several reasons. Since Gaucher disease is a rare genetic disease, mostly found among Ashkenazi Jews, ELELYSO™ has limited production needs. The FDA also accelerated (fast tracked) the approval process as a treatment for a rare disease. Additionally, the drug is produced in carrot cells using a large bioreactor under very stringent conditions. This process is different from production of other PMF products, which generally use entire leaves, fruits, seeds, or whole plants to produce the recombinant pharmaceutical. The production and approval of ELELYSO™ still represents a major step forward for the whole field of PMF. Many companies have now explored and started product pipelines utilizing plant-expression systems (see Table 2 ).

Identifying potential genes suitable for PMF and general approaches is becoming more simple and straight forward. Facilitated by the rapid progress in genomics, proteomics, and bioinformatics, a greater number of useful genes are being identified and characterized. Additionally, relatively routine molecular methods have become available for placing the genes of interest into plant expression vectors and transforming them into plants (see Figure 1 ). An example of an early proof of concept for PMF is the production of plant-derived edible human vaccines, using leafy plants or fruits [30] [31] [32] [33] [34] [35] . Edible vaccines are an ideal product in concept since the vaccine could be administered to people orally, theoretically without the need of professional health care workers or sterile injections. The tedious and complicated process of purifying and storing the vaccine would also be eliminated since the food product itself would be the vaccine. The plants could also be grown locally, thus negating the cost of long distance transportation and storage. Edible vaccines also avoid the potential risk of infecting patients with a contaminated product since, in general, organisms causing plant diseases do not infect humans. The edible vaccine concept was first proposed by Charles Arntzen and coworkers, after HBsAg (Hepatitis B Virus antigen) was produced in tobacco plants. Mice fed HBsAg-transgenic potatoes exhibited a robust immune response [36] . Uncooked potatoes from transgenic potato plants producing HBsAg were later tested orally in humans. Greater than 60% of the volunteers exhibited strong systemic and mucosal immunity after three doses of potato were consumed [37] . These results demonstrated that plant-edible vaccines could be used in global immunization projects at a low cost. Subsequently, many other vaccine genes were expressed in a variety of crops, including lettuce, banana, and tomato fruits. Several plant vaccines are now in clinical trials that have produced encouraging data [38] . PMF production strategies and challenges, such as biosafety, appropriate expression systems, possible and potential applications, are discussed in the present review with the idea of demonstrating a feasible approach for the potential commercialization of a PMF product.

A very advantageous aspect of PMF research is that it does not require a large financial investment to conduct initial studies. Plants can be grown in a greenhouse or even in a biosafety lab if required. The cost of plant maintenance for PMF is low, relative to E. coli, yeast, or mammalian cell expression systems (see Table 3 ); and the source (plant leaves or seeds) for making the recombinant protein is potentially unlimited [7, 8, 35] . Plant expression systems have several major advantages over prokaryotic and other eukaryotic cell systems in regards to production speed, cost, and safety. Plants can correctly fold and assemble complex proteins, such as secretory antibodies, full size immunoglobulins and the homodimeric vascular endothelial growth factor (VEGF). Human VEGF produced in barley grain has been commercialized for research use [39, 40] . Plant-expressed human VEGF is used as treatment for thinning hair (UNICO Enterprises, Pasadena, CA, USA). Plants also have the capacity to introduce post-translational modifications. The use of plants also eliminates potential contamination of the therapeutic drug with animal pathogens (prions, viruses, and mycoplasmas), thus increasing safety. In general, the cost of PMF-derived products is only 0.1% of mammalian cell culture systems and 2%-10% of microbial systems. An example of an early proof of concept for PMF is the production of plant-derived edible human vaccines, using leafy plants or fruits [30] [31] [32] [33] [34] [35] . Edible vaccines are an ideal product in concept since the vaccine could be administered to people orally, theoretically without the need of professional health care workers or sterile injections. The tedious and complicated process of purifying and storing the vaccine would also be eliminated since the food product itself would be the vaccine. The plants could also be grown locally, thus negating the cost of long distance transportation and storage. Edible vaccines also avoid the potential risk of infecting patients with a contaminated product since, in general, organisms causing plant diseases do not infect humans. The edible vaccine concept was first proposed by Charles Arntzen and coworkers, after HBsAg (Hepatitis B Virus antigen) was produced in tobacco plants. Mice fed HBsAg-transgenic potatoes exhibited a robust immune response [36] . Uncooked potatoes from transgenic potato plants producing HBsAg were later tested orally in humans. Greater than 60% of the volunteers exhibited strong systemic and mucosal immunity after three doses of potato were consumed [37] . These results demonstrated that plant-edible vaccines could be used in global immunization projects at a low cost. Subsequently, many other vaccine genes were expressed in a variety of crops, including lettuce, banana, and tomato fruits. Several plant vaccines are now in clinical trials that have produced encouraging data [38] . PMF production strategies and challenges, such as biosafety, appropriate expression systems, possible and potential applications, are discussed in the present review with the idea of demonstrating a feasible approach for the potential commercialization of a PMF product.

A very advantageous aspect of PMF research is that it does not require a large financial investment to conduct initial studies. Plants can be grown in a greenhouse or even in a biosafety lab if required. The cost of plant maintenance for PMF is low, relative to E. coli, yeast, or mammalian cell expression systems (see Table 3 ); and the source (plant leaves or seeds) for making the recombinant protein is potentially unlimited [7, 8, 35] . Plant expression systems have several major advantages over prokaryotic and other eukaryotic cell systems in regards to production speed, cost, and safety. Plants can correctly fold and assemble complex proteins, such as secretory antibodies, full size immunoglobulins and the homodimeric vascular endothelial growth factor (VEGF). Human VEGF produced in barley grain has been commercialized for research use [39, 40] . Plant-expressed human VEGF is used as treatment for thinning hair (UNICO Enterprises, Pasadena, CA, USA). Plants also have the capacity to introduce post-translational modifications. The use of plants also eliminates potential contamination of the therapeutic drug with animal pathogens (prions, viruses, and mycoplasmas), thus increasing safety. In general, the cost of PMF-derived products is only 0.1% of mammalian cell culture systems and 2%-10% of microbial systems. 

Current methods in plant biotechnology cannot precisely control the expression level of transgenes in plants in a consistent manner and not every plant species is readily transformed. This means that the amount of pharmaceutical produced may vary in each plant species, or even in different plant parts (i.e., leaves, fruit, and seeds). Levels of expression in subsequent generations may also vary. Given this scenario, it is very difficult to accurately quantify the appropriate dosage of edible vaccines for children and adult patients. Edible vaccines can also trigger immune tolerance after oral administration. Lastly, most of the ingested protein will be degraded by digestive processes. Collectively, these disadvantages greatly restrict the clinical use of edible vaccines [30, 32] .

While the science of PMF is relatively new, microbial and animal cell expression systems have been used for over 30 years, and industry has developed standard and high-throughput purification protocols. In contrast, protocols for the purification of plant-derived pharmaceutical proteins are varied. While plants such as tobacco, maize, and rice, have been used for greenhouse or open-field production of PMF products, each plant species consists of unique sets of proteins and metabolites. As a result, each PMF platform requires its own purification protocol that is tailored to the product being generated and the plant production system [14] [15] [16] [41] [42] [43] . Factors, such as plant phenolic compounds, plant pathogens, secondary metabolites, pesticides, and fertilizers, increase the difficulty of purifying a PMF product at an industrial level. Field crop-based PMF platforms, such as maize or rice, have pollen contamination issues which raise biosafety concerns as the pollen may contaminate non-transgenic crops that are part of normal agricultural production [44] [45] [46] . Currently, FDA has a restricted policy for using food crops for the production of recombinant pharmaceutical compounds [47] .

ProdiGene, Inc. company (College Station, TX, USA) launched the large-scale production of transgenic maize that produces trypsin. However, the USDA discovered that plant remnants from a previous ProdiGene trial had contaminated a nearby conventional field. ProdiGene was fined $250,000 and charged $3 million to cover the cleanup operation of mishandling the field test. The punitive action forced ProdiGene into bankruptcy. This event represented a significant setback to the commercial use of PMF [48] . In order for PMF to succeed, both standard biosafety procedures and purification protocols need to be established. In this regard, tobacco is a very good candidate for PMF production since it is not a food crop and cannot contaminate other crops by the spread of transgenic pollen [41] . The procedure for gene transfer and expression in tobacco is also simple and well established. Transgenic tobacco plant can be produced in six months and it can produce the protein of interest in both leaves and seeds.

Some reports have indicated that subcutaneous injections of plant-derived proteins could induce an immunogenic response to plant-specific glycans [49] [50] [51] [52] [53] . Topical applications of plant-derived glycoproteins in humans, however, have not resulted in any adverse effects and therefore represent a potential approach for PMF-based products. Topical application of a recombinant plant secretory antibody prevented oral Streptococcal colonization for at least four months in humans [54] . Topical application of soybean-derived monoclonal antibodies (mAbs) readily diffused in human cervical mucus and prevented herpes simplex virus 2 (HSV-2) infection [55] . Tan et al. (2014) [56] expressed human acidic fibroblast growth factor 1 (FGF-1) in the medicinal plant Salvia miltiorrhiza. The product combined the medicinal function of both FGF-1 and bioactive compounds within the medicinal plant. Topical application of extracts obtained from the transgenic medicinal plant significantly stimulated fibroblast cells, promoted blood vessel formation, and accelerated the healing process of burn wounds in mice. This is an example of how PMF can be used to combine the therapeutic function of a recombinant protein and the inherent properties of a medicinal plant. Topical application of a plant extract would significantly reduce the cost of purification and downstream processing. In general, topical application is safer than oral consumption or injection, which would help to address concerns about public safety.

A primary objective of PMF is to reduce the cost of producing novel therapeutic proteins. Using PMF to create a vegetable, seed or fruit health supplement could be a practical alternative to using PMF to develop a processed pharmaceutical drug. [57, 58] expressed lumbrokinase, an anti-thrombotic enzyme from earthworm, in sunflower kernels. Mice and rats that were fed the transgenic kernels exhibited a strong degradation of blood clots [58] . Unlike a vaccine or a therapeutic protein, lumbrokinase has been widely sold and used as a health supplement to dissolve blood clots and maintain healthy cardiovascular function in people. This makes lumbrokinase a good candidate for PMF since, in general, health supplements do not need a medical prescription and have less regulations for commercialization [59] .

It often takes six months to a year or more to produce transgenic plants. Several generations are required to generate plants that are homozygous for the transgene. Most transformation technologies also result in the gene being inserted randomly into the plant genome. The factor of random insertion, along with the need to identify regulatory elements (promoters) to drive a high-level of foreign gene expression, often results in a low yield of the recombinant protein even in plants that have been stably transformed. The time needed to utilize a standard PMF approach is unsuitable for addressing sudden viral epidemics, such as an outbreak of Ebola, severe acute respiratory syndrome (SARS), or MERS-CoV. Transient expression systems can be used as an alternative approach to produce recombinant protein within three to five days [60] [61] [62] [63] (see Figure 2) .

Various viral vectors have been developed for small-or medium-scale PMF production. For example, Mason et al. (2010) [64] developed a highly efficient, bean yellow dwarf virus (BeYDV) based single-vector DNA replicon system, which incorporated multiple DNA replicon cassettes. They were able to produce 0.5 mg of antibody per gram leaf (fresh weight) in tobacco leaves within four days following infiltration. Mapp Biopharmaceutical Inc., using similar methodology, transiently expressed the humanized antibodies, MB-003 and ZMab, in tobacco leaves. MB-003 and ZMab were later combined and designated as ZMapp. The use of these antibodies as a pharmaceutical drug was able to cure 100% of Ebola infected rhesus macaques primates [65] . 

Plant-cell-culture based bioreactors currently show more promise than traditional PMF using whole plants to produce pharmaceuticals [67] [68] [69] . Similar to microbial or mammalian cell bioreactors, plant cells are cultured in a sealed, sterilized container system without human pathogens or soil contamination. Biosafety concerns associated with the unintentional distribution of pollen and crossfertilization are also eliminated. Cultured plant cells require only simple nutrients to grow, so the operational cost is much less expensive than mammalian or microbial bioreactors. Downstream purification and processing of the recombinant protein is less complicated in the absence of complex plant fibers and an array of secondary metabolites, which significantly reduces production costs. As previously noted, the first FDA approved PMF-based pharmaceutical, taliglucerase alfa, used to treat Guacher's disease, was produced in carrot cell suspension cultures (Tables 1 and 2 ) [17] . Since Gaucher's disease is a very rare disease, treatment of this disease with an orphan drug is very costly ($200,000 US annually per patient for life). Using a carrot cell production system, however, reduces the cost to $150,000/patient/year. More than 20 recombinant proteins have been produced from plant cell culture systems [41] . The tobacco strains, BY-2 and NT-1, are the most popular plant cell-culture based strains used as bioreactors in PMF. The recombinant protein can be expressed to be secreted into the culture medium to simplify the downstream purification process. The pore size in plant cell walls, however, may prevent some foreign proteins from being secreted depending on their size and folded architecture [67] . Proteolytic activity in cultured cells may also result in a low yield of antibodies. Magy et al. [68] demonstrated that the proteolytic profile is host species specific. They tested the combination of the isotype, culture conditions, and host species and found that the best 

Plant-cell-culture based bioreactors currently show more promise than traditional PMF using whole plants to produce pharmaceuticals [67] [68] [69] . Similar to microbial or mammalian cell bioreactors, plant cells are cultured in a sealed, sterilized container system without human pathogens or soil contamination. Biosafety concerns associated with the unintentional distribution of pollen and cross-fertilization are also eliminated. Cultured plant cells require only simple nutrients to grow, so the operational cost is much less expensive than mammalian or microbial bioreactors. Downstream purification and processing of the recombinant protein is less complicated in the absence of complex plant fibers and an array of secondary metabolites, which significantly reduces production costs. As previously noted, the first FDA approved PMF-based pharmaceutical, taliglucerase alfa, used to treat Guacher's disease, was produced in carrot cell suspension cultures (Tables 1 and 2 ) [17] . Since Gaucher's disease is a very rare disease, treatment of this disease with an orphan drug is very costly ($200,000 US annually per patient for life). Using a carrot cell production system, however, reduces the cost to $150,000/patient/year. More than 20 recombinant proteins have been produced from plant cell culture systems [41] . The tobacco strains, BY-2 and NT-1, are the most popular plant cell-culture based strains used as bioreactors in PMF. The recombinant protein can be expressed to be secreted into the culture medium to simplify the downstream purification process. The pore size in plant cell walls, however, may prevent some foreign proteins from being secreted depending on their size and folded architecture [67] . Proteolytic activity in cultured cells may also result in a low yield of antibodies. Magy et al. [68] demonstrated that the proteolytic profile is host species specific. They tested the combination of the isotype, culture conditions, and host species and found that the best combination resulted in 10-fold differences in the expression level. More than 30 mg/L intact antibody was produced in optimum conditions. A yield of 20 mg/L of the human monoclonal antibody M12 was produced in tobacco-BY-2 cell cultures in a 200 L bioreactor [69] . A commercial, cost-effective plant cell culture platform, ProCellEx™ (Protalix Biotherapeutics, Karmiel, Israel) has been developed that significantly reduces costs for industrial scale recombinant protein production [70] .

Currently, the use of moss, a non-seed plant, is being investigated as a candidate for the production of pharmaceutical proteins in bioreactors [71] [72] [73] . The ability of moss cells to photosynthesize in culture significantly reduces the cost of culture nutrients. As in yeast and plant cell suspension cultures, recombinant proteins can be designed to be secreted into the culture medium in moss cultures, which facilitates downstream processing and purification of the recombinant protein. Using genetic manipulation, moss cells can also produce a humanized form of a glycosylated protein, lewis Y-specific mAb MB314 [72] , which reduces concerns, as noted below, about plant-specific glycosylation. Some recombinant therapeutic proteins, such as Epidermal Growth Factor [74] , α-galactosidase [74] , α-amylase [75] , a glyco-optimized version of the antibody IGN311 [76] , a multi-epitope fusion protein from the human immunodeficiency virus [77] , etc. have all been produced in moss cultures. Cultures of the moss, Physcomitrella patens (P. patens), are the most commonly used plant material within bioreactors to enable protein production. A German biopharmaceutical company, Greenovation Biotech GmbH, has developed a P. patens-based platform (Bryo-Technology) for large-scale, high quality, recombinant protein production. Examples include Moss-GAA for Pompe Disease, Moss-GBA for Gaucher's Disease, and Moss-AGAL for Fabry Disease. Moss-AGAL has completed preclinical trials and is entering Phase I testing (Table 1) .

Microalgae cultures have been used for biofuel and foreign protein production for many years [78] [79] [80] . Microalgae have a very simple structure, and can be unicellular, colonial, or filamentous. Algae can produce large amount of biomass within a very short period due to their short life cycle. The downstream purification of recombinant proteins in algae is similar to yeast and bacterial systems, and is therefore generally less expensive than whole plant production systems. However, recombinant proteins produced from algae do not undergo certain post translational modifications, and as a result, may not be suitable for the production of some glycoproteins. For example, algae may not be able to produce human forms of glycosylated proteins due to a lack of the proper enzymatic machinery [81] . However, a variety of therapeutic and diagnostic recombinant proteins, including vaccines, enzymes, and antibodies, have been produced in algal systems [82] [83] [84] . In some cases, however, foreign genes are only expressed transiently in algae [85] [86] [87] . Efficient and stable expression of foreign genes in algae is greatly improved by the use of strong promoters, proper codon usage, intron integration, and specific transformation methods [88] . An optimized microalgae production system has been developed by a USA-based algae bioreactor company, PhycoBiologics [24] . Recombinant protein yields up to 20% of total soluble protein have been obtained, which makes the algal platform a promising approach for the production of commercial pharmaceuticals [24] .

Protein stability is also an important issue in the storage of harvested PMF-based recombinant products. Currently, most pharmaceutical proteins are synthesized in leafy crops for optimum biomass. Leaf proteins, however, are subject to rapid proteolytic degradation after they are harvested [41] . Long-term storage of leaf material is also very challenging. Overexpression of foreign proteins in leaf cells may also result in necrosis and subsequent cell cell death [89] . Our own prelimary studies demonstrated that transient expression of various blood clot-dissolving serine proteinases, such as vampire bat plasminogen activator (DSPAα1), nattokinase, and lumbrokinase, in leaves resulted in leaf necrosis four days after infiltration (Figure 3 ). When these proteins were targeted to in seeds, however, cell necrosis was not observed and the purified proteins exhibited the ability to dissolve fibrin and blood clots (unpublished data). Therefore, targeting the production of PMF-based products to seeds is becoming an attractive alternative [8, 57, 58, [90] [91] [92] [93] [94] .

Int. J. Mol. Sci. 2015, 16, page-page however, cell necrosis was not observed and the purified proteins exhibited the ability to dissolve fibrin and blood clots (unpublished data). Therefore, targeting the production of PMF-based products to seeds is becoming an attractive alternative [8, 57, 58, [90] [91] [92] [93] [94] . 

Arabidopsis [90, 92] , tobacco [91] , rice [93] , and corn [94] . Recombinant proteins targeted to seeds have been reported to accumulate to very high levels. The use of a seed specific, Phaseolus vulgaris regulatory sequence to drive transcription of a murine single chain variable fragment (scFV) in Arabidopsis resulted in high yield of recombinant protein (36.5% of total soluble seed protein) [90] . The abundance of human lysozyme in transgenic rice grains was 1% of grain weight [95] . The level of scFV accumulation approached 25% of total protein in tobacco seeds when scFV was fused with elastin-like polypeptides [96] .

Different subcellular compartments have been targeted for recombinant protein in order to increase protein stability in seeds. The ER (endoplasmic reticulum) compartment in plant cells has been demonstrated to contain few proteases and therefore represents a relatively protective environment. Retention of proteins in the ER therefore increases protein stability and yield due to the lack of protein degradation. KDEL, an ER signal peptide, was used to target the deposition of a recombinant protein to the ER [91, 97] . Recombinant protein targeted to seeds also allows long-term storage (up to three years) at room temperature without a detectable loss in activity [8] .

Targeting of PMF-based products to seeds allows for long-term protein stability, as well as easy harvesting, storage, and transportation. Relative to leaves, seeds contain fewer native proteins and less phenolic compounds and secondary metabolites. Seeds are easy to surface wash and sterilize which also facilitates commercial production. Collectively, the advantages of producing recombinant proteins in seeds make this platform a cost-effective platform for PMF-based products [8, 98] .

Greater than 50% of human proteins are glycosylated. One third of all approved pharmaceuticals are glycoproteins [41] . Plants present an advantage for the production of recombinant proteins due to their capability of performing a variety of post-translational modifications, including glycosylation and lipid addition. Plants and animals share a similar early stage glycosylation pathway before nascent N-glycan reaches the Golgi apparatus. This pathway produces β-(1,4)-linked galactose and sialic acid in animals and N-glycan (core β-(1,2) xylose and core α-(1,3)) fucose moieties in plant. Glycosylation of native plant proteins is essential for their proper function during plant growth and development [99] [100] [101] .

Plant-specific, hyperglycosylated proteins, however, may be immunogenic in humans. Immunization of mice and rats with horseradish peroxidase elicited the production of antibodies Seed-based systems for PMF have been developed in various plant species, including Arabidopsis [90, 92] , tobacco [91] , rice [93] , and corn [94] . Recombinant proteins targeted to seeds have been reported to accumulate to very high levels. The use of a seed specific, Phaseolus vulgaris regulatory sequence to drive transcription of a murine single chain variable fragment (scFV) in Arabidopsis resulted in high yield of recombinant protein (36.5% of total soluble seed protein) [90] . The abundance of human lysozyme in transgenic rice grains was 1% of grain weight [95] . The level of scFV accumulation approached 25% of total protein in tobacco seeds when scFV was fused with elastin-like polypeptides [96] .

Different subcellular compartments have been targeted for recombinant protein in order to increase protein stability in seeds. The ER (endoplasmic reticulum) compartment in plant cells has been demonstrated to contain few proteases and therefore represents a relatively protective environment. Retention of proteins in the ER therefore increases protein stability and yield due to the lack of protein degradation. KDEL, an ER signal peptide, was used to target the deposition of a recombinant protein to the ER [91, 97] . Recombinant protein targeted to seeds also allows long-term storage (up to three years) at room temperature without a detectable loss in activity [8] .

Targeting of PMF-based products to seeds allows for long-term protein stability, as well as easy harvesting, storage, and transportation. Relative to leaves, seeds contain fewer native proteins and less phenolic compounds and secondary metabolites. Seeds are easy to surface wash and sterilize which also facilitates commercial production. Collectively, the advantages of producing recombinant proteins in seeds make this platform a cost-effective platform for PMF-based products [8, 98] .

Greater than 50% of human proteins are glycosylated. One third of all approved pharmaceuticals are glycoproteins [41] . Plants present an advantage for the production of recombinant proteins due to their capability of performing a variety of post-translational modifications, including glycosylation and lipid addition. Plants and animals share a similar early stage glycosylation pathway before nascent N-glycan reaches the Golgi apparatus. This pathway produces β-(1,4)-linked galactose and sialic acid in animals and N-glycan (core β-(1,2) xylose and core α-(1,3)) fucose moieties in plant. Glycosylation of native plant proteins is essential for their proper function during plant growth and development [99] [100] [101] .

Plant-specific, hyperglycosylated proteins, however, may be immunogenic in humans. Immunization of mice and rats with horseradish peroxidase elicited the production of antibodies (Abs) specific for plant glycans [52] . Immunization of rabbits with a plant-derived, human monoclonal antibody resulted in a strong immune response to a plant specific glycan [49] . A human sera test also detected antibodies to plant glycans. The anti-plant glycan immune response is highly undesirable and could prevent regulatory approval of a glycosylated PMF when the recombinant protein is intended to be administered by injection [11] . Few human trials have investigated whether or not PMF-based recombinant proteins can elicit an immunogenic response in human patients, although it has been estimated that at least 20% of patients would be allergic to plant specific N-glycan [50] . Even though no clinical trials have revealed an adverse effect from an immune response to plant glycans, the subject is still a source of debate and has hampered the development of PMF-based pharmaceuticals [11, 102] .

Researchers have addressed concerns about plant-specific glycosylation by altering the pathway that plant cells use to process the recombinant protein.

Recombinant proteins have been targeted to the endoplasmic reticulum (ER) where non-immunogenic, high-Man-type N-Glycans are produced [103] . Other studies have tried to prevent the production of plant glycan moieties using a knockout [104] or by silencing α-(1,3) FucT and β-(1,2) XylT with RNAi or an antisense approach [79, 105, 106] . Strasser et al. (2004) [104] produced a triple knockout in Arabidopsis, resulting in the absence of plant-specific glycans, in order to produce recombinant protein with humanized glycan structures. A recombinant human antibody fragment and an active enzyme were successfully produced with a controlled glycosylation pattern using this triple knockout Arabidopsis mutant [99] . RNAi lines of Lemna [79] , alfalfa [105] , and tobacco [106] have been developed that down regulate β-(1,2) XylT and α-(1,3) FucT activity. All of these RNAi lines were capable of producing human proteins without the addition of plant-specific glycans.

Efforts to optimize the plant glycosylation pathway have improved the therapeutic safety of PMF-derived recombinant proteins and reduced concerns about plant-specific glycan immunogenicity [103] . Protalix Biotherapeutics has produced a ""glycan-better"" taliglucerase alfa (ELELYSO™) to treat the rare genetic disorder, Gaucher's disease. The enzyme is targeted to the vacuole of carrot cells. Unlike the equivalent product produced in CHO cells, which express a recombinant protein with a terminal sialic residue that needs to be removed by an exoglycosidase to expose the terminal mannose moiety, the recombinant enzyme produced in a plant-based system already exhibited terminal mannose residues capable of specifically binding to the receptor on a microphage. The plant derived recombinant protein does not need an enzyme to expose mannose residues, which significantly lowers the cost and complexity of downstream processing; thus reducing the costs of production and therapy. Another example is Lemna (duckweed)-derived mAbs (Biolex/Synthon, Durham, NC, USA). RNAi technology was used to reduce the level of enzymes involved with plant core β-(1,2) xylose and core α-(1,3) fucose synthesis The recombinant mAbs produced in the duckweed system contains a single major N-glycan that has a 20-fold better antibody-dependent cell-mediated cytotoxicity and a 10-fold higher cell receptor binding activity than mAbs produced in CHO cells [79] .

Extraction and purification of PMF-derived pharmaceutical proteins can be complex and costly from an industrial perspective. It has been estimated that purification and downstream processing represents 80% to 90% of the cost of producing PMF-derived pharmaceuticals [42] . For each platform, a specific recovery and purification protocol has to be optimized. Numerous protein recovery and purification processes have been developed on a case-by-case basis [107] . For example, Kentucky Bioprocessing Inc. (Owensboro, KY, USA) has developed a platform of protein expression, production, and processing in transgenic tobacco that conforms to Current Good Manufacturing Practices (cGMP) [23] . Healthgen Biotechnology Corp. (Wuhan, China) has developed an rice seed-based platform, OryzExpress, for producing a variety of products, such as recombinant human albumin, antitrypsin, protease inhibitor, IGF-1, etc. [29] . Plant substances, such as waxes, phenolic compounds, pigments, lignin, and endogenous proteases, also create problems in downstream processing. For instance, phenolic oxidation can result in protein aggregation and precipitation, or endogenous proteases can cause proteolysis [88] . The overall strategy used for downstream processing has to be economically competitive, robust, and compliant with cGMP [108] . In 2015, Caliber Biotherapeutics [109] built a plant-based manufacturing facility that has capacity to process over 3500 kg of tobacco (Nicotiana benthamianai) biomass per week. A transient expression approach is used to produce ""biobetter"" monoclonal antibodies (mAb) and anti-viral mAb. The downstream processes in this facility include an automated system for the production of various buffers used for separation, high-capacity chromatography, and formulation (cryostorage). Yields of the recombinant protein are in the range of 62%-68% of total protein and the purity of the final product is 95%-98% [110] . The costs of producing, purifying, and formulating the PMF products has been significantly reduced. For example, plant-derived vaccines for the flu cost $0.10 to $0.12 per 50-µg dose [110] .

Plants have the potential to rapidly produce recombinant proteins on a large scale at a relatively low cost compared to other production systems. PMF-based production of pharmaceuticals, topical compounds, and nutritional supplements is feasible, however, concerns about biosafety, human health (allergenic response to plant-specific glycans), and other factors need to be adequately addressed. Downstream processing and purification of PMF products is currently tedious and costly. Systems need to be developed that simplify the purification process in order to make the production of industrial quantities of PMF-based products feasible and cost effective. The right candidate genes, a strong commercial need, and a good production system will build a bridge between basic research on PMF and its commercial application.

",0.6429668851281435
Barriers to the use of face protection for standard precautions by health care providers HHS Public Access,Health care providers sometimes choose not to use face protection even when indicated as part of standard precautions. We performed a survey of pediatric health care providers to determine barriers to the routine use of face protection. Lack of availability at the point of care and a perceived lack of need were the most commonly cited issues. Continuing education is needed regarding situations in which face protection is indicated for standard precautions.,"We created a survey to assess practices and attitudes regarding the use of face protection by health care providers. The 10-item survey addressed frequency of use of face protection when suctioning and barriers to use. Items were written based on expert input from infection preventionists, hospital epidemiologists, and an infectious diseases fellow. The survey was pilot tested prior to fielding, and revisions were made based on the feedback provided.

We administered the survey in December 2013 using both electronic and paper modes to designated nursing leaders in intensive care units and medical floors. The nursing leaders then e-mailed it to nursing distribution lists and provided paper copies for other clinicians working on those units. The survey was considered a quality improvement activity, and institutional review board approval was not sought.

The survey was sent to 606 staff, and 221 individuals (36%) responded. Registered nurses accounted for most respondents (81%); other participants included respiratory therapists (9%), physicians (3%), and other clinicians (7%). Forty-six percent had >10 years of clinical experience, 26% had between 6 and 10 years, and 28% had <6 years of clinical experience.

A minority of respondents reported always or usually wearing a mask or eye protection while suctioning a patient (Fig 1) . When asked why they did not use face protection in this clinical scenario, 48% reported that in an emergency, face protection is not a priority. Of the respondents, 35% identified that face protection is not readily available in the patient's room, and 15% felt they had to walk too far to obtain face protection. One-quarter of respondents reported they did not think face protection was necessary during open suctioning. Fourteen percent reported they wore eye glasses and therefore did not require eye protection, 8% felt that eye protection impedes their vision, and 7% thought face protection was too uncomfortable to wear.

To assess the availability of face protection, we asked where on each unit it was located. Locations identified by respondents included the clean supply room (43%), precaution carts outside the patient's room (31%), and inside the patient's room (26%). More than half (52%) thought face protection supplies should be kept within the patient's room to make it more accessible, and 35% thought these supplies belonged on the precaution carts outside the room.

When asked if they had ever regretted not wearing face protection, 34% responded yes. If face protection was more accessible, 37% said they would be more likely to use it, and 14% said they would not use it.

We identified that at our hospital, clinicians are not consistently compliant with using face protection as part of standard precautions while performing open suctioning. A common barrier cited was the lack of readily accessible supplies at the point of care. In our facility, the variability in patient room and unit design makes it difficult to identify a single standard location that providers could associate with finding goggles or face shields. Most respondents felt that equipment should be available either in the patient's room or just outside the room with other precautions supplies, such as gowns and gloves. It is important for hospital infection prevention programs to recognize that if face protection supplies are not available at every bedspace or room, a substantial proportion of providers may choose to omit using face protection in situations in which it is indicated. 4, 5 In addition, our survey revealed that face protection was perceived by many providers as unnecessary, despite our inclusion of education about standard precautions as part of our hospital's required annual computer-based training for all clinicians.

Our survey had several limitations. We did not attempt to verify self-reported responses about the use of face protection, and we do not know whether nonrespondents would have answered differently. In addition, social desirability bias may have influenced the responses. Despite these limitations, it seems clear that more detailed education is warranted for clinicians about specific situations in which standard precautions should be used. Clinicians may be making a judgment that open suctioning poses no risk of splashing or spraying; if so, it is important to elicit such opinions to initiate a meaningful discussion about the potential benefits of face protection. The choice to omit face protection may also reflect the inconvenience of wearing goggles or a face shield during clinical care, as some of our respondents noted. Offering multiple design options and including frontline staff in decisions about product selection may help ameliorate these concerns somewhat, but ultimately the decision to use face protection will be made by individuals based on their assessment of risks versus benefits. The role of the infection prevention program should be to ensure that reasonable options are easily accessible and that providers fully understand the potential risk of transmission of infection at the time they make their decision. 

",0.6421371598600903
a Stakeholder Survey on live Bird market closures policy for controlling Highly pathogenic avian influenza in Vietnam,"Extensive research in Vietnam and elsewhere has shown that live bird markets (LBMs) play a significant role in the ecology and zoonotic transmission of avian influenzas (AIs) including H5N1 and H7N9. Vietnam has a large number of LBMs reflecting the consumer preferences for live poultry. Under pressure to mitigate risks for H7N9 and other zoonotic AIs, Vietnam is considering, among other mitigation measures, temporary closures of LBMs as a policy to reduce risk of AI outbreaks. However, the efficacy of market closure is debated, particularly because little is known about how poultry traders may react, and whether trading may emerge outside formal marketplaces. Combining efforts of anthropologists, economists, sociologists, and veterinarians can be useful to elucidate the drivers behind poultry traders' reactions and better understanding the barriers to implementing risk mitigation measures. In this paper, we present results from a stakeholder survey of LBM stakeholders in Vietnam. Our qualitative data show that trading outside formal markets is very likely to occur in the event of a temporary LBM market closure. Our data show that the poultry value chain in Vietnam remains highly flexible, with traders willing and able to trade poultry in many possible locations. Our results indicate that simplification of the poultry value chain along with strict enforcement, engagement of stakeholders, and adequate communication would be a necessary prerequisite before market closure could be an effective policy.","Extensive research in Vietnam and elsewhere has shown that live bird markets (LBMs) play a significant role in the ecology and zoonotic transmission of avian influenzas (AIs) including H5N1 and H7N9. Vietnam has a large number of LBMs reflecting the consumer preferences for live poultry. Under pressure to mitigate risks for H7N9 and other zoonotic AIs, Vietnam is considering, among other mitigation measures, temporary closures of LBMs as a policy to reduce risk of AI outbreaks. However, the efficacy of market closure is debated, particularly because little is known about how poultry traders may react, and whether trading may emerge outside formal marketplaces. Combining efforts of anthropologists, economists, sociologists, and veterinarians can be useful to elucidate the drivers behind poultry traders' reactions and better understanding the barriers to implementing risk mitigation measures. In this paper, we present results from a stakeholder survey of LBM stakeholders in Vietnam. Our qualitative data show that trading outside formal markets is very likely to occur in the event of a temporary LBM market closure. Our data show that the poultry value chain in Vietnam remains highly flexible, with traders willing and able to trade poultry in many possible locations. Our results indicate that simplification of the poultry value chain along with strict enforcement, engagement of stakeholders, and adequate communication would be a necessary prerequisite before market closure could be an effective policy.

Keywords: avian influenza, live bird market, market closure, trader opinion, poultry value chain, risk mitigation iNtrOductiON aNd purpOSE A number of studies point to the significance of live bird markets (LBMs) in the maintenance, transmission, and spread of avian influenza (AI) viruses in poultry populations, and highlight the role of LBMs in transmission of zoonotic influenza viruses to human populations (1) (2) (3) (4) (5) (6) (7) . Studies in Vietnam have shown that LBMs are at high risk for presence of AI viruses (8) , and market practices were significantly associated with AI virus contamination (9) (10) (11) . In China, epidemiological studies have indicated that exposure to live poultry or contaminated environments, especially markets where live birds are sold, were significant risk factors for influenza A (H7N9) infection in human (6, 12) . In a number of Chinese cities, government authorities have closed live poultry markets as part of the effort to control the epidemic. 1 Emerging subtypes or clades of highly pathogenic avian influenza (HPAI) viruses could be detected in Vietnam months or years after similar viruses were detected in China, e.g., H5N1 clade 1 and clade 2.3.2.1a and H5N6. Cross-border trade in poultry is suspected to be an important mechanism for the introduction of new zoonotic and HPAI viruses into Vietnam (14) . The government of Vietnam is considering closure of LBMs as a possible emergency intervention if H7N9 or other zoonotic influenza viruses are detected in the market or in a person who has visited the market. Besides reducing direct contact between poultry and people, temporary market closure would enable cleaning and disinfection aimed at reducing virus accumulation, amplification, and spread among poultry population and transmit to humans. Market closure would include culling and disposal of all poultry on the day of closure, and the prohibition of holding or selling poultry in the market for 7 days while cleaning and disinfection would be conducted [Vietnam Ministry of Agriculture and Rural Development Action Plan (2014) on Emergency Response to Dangerous Avian Influenza Virus Strains with Potential Infection on Humans].

Although scientific consensus exists about the role of LBMs in the ecology of AI, significant debate remains about whether closure of LBMs will effectively transform this viral ecology and reduce risk of AI transmission. Both qualitative and quantitative studies of China's 2013 LBM closures suggested that they were effective in reducing the number of human infections with H7N9 (15) (16) . Studies in Hong Kong, in particular, have shown that emergency closure can transform the ecology of AI and reduce risk of transmission (17, 18) . However, Fournié and Pfeiffer (19) question whether market closure can be an effective long-term strategy or can be utilized in resource-poor settings. In particular, they suggest that closure may not be as effective in a future epidemic if informal marketing channels develop. Parallel informal trade routes could spread the virus to new locations, transform the structure of viral transmission networks, and worst of all, render existing targeted surveillance and risk management activities less effective (13) . Fournié and Pfeiffer (19) highlight the importance of assessing the feasibility of closing markets and the likelihood of unintended adverse results, before implementing such a measure.

Although previous studies analyzed the natural ecology of poultry and AI viruses in LBMs, they left unanswered this fundamental question about the feasibility of market interventions: how will poultry traders and LBM market managers respond to market interventions, including temporary market closure? This study aims to answer this question through a qualitative, participatory survey of the perceptions and opinions of LBM stakeholders toward disease risk in LBMs and toward temporary market closure as part of government risk mitigation interventions.

Previous surveys in LBMs focused on hygiene practices and risk behaviors (20) , quantifying trader scale, and analyzing market chains (9, 11, 21) , but provided minimal information about the perceptions and motivations of traders or market managers. To date, a few studies have analyzed social and cultural factors impacting the ecology of AI, focusing on farmers (22, 23) or consumers (24) . This study fills this remaining gap in the understanding of social and cultural factors that are relevant for the ecological dynamics of AI through an in-depth survey of the perceptions and opinions of poultry traders and market managers in LBMs, with a particular focus on their perceptions toward temporary market closures. The aim of this study is to provide policy makers with field evidence for developing adequate risk mitigation policies in response to new introductions or detection of zoonotic AI viruses.

The study employed a participatory stakeholder approach that investigates perceptions and opinions of stakeholders about problems and policies (25) . Participatory research can be defined as ""systematic inquiry, with the collaboration of those affected by the issue being studied, for the purposes of education and taking action or effecting change"" (26) . Previous research has shown that early stakeholder involvement in the response to an environment or health problem is more effective in terms of reducing negative impact and adverse reactions than post hoc surveys of stakeholder reactions to a policy intervention (25, 27) . In this study, we adopted a prospective approach by surveying the opinions and perceptions of stakeholders in advance of policy implementation.

The study was designed by an interdisciplinary team of trained anthropologists and sociologists, including both Vietnamese and international researchers, and in consultation with experts in animal health and AI. Guiding questions were prepared and pretested in one of the LBMs in Ha Noi, which are similar in structure and trading operations with LBMs in survey areas. The piloting markets were excluded from the survey. The interviews conducted by a team of three researchers with in-depth experience in participatory survey methods. The interviewers exchanged information at the end of each interview day in the field to ensure consistency of the field interviews. Questions were addressing aspects related to market closures such as reaction of stakeholders, impact of market closure on the livelihoods, other trading options for poultry in case of market closure, reaction on compulsory culling of poultry, and willingness of stakeholders to collaborate and under which conditions traders would comply with government policy on market closure. Interviews were conducted from February to March 2014. In total, 91 faceto-face interviews were conducted with poultry wholesalers, middlemen, transporters, and retailers. The interviewees were selected randomly in the survey markets. Notes were taken by the interviewers, and data were subsequently analyzed by coding of interviews. Interviews were also conducted with market management boards in six of eight LBMs. Two markets did not have market management boards, as they were open street markets. During interviews, stakeholders were presented with the possibility that markets would be closed by the government for a temporary period of time, either 7 or 21 days, based on the Vietnam action plan on emergency response to dangerous AI virus strains with potential infection on humans. Under this plan, the decision between 7 and 21 days closure should be based on the magnitude of the disease situation. While a closure of 7 days would apply as emergency control response in markets in a small geographic area, a 21-day closure would come into force in case of geographic spread of the disease and would include markets in a wider geographic area. These stakeholder groups were defined as follows: (1) a market manager appointed by local government to manage the market; (2) a wholesaler trades a high volume of poultry, primarily purchasing from farms and selling to other traders; (3) a middleman trades a small to medium volume of poultry, purchased from wholesalers and sold to other traders; (4) a retailer trades a small volume of poultry and sells directly to the end user (consumer); and (5) a consumer is a purchaser and end user of poultry. . The markets were selected to represent diversity in scale, management, trading operations, and mode of construction, which would need to be considered by government interventions in case of HPAI outbreaks.

The term ""traders"" ( Table 1) includes live poultry wholesalers, middlemen, and retailers. The number of live poultry traders in each market range from 10 to 190, with a mean of 62 and a median of 57. The largest market is Ha Vi market, with 190 wholesalers and middlemen trading over 30,000 birds per day.

All the surveyed markets operate 7 days per week. Three out of the eight markets are sheltered or roofed and enclosed. Traders with permanent stalls in the market pay a monthly hygiene fee of about 50,000 VND 2 to the market management board, which hires cleaners to clean the market at the end of each day. In addition, investments have been made in two of these markets to improve the hygienic situation. For example, Ha Vi market was built during 2007-2011 with funds from the World Bank through the Vietnam Animal and Human Influenza Control and Preparedness (VAHIP) project. The VAHIP invested in a waste water treatment and drainage system. However, in all three of 2 Equivalent to 2.2 USD. these markets, drainage systems remained clogged by solid wastes and therefore ineffective.

Four of the eight markets located on open streets. In these street markets, traders pay a daily market fee of about 3,000-5,000 VND 3 per trader. Some of these street markets are nearby to official, enclosed marketplaces that do not sell live poultry. One market is neither indoors nor on a public street, but on an area of barren land. When it rains, the ground turns to mud.

In response to the possible 7-day market closure, all stakeholders pointed to the likelihood that parallel trading outside the market would emerge. At the same time, responses to parallel trading diverged according to the scale of the traders' operations. The opinion of stakeholders on market closure for 7 days is summarized in Table 2 .

However, in response to the possible 21-day market closure, the divergence of opinions shifted. In this case, middlemen and retailers joined wholesalers in declaring that they would halt trading of live poultry altogether. They stated that with such a long, and probably widespread closure of markets, the market demand for poultry would likely decline sharply. Many traders suggested they would temporarily shift to other jobs, such as agricultural work, or trading other products (vegetables, rice, pork, kittens, puppies, etc.). In addition, they would request for exemption or reduction of taxes and other charges (market fee and charges). Market managers remained consistent in declaring they would comply with regulations, and also noted that they would request remissions of taxes or revenue charges.

Although wholesalers had declared they would halt poultry operations during both 7-and 21-day market closures, further inquiry revealed that they disagreed with compulsory culling of poultry associated with market closures in case of market closures as specified in the Vietnam action plan. Wholesalers argued that their birds have been carefully selected and have farm origin and vaccination certificates issued by animal health authorities. Therefore, they believed the birds could not be responsible for any AI outbreaks, and therefore should not be culled. If the authorities forcibly cull poultry, wholesalers argue that they should be compensated according to the purchase (farm) price or at 50-70% of the birds' market value. In addition, they called for assistance, such as preferential loans, following the end of the outbreak and the resumption of normal market activities. Middlemen and retailers also disagreed with the culling policy, in particular the culling of ""healthy looking"" birds. Both groups of stakeholders called on the government to compensate for any culled birds at market or farm price. Retailers also suggested that they might try to bring birds home to avoid being culled. The market managers worried that traders would protest against any culling of poultry. They suggested that any decision to cull poultry should combine strict enforcement with good communication and explanation. They noted that the government does have a mechanism for assistance and compensation in the case of poultry culls and suggested it should be used to enhance compliance of traders. If a zoonotic influenza virus is detected in a market, the veterinary authorities will also need to go beyond local market interventions and rapidly identify the source of the infection to focus control measures at the origin. When reporting about the willingness and ability to locate the farms of origin, responses varied according to the scale and structure of trader operations. Wholesalers claimed that tracing their poultry back to the farm of origin would be easy since their poultry typically have origin and vaccination certificates issued by animal health authorities. Wholesalers would be able and willing to provide the addresses of the farms of origin.

Middlemen claimed they purchased birds both directly from farms and from other markets or street vendors. Birds purchased directly from farms would be easy to trace, but it would be difficult or impossible to trace birds purchased at markets or on roads. Finally, retailers suggested that it would be difficult or impossible to trace the origin farm of their birds, because they purchase birds from different sources.

Research in Vietnam indicates that markets connected through trade networks can contribute to large-scale epidemics, while providing opportunities for effective control as well. Targeting network hubs for surveillance, hygiene and biosecurity interventions at LBMs could reduce the transmission of virus through the network (11, 28) for China (4) . The results of our study reveal that despite their position as hubs in trade networks, temporary ""emergency"" market closures of 7 days in case of new detection of AI viruses are unlikely to reduce the spread of AI viruses. Poultry traders, in particular middlemen and retailers, maintain a highly flexible practice of market transactions along the poultry value chain. The physical location of the LBM is only one among many possible transaction sites. Our results showed that temporary market closure for 7 days is likely to lead to establishment of parallel, informal, and uncontrolled live poultry trade, which could lead to virus introduction into non-affected areas (29) . Our study concludes that given the structure of Vietnam's poultry value chain, which remains highly flexible with numerous middlemen between producer and consumer, closure of LBMs, unless implemented on a longer term and in a larger geographical area or nationwide, will be an ineffective strategy for reducing the risk of AI. However, decisions on longer term closures, would need to take into consideration the economic effects on the poultry sector (30) . The importance of timely and appropriate compensation following simple procedures for culled birds appeared consistently among the responses of all the stakeholders. While current Vietnam government regulations do foresee financial compensation for compulsory culling of poultry at farms, there is at present no provision for compensation for poultry culled at markets. If stakeholders do not perceive culling as a justified measure, they will be more prone to disobey regulations and trade their poultry through unofficial channels. Proper communication of compensation schemes has shown to be crucial to improve compliance and avoid unintended effects (31, 32) .

The results of our study demonstrated that poultry value chains in northern Vietnam contain a high degree of flexibility. In an agricultural value chain, ""actors are connected along a chain producing, transforming, and bringing goods and services to end-consumers through a sequenced set of activities"" (33) . Value chain analysis has tended to provide formalistic accounts of market relationships, focusing on the vertical links that bring a product ""from farm to fork. "" We propose the concept of the ""flexibility"" of the value chain to describe the capacity of a value chain to shift spatially, or to forge new transaction links, in the event that a particular site or relationship of exchange is eliminated (e.g., through market closure). Our study exposed the high degree of flexibility of the value chain in Vietnam. The flexibility of market transactions far exceeds the physical space of the marketplace, i.e., the LBM. If the LBM is closed for 7 days, trading would continue in other forms and locations. At the same time, this flexibility itself was not shared equally by all stakeholders. Wholesalers, due to the large scale of their operations, were more closely bound to the institutional setting and physical site of the formal LBM marketplace. Furthermore, they reported that they purchase poultry with official certification of farm of origin, indicating a relatively stable and traceable part of the value chain. Small retailers, by contrast, purchase birds from middlemen or wholesalers each day and sell them again, often on the side of small streets or by delivering them to small restaurants. Middlemen buy and sell poultry with the highest degree of flexibility: they report that they could easily shift operations to other markets in the event of market closure for 7 days. The restructuring and simplification of the poultry value chain as suggested by several poultry value chain studies conducted in Vietnam (FAO unpublished data), by reducing the number of middlemen and small-scale traders, could decrease the overall flexibility of trading and therefore improve the effectiveness of market closure. The flexibility of the poultry value chain explains why market closure may not be an effective strategy for reducing the spread of AI or AI incursion risk. The closure of the marketplace is intended to eliminate a key node in the network of AI transmission. But LBM traders do not necessarily confine their trading to the LBM. As traders exploit the flexibility of the value chain and shift transactions to parallel trading sites, live poultry trading networks may expand and fragment, increasing rather than reducing AI transmission and risk.

Finally, our results also revealed important limits to the flexibility of the poultry value chain. A minor limit exists in the length of market closure. During a 7-day closure, all stakeholders described how they would adapt by shifting market operations to other locations, but during closure of minimum 21 days, all stakeholders reported that they would halt trading operations. Rather than trading live poultry in alternate locations along the value chain, they reported that they would shift to other forms of economic activity: trading non-poultry products or even returning to farm work. However, prolonged market closures may result in high economic losses and impact livelihoods in the poultry production sector (34) .

The present study shows that analyzing perceptions of stakeholders regarding risk mitigation interventions, such as the temporary closures of markets, are crucial for the design of effective policies and to avoid adverse results.

To date, the implementation of market closures has been based on viral surveillance data relying on virus detection alone.

Neglecting the fact that LBMs are human, social and cultural institutions may render disease control policy ineffective. In fact, the role of LBMs in the chain of influenza transmission is conditioned by the practices and perceptions of LBM stakeholders. As a result, although market places may be closed, marketing practices and networks may continue to operate in a shifted form and facilitate AI virus spread. The position of LBMs in the poultry value chain in the North of Vietnam exemplify how the natural ecology of AI is shaped as a consequence of human perceptions and reflexive practices (35) . In such cases, understanding the ecology of the virus and how to manage its risks relies on understanding the human stakeholders that construct, and can unexpectedly reconstruct, the links in the chains of viral transmission.

Stakeholder participation should be an integral part of the development of science-based policy interventions, not only for reasons of equity and ownership but also more importantly to provide accurate knowledge about natural ecology itself (25) and to ensure planning and implementation of more effective risk mitigation measures.

The study was considered low risk as the main risks to study respondents were believed to be abreact of confidentiality and privacy. To mitigate these risks, the following safeguards were put in place. Verbal informed consent was received from study participants during the recruitment process. Another verbal informed consent was received from participants before each interview or group discussion after clear explanation about the objectives and content of the study. Each study participant was assigned a code to maintain confidentiality during data collection and analysis. No personal identifiers, including names, were collected at any time throughout the study.

All the authors: substantial contributions to the design of the study, collecting data in the field, and analysis and interpretation of the data; drafting the report; approval of the version to be published; and agreement to be accountable for all aspects of the manuscript.

This study was financially supported by the United States Agency for International Development (USAID) under grant number GHA-A-00-06-00001. 

",0.6417035830669487
Self-contamination during doffing of personal protective equipment by healthcare workers to prevent Ebola transmission,"Background: Healthcare workers (HCWs) use personal protective equipment (PPE) in Ebola virus disease (EVD) situations. However, preventing the contamination of HCWs and the environment during PPE removal crucially requires improved strategies. This study aimed to compare the efficacy of three PPE ensembles, namely, Hospital Authority (HA) Standard Ebola PPE set (PPE1), Dupont Tyvek Model, style 1422A (PPE2), and HA isolation gown for routine patient care and performing aerosol-generating procedures (PPE3) to prevent EVD transmission by measuring the degree of contamination of HCWs and the environment.","Ebola virus disease (EVD) is a severe infectious disease with a high fatality rate of approximately 50% [1] . The virus in the blood and body fluids of a patient can enter another person's body through skin lesions or mucous membranes of the eyes, nose or mouth. Therefore, health care workers (HCWs) should wear protective gear and adopt strict infection control measures when caring for suspected patients [2, 3] .

The EVD outbreak has recently prompted interest in personal protective equipment (PPE) apparel and their use [4] . PPE comprise gowns, gloves, hood, face shield, boots, masks or respirators, which are used to protect HCWs from contact with infectious agents. However, although equipped with protective clothing, HCWs can be contaminated if the PPE apparel is improperly removed [3] . PPE must be removed slowly, deliberately and in the correct sequence to reduce the possibility of self-contamination or exposure to EVD [5] .

Several healthcare organisations developed PPE protocols based on the best locally available components. However, HCWs may be hesitant to use a PPE with no empirical validation [4] . Thus, crucial precautions during PPE removal must be determined to effectively protect HCWs [6] .

The Hospital Authority (HA) of Hong Kong is a statutory body that manages Hong Kong's public hospital services [7] . The HA recommends a PPE ensemble with a neck-to-ankle overall without skin exposure to meet the current recommendations of the Centers for Diseases Control and Prevention (CDC) on the PPE to be used by HCWs during management of patients with confirmed EVD [5] . A waterproof hood and a water-resistant gown were designed to cover the head, neck and body of HCWs. Previous studies [8, 9] reported that a water-resistant gown can provide a good physical barrier via preventing the absorption of liquid contaminants, and thus, conferring protection to HCWs who come in contact with body fluids and secretions of patients with EVD. Our previous study has shown that the barrier protection performance and usability of PPEs are affected by the covered area and ergonomic features [8] . However, systematic data on the risk of self-contamination of different PPE types for Ebola prevention remain lacking. In the present study, three types of PPEs, namely, Hospital Authority Standard Ebola PPE set (PPE1), Dupont Tyvek Model, style 1422A (PPE2) and HA isolation gown for routine patient care and performing aerosol-generating procedures (PPE3), were tested. We compared the PPE ensembles used to prevent EVD transmission in terms of protocol deviations during usage and the degree of contamination during doffing.

This research was an experimental study of one group using multiple comparisons.

A total of 59 HCWs were recruited for this study. The sample size was determined as previously described by Guo et al. [8] , who have examined body-contamination rates and environmental-contamination levels during doffing of different PPE types in accordance with the protocol recommended by the HA. Pregnant females and participants suffering from upper respiratory tract infection and respiratory diseases requiring treatment were excluded.

Among the participants, 57.60% (n = 34) were female with an age range of 21-60 years old. The participants were either registered nurses (n = 50, 84.80%), advanced practicing nurses (n = 4, 6.80%), nursing officers (n = 3, 5.10%) or nurse educators (n = 2, 3.40%). The participants worked in units with high infection risk, including the intensive care unit, emergency department, infection control units and respiratory wards, accounting for 47.50% (n = 28), whilst the rest worked in units with relatively low infection risk (i.e., other clinical units apart from the units mentioned above [n = 31, 52.50%]). All the participants have not yet worn PPE2 because this ensemble is generally not adopted in local hospitals for HCWs. Participants who are currently working in high-infection -risk units have more opportunities to wear PPE1 and PPE3 in daily practice.

Three PPE ensembles were tested (Additional file 1: Figure S1 , Additional file 2: Figure S2 , Additional file 3: Figure S3 , Additional file 4: Figure S4 , Additional file 5: Figure S5 , Additional file 6: Figure S6 ). (1) HA standard Ebola PPE set (PPE1) is a neck-to-ankle overall with an overlying water-resistant gown (Halyard, AAMI Level 4 Liquid Barrier Standard), double and long nitrate gloves, boots, hood, disposable face shield and N95 respirator. A bow was tied at the lateral of the waist to minimise the risk of front contamination. (2) DuPont™ Tyvek®, Model 1422A (PPE2) is commonly adopted in clinical settings to prevent Ebola transmission in countries, such as the US [10, 11] and South Korea [3] . Its protective clothing is also fluid resistant, but the design is a one-piece head-to-ankle overall with a zipper on the front. The whole outfit includes double gloves, boots, disposable face shield and an N95 respirator. A plastic apron was used to cover up the front zipper before use.

(3) PPE3 is a HA isolation gown (Medicom®) for routine patient care and performing aerosol-generating procedures. PPE3 was selected as the reference PPE in the present study. A commercially available pure cotton surgical scrub suit (upper and lower working clothes) was worn inside the individual PPE ensembles during testing. Participants were free to select the appropriate size of gowns and gloves and the known best-fitted respirator model (3 M 1860, 1860s and 1870). Table 1 shows the comparison of the three PPE ensembles.

Data collection was performed in an air-conditioned room with an average temperature of 23°C ± 2°C and a relative humidity of 60% ± 3%. Information about the purpose and procedures of the study was provided to the participants, and written consent was obtained prior to the study.

The participants' socio-demographic data, including gender, age, educational background, specialty, working units and clinical experience, were collected. Each subject received a 30 min briefing from a trained research personnel. The donning and doffing procedures for PPE1 and PPE3 were designed based on the recommendations by the HA, whilst the World Health Organisation (WHO) protocol was followed for PPE2 doffing [12] . On the testing day, the participants watched a video about donning and doffing of the PPE ensembles to familiarise themselves with the procedures. The total duration for donning and doffing of PPEs in the videos was 8.74, 10.68 and 4.59 min for PPE1, PPE2 and PPE3, respectively. Posters related to donning and doffing procedures were pinned up in the venue. Participants with long hair were asked to tie up their mane. Watch and jewellery were removed to minimise the risk of exposure during the procedures. Afterwards, the participants donned and doffed the three PPEs in a random order as decided by a computer-generated randomised table.

The experiment was sequentially conducted in three areas. Area A was the 'clean zone' , where the participants donned the working clothes and clean PPE ensemble in front of a mirror. Area B was the 'preparation zone' , where the PPE of the participants was contaminated with a fluorescent solution (UV GERM Hygiene Spray, Glow Tec Ltd., London, England) that mimics contaminated bodily fluids or secretions spread via contact route. Fluorescent solution was sprayed onto the face shield, two upper limb/ gloves and anterior surfaces of the gown at a distance of 60 cm from the participants, which represents the length of a stethoscope, simulating the usual working distance between a patient and an HCW [8] , with an average of 1.99 g fluorescent solution/per stroke [9] . This value was determined using an electronic analytical balance with a precision of 0.1 g (NJW-3000, Xiangxin, Taipei, Taiwan) via obtaining the average of 20 trial cases. A standard of three strokes was sprayed on each body part with a total of 12 strokes made for each case. The weight of the splash in 1 stroke was 1.99 g in this study when the density of the solution was assumed as 1. Area C is the 'degown and test zone' , wherein the participants were required to doff the PPE. A video camera with a high-density capability was set up for subsequent evaluation of protocol deviations during donning and doffing. Protocol deviations are defined as accidental or noncompliance with the donning and doffing procedures of the PPEs under testing. The performance of the participants was monitored using a checklist. The participants were notified immediately of any deviation being committed. For evaluation, all protocol deviations were recorded. The participants were timed and videotaped whilst donning and doffing the PPE. The timer was stopped when the participants removed the final item of the protective clothing.

During the procedures, hand washing with liquid soap and water was performed according to the procedures of individual PPEs. Immediately after doffing, the participants were scanned for the presence of fluorescent solution. The participants' body (hair and head, face, anterior/ posterior neck, left/right arms, hands or wrists, upper/ lower working clothes and clogs) and the surrounding environment (rubbish bin cover, chair, faucet and sink) were examined using an ultraviolet lamp (CheckPoint, 220-240 V/50 Hz; Glow Tec Ltd., London, England) under dim light. Areas of contamination were counted and measured in square centimetres, and the fluorescent patches of different sizes were counted. Contaminated stains were defined as either small-(≤1cm 2 ), medium-(1cm 2 to <3cm 2 ), large-(≥3cm 2 to 5cm 2 ) or extra-large patch (≥5cm 2 ) [8, 13, 14] . The number of contaminated patches and the time consumed by the participants during donning and doffing were recorded. The environment was thoroughly cleaned, and the areas were rechecked for any contamination with the ultraviolet lamp before the next trial. A 15 min break was given, and water was provided to the participants before testing the next PPE to prevent fatigue, which may affect performance.

All data were analysed with IBM SPSS Statistics 23. Descriptive statistics were used for all independent Tyvek, a brand of flash spun high-density polyethylene fibers, a synthetic material. Apparel with elasticated wrists and ankles.

Water resistant isolation gown (Medicom®)

No zipper; the bow of the water-resistant gown is tied at the lateral side of the waist.

Zipper along the center front of the coverall, covered by a plastic apron. (Table 4 ).

Our study demonstrated considerable self-contamination during doffing. This result raised concerns on pathogen contamination of the skin or clothes of HCWs during PPE removal, which may result in self-inoculation and spread of the virus to patients and other HCWs through contaminated body fluids, including blood, urine, vomitus and stool. Gastrointestinal fluid losses of patients with EVD can be massive (5-10 L/day), droplet dispersion can be greater than 10 ft. and serum viral loads of dying patients with EVD can reach 10 billion copies/mL [15] . Given that no licensed vaccines nor proven effective antiviral therapies for EVD are currently available, PPE plays a crucial role in mitigating the risk of HCW exposure to contaminated body fluids in the care of patients with EVD [16] . The frequent occurrence of self-contamination during PPE doffing is also consistent with the findings of previous studies [6] [7] [8] [13] [14] [15] [16] [17] [18] [19] [20] [21] . The most likely contaminated areas include the neck, hands and fingers, arms and wrists and face [14, 17] . A study conducted in South Korea that estimated the degree of contamination during PPE doffing of HCWs reported that the most vulnerable processes comprise the removal of the respirator, shoe cover and hood [3] .

The current study indicated that contamination of the working clothes occurred less frequently during PPE1 removal than during the removal of PPE2 and PPE3, which may be due to the ergonomic features of individual PPEs under testing. PPE1 consists of a neck-to-ankle outfit and includes a hood covering the neck. PPE2 is a head-to-ankle overall, and is a PPE ensemble frequently used in overseas settings to prevent Ebola transmission [3, 10, 11] . However, PPE removal is complicated because of the head-to-ankle, one-piece design and the elastication in the facial opening, wrists and ankles. HCWs have to take off the hood, unzip the front zipper, remove the overall and outer gloves together and place the trousers on the chair, thereby resulting in easy contamination of the hair and head, hands, working clothes, clogs and chair [17] . The elasticated one-piece coverall hood creates a potential contamination risk because the elastic contracts and pulls the outer part of the hood inwards and towards the participants' hair and neck during removal [22] . The zipper and its flap are also placed along the PPE2 centre front. Therefore, a plastic apron is worn to minimise the risk of body fluids being trapped in the zipper region. Herlihey et al. [22] also reported that when the subjects unzip the coverall, the zipper is stuck in the surrounding fabric or the gloves are stuck to the adhesive of the PPE, while unsealing the flaps covering the zipper results in ripping [22] .

The WHO protocol requires the overall to be removed from top to bottom, followed by the removal of outer gloves whilst pulling the arms out of the sleeves of the overall. Special caution is needed to prevent self-contamination. PPE3 is recommended by HA for routine patient care, in which the neck, lower part of the legs and shoes are incompletely covered. Compared with PPE 1, additional sites, including the neck, arms, working clothes and clogs, were heavily contaminated when wearing PPE3 because it cannot provide adequate protection for HCWs caring for patients with EVD. These contaminated regions may be caused by self-contamination during doffing and contamination when the fluorescent solution was sprayed. Considering the possible underclothing contamination during doffing, the working clothes worn under the PPE ensembles should be frequently changed, especially when contamination is suspected.

During PPE1 or PPE2 doffing, the participants have to wear the clean clogs after removing their boots. However, the clogs may be possibly contaminated by the gowns or the environment in some cases. Hence, using footwear covers is an unideal option. During boot cover removal, HCWs struggle to balance their legs in the air [20] . Shoe covers are also difficult to doff, thereby often requiring assistance and increasing the risk of cross-contamination among workers [22] .

The CDC and WHO recommend the use of double gloving with at least the outer pair possessing an extended cuff that reaches beyond the wrist [6] to decrease the incidence of hand contamination and provide improved protection for HCWs during PPE removal [16, 23] . Although double gloving is incorporated into the protocols for PPE use, the removal of the outer and inner gloves should be done with caution, followed by proper hand hygiene.

Previous studies defined contamination as small fluorescent stains (<1cm 2 ) and large patches (>1cm 2 ) [8, 13, 14] and revealed that fluorescent stain sizes are affected during gown removal [8] . In the present study, a precise estimation of the contaminated regions was performed in terms of the size of patches, that is, small (≤1cm 2 ), medium (1cm 2 to <3cm 2 ), large (≥1cm 2 to 5cm 2 ), or extra large (≥5cm 2 ). The stain sizes can be associated with either inadequate PPE coverage or because of self-contamination during PPE removal. For example, PPE3 cannot fully cover the neck of the participants, which resulted in many small or extra-large patches in the anterior and posterior neck region after spraying of the fluorescent solution onto the face shield and anterior surfaces of the gown. Meanwhile, PPE2 offers a high coverage area during fluorescent solution spraying. However, the hair/head, hands or wrists of the participants were heavily In this study, the older staff showed significantly less small-sized contaminated patches on their working clothes than the younger staff. This result may be due to the additional cautiousness of the older staff, whilst working than the younger staff. However, this finding cannot be generalised because of the low number of older staff (n = 4) who participated in this study.

In addition to self-contamination of HCWs during PPE doffing, environmental contaminations, such as those in the lid of the rubbish bin, chair, faucet and sink, were observed. Human-to-human transmission of EVD is also possible via indirect contact with the environment contaminated with such fluids [24] . The virus can survive for several hours on dry surfaces, such as doorknobs and countertops, to several days at room temperature in body fluids, such as blood [25] ; virus-positive samples were still observed 7 days post-mortem [16] .

Considering that hand hygiene methods using alcohol hand sanitiser fail to remove the fluorescent solution, handwashing with soap and water was performed by the participants. Thus, the sink may be contaminated because of handwashing, and the working clothes that came in contact with the sink may be contaminated because of the repeated handwashing. These results suggested that the height and width of the sink must be at a good working level of HCWs to prevent self-contamination during handwashing. Although alcohol gel is commonly used nowadays during PPE donning/doffing, hand cleansing with soap and water is recommended in cases of visible contamination in various situations, such as when areas are contaminated by vomitus, respiratory secretions, or fecal matter. Discarding used PPEs should be given much attention because of frequent contamination of rubbish bin covers [13] .

Deviations of the donning procedure may increase the risk of self-contamination whilst doffing [20] . Although the participants watched a video on PPE donning and doffing to familiarise themselves with the steps on the day of testing, they can also refer to the posters related to the procedures available in the venue. PPE1 exhibited the lowest overall deviation rate among the three PPE ensembles during doffing (2.95, 9. 48 and 3.52% for PPE1, PPE2 and PPE3, respectively). This finding was expected because of the complexity of PPE2, as described above. The highest deviation rate (58.33%) was observed during the simultaneous removal of the overall and outer gloves in PPE2. As mentioned above, this result agrees with the WHO protocol for doffing overall [12] . This protocol requires the participants to remove the inner gloves, which were covered by the coverall. This procedure is difficult for many participants because they can only 'feel' the inner gloves during removal and cannot see them. Therefore, several participants cannot remove the overall and outer gloves together, or in certain situations, they removed both the inner and outer gloves simultaneously. Apart from the emphasis on regular training for HCWs to perform the procedure smoothly, the doffing procedure should be evaluated to increase its practicability for the users.

Being an international aviation hub, Hong Kong is frequently visited by travellers from all around the world. Moreover, contacts between Mainland China and African [3, 17, 26, 27] . On the average, participants used the longest time for donning and doffing PPE2, followed by PPE1 and PPE3. A study reported that HCWs may show poor compliance with proper PPE removal protocol because of time constraints [28] . The most time-consuming processes include removing the shoe covers, putting on gloves and removing the outer gloves [3] . Thus, a short duration of doffing PPE is important for the faultless completion of removal protocol. Familiarisation of the HCWs with the procedures via frequent training and improved ergonomic features is necessary for the PPE design not only to prevent HCWs from self-contamination but also to shorten PPE donning and doffing time.

This study has several limitations. Results showed the possibility of Hawthorne effect because the participants knew that they were being observed during the study. Therefore, compared with previous findings, real-life contamination rates or protocol deviations can be poorer than the findings presented in this study. Result generalisation is limited given the small number of participants, most of which are relatively young staff. A larger sample size with a better balance of staff seniority than that of the present study should be considered in future trials to evaluate whether clinical experiences influence the PPE performance.

The fluorescent solution used in this study was intended to mimic the mechanical effects of body fluids or secretions of patients with EVD. Although this method can provide a visualisation of contamination, it cannot provide information about viral load and shows no response to alcohol-based hand sanitiser, similar to EVD. Future studies may consider using surrogate viruses, such as MS2 (a surrogate for non-enveloped human viruses) and bacteriophage ϕ6 (a surrogate for enveloped viruses such as Ebola) to allow researchers to obtain quantitative data on virus transfer events and risks to HCWs without exposing participants to the risk of infection [16, 19, 23] .

The ethical approval was obtained from the human subject ethics subcommittee of the university [HSEARS20160605001]. All participants provided written informed consent prior to participation in the study.

Were taken from each person in the photos.

",0.6415473718629678
BMC Public Health Non-pharmaceutical public health interventions for pandemic influenza: an evaluation of the evidence base,"In an influenza pandemic, the benefit of vaccines and antiviral medications will be constrained by limitations on supplies and effectiveness. Non-pharmaceutical public health interventions will therefore be vital in curtailing disease spread. However, the most comprehensive assessments of the literature to date recognize the generally poor quality of evidence on which to base non-pharmaceutical pandemic planning decisions. In light of the need to prepare for a possible pandemic despite concerns about the poor quality of the literature, combining available evidence with expert opinion about the relative merits of non-pharmaceutical interventions for pandemic influenza may lead to a more informed and widely accepted set of recommendations. We evaluated the evidence base for non-pharmaceutical public health interventions. Then, based on the collective evidence, we identified a set of recommendations for and against interventions that are specific to both the setting in which an intervention may be used and the pandemic phase, and which can be used by policymakers to prepare for a pandemic until scientific evidence can definitively respond to planners' needs.","Ongoing concerns about the emergence of an influenza pandemic continue as the number of avian and human infections with the H5N1 virus mount [1, 2] . Adequate amounts of vaccine or antivirals are unlikely to be available early on in a pandemic, and the latter could become ineffective because of resistance [3] . These factors have focused attention on the use of non-pharmaceutical public health interventions to inhibit human-to-human transmission and fueled interest in answering important questions about influenza epidemiology and transmission [4] [5] [6] [7] [8] [9] . However, the most comprehensive assessments of the literature to date recognize the generally poor quality of evidence on which to base pandemic planning decisions [8] [9] [10] . In light of the need to prepare for a possible pandemic despite concerns about the poor quality of the literature, combining available evidence with expert opinion about the relative merits of non-pharmaceutical interventions for pandemic influenza may lead to a more informed and widely accepted set of recommendations.

At the request of the US. Department of Health and Human Services, we evaluated the evidence base for nonpharmaceutical public health interventions by reviewing recent published literature, including historical reviews, convening a meeting of experts, and formally eliciting and quantifying expert opinion about the relative efficacy and effectiveness of specific non-pharmaceutical interventions for pandemic influenza. Based on this collective evidence, we identified a set of recommendations for and against interventions that are specific to both the setting in which an intervention may be used and the pandemic phase, and which can be used by policymakers to prepare for a pandemic until scientific evidence can definitively respond to planners' needs. We also identified important areas of uncertainty that warrant further research.

We identified scientific articles through a MEDLINE search that combined communicable and infectious disease with non-vaccine and non-pharmaceutical intervention search terms. We Table 1 . Using a modified rating scale derived from West et al., we then formally rated the strength of the scientific evidence presented in each of the relevant articles [11] . Table 2 presents the modified rating scheme and reports the number of individual articles that fell into each rating category. We have presented a list of the final articles selected for full review and classification [see Additional file 2].

While completing the literature review, we convened a meeting of experts on January 17 and 18, 2006, in Arlington, VA. Attendees represented a broad range of disciplines, including biomedical research, virology, clinical practice, infection control, epidemiology, public health, ethics, law, history, and health policy. All panelists excepting one were based in North America. Participants identified a set of non-pharmaceutical public health interventions that could potentially mitigate an influenza pandemic and grouped them into four categories: (1) infection control and prevention to reduce transmission when contact occurs between infected and uninfected individuals; (2) patient management to reduce contact between infected and uninfected individuals; (3) contact management to reduce contact between possibly infected and uninfected individuals; and (4) voluntary and mandatory community restrictions to reduce contact between groups that may contain infected persons (Table 3) .

Participants qualitatively evaluated each intervention considering a broad range of factors, including efficacy (effects under ideal conditions) and various aspects of effectiveness (effects under real-world constraints). The latter included feasibility, cost, logistics, operational and infrastructure constraints, and acceptability in terms of concerns surrounding legality and ethics, equity, public confidence, and potential unintended consequences. In their deliberations, the experts also considered the different settings in which these interventions might be applied (healthcare facilities, community-based institutions, and private households). They also considered the epidemic phases in which the use of these interventions should be evaluated (no pandemic, no US pandemic, early localized US pandemic, and advanced US pandemic). Following the meeting, we asked each expert to rate 200 unique intervention-setting-phase triads identified during the meeting based on the totality of their knowledge. Thirteen of seventeen responded. Ratings were on a scale of 1 to 5, with 1 being 'not recommended' and 5 being 'strongly recommended'. We scored the ratings using an adaptation of the RAND/UCLA Appropriateness Method [12, 13] . We first discarded the two extreme high and low ratings for each item. We then defined agreement as all ratings falling within a single 2-point range and all other outcomes as disagreement. Among those items for which there was agreement, items with ratings of 4 or 5 were classified as a recommendation for use, and items with ratings of 1 or 2 were classified as a recommendation against use [12] .

Our formal ratings of the articles revealed few high quality studies to inform the evidence base for non-pharmaceutical interventions for influenza. The majority of topically relevant articles we identified were narrative reviews, case reports, observational studies or expert opinion, editorials and commentaries (Table 2) . We found only 9 systematic reviews of relevant material and 3 randomized clinical trials. Additionally, few of the topically relevant articles were directly on-point.

In light of the evident lack of scientific evidence about specific non-pharmaceutical interventions in the context of seasonal or pandemic influenza, there was limited directly useable information from the majority of the studies identified in the formal Medline search. For this reason, we turned to expert opinion to inform and categorize the findings [14] . Expert panels are often used to develop guidelines and recommendations when compelling evidence is lacking. Drawing on both qualitative discussion at the expert panel and quantitative results from the follow-up survey discussed above, we classified the nonpharmaceutical interventions into two broad categories, those whose use was recommended by the panel and those whose use was not recommended. Table 4 (see Figure 1) summarizes the results from the survey questionnaire, providing the complete list of non-pharmaceutical intervention-setting-phase combinations that were queried and the number and proportion of items for which there was agreement (41.5%) or disagreement (58.5%). We included relevant findings from the literature, where available, in our discussion of the specific interventions, and we cited some of the selected studies from the formal Medline search, as well as others that supplemented the search, to provide necessary background information when appropriate and to support some commonly held views about infection control activities. We also note below interventions about which there was disagreement or no recommendation by the panel.

Hand hygiene and respiratory etiquette Hospital-based infection control measures such as hand hygiene and respiratory etiquette to prevent the spread of infection are widely supported in the literature and broadly accepted [15] [16] [17] [18] [19] [20] . Many controlled studies have shown a protective effect of hand hygiene in reducing upper respiratory infections, although few of the infections studied were due to influenza [9, [21] [22] [23] [24] [25] [26] [27] . Some studies suggest that use of an alcohol-based hand sanitizer is more effective in preventing carriage of non-spore forming bacteria and direct spread of most infections than antimicrobial soap or no hand washing, but antimicrobial handwashing products have not been shown to offer an advantage over soap and water [9, 23, [28] [29] [30] [31] [32] [33] [34] .

The experts recommended rigorous and routine hand hygiene as an important strategy for healthcare workers and the general public in all settings and at all phases of a pandemic, including prior to a pandemic (Table 4 -see  figure 1 ). However, important barriers to the effective use of hand hygiene were noted, including adherence, the supply and cost of commercially available disinfectant soaps and alcohol-based rubs, and the pervasive practice of hand-shaking.

Experts also recommended respiratory etiquette as an important means of preventing transmission for all patients and providers in all pandemic phases, and in the community and/or home when the US pandemic is early and localized. Respiratory hygiene and cough etiquette is generally held to include covering the mouth and nose with a tissue or into the upper sleeve when coughing or sneezing, and refraining from spitting [35] . However, they urged that the promotion of respiratory etiquette be coupled with a compelling public education campaign that includes information regarding the signs and symptoms of influenza.

We found little direct empirical evidence on the efficacy or effectiveness of surveillance and case reporting in the context of influenza. Nevertheless, in light of experience with SARS, the experts recommended both as important to containing the spread of a pandemic [36] . Influenza surveillance supports a range of necessary preparedness activities, including: 1) providing information regarding the presence and epidemiology of influenza viruses in the community, 2) determining appropriate interventions, 3) targeting interventions, and 4) generating current accurate information for public health officials, providers and the public. While the experts agreed that human surveillance and case reporting are efficacious and likely to be effective during any pandemic phase, broad endorsement was qualified by concerns about resource constraints, especially in a large outbreak, potential difficulties in cooperation between providers and governmental and nongovernmental entities, the cost of scaling up capacity to report and investigate influenza-like illness, privacy rights and the right to informed consent [5] .

The key limitation of currently available rapid tests for influenza is suboptimal sensitivity, especially in adults [37] [38] [39] [40] . New and more sensitive technologies for rapid diagnosis of influenza that can reliably identify influenza among patients with respiratory syndromes would greatly aid in the efficient allocation of limited resources such as isolation facilities. The experts agreed on the need for increased investment in the development and deployment of improved rapid diagnostic tests for influenza, arguing that such testing will be invaluable for effective surveillance and in managing all but the most advanced phases of a US pandemic. Moreover, since viral diagnosis of influenza is currently not routine practice, the experts reasoned that education regarding the importance of improved tests will be necessary to increase the adoption of such tests in the US health system. If new tests can be packaged in a way that facilitates use in non-clinical settings, their potential to facilitate disease containment efforts will be even greater. However, the lack of incentives for routine use of costly tests could limit development and production of new technologies, creating shortages in a pandemic emergency.

Uncertainty about the mode of influenza transmission has influenced debate about when and whether to use masks or N95 respirators for pandemic influenza. Droplet transmission is thought to be the primary mode of transmission, and provides the basis for CDC guidelines that health-care personnel wear masks for close patient contact (i.e., within 3 feet) to control influenza transmission dur-Survey questionnaire results Figure 1 Survey questionnaire results. *Hospital = inpatient acute care hospital, inpatient long-term care facility or any inpatient setting; 'Ambulatory' = emergency departments, ambulatory hospital care, urgent care centers, providers' offices, clinics or other community-based healthcare settings and includes care delivered in the community by first responders; 'Community' = schools workplaces, churches, malls, stadiums, etc,; 'Home' = care delivered in private residences. †None = overseas cases only; Elsewhere = no cases in your state/locality/jurisdiction; Early Localized = cases your state/locality/jurisdiction; Advanced = widespread human-to-human transmission in the US §Since some items were left blank, the indicated results were based on 9, 10, ing the influenza season [41] . But experience from seasonal influenza also provides evidence of contact, droplet and aerosol transmission of influenza that lend support for N95 respirators, which are designed to stop up to 95% of small airborne particles [42] . A recent Institute of Medicine (IOM) study found that empirical evidence about the efficacy or effectiveness of inexpensive, disposable masks and respirators against influenza is limited [43] [44] [45] [46] . Our experts recognized this as an area of significant controversy and complexity, but they generally recommended reserving surgical masks, N95 respirators and other personal protective equipment for hospital and ambulatory patients and providers when a community outbreak begins or when the pandemic was widespread. Moreover, surgical masks and N95 respirators were recognized as a non-invasive technology that would induce no antiviral drug resistance. The experts qualified their recommendation, noting that poor training, improper use and, for N95 respirators, the need for fit-testing may compromise the overall effectiveness of these measures.

The amount of influenza virus shed by symptomatic individuals is greater than in the asymptomatic phase, but viral shedding typically begins shortly after infection and before the onset of symptoms. This limits the efficacy of isolation except for individuals completely quarantined almost immediately after contact with an infected person [47] . However, more recent studies report that when numbers are small, isolation in hospitals using appropriate infection control measures may be effective [47] . While discussions were uniformly supportive of routine infection control measures, the experts did not agree on recommendations for mandatory isolation in any specific setting. This was because of the inconclusive nature of the evidence, the concern that healthcare facilities are likely to be rapidly overwhelmed, and that overflow into difficult to manage public settings such as arenas would be less effective. Moreover, mandatory isolation outside of healthcare settings, even if effective and enforceable, raises a range of legal, political and ethical issues that can, at a minimum, erode public acceptance of these policies. Despite the skepticism about mandatory isolation strategies, voluntary self-isolation in the home was recommended for all phases of a US pandemic.

With the exception of some evidence from SARS, we did not find any published data that directly support the use of masks, respirators, or other personal protective equipment by the public, or other steps such as disinfecting surfaces beyond usual practices. The expert's views were mixed. There was uncertainty regarding requirements for masks or respirators because of uncertainty about the relative roles of droplet versus aerosol transmission. Concerns about supply, competency in mask and especially respirator fitting and use, adherence by the public, and social impact of mask-wearing all served to undermine the panel's confidence in the feasibility and acceptability of widespread use. On the survey, experts recommended against the use of masks or respirators by the public prior to the arrival of pandemic influenza and in the early localized phase. For similar reasons, experts recommended against the public use of other protective equipment such as gowns, gloves and protective eye wear.

Although social distancing measures such as workplace closures, limitations on location-based gatherings and events, and mandated travel restrictions have been a recent focus of investigation, and some of these measures were implemented in Asia and North America during SARS, their effectiveness in an influenza outbreak has not yet been established [5, 8, 48] . Despite the propensity of influenza epidemics to be amplified in primary schools, data on the effectiveness of school closures for reducing community transmission are contradictory. Most empirical studies suggest a decline in community transmission rates of respiratory infections with school closures [49, 50] , but the WHO Writing Group also noted older studies showing increases in the spread of disease and subsequent illness after a school holiday, and protective effects when schools remained open [9] . Recent modeling studies generally support school closure and confinement in the home as an effective means of reducing overall attack rates within communities when coupled with antiviral prophylaxis, but predicting the effect of closing schools and workplaces is difficult, since infectious individuals may be displaced into other settings [47, 51, 52] . Models suggest that cancellation of non-essential public gatherings and restrictions on long-distance travel might help to decrease rates of transmission and overall morbidity, but the effectiveness of these interventions has not been quantified.

The experts generally thought that community restrictions could be considered on a case-by-case basis, for example, cancellation of an event to which thousands would travel. However, efforts to forcibly limit public assembly or movement were seen as legally and ethically problematic, especially when there is limited scientific evidence supporting such restrictions. There are also important practical and logistical limitations to mandatory long-term community restrictions and compulsory quarantine, in addition to the problem of likely public opposition to such measures [5, 53, 54] . The experts recommended against these restrictions when outbreaks were elsewhere, and they did not agree otherwise. The same is true for school closures.

Consistent with the literature, the experts contended that widespread and sustained screening of travelers would ultimately be impractical and inefficient as long as detecting asymptomatic shedding is not feasible. Difficulties with the rapid diagnosis of influenza means that travel bans and screening programs risk detaining a large number of symptomatic persons who do not have influenza. There is also the possibility of such measures leading to an international backlash, decreasing cooperation at a time when increased is needed. However, voluntary measures and guidelines would likely be more acceptable and thus more effective. The experts recommended against any mandatory travel restrictions in the advanced phases of a pandemic and did not recommend restrictions on domestic or international departures or entry screening when a pandemic is in the early localized phase.

We evaluated the evidence-base for non-pharmaceutical public health interventions in an influenza pandemic by reviewing the recent scientific literature, convening a multidisciplinary meeting of experts, and eliciting expert knowledge qualitatively and quantitatively. Despite the poor quality of the evidence, the use of expert opinion has enabled us to identify strategies that are likely to help slow influenza transmission in a pandemic setting and also do no harm. Our findings highlight the importance of specifying the setting in which a non-pharmaceutical public health intervention will be used, as well as when its use should be considered. Although the interplay of these factors and deep uncertainty about the relative efficacy or effectiveness of specific non-pharmaceutical public health interventions prevents us from conclusively pinpointing an optimal set of non-pharmaceutical public health interventions for every circumstance, our study provides some important insights about pandemic planning.

Consistent with others, we found that the published literature revealed scant confirmatory evidence on efficacy and overall effectiveness of non-pharmaceutical public health interventions in an influenza pandemic, effectively forcing policymakers to turn to expert opinion [8, 9] . Some infection control studies classified as systematic reviews, observational studies or evidence-based guidelines constituted stronger evidence, at least for spread of respiratory disease [14] . The remaining scientific evidence is of low strength (further research is very likely to change estimates of effect) or very low strength (effects are quite uncertain), specifically with respect to seasonal and pandemic influenza [14] . The published evidence consisted mainly of narrative reviews of past pandemics, contemporary observations, editorials, commentaries and case reports or series. It also included articles drawing inferences from biology and pathophysiology, clinical epidemiology and mathematical modeling studies, rather than from rand-omized controlled trials evaluating interventions. In this context, expert opinion is particularly useful, and several clear messages emerged.

First, policymakers should actively promote personal responsibility for slowing spread of infection through good hand hygiene and respiratory etiquette in all settings and at all times. Use of disinfectant hand soaps and alcohol-based rubs should also be encouraged. Second, developing the capability and capacity for early rapid viral diagnosis should be a high priority. Third, healthcare providers need to be better trained to maximize the effectiveness of infection control measures, including use of masks, respirators, and other personal protective equipment. It may be reasonable to recommend limited use of personal protective measures in certain other settings such as mask-wearing by the ill or perhaps surface disinfection of very heavily trafficked public areas. However, other use of personal protective equipment by the general public is not recommended at this time. Fourth, and also consistent with other studies, we founds that widespread government mandates to segregate individuals, including isolation, quarantine, sheltering, location-based community restrictions, and travel restrictions, are less likely than voluntary measures to be recommended, especially over the longer-term [55] . Instead, less invasive voluntary efforts to reduce social contact, especially self-isolation of the sick at home, self-quarantine of the exposed, and, when feasible, sheltering by the well ought to be widely supported. This will require education, persuasion, and social support to ensure that medical and non-medical needs are met, with the latter being central to the success of sequestration measures in all settings. Fifth, information needs are pervasive. Very little of the literature is on point, and the experts disagreed 60 percent of the time. Well-controlled observational and especially interventional studies are needed, especially in the context of seasonal influenza.

The demand for scientific evidence on non-pharmaceutical public health interventions for influenza is pervasive, and policy recommendations must rely heavily on expert judgment. In the absence of a definitive science base, our assessment of the evidence identified areas for further investigation as well as non-pharmaceutical public health interventions that experts believe are likely to be beneficial, feasible, and socially and politically acceptable in an influenza pandemic. Taken together, the literature and expert opinion reveal the kinds of explicit judgments required to translate existing knowledge into policy-relevant terms. These findings should be considered in forming national, state, local, and facility pandemic plans.

",0.6411630705483493
Commentary Considerations for Recommending Extended Use and Limited Reuse of Filtering Facepiece Respirators in Health Care Settings,"Public health organizations, such as the Centers for Disease Control and Prevention (CDC), are increasingly recommending the use of N95 filtering facepiece respirators (FFRs) in health care settings. For infection control purposes, the usual practice is to discard FFRs after close contact with a patient (""single use""). However, in some situations, such as during contact with tuberculosis patients, limited FFR reuse (i.e., repeated donning and doffing of the same FFR by the same person) is practiced. A related practice, extended use, involves wearing the same FFR for multiple patient encounters without doffing. Extended use and limited FFR reuse have been recommended during infectious disease outbreaks and pandemics to conserve FFR supplies. This commentary examines CDC recommendations related to FFR extended use and limited reuse and analyzes available data from the literature to provide a relative estimate of the risks of these practices compared to single use.","Analysis of the available data and the use of disease transmission models indicate that decisions regarding whether FFR extended use or reuse should be recommended should continue to be pathogen-and event-specific. Factors to be included in developing the recommendations are the potential for the pathogen to spread via contact transmission, the potential that the event could result in or is currently causing a FFR shortage, the protection provided by FFR use, human factors, potential for self-inoculation, the potential for secondary exposures, and government policies and regulations. While recent findings largely support the previous recommendations for extended use and limited reuse in certain situations, some new cautions and limitations should be considered before issuing recommendations in the future. In general, extended use of FFRs is preferred over limited FFR reuse. Limited FFR reuse would allow the user a brief respite from extended wear times, but increases the risk of self-inoculation and preliminary data from one study suggest that some FFR models may begin to lose effectiveness after multiple donnings.

The Centers for Disease Control and Prevention (CDC)-including the National Institute for Occupational Safety and Health (NIOSH), as well as the Occupational Safety and Health Administration (OSHA) and the Food and Drug Administration (FDA)-develop regulations and/or recommendations for the use of respiratory protection in health care settings, and each agency plays a different role which impacts the use of them in health care. CDC develops recommendations for the use of respirators to reduce the spread of disease in health care settings. NIOSH certifies respirators and develops recommendations on the use of respiratory protection in health care workplaces to protect workers. OSHA develops and enforces workplace regulations on respiratory protection. FDA clears the sale of certain types of respirators as medical devices.

The most commonly used type of respirator in health care settings are NIOSH certified N95 filtering facepiece respirators (FFRs). These devices are disposable, tight-fitting airpurifying respirators that have a filter efficiency of 95% or greater for a standard test aerosol. (1) FFRs are also used by workers in many industries to reduce the amount of harmful dusts and aerosols they inhale. Workers are expected to wear their FFR during all periods of exposure. However, there are times of non-exposure when workers need to remove their FFR (e.g., take a drink of water, use the restroom, or go on a rest break) or situations during use when their FFR must be replaced.

Employers have several options for FFR usage to handle these situations. During ""single use,"" users put on (""don"") a new FFR each time they need one and discard their used FFR each time they take it off (""doff""). Another option is commonly referred to as ""FFR reuse."" Reuse involves donning and doffing the same FFR more than once until the FFR is discarded. Employers benefit from FFR reuse compared to single use by extending the lifetime of the FFR so that fewer need to be purchased. There is no specific restriction on the number of uses or donnings. Rather, historical guidance is focused on the length of time the FFR can be used and identifying situations when the FFR should be discarded. In general, NIOSH (2) specifies that the service life of all filters on NIOSH-approved respirators is limited by considerations of hygiene, damage, and breathing resistance and that any filter should be replaced if it becomes soiled, damaged, or causes noticeably increased breathing resistance. In workplaces that could produce high cumulative particulate filter loading (i.e., >200 mg), the service time for N95 FFRs should only be extended beyond 8 hr of use (continuous or intermittent) by performing an evaluation that demonstrates that continued use will not reduce the filter efficiency.

(product code = MSH) occurred in 1996, after FFRs were first recommended by CDC as the minimum level of protection for health care workers (HCWs) treating patients with tuberculosis (4) and NIOSH updated its certification requirements to create the N95 class of filters. (1) Most (22/31 = 71%) of the surgical N95 respirator models in the FDA database were cleared after 2005, which coincides with a period of increased interest in these types of products due to concerns about an infectious disease pandemic.

Because of the concerns that previously used FFRs may be contaminated with infectious material (i.e., act as a fomite), the factors that a health care employer considers in formulating FFR use policies (e.g., single vs. reuse) for its employees are also different from employers in industrial settings. Despite this concern, FFRs are reused under certain conditions in health care. (11) In the health care context, reuse is defined as a HCW donning the same FFR for a series of close patient contacts and doffing it at the end of each of the close patient contacts before it is discarded. Even when FFR reuse is practiced or recommended (discussed in the next section), restrictions are in place (e.g., discard when FFR is contaminated or damaged, becomes difficult to breathe through, and so on) which limits the number of times the same FFR is reused. Thus, FFR reuse is sometimes referred to as ""limited FFR reuse."" Options for limited FFR reuse were provided when FFRs were first introduced as the minimum level of respiratory protection for HCWs in close contact with patients with tuberculosis. (4) (5) (6) Another related FFR use practice, termed ""extended use,"" involves donning a FFR and wearing it for multiple patient encounters without doffing and redonning between patient visits. Thus, the same FFR is worn continuously (for up to several hours) across multiple patient encounters before it is doffed. This practice is only practical when bundled with the practice of cohorting, which involves locating patients with a common diagnosis in the same unit, ward, or zone. Extended use can be implemented separately from reuse (i.e., like single use, discard the FFR once it is doffed) or combined with reuse. Compared to single use and reuse, recommendations for extended use in health care are fairly recent. The first time extended use of FFRs was identified as an option was during the 2009 H1N1 pandemic. (12) Both extended use and limited reuse of FFRs allow the employer to reduce its consumption of FFRs, prolonging existing supplies during a pandemic or respiratory pathogen outbreak or to save money and reduce waste during day-to-day operations (e.g., close contact with tuberculosis patients) by using fewer FFRs, (13) similar to the benefits found for industrial settings. This commentary examines recommendations related to extended use and limited reuse of FFRs in health care. Key scientific and policy issues are highlighted along with considerations for policy makers to weigh when making decisions on whether to recommend extended use and/or limited reuse of FFRs during routine health care situations and for public health emergencies involving respiratory pathogens that have the potential for aerosol transmission. Finally, key knowledge gaps are discussed to identify additional data needs that could enhance understanding of the risks for transmission of diseases associated with FFR extended use and limited reuse. Table I summarizes past and current recommendations for extended use and limited reuse of  FFRs. CDC recommendations were selected for this analysis because of their widespread  recognition in health care. In 2007, CDC published general infection control guidance for isolation precautions, which included a list of all pathogens and medical procedures in which respiratory protection was recommended. (14) For certain pathogens affecting defined populations (e.g., TB) or infectious agents of special interest to health care (e.g., epidemiologically important organisms such as severe acute respiratory syndrome (SARS)] and influenza), CDC publishes detailed specialized infection control guidance. For this analysis, we selected all of the respiratory pathogens in which specialized infection control guidance was published as either interim or final recommendations and included the use of respiratory protection (N95 FFR or higher). This strategy provided a diversity of respiratory pathogens for analysis. These situations include two recent outbreaks/pandemics (2004 SARS and 2009 H1N1 flu), two routine situations (TB and seasonal influenza), and two pathogens of concern (Avian Influenza A (H5N1) and Avian Influenza A (H7N9)).

Cost can be a consideration for adopting extended use and limited reuse practices as it was in adopting the recommendation to allow limited reuse of FFRs when working in close contact with TB patients. However, the CDC recommendations on limited reuse and extended use have primarily considered the specific pathogens involved and the specific characteristics of the event. The first key factor is whether contact transmission is possible for the pathogen. Contact transmission of pathogens occurs through direct or indirect contact with the patient or the patient's environment via blood or body fluids (e.g., respiratory secretions). For pathogens in which contact transmission (e.g., fomites) is not a concern, limited reuse of FFRs has been determined to be a viable option. For TB, the CDC maintains that ""a respirator classified as disposable can be reused by the same HCW as long as it remains functional and is used in accordance with local infection control procedures."" (15) Infection control guidelines for TB (14) recommend only airborne precautions; contact isolation precautions are only needed if extrapulmonary lesions are draining, which occurs rarely. Contact transmission of TB is thought to be highly unlikely. (16) This contrasts with the recommendations for seasonal influenza where contact with contaminated surfaces and objects is considered a possible mode of transmission. (17) In situations where airborne precautions are recommended, and contact precautions are recommended or contact transmission is possible, the second key factor in the CDC recommendations is the likelihood of a localized shortage of the FFRs needed to protect HCWs during high-risk procedures. The use of FFRs for protection of HCWs during routine infectious disease procedures generally does not result in a FFR shortage, as evidenced by CDC's guidance to wear a FFR during aerosol generating procedures (AGPs) on patients diagnosed with seasonal influenza; this does not include an option for FFR extended use or reuse. (17) extended use, or limited reuse, but instead refer back to the general CDC infection control guidance (14) which specifies single use. These recommendations are consistent with the other four recommendations in Table I based on the potential for contact transmission of these pathogens and that FFR shortages are unlikely in the near-term.

However, during periods of high usage (e.g., public health emergencies such as an influenza pandemic (20) or widespread respiratory pathogen outbreak), supplies of FFRs can quickly become depleted because most hospitals maintain only a small inventory of FFRs. Not surprisingly, shortages were reported at the hospital level during both the 2004 SARS outbreak and the 2009 H1N1 influenza pandemic. (21) (22) (23) In a recent evaluation of respiratory protection programs in California hospitals, it was reported that half of the hospital managers interviewed (n = 48) reported shortages of FFRs during the 2009 H1N1 outbreak due to increased demand and supplier lag time in filling orders. (11) During the 2004 SARS and 2009 H1N1 events, recommendations were made allowing the option for extended use and limited reuse, although both recommendations acknowledged situations in which these strategies would not be appropriate.

For SARS, CDC stated in its interim guidance that ""health care facilities may consider reuse as long as the device has not been obviously soiled or damaged (e.g., creased or torn)"" and ""if a sufficient supply of respirators is not available."" (24) The recommendation recognized the importance of preventing contamination through contact with infectious material on the outside of the respirator. CDC also addressed concerns about a shortage of FFRs during the 2009 H1N1 flu pandemic with supply-conserving strategies for hospitals that included the possibility of extended use and limited reuse of FFRs, with extended use preferred over limited reuse. (12, 25) Reuse of FFRs was reported to occur quite often in California hospitals during 2009 H1N1 as either a response to shortages or as standard practice; 81% of survey respondents indicated that their hospital had a plan to implement reuse, while only 12.5% indicated plans to apply extended use. (11) 

As shown in Table I and discussed above, prior and current CDC recommendations made for FFR extended use and reuse were largely based on the type of infection control precautions or transmission mode(s) associated with that pathogen and whether shortages of FFRs were observed or anticipated. Those recommendations were based upon the data available at that time, which often lacked evidence to answer key questions regarding the effectiveness of extended use or limited reuse and the risk of disease transmission from handling potentially contaminated FFRs. In 2006, the Institute of Medicine (IOM) addressed (26) the reusability of facemasks, and summarized the data available to support previous recommendations. The committee agreed with the previous CDC guidance and recommended that ""avoiding contamination [of FFRs] will allow for limited reuse."" The IOM also identified key knowledge gaps that served as a catalyst for increasing awareness of the research needs.

Since publication of the IOM report, numerous research groups have attempted to address some of these knowledge gaps. In the following sections, we discuss studies published since Fisher 2006 that address key areas of FFR extended use and reuse, including FFR protection, human factors (e.g., physiological/psychological effects), self-inoculation, and secondary exposures (e.g., from particle reaerosolization and co-contaminants). Some earlier studies (pre-2006) are also discussed to provide context where needed. The purpose of this analysis is to improve the scientific basis for future recommendations for employers in health care settings to consider when implementing FFR extended use and/or limited reuse. For each of the issues below, a qualitative assessment of the risks of extended use and limited reuse versus single use is presented (see Table II) .

One possible concern with FFR extended use and reuse is that extending the useful life of a FFR could reduce its protective effectiveness (i.e., when worn properly and used in a complete respiratory protection program it provides exposure reduction consistent with the assigned protection factor for this class of respirator). The protection provided by a properly used FFR results primarily from a combination of its ability to filter out (remove) biological aerosols from the inhalation air stream of the wearer and seal tightly to the face (i.e., ""fit""). Each of these concerns has been studied (to some extent) or can be assessed using existing data.

Filter Media-Most N95 FFRs contain a polypropylene electret filtering medium within the layers of a FFR ( Figure 1 ). The electret filtering medium has been shown to capture and retain a majority of airborne biological particles compared to the layers next to the face and farthest from the face, although particle size could affect particle deposition location. (27, 28) Electrets and other similar types of nonwoven air filter media are not unique to FFRs. (29) They are commonly found in various dust collection systems (e.g., vacuum cleaners, clean rooms, and home heat ventilation and air conditioning (HVAC) systems). Recommended replacement life for electret filters in air cleaning systems is typically 3 months of normal use, as the fundamental mechanisms (diffusion, interception, impaction, electrostatic, and so on) of these types of filters do not readily degrade over time with normal use.

Only a few studies have been done to verify FFR performance in extended use or reuse type scenarios. Moyer and Bergman (30) conducted a laboratory evaluation of the intermittent use (short-term use once per week) of N95 filters over several months. Filtration efficiency was reduced to below 95% for filters from 2 of the 3 manufacturers after 9 and 13 weeks of simulated reuse. Researchers at the Institut de recherché Robert-Sauvé en santé et en sécurité du travail (IRSST) validated the long-term filter performance of a single N95 FFR model. (31) For inert particles below 200 nm, filter efficiency levels remained above 97.3%, even after 5 hr of particle loading (i.e., continuous use). Not surprisingly, another study found that samples from 19 of 21 N95 FFR models stored for up to 10 years had expected levels of filtration performance. (32) Fit-Fit is a measurement of the efficacy of the seal between the FFR and the face of the wearer. Components of the FFR, such as straps, face seal material, shape, and adjustable nose bands influence FFR fit. Several studies have analyzed strap performance and fit for multiple donnings of FFRs. Roberge simulated donnings and reported reduction in the strap load for each successive donning with the majority of the reduction occurring after the first donning. (33) However, the FFR model with the lowest restorative strap performance load was still able to pass fit-testing. Bergman et al. examined the effect of FFR reuse on fit by measuring the fit factors of 6 FFR models donned by 10 subjects up to 20 times with wear times of approximately 2 min between each donning. (34) FFR fit gradually decreased over multiple consecutive donnings; however, good fit was observed for some subjects on some models even after 20 donnings. The best levels of fit were observed for the first five donnings, likely because of the relatively little wear on FFR components (e.g., head straps and nosepieces) compared with later donnings.

It was concluded from that study that five donnings could be performed before fit factors started to drop below 100. Catastrophic failure of the FFR (e.g., complete head strap breakage, nosepiece becomes damaged, and so on) should have no effect on risk, if users diligently perform device inspection procedures required during the FFR donning process. Fit of FFRs is also a concern for extended use where the FFR may become wet and deformed due to moist exhaled breath and facial perspiration. Hauge et al. measured realtime fit while HCWs performed three 10-min simulated patient care scenarios. It was determined that initial fit was predictive of fit during the tasks as the five subjects with initial fit factors greater than 200 registered simulated workplace protection factors greater than 400, and the three subjects with initial fit factors less than 200 had simulated workplace protection factors ranging from 132 to 326. (35) Although the tasks were only a combined 30 min, the study design could be considered an extended use scenario covering three patient encounters.

Workplace Protection Factors-Few studies in health care settings measure workplace protection factors (WPF). WPF is a measure of the protection provided by a properly functioning respirator when correctly worn and used in the workplace and is determined as the ratio of the particle concentration outside the respirator over the particle concentration inside the respirator. Infectious bioaerosols are hard to detect and differentiate from noninfectious bioaerosols. (36) Furthermore, assuring compliance during all periods of exposure in the health care setting is challenging. (37) Several studies in other workplaces have assessed protection over extended periods of continuous use by measuring the WPFs: up to 224 min in a steel foundry (38) ; 172 min in a concrete factory (39) ; and 60 min on farms. (40) All three studies concluded that the N95 FFRs provided levels of protection consistent with expectations (i.e., protection factors were ≥ the assigned protection factor of 10), with reported geometric mean WPF values ranging from 18 to 223. No evidence of reduced protection as a function of time was noted in these studies. The aerosol challenge encountered at the farm locations consisted of biological aerosols such as endotoxins and fungal spores which are more closely related to the bioaerosols in a hospital than to the dust encountered at the foundry and concrete factory.

Summary-Overall, the scientific studies provide evidence that extended use is unlikely to reduce the protection afforded by a FFR (see Table II ) and support the CDC TB infection control guidance which states in the Frequently Asked Questions section, ""Disposable respirators can be functional for weeks to months."" However, as noted in Table II , some additional cautions may apply for reuse. Reuse involves multiple repeated uses (donnings) of the same device, and it is possible that some components (straps, nose clips, and so on) could begin to degrade over time and reduce protection. These effects are likely specific to each model of FFR, but the only study published (34) to date on this topic suggests that limiting FFR reuse to no more than five donnings or reuses would provide an adequate safety margin.

One of the consequences of extended use is the need to wear the FFR continuously for up to several hours, compared with single use or reuse in which the FFR would only be worn during the period of close contact with the patient (typically less than 15-20 min). Thus, questions have been raised regarding the safety of long-term FFR use and, if safe, how long HCWs can physiologically and psychologically tolerate extended use.

NIOSH researchers found that FFR use caused no or minimal increases in heart rate, respiratory rate, and transcutaneous carbon dioxide as well as no differences in oxygen saturation on test subjects during 1 hr of low-moderate treadmill exercise when compared with wearing no respirator (control). (41) (42) (43) They also reported that 2 hr of continuous FFR use at low-moderate work rate did not cause a change in core body temperature, (42) and there was no significant increase in FFR deadspace heat or humidity after the first hour. (44) Taken together, these studies suggest that FFR use for 1-2 hr should cause minimal physiological stress to individuals medically cleared to wear FFRs.

A study by researchers affiliated with Department of Veterans Affairs reported how long 27 HCWs could tolerate multiple bouts of 2-hr-long extended use periods, interspersed with 15-30 min breaks. (45) Median tolerance times of 6.6 hr and 5.8 hr were reported for the two FFR models without exhalation valves. Only 16 and 18 of the 27 subjects using those two models were able to complete all four 2-hr use periods of continuous use; the most reported reason for stopping use was head and facial discomfort (e.g., heat). In a follow-up analysis of the same data, it was concluded that FFR discomfort negatively affects respirator tolerance over time, but respirator intolerance is not associated with perceived self-reported exertion. (46) Although the number of participants was small, a recent study reports greater tolerance of extended use of FFRs among HCWs. (47) They reported that 9 of 10 study participants (nurses) were willing to wear FFRs for the entirety of two full 12-hr shifts, stopping only to eat and drink, because it was the end of their shift, or because the FFR was too uncomfortable. The nurses tolerated FFR continuous wear for an average of 223 min on day A study conducted in a teaching hospital in Brazil considered changes in appearance and possible physical damage resulting from FFR reuse. (48) A new N95 FFR was distributed to each nurse once per month and reused as needed until the next new N95 FFR was provided. The researchers found that within 5 days, the majority of the distributed cone-shaped FFRs exhibited visible ""wear and tear,"" indicating possible physical damage (caused by folding them for storage in a pocket) and visible stains/dirt on the FFR interior and exterior surfaces.

Although the performance of the respirators was not assessed, the data suggest that some models may be more suitable for reuse (e.g., those that fold easily) or that hospitals should enforce some restrictions on reuse (e.g., replace every 5 days, rather than every 30 days).

Overall, the available scientific studies provide evidence that HCWs will experience greater discomfort during periods of extended continuous wear of FFRs, but this discomfort will likely be tolerable for most HCWs. Continuous FFR use over extended periods of time up to 12 hr is unlikely to harm workers (see Table II ) who have been medically cleared for respirator use. Furthermore, because HCWs need to take occasional breaks during their work shift (e.g., to use the rest room, eat or drink, and so on) FFR extended use of greater than 4 consecutive hours is unlikely in most settings.

One knowledge gap often cited against allowing FFR extended use and limited reuse is whether a FFR worn during close contact with an infected patient is likely to serve as a fomite. Historically, little data were available to assess the transfer potential of respiratory pathogens from the FFR to the hands of the HCW, resulting in the potential for selfinoculation. Similar to other potential fomites (e.g., surfaces, medical devices, and stethoscopes (49) ) assessing the level of risk of self-inoculation associated with touching a used FFR is complex. It is very difficult to trace a specific hospital-acquired infection to a particular object. Thus, while no studies have identified the use of a contaminated FFR as a source of infection, the possibility cannot be ruled out.

Nicas and Sun and Nicas and Jones have provided models for transmission of pathogens, including influenza, in health care settings. (50, 51) Nicas and Sun considered fomite hazards of textile and nontextile surfaces and in room air to estimate the expected pathogen dose to a HCW's mucous membranes and respiratory tract. (51) Nicas and Jones modeled four influenza virus exposure pathways including fomite transmission. A similar approach is used below to estimate the potential fomite hazard of used FFRs. Factors that influence the risk of self-inoculation directly associated with handling a contaminated FFR include the quantity of respiratory pathogens deposited on the FFR surface (i.e., contamination levels), viability of the pathogen, transfer efficiency of the pathogen from FFRs to the hands of the wearer, and area of hand contact with the contaminated surface.

FFR Contamination Levels-There are no published studies that quantify the amount of pathogens on FFRs used in clinical settings. However, simple mathematical models can be used to provide some estimates. In one study, influenza aerosol concentration, breathing rate of the wearer, time of patient interaction/FFR use, and particle retention efficiency of the FFR were used as inputs to a linear model to estimate influenza contamination levels inside and on the surface of the FFR (C FFR ). (52) Using this model, an increase in any parameter results in higher levels of C FFR (i.e., total number of pathogens on the FFR). For a typical HCW scenario, using model input values estimated from the peer-reviewed literature, the model calculated that C FFR would be approximately 4500 influenza viruses given an influenza aerosol concentration of 12,000 viruses m -3 , a breathing rate of 1.140 m 3 hr -1 , a particle retention efficiency of the FFR of 0.991, and a 20-min patient interaction/FFR use time. The values for influenza aerosol concentration and wear time found in the literature varied more than other parameters used in the model. Thus, for extended use which involves longer wear times, the number of pathogens available for transfer to the hands is increased.

This model illustrates the need to take into account HCW procedures (e.g., AGPs) which can increase C FFR by up to 2200% and the potential for administrative controls such as source control of patients (e.g. asking patients to wear face-masks) which can reduce C FFR by up to 71%. (53, 54) In addition to administrative controls, the use of engineering controls such as local exhaust ventilation might reduce C FFR . Similarly, previous recommendations issued during the SARS outbreak suggested the use of a surgical mask or faceshield on top of a FFR to reduce C FFR (26) ; although subsequent work has identified a number of potential concerns, including regulatory compliance with this approach. (55) While developed for influenza, this model could be used to approximate C FFR resulting from any respiratory pathogen if estimates of the concentrations of the pathogen near the breathing zone of the HCW could be obtained.

Pathogen Survival-Given that FFRs can become contaminated with pathogens when used in close contact with infectious patients, the next factor under consideration is how long these pathogens can survive (remain infectious) and, for some types of microorganisms, grow (propagate) on the FFR surface. Some studies in the early 1990s found that under ideal conditions (e.g., humidity >78%), fungi and certain bacteria could grow on filters made of cellulose because they are capable of digesting cellulose. (56, 57) However, modern (post-1995) FFRs are made of polypropylene, which cannot serve as a nutrient for bacteria. (58) Studies confirmed that surrogates for TB were not able to grow on polypropylene-based filter media, even under incubation conditions. (58) (59) (60) (61) Although bacteria were found to survive for several days, this was not considered a concern because contact transmission for TB is considered unlikely (see Table I ). These studies lent support for the FFR reuse guidance being drafted at that time.

Respiratory viruses have received more recent attention. While growth is not an issue because these pathogens require a host organism to propagate, their persistence or survival on surfaces is a concern. In general, the evidence indicates that viruses are more persistent on nonporous substrates compared with porous materials such as FFRs. Bean et al. reported laboratory-grown influenza A and influenza B survived for 24-48 hr on hard, nonporous surfaces but survived for <8-12 hr on porous substrates. (62) Similarly, another study (63) found that influenza remained viable for 8 hr on FFR samples, but infectivity dropped below detection limits at <24hr. However, Tiwari et al. examined the persistence of two avian respiratory viruses including influenza H13N7 on various substrates and although they found that both viruses survived longer on nonporous surfaces than on porous ones, the viruses remained active for up to 6 days. (64) In one laboratory study, pH1N1 was detected on FFRs for up to 6 days with an average of 90% reduction (1 log) in viability during this time period. (65) Similar findings were found using MS2 phage as a surrogate for respiratory viruses. (66) A surrogate for SARS coronavirus, transmissible gastroenteritis virus, was shown to remain viable for 24 hr on FFR samples with an estimated 99% (>2 log) reduction in titer. (67) )

Another study found that inactivation of Φ6 bacteriophage spiked on a N95 FFR surface was highly sensitive to environmental conditions, with a ∼1 log reduction over 24 hr at 40% relative humidity versus ∼4 log reduction over 24 hr at 60% relative humidity. (68) Although it is difficult to generalize from these conflicting findings, it is clear that for reuse during a work shift with short storage times (< 1 hr) most of the trapped pathogen will remain viable. Some reduction in viability might occur for overnight (>12 hr) or weekend (>24 hr) storage depending upon storage conditions (temperature, humidity, light, and so on) and pathogen type and strain.

In many cases where pathogens remain persistent and pose a contact threat, cleaning and disinfection regimens are routinely used. For example, countertops, exam tables, and other surfaces of patient rooms are often cleaned when a patient is discharged. Research has been conducted on cleaning and disinfecting procedures for FFRs for possible reuse by the same HCW. Although the results appear to be promising, the practice is not currently recommended (69) (70) (71) (72) and thus is not a viable solution at this time to reduce the fomite potential of a reused FFR.

Recent improvements in antimicrobial chemistries have allowed some manufacturers to begin incorporating these technologies into FFRs. There are now a few surgical N95 FFRs incorporating antimicrobial technologies (product code = ONT) that have been cleared by the FDA as medical devices. (9) Interestingly, one device has been cleared by the FDA with claims of 8 hr of continuous use. Unfortunately, none of these devices has been evaluated in the peer-reviewed literature for claims regarding their ability to reduce fomite potential. However, several studies (73) (74) (75) (76) (77) have looked at the ability of prototypes or devices, not cleared by FDA, that incorporate some type of antimicrobial chemistry in them to render trapped pathogens inactive over time (i.e., storage time between uses). These studies suggest that efficacy of antimicrobial FFRs for this application is dependent on the pathogen, antimicrobial agent, storage conditions, and specific test method used which makes generalization of findings difficult. (73, (78) (79) (80) (81) Although promising, the lack of conclusive evidence suggests that additional work is needed before FFRs incorporating antimicrobial technologies can be factored into FFR reuse recommendations.

Transfer Efficiency-Because FFRs can become contaminated with pathogens likely to remain infectious during typical extended use and reuse scenarios, the next factor to assess is the likelihood of pathogens transferring from the FFR to the hands of HCWs. Unfortunately, no studies exist that quantify the percentage of pathogen transferred from the FFR to the hands of HCWs. However, similar to estimating contamination levels, models can be used where estimates of the key input parameters are available. A simple model for estimating the amount of pathogen transferred to the hands (C hand ) of HCWs from contaminated FFRs uses C FFR (the number of pathogens on the FFR as discussed above), transfer efficiency of the pathogen (E t ), and contact area of the hands (A h ) and the FFR (A FFR ).

Unfortunately, no peer-reviewed sources are available on the transfer efficiency of relevant pathogens from a FFR to skin and others surfaces. However, an unpublished conference presentation reports the transfer efficiency of a bacterium, Bacillus atrophaeus, from FFRs to synthetic skin as 0.005% and 0.05% for touching and rubbing, respectively. (82) Other microbial transfer studies for porous surfaces have shown similar results. For example, Rusin et al. reported transfer efficiencies for a bacterium, Micrococcus luteus, of 0.13% from a 100% cotton substrate and 0.06% from a 50:50 cotton/polyester substrate. Even lower transfer efficiencies (<0.01%) from those surfaces were reported for bacteriophage PRD-1. (83) Another recent study compared the transfer efficiency of bacteria and viruses from several porous and nonporous surfaces to the fingers. (84) In general, the lowest transfer efficiencies were found for porous surfaces under low relative humidity. Isoelectric point and hydrophobicity of the surface were also important factors.

As discussed previously, C FFR can be estimated. For simplicity, we use the influenza values reported above from Fisher et al. as a surrogate for all respiratory pathogens. (52) The contact area of the hands depends upon the action of the HCW (Table III) . For extended use, it is likely that only the finger tips are used to touch the FFR surface (e.g., to reposition the FFR). The total surface area of the volar portion of the fingertips has been estimated to be 7.34 cm 2 . (85) However, when implementing FFR reuse, the proper donning process requires a user seal check step, which requires the user to cover the entire FFR surface by cupping both hands around the filter surface. In this situation, A h would be very similar to A FFR , which has been estimated to be approximately 175 cm 2 , but varies among the various FFR models. Assuming uniform deposition of the pathogen over the surface of the FFR, applying input values of 4,500 FFR -1 for C FFR and 0.1% as an approximation for E t to the equation results in an estimated 4.5 pathogens being transferred to the hands of the HCW during the user seal check step and <1 pathogen for each touch involving a fingertip.

Summary-While the model above indicates that some pathogens from a contaminated FFR could transfer to the hands, other factors also affect the risk of infection. Steps in the fomite pathway such as the transfer of viable pathogens from hands to respiratory tract ports of entry, transport of viable pathogens to the site of infection, and the infectious dose of the pathogen are not unique to extended use and reuse of FFRs, but are common to any potential fomite. A full assessment that takes into account these steps is beyond the scope of this commentary. However, the model developed by Nicas and Sun indicates that each successive step in the fomite pathway further reduces the number of infectious pathogens reaching the site where infection can occur, reducing the risk of self-inoculation from practicing FFR extended use and/or limited reuse. (51) In theory, extended use should not present a significant self-inoculation hazard, as ideally, the HCW's hands should never come in contact with the contaminated filtering surface when proper doffing protocols are followed. (86) However, the Rebmann study (47) reported that HCWs touched or adjusted their FFR on average 10-20 times per 12 hr. shift. Even with this amount of contact, our analysis, based on the data and the models discussed above, suggests that very few pathogens are likely to make it to the site of infection each time the hand or fingertip comes in contact with the FFR. Thus, extended use is considered minimal risk for typical patient interactions (Table II) when coupled with training and education to reinforce proper use (e.g., don't touch the FFR surface) and adherence to hand hygiene recommendations.

Reusing FFRs provides multiple opportunities for the hands of HCWs to come in contact with any infectious microbes on the respirator surface and thus involves a higher level of risk compared to extended use (Table II) . HCWs' hands would presumably contact the contaminated FFR surface when placing the FFR onto the face, adjusting the position of the FFR and flexible strap across the nasal bridge (if applicable), and when performing the user seal check, a requirement after donning a respirator and after each adjustment to the respirator. Similar to extended use, fomite risks from FFR reuse can be mitigated through training and education to reduce unnecessary touching of the FFR and rigorous adherence to hand hygiene. Steps to limit FFR contamination (e.g., masking patients, use of engineering controls, face shields, and so on) can also limit fomite risks, as C hand is proportional to C FFR .

Concerns have been raised that extended use of FFRs could result in additional opportunities for pathogen transmission to co-workers and patients due to reaerosolization of trapped pathogens to the environment from a sneeze, cough, or through rough handling. Several studies have addressed this issue. Most recently, Fisher et al. examined virus reaerosolization from FFRs and concluded that the risk of virus transfer to the environment from the FFR was negligible, a finding key to extended use and reuse. (87) FFRs were challenged with virus-containing droplet nuclei with a size range of 0.65 to 7.0 μm (with the majority <1.1 μm) and challenged with reversed airflow to simulate a sneeze or cough. The highest reaerosolization of 0.21% occurred with a droplet nuclei challenge while a droplet challenge led to reaerosolization of less than 0.0001%. These findings are consistent with earlier studies that examined reaerosolization of bacteria and inert particles. Qian et al. and Willeke and Qian reported the reaerosolization of less than 0.2% for bacteria deposited on N95 FFRs as aerosols and challenged with a reverse airflow consistent with a violent sneeze or cough. (88, 89) Kennedy and Hines found that less than 0.3% of polystyrene latex microspheres reaerosolized from FFRs when dropped from a height of 3 feet (90) Overall, these data provide evidence that the risks of secondary exposure due to reaerosolization or rough handling associated with FFR extended use or limited reuse can be considered negligible (Table II) . Similar to the fomite concerns discussed above, secondary exposure risks could increase as C FFR , the number of pathogens on the FFR, increases (i.e., higher C FFR = higher levels of reaerosolized pathogen), so steps taken to limit FFR contamination (e.g., masking patients, faceshields, local exhaust ventilation systems) should be implemented where possible.

In situations where patients are under contact precautions, such as those co-infected with common health care pathogens with the ability for prolonged environmental survival (e.g., Vancomycin-resistant enterococci, Clostridium difficile, and norovirus), it may be prudent to have HCWs discard FFRs after close contact because these pathogens could be transferred to other patients via the unclean hands of the HCW.

Sharing FFRs among HCWs could also result in a secondary risk if at least one of the users is infectious (symptomatic or asymptomatic). For example, a specialized face mask containing electret filter media (similar to those found in N95 FFRs) was worn in one study (92) to successfully collect infectious virus from the exhaled breath of symptomatic test subjects. Because of respirators' ability to trap respiratory pathogens, sharing a contaminated FFR could result in disease transmission. However, proper labeling, training, and education can be effective at limiting any inadvertent sharing of FFRs during reuse.

We also conducted an Internet search and reviewed FFR extended use and reuse recommendations issued by other United States agencies (e.g., FDA and OSHA) and professional organizations (e.g., Association for Professionals Infection Control and Epidemiology). (93, 94) In terms of FFR extended use and limited reuse, we identified no major discrepancies among the recommendations from the Association for Professionals in Infection Control and Epidemiology (APIC), OSHA, and the CDC recommendations in (Table I) . For example, OSHA TB guidance (7, 95) indicates that disposable respirators (i.e., FFRs) can be reused by the same HCW, as long as the functional and structural integrity of the respirator is maintained and the outside of the filter is inspected before each use for signs of physical damage or soiling, and discarded if signs are present.

While OSHA is responsible for regulating employers to provide a safe workplace for their employees and CDC makes public health recommendations that are often adopted by hospitals, FDA has a different role in health care settings. The FDA regulates the manufacture and labeling of medical devices. (96) Medical devices are cleared by the FDA under the Food, Drug, and Cosmetic Act based upon data submitted by the manufacturer to support the claimed intended use of the product. Under 21 CFR 878.4040, FDA classifies surgical N95 respirators as a type of surgical apparel, intended to be worn by operating room personnel during surgical procedures to protect both the surgical patient and the operating room personnel from transfer of microorganisms, body fluids, and particulate material. As part of the labeling requirement, FDA recommends that manufacturers state whether a device is intended to be a reusable device or a single-use disposable device. (9) Some surgical N95 respirator models are cleared by the FDA with claims of being a singleuse device, while other manufacturers do not make such claims. (10) For surgical N95 respirators labeled as ""single use only,"" extended use or limited reuse could be considered as an ""off label"" use of these products. FDA has specific requirements for reuse (""reprocessing"") of single-use medical devices. (97) Unfortunately, as discussed earlier in this manuscript, some hospital use practices for these types of medical devices such as limited FFR reuse were first recommended (4) (5) (6) (7) and put into practice prior to FDA's involvement. There is also a general lack of awareness among infection control professionals and safety/ employee health administrators in understanding FDA's role in regulating surgical N95 respirators. (98) These factors contribute to the prevalence of ""industrial N95 FFRs"" used in health care settings. These industrial N95 FFRs are NIOSH-certified FFRs, but have not been cleared by the FDA as medical devices. Several of these industrial N95 FFRs were stockpiled by the CDC in the Strategic National Stockpile. (99) In the future the different regulatory and policy perspectives will need to be factored into FFR extended use and limited reuse recommendations. For example, recommendations for operating rooms, where soiling and potential contamination from blood borne pathogens will likely occur, might be different. In those situations, limited reuse should only be considered after consultation with the surgical N95 respirator manufacturer and local hospital infection professionals. benefits (e.g., cost savings, ability to extend existing supplies, reducing the ""burn rate,"" and so on) of FFR extended use and limited reuse.

The primary purpose of this article is to assess recent scientific findings to assist policy makers when making decisions on whether to recommend that employers in health care settings permit FFR extended use and/or limited reuse during routine operations and for future public health emergencies. The authors acknowledge that the evidence discussed above is not always as sufficient as desired to develop evidence-based policy decisions. However, decisions on how to protect exposed workers must be made in the present and cannot wait until additional evidence is available. In the interim the available evidence can be useful for policy-based and pragmatic public health decision ideologies. (100) As discussed by Rosella and coauthors, (100) emerging public health situations require a balance between various factors. Both evidential and policy considerations are important. Policy makers need to use the best evidence available to them, even when it has substantial limitations, acknowledge the uncertainties, and account for them in as practical a way as possible.

For recommending FFR extended use and/or limited reuse for routine events, policy makers should weigh the increased risks for disease transmission from FFR extended use and limited reuse against the inconvenience, cost, and waste of single use. In public health emergencies, policies on FFR extended use and limited reuse should weigh the risks for disease transmission against the risk of disease transmission associated with sacrificing because of FFR shortages (e.g., foregoing respiratory protection or using surgical masks for pathogens or activities where N95 FFRs are recommended). Decisions regarding whether FFR extended use or limited reuse should be recommended need to continue to be pathogenand event-specific. The two most important factors driving this decision should be whether the pathogen is likely to spread (in part) via contact transmission and whether the event could result in or is currently causing a FFR shortage.

This analysis of recent research (post-2006) generally supports CDC guidance issued since 2004 for FFR extended use and limited reuse for routine events such as TB and seasonal influenza (during AGP) as well as the public health emergencies such as the 2004 SARS and 2009 H1N1 flu pandemics. While recent findings largely support these CDC recommendations, some new cautions and limitations should be considered in recommendations issued in the future as discussed subsequently.

Extended use offers a lower risk of self-inoculation compared to limited reuse given that the HCWs hands should ideally rarely contact the contaminated FFR surface. Training and education should be stressed to reinforce the need for strict adherence to guidance to minimize unnecessary contact with the FFR surface and strict adherence to hand hygiene practices. Extended use poses no additional health risk to a medically cleared respirator user and despite the additional discomfort should be tolerable for most HCWs. For these reasons, extended use should be preferred over limited reuse, even though FFR reuse requires the least change to current practices.

Limited FFR reuse would allow the HCW to doff the FFR to provide a brief respite from the psychological and physiological factors that decrease FFR comfort, but increases the potential for contact transfer when donning the used FFR and performing the user seal check. However, fomite transfer models indicate that the potential for transfer of pathogens from FFRs to the hands of the wearer is small suggesting that limited FFR reuse can be employed with minimal additional risk in most cases. An exception is reuse of FFRs after AGPs, where higher FFR contamination levels are likely to occur. Education and training should be emphasized to reinforce the need for proper hand hygiene when redonning the FFR, including inspection of the device for physical damage and performing a user seal check. Strict adherence to these steps should further reduce the potential to transfer virus from the hands to the points of entry of infection.

While limited FFR reuse remains a viable option for reducing usage rates and for situations involving a pathogen that does not spread via contact transmission, data suggest that FFR protection can begin to be reduced for some models after multiple donnings or uses. Guidance should emphasize the need for the employer to consult with the respirator manufacturer regarding the maximum number of donnings or uses suggested for the FFR models used in that location or to presumptively limit the number of reuses to no more than five to ensure an adequate safety margin, in the absence of new information to the contrary. 

",0.6409368173045428
Avian Influenza Risk Perception and Preventive Behavior among Traditional Market Workers and Shoppers in Taiwan: Practical Implications for Prevention,"Background: Avian influenza (AI) can be highly pathogenic and fatal. Preventive behavior such as handwashing and wearing face masks has been recommended. However, little is known about what psychosocial factors might influence people's decision to adopt such preventive behavior. This study aims to explore risk perception and other factors associated with handwashing and wearing face masks to prevent AI.","A total of 520 cases of avian influenza (AI) had been reported around the world, resulting in 307 deaths, with a case fatality rate of 59.0% [1] . The AI virus has been found in domestic poultry (e.g., chickens, ducks, geese), wild birds [2] , waterfowl and shorebirds [3] . An extensive review indicated that the AI virus could be transmitted through direct contact with infected poultry, including holding diseased or dead poultry, slaughtering, defeathering, or preparing sick poultry for cooking [4] .

Taiwan experienced sporadic AI outbreaks in 2004 and most recently in 2009, including major local outbreaks in Kaohsiung (a region located in southern Taiwan) in 2008. Limited information released in government reports [5] later confirmed that these outbreaks were caused by H5N2 virus. Although there had not been any highly pathogenic avian influenza (HPAI) H5N1 cases reported in Taiwan, yet considering the geographical location of Taiwan being an important stopover for migrating birds [6] and previous H5N1 outbreaks in neighboring Asian countries such as Thailand and China [7] , plus increasing travel and direct transportation links with other countries, Taiwan is at risk for HPAI outbreaks as well. The most worrisome scenario is that human-to-human transmissions may begin to take place if there is a change in the viral genome [8] , and according to a relatively conservative estimate by the World Health Organization (WHO), such transmissions may cause 2 million to 7.4 million deaths [9] .

In Taiwan, people have the habit of shopping at traditional markets for live poultry [10] , and such traditional markets with live chicken for sale provide a possible AI viral reservoir [11] , thereby placing traditional market workers and shoppers at risk for contracting AI. Poultry are usually selected and purchased by shoppers while they are still alive and slaughtered on site as shoppers in Taiwan commonly believe that live poultry preserves the freshness [11] . During the purchasing process, shoppers may come into contact with live poultry, thereby exposing shoppers to risk of contracting AI, and thus it is important that shoppers wash their hands after any contact with poultry. Normally, a poultry vendor would not sell other foods; however, in a traditional market, a poultry stand could be adjacent to any other food stands without any partitions separating them. Notably, AI viruses can also be transmitted to humans through the particles raised up by the movement of the poultry [12] , and therefore, regardless of whether the shoppers are purchasing live poultry, they are advised to wear a face mask while in a traditional market. Given the modes of AI transmission as described above, WHO [13] and Taiwan Centers for Disease Control (Taiwan CDC) [14] both recommended washing hands after any contact with poultry and using a face mask when coming into contact with poultry to prevent AI infection and its spread. Handwashing [15, 16] and wearing face masks [17] have been documented as effective preventive measures against respiratory disease in community settings. In addition, these two preventive behaviors are relatively easy to practice. Taken together, both handwashing and wearing face masks could be a cost-effective way of preventing AI in the general public. Therefore, an enhanced understanding of the factors associated with AI preventive behavior could inform renewed prevention efforts to more effectively promote the recommended preventive behavior in different target populations.

Previously, studies have been conducted in various countries in regard to AI. For example, a study in Turkey investigated AI knowledge and anticipated attitudes in the general population; however, this cross-sectional survey only examined the anticipated preventive measures rather than actual preventive behaviors [18] . Another study in Italy focused merely on poultry workers, exploring the relationships of their knowledge, attitudes, and compliance with precautions at work, such as self-reported use of face masks and gloves [19] . An earlier study reported its findings in a letter to the editor, presenting regional differences in AI knowledge, risk perceptions, and AI-related behavior changes among Laotians after HPAI outbreaks [20] . Further, a telephone survey in the Hong Kong general population examined participants' AI risk perception in relation to their live chicken purchasing behavior [21] . Similarly, a study in Taiwan conducted by marketing and business management researchers assessed consumer knowledge and risk perceptions of AI in association with chicken consumption behavior, instead of AI preventive behavior under the threat of AI [11] .

However, relatively little is known about the recommended handwashing behavior, especially in combination with face mask wearing behavior, to prevent AI. Therefore, considering the modes of AI transmission taking place at traditional markets in Taiwan as reviewed earlier, the current study aimed to estimate the prevalence of the recommended AI preventive behavior (i.e., when in a traditional market, wearing a face mask and washing hands after any contact with poultry) among traditional market workers and shoppers in Taiwan and examined their knowledge and risk perception in relation to AI preventive behavior.

Participants of this cross-sectional study were market workers and shoppers in traditional markets located in northeastern and central Taiwan. As described in the Introduction, there were major AI outbreaks in Kaohsiung (in southern Taiwan), and therefore, the current study aimed to explore the risk perception about AI, as well as the recommended AI preventive behavior, among traditional market workers and shoppers in central and northeastern Taiwan, where there have not been reported AI outbreaks. Accordingly, two traditional markets were selected in central Taiwan and two others were selected in northeastern Taiwan. Since this study sought to examine simultaneously both groups of participants (i.e., traditional market workers and shoppers), we decided to interview market workers and shoppers with a one-to-one ratio. With this particular purpose in mind, purposive sampling was employed in traditional markets: market workers were interviewed by trained interviewers during their breaks; because there were far more market shoppers than workers, market shoppers were randomly selected for an interview at the market entrance if they happened to step into the market when an interviewer just became available to conduct a survey after completing the previous interview. The interviewer-administered survey using a structured questionnaire was conducted in December 2009 through January 2010. A total of 352 anonymous interviews were completed with a response rate of 95.1%. Each interview was conducted by a trained interviewer and took the participant 5-10 minutes to complete the questionnaire. Traditional market workers and shoppers were first approached and informed of the study's goals and procedures by our interviewers to obtain verbal informed consent before each interview. Every attempt made to approach potential participants, be it successful or unsuccessful, was documented in order to calculate the response rate. We also provided a gift worth approximately US $1 as incentives to increase the response rate. The study protocol and verbal consent procedure were reviewed and approved by the Institutional Review Board of the National Taiwan University College of Public Health.

The survey collected sociodemographic information such as the participant's age, gender, region of residence, education, and whether the participant was a market worker (yes/no). This study also classified each participant's risk of AI by type of work: 1) nonmarket worker (i.e., shopper), 2) low risk market worker (e.g., flower vendor, vegetable vendor, cleaner, administrative staff), 3) medium risk market worker (e.g., pork vendor, beef vendor, seafood vendor, mutton vendor, cooked poultry vendor), and 4) high risk market worker (e.g., chicken butcher, chicken vendor, poultry organ vendor). It is noteworthy that the AI risk classification above takes into account the nearness in space to live poultry. For example, meat and seafood stands are normally located in the same section as poultry stands in a traditional market, and hence are closer to poultry vendors than are flower and vegetable vendors. As such, pork vendors, beef vendors, etc. are classified as medium risk market workers, whereas flower vendors and vegetable vendors are classified as low risk market workers.

Further, this study assessed the participant's knowledge about AI such as its transmissibility and fatality rate (Table S1 ) [31] . The survey also evaluated the participant's risk perception of AI, including whether the participant anticipated an AI epidemic in Taiwan, whether the participant knew about severe cases of AI, and whether the participant knew about AI outbreaks in Kaohsiung (a region located in southern Taiwan). The variables regarding knowledge and risk perception of AI were binary (e.g., yes/no, correct/incorrect). The outcome measure of recommended AI preventive behavior was assessed by asking: ''When you are in a traditional market, do you wear a face mask and also wash your hands after any contact with poultry (yes/no)?'' Notably, each participant's response to this behavioral outcome measure was validated by the interviewer through direct observation in regard to the face mask wearing aspect of the preventive behavior. In other words, since all interviews were conducted in traditional markets, only participants who were wearing a face mask at the time of the interview would be coded as adopting the recommended AI preventive behavior as defined in this study. Several theoretical models have been used for understanding protective health behaviors, including the Health Belief Model (HBM) [22] [23] [24] . The risk perception measures in this study were derived from theoretical constructs of the HBM, which posits that risk perceptions such as perceived severity and perceived susceptibility are associated with adoption of healthrelated behaviors [25] . This model has also been used in a recent AI study [26] .

Descriptive statistics were first examined for sociodemographic data. Sociodemographic variables, knowledge, and risk perception of AI were cross-tabulated with AI preventive behavior, and x 2 comparisons were performed to test for group differences between participants who practiced and those who did not practice the recommended AI preventive behavior. T-test was used for comparing the ages of participants who practiced the recommended AI preventive behavior and those who did not. Then, variables with significant x 2 or t-test results were included as candidates in subsequent stepwise logistic regression modeling. The final model was adjusted for age and gender as control variables. All statistical analyses were carried out with SPSS (version 17) and P,.05 indicated statistical significance.

The sociodemographic characteristics of the participants (n = 352) are as follows: the mean age was 43.9 years; 62.5% were female; 59.4% lived in central Taiwan; 18.2% had a degree from university or above, and 41.2% had a senior high-school diploma. About half (49.7%) of the participants were shoppers; 22.7%, 18.2%, and 9.4% were market workers at low, medium, and high risk for AI, respectively. Slightly more than half (52.6%) of the participants adopted the recommended AI preventive behavior. Table 1 shows the associations of sociodemographic characteristics and AI risk perception with the recommended AI preventive behavior. Younger participants were more likely than older participants (mean age = 42.0 vs. 45.6, t = 3.00, p = .003), females were more likely than males (50.0% vs. 43.2%, x 2 = 1.54, df = 1, p = .22), and participants who lived in northeastern Taiwan were more likely than those in central Taiwan (62.9% vs. 36.8%, x 2 = 23.19, df = 1, p,.0001) to practice the recommended AI preventive behavior. Furthermore, participants with a university or higher degree (70.3%) were the most likely to adopt the AI preventive behavior, followed by those with a senior high-school diploma (53.8%) and those with a junior high-school or less education (30.8%) (x 2 = 37.71, df = 2, p,.0001). Finally, compared with their counterparts, participants who had correct knowledge about AI fatality rate (68.8% vs. 39.8%, x 2 = 23.16, df = 1, p,.0001), who anticipated an AI epidemic in Taiwan (54.8% vs. 41.6%, x 2 = 6.08, df = 1, p = .01), who knew about severe cases of AI (55.0% vs. 26.1%, x 2 = 22.78, df = 1, p,.0001), and who knew about AI outbreaks in Kaohsiung (63.5% vs. 37.2%, x 2 = 23.20, df = 1, p,.0001) were more likely to practice the recommended AI preventive behavior. Table 2 presents the final multivariate logistic regression model for covariates of AI preventive behavior. Participants who lived in northeastern Taiwan were 6 times as likely as those in central Taiwan to practice the recommended AI preventive behavior (adjusted odds ratio [AOR] = 6.01, 95% confidence interval [95% CI] = 3.40-10.61). Gender did not have a statistically significant effect; however, male gender was found to interact with senior high-school education (AOR = 0.34, 95% CI = 0.12-0.98). Hence, females with a senior high-school diploma were more than 3 times as likely (AOR = 3.33, 95% CI = 1.56-7.07), and participants with a university or higher degree were nearly 7 times as likely (AOR = 6.86, 95% CI = 2.60-18.06) to adopt the AI preventive behavior, compared with their counterparts with a junior high-school or less education. Further, participants who had correct knowledge about AI fatality rate were more than 4 times as likely (AOR = 4.18, 95% CI = 2.25-7.75), those who knew about AI severe cases were approximately 2 times as likely (AOR = 2.13, 95% CI = 1.13-3.99), and those who knew about AI outbreaks in Kaohsiung were more than 2 times as likely (AOR = 2.24, 95% CI = 1.28-3.92) to practice the AI preventive behavior, compared with their counterparts.

An earlier study reported that greater knowledge of AI (i.e., knowing correctly the modes of AI transmission, occupational groups at risk for AI infection, and proper AI preventive measures) among poultry workers was associated with increased odds of adopting preventive measures, including wearing protective clothing and face masks [19] . Adding to the literature, our study further found that, compared with participants who misperceived that AI fatality rate is lower than that of pandemic H1N1, those with correct knowledge were more than 4 times as likely to practice the recommended AI preventive behavior. Another study, conducted among adults in the general population, also reported that participants who had correct knowledge about AI were more likely to practice AI preventive behavior [27] . While these prior studies also identified knowledge of AI, such as modes of AI transmission, to be a significant factor for increased preventive behavior, our study discovered that to promote the recommended AI preventive behavior, it is crucial to inform the public specifically of the AI fatality rate. In addition, unlike most previous research [18] [19] [20] which focused on poultry workers or the general public, this study expanded this line of research by examining AI preventive behavior and related factors among traditional market workers and shoppers.

This study also found that participants with greater risk perception of AI (i.e., those who knew about AI severe cases and those who knew about AI outbreaks in Kaohsiung) were more likely to practice the recommended AI preventive behavior. The greater adoption of precautionary measures among these participants with higher risk perception in the current study could be explained by their possibly elevated anxiety levels as posited in an earlier study [28] . These findings further underscore the importance of transparency in dealing with epidemic information, specifically, AI severe cases and local outbreaks, if any, as in this study. Accordingly, future public service announcements or pandemic control initiatives should consider disseminating the aforementioned specific information to the public in the face of an AI epidemic.

Participants living in northeastern Taiwan were found to be much more likely to practice the recommended AI preventive behavior than those living in central Taiwan in the current study. Such regional differences in preventive behavior were also reported in a Laotian population [20] and were attributed to different participant characteristics in urban and rural areas. In light of such findings, the present study also took into account gender, age, education, and other covariates in the multivariate regression model. However, the regional differences still remained. A possible explanation is that participants in northeastern Taiwan, which is more rural and has fewer healthcare resources than central Taiwan, may choose the relatively easy and cost-effective measures such as wearing face masks and washing hands to protect against AI. Another study in China also found differences between Guangzhou and Hong Kong in regard to participants' AI risk perception and live poultry purchase [29] ; while its outcome variable of interest was not AI preventive behavior, regional differences were present and were ascribed to cultural differences. Notably, the current study found that participants who lived in rural areas were more likely to practice the recommended AI preventive behavior (i.e., both face mask wearing and handwashing) than those who lived in urban areas; however, prior research has found that people living in rural areas were more likely to practice the AI risky behavior (i.e., live poultry purchase) than those living in urban areas [29] . Caution needs to be exercised in interpreting such inconsistency, because different behaviors could have different determinants, and therefore, it is possible that these two studies found inconsistent results due to the different natures of these behaviors in question. These findings suggest that future studies may investigate potential cross-national differences in AI preventive behavior, and that qualitative research is also needed to explore regional differences caused by cross-cultural differences as well as other possible causal mechanisms.

Gender differences in the practice of protective behavior against emerging infectious diseases, including SARS and AI, have been explored in prior research, although the results have been inconclusive. For example, a review article on handwashing practices during and after SARS outbreak indicated that females in general were more likely than males to adopt the protective behavior, suggesting that females might be more health conscious and risk averse, although the reported differences were not always statistically significant [16] . Similarly, in a limited number of AI studies which included gender as a variable, its effect on the AI preventive behavior was generally found to be not statistically significant [e.g., 19, 26] . On the other hand, higher education levels have repeatedly been reported to be associated with increased knowledge and intention to adopt the recommended AI preventive behavior [e.g., 18, 19, 26] . Consistent with these findings, our study also found participants with higher education levels to be more likely to practice the AI preventive behavior. Interestingly, while gender difference was not statistically significant in the current study, we found significant interactive effect between gender and education among participants with a senior high-school education, males being less likely to adopt the recommended AI preventive behavior than their female counterparts. To our knowledge, such findings have not been reported in previous research and warrant further investigations to elucidate possible mechanisms.

Moreover, the x 2 comparison in this study found an alarming pattern that market workers at higher risk for AI appeared to be less likely to adopt preventive behavior than shoppers and other market workers with lower risk for AI. Further, an ancillary analysis (data not shown) indicated that these high-risk market workers also had a significantly lower level of education, which was associated with lower compliance with recommended preventive behavior. Taken together, more attention should be paid to this group of high-risk market workers. It is worth noting that the aforementioned x 2 comparison of AI preventive behavior was not statistically significant, possibly owing to relatively smaller cell counts of high-risk market workers and hence reduced statistical power. Therefore, future studies may consider increasing not only the total sample size but also the number of high-risk market workers so as to confirm the above-noted pattern.

Limitations of this study include the potential reverse causality due to the cross-sectional design; however, a number of variables identified to be significantly associated with AI preventive behavior (e.g., region of residence and education) are likely to precede temporally the outcome measure, thereby lending additional support to our explanations discussed earlier. Also, combining wearing face masks and handwashing as the outcome variable without assessing them separately could be a limitation of this study because determinants of these two practices could be different. On the other hand, however, in view of the modes of AI transmission through contact and air particles, practicing both face mask wearing and handwashing behaviors could provide better protection against AI infection. Another potential limitation is that this study was not based on national data but on data from northeastern and central Taiwan; yet, regional differences were uncovered. Therefore, future studies should consider drawing a national sample to explore other possible cross-regional differences in Taiwan. In addition, cross-national comparisons may also reveal interesting differences across various countries and cultures in preventive behaviors associated with such emerging infectious diseases. Lastly, while self-efficacy (a construct borrowed from Bandura's Social Learning Theory) was not added to the HBM until 1988 [30] , it has increasingly become an important construct in the HBM but was not included in our study. Hence, future research should consider measuring all other HBM constructs, including self-efficacy, when examining AI preventive behavior. 

In conclusion, this study found that specific information concerning AI risk perception was associated with the recommended AI preventive behavior. In particular, having correct knowledge about the fatality rate of AI, and being informed of severe cases of AI and local AI outbreaks, were linked to increased AI preventive behavior. These findings have important implications for future practice as they could inform policy-making and renewed prevention efforts to more effectively promote the recommended AI preventive behavior in the public.

Table S1 Questions used to measure AI knowledge, risk perception, and preventive behavior. (DOC)

",0.6408082550246038
Contamination during doffing of personal protective equipment by healthcare providers,"Objective In this study, we aimed to describe the processes of both the donning and the doffing of personal protective equipment for Ebola and evaluate contamination during the doffing process.","Ebola was first identified 40 years ago. 1, 2 However, little interest has given to this highly contagious pathogen until an outbreak was confirmed by the World Health Organization in 2014 in Congo. 3, 4 Disease spreads across nine countries and its death was toll of over 11,000 person in three West African nations, and people around the world began to concern about the Ebola nowadays. Ebola spreads through human-to-human transmission via direct contact (through broken skin or mucous membranes) or indirect contact with infected materials, such as clothing. [5] [6] [7] As other infectious diseases, healthcare workers are at risk of infection because they have to work at the frontline, often without protections. 8, 9 Cases of contamination while treating Ebola patients rose rapidly during the current outbreak, and the Centers for Disease Control and Prevention (CDC) announced the new personal protective equipment (PPE) guidelines for healthcare providers treating Ebola patients. [10] [11] [12] However, even with protective clothing, a healthcare worker can be contaminated if removal is not done in a manner that prevents exposure. [13] [14] [15] A limited numbers of studies have been performed regarding contamination with PPE usage. In this study, our primary aim was to evaluate contamination during doffing. The secondary purpose was to describe errors and delays during donning and doffing.

This study was performed in a single tertiary-level academic hospital (Samsung Medical Center, Seoul, Korea). The hospital has a disaster plan and equipment such as PPE. However, no official training or education was provided before this study. We recruited 29 participants for the study from the emergency department physician and nursing staff. They were informed of the purpose of the study, and each gave written consent.

Participants received a 20-minute lecture. After the lecture, a 20-minute demonstration with PPEs was given, along with an educational movie produced by the Korea Centers for Disease Control and Prevention. The course content contains donning and doffing procedure for PPE (gowns, gloves, respirators, and goggles), according to a 2014 CDC protocol. A 10-minute question and answer session followed the demonstration. The lecturer was a certified advanced disaster life support instructor, experienced with several training sessions.

After the training, each participant was paired with another, and completed the donning and doffing procedures with help from the partner. Each team was blinded from the others; however, the second participants were more exposed to the process after watching the first. Each person was allowed to watch the protocol and to consult their partners. Though the 2014 CDC protocol was close to level D (level of PPE is divided A to D and level D is the lowest protection), we modified the airway protection by upgrading the N95 mask (particulate filtering face-piece respirators) to a gas mask. We also modified the 2014 CDC leg cover process. Fig. 1 demonstrates the overall look of our study PPE. Table 1 shows the difference between the CDC guideline and study protocol.

A demographic survey was performed with all participants. The survey also included questions on job experience and previous training with PPE. During a simulation study, two separate cameras with high-density capability were set up at an approximate 120 degree angle to document the process. All processes were videotaped during PPE donning and doffing. Afterward, a trained examiner reviewed all video recordings and coded timer intervals and errors. Time stamps were determined according to a 2014 CDC protocol. Each procedure was initiated when the participant picked up the equipment. The procedure ended when the next procedure was initiated. Errors were determined when the participants violated the order of procedures even with the help of their partners. For example, if a participant skipped putting on a second glove and proceeded to the next procedure, this was counted as an error. Errors during the process were explained by partners and instructors, so participants could resume the normal process.

The primary outcome was a potential incident of self-contamination during the doffing procedure, defined as the touch of the outside of PPE to the participant's body or clothing. The determination of contamination was performed by a single examiner on the basis of two recordings. If one of the recordings did not have sufficient information, the determination was carried out with only one. If two recordings suggested opposite conclusions, the outcome was determined as negative. A single trained examiner was considered sufficient for the examination because the outcome measure was relatively simple. This choice was supported by pilot cases before the study began.

Statistical analysis was performed with STATA ver. 13 (Stata Corp., College Station, TX, USA). Demographic data were reported in a descriptive manner. Continuous variables were presented as means with standard deviations, medians and interquartile ranges, or A pilot study on the contamination of PPE frequencies. Categorical variables were described as numbers and percentages. Differences between the two groups were tested using the independent two-sample t-test or the Mann-Whitney Utest for continuous variables and the chi-square test for categorical variables. P < 0.05 was considered significant.

Demographic characteristics of the study participants are shown in Table 2 . Overall, 29 participants enrolled in the study. Twenty (69.0%) were female, and mean age was 29.2 years (standard deviation [SD], 2.39). The mean work experience was 3.33 years (SD, 2.67). Among participants, 20.7% had previously received PPE training (Table 2 ). Values are presented as median (interquartile range) or number (%). One to twelve number of the Table 3 shows step by step process. PPE, personal protective equipment. Values are presented as median (interquartile range) or number (%). One to seventeen number of the Table 4 shows step by step process

The average donning process interval was 234.2 seconds (SD, 65.7) from start to finish. The most time-consuming process was putting on the gown, putting on shoe covers, and putting on the respirator. The most frequent errors occurred while putting on outer gloves (27.6%), respirator (20.7%), and hood (20.7%). The entire donning procedures refer to Table 3 .

The average interval during doffing was 183.7 seconds (SD, 38.4) from start to finish. The most time-consuming processes were removing the shoe covers, putting on gloves, and removing outer gloves. The most frequent errors occurred during disinfecting feet (38.0%), discarding scrubs (17.2%), and putting on gloves (13.8%). The entire doffing procedures refer to Table 4 . Fig. 2 illustrates contamination locations. During the overall doffing process, 65 contamination incidents occurred. The most vulnerable process was removing respirators, which caused 23 contaminations (79.3%). Two of these were on the head (6.9%), and 21 were on the neck (72.4%). Removal of shoe covers was also associated with a high probability of contamination; 19 incidents (65.5%) were reported. All locations of contamination were consistent with the doffing step related to that part of the body.

Even after standardized education, 65 numbers of contamination were reported, or 2.2 per participant. Further studies are required to minimize this number.

This study provides a valuable first step in the evaluation of PPE used by healthcare workers. This study suggests healthcare work- A pilot study on the contamination of PPE ers should be cautious about decontamination and that they need training. Donning, doffing, and decontamination procedures should be optimized for specific clinical situations. The strengths and limitations of each protective system need to be considered when recommendations are made about PPE implementation.

This study shows that a significant number of contamination incidents occurred during the process of removing PPE. This is consistent with previous reports, which have pushed the CDC into announcing recommendations for PPE handling. 16, 17 Although the study could not conclude which factors resulted in contamination, it is important to comment that more training with technical support is required for the safety of workers. Potential measures include interactive audiovisual devices to guide the procedures, or trained personnel specialized in assisting others with the procedures. Intensive and repetitive training is also required.

Recommendations for decontamination mainly emphasize hand washing. However, despite hand washing, healthcare workers could touch other parts of their bodies or clothing that has not been properly decontaminated and consequently infect themselves. 18 More intensive education and training is required for safe doffing. Evaluation of the effectiveness of training is also required.

Previous studies have focused on the importance of PPE, potential risks of doffing procedures, and doffing procedure experiments comparing different systems with a few subjects. 18 One study demonstrated contamination rates of 26% and 96% with two distinct methods; this study included only one subject for each arm, which makes it difficult to compare the outcome with the current study. 18 This study gives additional information because of the larger numbers of subjects, and because the scenario has more generalizability than previous studies.

This study does have some limitations. First, the study setting is a single center without an existing PPE training program. Inhospital staff people were inexperienced, which may have been a factor increasing errors and contamination rates. However, a majority of hospitals are inexperienced with hazmat and PPE incidents, so these study results have general applicability.

Second, the study number was very small, including only emergency department staff people. This makes it difficult to generalize to a broader population of hospital staff.

Third, contamination sometimes appeared obscure on video and was subject to examiner's decision. Though the examiner reviewed recordings several times from different angles, there could be blind spots and unobservable touches. This could have made the rates underestimated, in other words the false negative rate of the outcome measure could have increased. Also, the video review could have missed subtle contacts. Depending on a single examiner also could have influenced the accuracy of the outcome.

However, even if contaminations were underestimated, the number of reported incidents is still alarming.

Finally, the study protocol was not identical to 2014 CDC guidelines. It excluded aprons, leg covers, and tape seals between parts of the gear. The donning and doffing procedures refer to Tables 3  and 4 . This could be the reason that donning and doffing procedures were finished in very short intervals.

The advantage of this study is as a pilot study, exploring the need of further, more accurate investigations.

",0.6407930097998196
Establishment of minimal positive-control conditions to ensure brain safety during rapid development of emergency vaccines,"With the increase in international human and material exchanges, contagious and infectious epidemics are occurring. One of the effective methods of epidemic inhibition is the rapid development and supply of vaccines. Considering the safety of the brain during vaccine development is very important. However, manuals for brain safety assays for new vaccines are not uniform or effective globally. Therefore, the aim of this study is to establish a positive-control protocol for an effective brain safety test to enhance rapid vaccine development. The blood-brain barrier's tight junctions provide selective defense of the brain; however, it is possible to destroy these important microstructures by administering lipopolysaccharides (LPSs), thereby artificially increasing the permeability of brain parenchyma. In this study, test conditions are established so that the degree of brain penetration or brain destruction of newly developed vaccines can be quantitatively identified. The most effective conditions were suggested by measuring time-dependent expressions of tight junction biomarkers (zonula occludens-1 [ZO-1] and occludin) in two types of mice (C57BL/6 and ICR) following exposure to two types of LPS (Salmonella and Escherichia). In the future, we hope that use of the developed positive-control protocol will help speed up the determination of brain safety of novel vaccines.","In the 21st century, various infectious diseases threaten humans and animals around the world due to ever-increasing global movements of people, animals, and materials [19, 23] . Nonetheless, the most effective way of approaching an epidemic that occurs at unexpected times and or in unexpected places is to rapidly supply vaccines related to the infectious diseases within a limited time [15] . However, during rapid vaccine developments performed to date, there has been little consideration given to establishing a precise protocol for ensuring brain safety. This oversight in the development of general and/or emergent vaccines should be corrected by developing an effective safety protocol that can reduce brain damage risks before distribution of the vaccine. The present study was undertaken to establish a protocol that can be used to determine if a material, such as a vaccine, will damage the brain by breaking down the brain-protective blood-brain barrier (BBB). Since the BBB is a critical morphological structure with selective permeability between blood vessels and brain tissue, it is very important to determine the conditions that can make the BBB vulnerable; for example, exposure to a pyrogenic inflammatory agent such as a lipopolysaccharide (LPS) administered via systemic injection [10] . To ensure the brain safety of newly developed vaccines and to establish positive-control conditions for use in a vaccine safety protocol, we used the specificity of brain anatomical structures to investigate the robustness of the BBB by varying LPS exposure times (0, 2, 4, and 24 h) in two different mouse strains (C57BL/6 and ICR). In this study, we describe an effective positive-control protocol for were tested in two different strains of mice (C57BL/6 [B6] and ICR). The intraperitoneally administered EB (4 mL/kg, 2% [w/v] in phosphate buffered saline [PBS]; pH 7.4) appeared in the homogenized brains following the injection of LPS(S) or LPS(E) in the different mice strains. Compared to the control group administered with vehicle (PBS) only, the amount of EB that penetrated the brain was significantly higher in both mouse strains after injection with both LPSs. Interestingly, for both LPSs the amounts of EB penetrating ICR mouse brains were greater than those of B6 mouse brains, and for both mouse strains the amount of EB penetrance was greater in LPS(S) than in LPS(E). assessing the brain safety of vaccines that will be developed and supplied in the future. The protocol may develop into a complete safety assurance protocol in the future.

Evans blue (EB) assay for brain permeability: Two types of 7-week-old mice (C57BL/6 and ICR) and two LPS sources [Salmonella enterica, LPS(S) and Escherichia coli, LPS(E)] were used. Based on mouse type and LPS source, the C57BL/6 (B6) and ICR mice were assigned to 6 groups (n = 4): group 1, B6/PBS; group 2, B6/LPS(S); group 3, B6/LPS(E); group 4, ICR/PBS; group 5, ICR/LPS(S); and group 6, ICR/LPS(E). We performed the EB assay according to the method described by Jangula and Murphy [14] with a few modifications. Initially, we prepared LPS solutions using two different LPSs. The LPSs from S. enterica serotype enteritidis (L6011; Sigma, USA) and E. coli O55:B5 (L2880; Sigma) were separately dissolved in phosphate buffered saline (PBS; pH 7.4) to a concentration of 1 mg/4 mL.

For treatment, PBS or LPS in PBS were intraperitoneally (i.p.) injected into each mouse (4 mL/kg body weight). At 1 h postinjection (PI) of PBS or LPS, EB dye (2% w/v in PBS, 4 mL/kg, E2129; Sigma) was i.p. injected into each mouse. All mice were sacrificed after 3 h of EB exposure (4 h PI). After sacrifice, blood was collected from heart and serum was separated by using centrifugation. The brain was removed after the mouse had been perfused with heparinized PBS (0.1 mg/L). Serum and homogenized brain hemisphere of each mouse were dissolved in a 50% (w/v) trichloroacetic acid in PBS solution to eliminate proteins and then subjected to centrifugation at 5,000 × g for 20 min. Supernatant of each sample was diluted 1:3 with ethanol.

A spectrophotometer (Infinite F200; Tecan, Switzerland) was used to determine the EB concentrations in the serum and brain hemisphere samples. The fluorescence intensity was measured at 620  ex and 680  em , and the EB concentration of each sample was calculated according to a standard curve that had been prepared with a range of 10 pg/mL to 10 ng/mL EB in PBS. In order to determine the penetration rate for EB into the BBB, the ratio of EB in brain to EB in blood was calculated. All experiments and procedures were approved by Soonchunhyang University Institutional Animal Care and Use Committee (approval No. SCH17-0024).

Brain tissue collection and preparation of RNA and protein samples: Based on the EB assay results (Fig. 1) , we selected the S. enterica-derived LPS for use in the following animal experiments. The mice were divided into 8 groups (n = 4) based on mouse strain and LPS exposure time: group 1, B6/0 h; group 2, B6/2 h; group 3, B6/4 h; group 4, B6/24 h; group 5, ICR/0 h; group 6, ICR/2 h; group 7, ICR/4 h; and group 8, ICR/24 h. The LPS solution was i.p. injected into all mice simultaneously and each group of mice was sacrificed at the planned LPS PI time. After euthanasia, the brain was removed from each mouse and was cut into two hemispheres. RNA was extracted from one hemisphere by using Trizol reagent (Ambion; Life Technologies, USA) and protein was extracted from the other hemisphere by using PRO-PREP (iNtRon Biotechnology, Korea) according to the manufacturers' instructions. Briefly, for RNA extraction, the homogenized hemisphere was incubated in Trizol and chloroform reagent and then subjected to centrifugation. The aqueous phase was removed and incubated with 100% isopropanol. After centrifugation, the obtained RNA pellet was washed with 75% ice-cold ethanol and then dissolved in RNase-free water. The obtained RNA extract solution was stored at −70 o C after undergoing 60 o C heat incubation. For protein extraction, the homogenized hemisphere of each mouse was incubated in PRO-PREP solution and centrifuged several times under ice-cold conditions. The refined protein extract solution was stored at −20 o C.

Quantitative real-time polymerase chain reaction (qPCR) was performed with SYBR Green dye by using a Step One Plus Real-Time PCR system (Life Technologies). For relative quantitation of gene expression, we used the comparative cycle threshold (Ct) method (2

). Results were normalized to that of the housekeeping/control gene glyceraldehyde 3-phosphate dehydrogenase (GAPDH). The primer sequences for zonula occludens-1 (ZO-1) and occludin were obtained from the National Center for Biotechnology Information nucleotide database [7] and are shown in Table 1 .

Protein concentrations were determined by using a BCA kit (iNtRon Biotechnology). Total proteins (20 g per sample) were loaded to each lane of 12% SDS-PAGE, electrophoresed, and transferred to PVDF membranes (Bio-Rad Laboratories, USA). Following transfer, membranes were blocked with TBST [100 mM Tris-HCl (pH 7.6), 0.8% NaCl, and 0.1% Tween-20] containing 5% skim milk (BD Biosciences, USA). The blocked membranes were incubated with diluted rabbit anti-ZO-1 (1:2,000; Thermo Scientific, USA), mouse anti-occludin (1:2,000; Thermo Scientific), and rabbit anti-GAPDH (1:5,000; Cell Signaling Technology, USA) primary antibodies at 4 o C overnight. After further washing, membranes were incubated with peroxidase-labeled anti-rabbit and anti-mouse secondary antibodies (Vector, USA). Immunoreactive signals were detected by using an enhanced chemiluminescence reagent (Abclon, Korea) and recorded by using a MicroChemi 4.2 system (DNR Bio-Imaging Systems, Israel).

Band intensity generated from western blotting was determined by using NIH Image 1.59 software to transform the mean gray level through application of the following formula: optical density = log (256/mean gray level). Data are presented as mean ± SE values for each experimental group. Differences between means were analyzed by using Student's t-test for single comparisons. P values of less than 0.05 were considered statistically significant.

The permeability of EB dye into brain parenchymal tissue after LPS injection is presented as a ratio of the EB dye amount in brain to that in blood (Fig. 1) . When LPS was injected into mice, EB penetrance increased, regardless of the LPS source. However, between the two different mouse strains, the EB ratio was significantly higher in ICR mice than in B6 mice. Interestingly, the LPS(S)-injected group showed a greater increase in EB ratio than that in the LPS(E)-injected group and that tendency was significant in ICR mice.

From LPS(S) injection to 24 h PI, the ZO-1 mRNA level did not significantly change in B6 mice. In contrast, in ICR mice, the ZO-1 mRNA level was significantly up-and downregulated. Until 4 h PI, ZO-1 mRNA level in ICR mice progressively increased, but, at 24 h PI, the level had returned to a level similar to that at injection (0 h PI; panel A in Fig. 2 ). The occludin mRNA levels showed a decreasing tendency over 24 h in B6 mice, but the changes were not significant. In ICR mice, occludin mRNA level in early period after LPS(S) injection (i.e., 2 and 4 h PI) did not increase significantly; however, there was a notable significant increase at 24 h PI (panel B in Fig. 2 ).

Western blot results showed that ZO-1 and occludin protein expression tendencies were similar to the mRNA results in both mouse strains. In B6 mice, there were small changes in ZO-1 and occludin expression levels between 0 and 24 h PI, but the changes were not significant. Interestingly, ZO-1 protein expression in ICR mice continuously and time-dependently increased during the 24 h after LPS(S) injection (panels A and B in Fig. 3 ); in contrast, the ZO-1 mRNA level at 24 h PI had decreased to a level similar to that at injection (panel A in Fig.  2 ). Occludin protein expression in ICR mice showed a temporal pattern that was similar to that for occludin mRNA expression. The occludin proteins in B6 mice did not change significantly, but, in ICR mice, there was a significant increase in occludin protein expression but only at 24 h PI, which was similar to the occludin mRNA level result in ICR mice (panel C in Fig. 3) . The results of mRNA and protein levels have correlation each other and putative mechanisms of LPS-inducing reaction were showed (panels A and B in Fig. 4) . The different tendencies of ZO-1 and occludin mRNA and protein expressions in ICR mice are also summarized with the stream graphs (panel C in Fig. 4) .

The essential requisites for the vaccine verification protocol are promptitude and accuracy. In this respect, the Evans blue assay result showed that BBB structure and function of ICR mice could be more vulnerable than those of B6 mice (Fig. 1) . And 4 h after LPS(S) injection into ICR mice, the ZO-1 mRNA and protein levels were thoroughly increased than 0 h PI, being compared with any other conditions (B6 mice, LPS(E) and 2 h, 24 h PI). qPCR method might be also suitable than WB method in terms of time requirement. Consequentially, the vaccine safety verification protocol for BBB and brain could be established by selecting optimum conditions and its results in Fig. 4 . Zonula occludens-1 (ZO-1) and occludin disruption and recovery processes in C57BL/6 and ICR mice. When lipopolysaccharide binds to a receptor (such as toll-like receptor 4), the nuclear factor kappa B (NF-B) pathway or other inflammatory pathways are activated and release various cytokines. ZO-1 are mislocated with occludin by cytokines, and ZO-1 or occludin proteins are degraded by proteasomes, followed by a recovery activity via protein synthesis from the mRNAs. However, this process might be different in B6 (A) or ICR (B) mice. Considering the low permeability of Evans blue dye and the low expressions of ZO-1 and occludin mRNA and protein, there may be a protective or inhibitory factor suppressing the inflammation activity or degradation of ZO-1 or occludin proteins. This process difference is thought to be related to the more than 24 h required for the protein restoration process of the proteins in brain epithelial cells in B6 mice (A). In ICR mice (B), however, disruption and restore pathways are rather more common than those in B6 mice. The arrow streams indicating translation activity show the different time-dependent patterns in ZO-1 and occludin protein synthesis from mRNA in brain endothelial cells of ICR mice (C).

How to apply the lipopolysaccharide (LPS) positive-control protocol to verify vaccine safety at the blood-brain barrier (BBB). Based on the results of our experiments, we have established a positive-control protocol for determining vaccine safety at the BBB. Salmonella enterica-derived LPS and vaccine are intraperitoneally injected into 7-week-old ICR mice (1.0 mg LPS/4 mL PBS/kg body weight) and brain hemispheres are removed after 4 h. The mRNA is extracted, followed by quantitative real-time polymerase chain reaction (PCR) using zonula occludens-1 (ZO-1), occludin, and glyceraldehyde 3-phosphate dehydrogenase (GAPDH) primers. By comparing the tight junction mRNA levels of the vaccine-injected group with those of the LPS-injected group, the potential danger to the BBB of the vaccine is evaluated.

this study (Fig. 5 ).

Yearly, even seasonally, new or variant viruses and bacteria are reported globally, and such microorganisms can threaten the health of people, regardless of nationality, race, gender, or age. Occasionally, pandemic microorganisms, such as avian influenza or Middle East respiratory syndrome, can rapidly become pervasive globally, and they can impair their host's immune, cardiovascular, respiratory, and other systems and, even worse, can result in death. To prevent and manage these varied epidemics, whether relatively simple or severe, numerous vaccines are produced. In urgent situations, vaccines need to be mass-produced in a short time; however, verification of their safety may be inadequate, particularly confirmation of their brain safety.

All bio-modulations in the human body are regulated and maintained mainly by the central nervous system, particularly, the brain. This important organ is defended by the protective BBB. The highly selective permeability of the BBB is very useful in preventing some macromolecules, toxic substances, and microorganisms from penetrating brain tissue from adjacent blood vessels [1, 22] . However, if the BBB is unexpectedly damaged or loses its protective function following injection of a novel vaccine with insufficient safety verification, brain impairment and malfunction can result. Therefore, there is a necessity to develop an expeditious vaccine assessment module that would assure the BBB was secure.

The BBB mainly consists of capillary endothelial cells, pericytes, and astrocytes. Nutrients and oxygen in the bloodstream can permeate endothelial cells into the basal lamina through a variety of transport routes [9] . These endothelial cells are layered and form a barrier by their close proximity to each other and by tight junctions (TJs) located on cell membranes [3, 13] . TJs such as occludin, claudin, junctional adhesion molecule (JAM) and zonula occludens (ZO) are key structures of the BBB and significantly reduce permeation of some macromolecular and toxic solutes [11, 22] . Therefore, disruption of TJs leads to barrier disruption. Many previous reports have shown that LPS, a toxin from bacteria, combines with the toll-like receptor on the endothelial cell membrane to stimulate inflammation via signals from various cytokines including nuclear factor kappa B, tumor necrosis factor alpha, interleukin (IL)-1, IL-6, and IL-8 [2, 6, 12, 18, 20] . Under excessive inflammatory conditions, these cytokines can break down TJs, particularly occludin, which is sensitive to cytokines [6, 24] . Therefore, we thought it prudent to establish a positive-control protocol that involved injecting an appropriate dose of LPS to an animal model and observing the resultant damage to occludin and ZO-1 TJs.

In that protocol, to assure that the alterations to occludin or ZO-1 mRNA levels actually result in damage to the BBB's selective permeability function, EB assays were performed. In that assay, EB penetrates into brain parenchymal tissues and an EB ratio (brain/blood EB content) is calculated. A high permeated EB ratio indicates a significantly damaged BBB structure. We used LPS from two sources (S. enterica and E. coli) and two strains of mice (B6 and ICR mice). Interestingly, the results from each LPS and mouse strain showed different trends. The BBB protective permeability of ICR mice was more intensely impaired than that of B6 mice, and LPS derived from S. enterica penetrated the brain more effectively than that derived from E. coli. It has been reported that LPS consists of three different structures; O-antigen, core polysaccharide, and lipid A. In particular, the composition of the O-chain varies notably from strain to strain and the O-chain is a recognizable target of host antibodies, as O-antigen. Therefore, it is possible that LPSs of different bacterial origins can produce different degrees of BBB permeability damage. As the purpose of this study was to establish an effective BBB safety verification positive-control protocol, we selected the LPS type that had the highest damage effect on BBB permeability. In addition, the EB results from the two mouse strains showed that a more sensitive positive-control protocol could be established by using ICR mice instead of B6 mice.

Meanwhile, in the developed protocol, very low levels of EB dye were detected in the brain parenchyma of B6/PBS and ICR/PBS mice (i.e., no LPS injected). Because EB dye has toxicity when injected in the body at a high concentration, it could slightly damage the BBB during circulation through the bloodstream [16, 17] .

Occludin is transmembrane protein, and ZO-1 is combined with the occludin C-terminal under the cell membrane in cytoplasm [4, 5, 8] . Under inflammatory conditions induced by LPS, it has been reported that TJs including occludin and ZO-1 are degraded along various pathways [21, 25] . In the present study, the qPCR results demonstrated that occludin and ZO-1 mRNA levels dynamically changed in accordance with exposure time after LPS injection, especially in ICR mice. The EB assay results showed that the BBB permeability was sufficiently damaged at 4 h PI. While developing the positive-control protocol we focused on the early PI period because we thought the less time spent determining the BBB damage point, the more the effective the vaccine safety verification protocol. The ZO-1 mRNA level increased until 4 h PI and was decreased at 24 h PI, recovering to the same level as that at 0 h PI in ICR mice brain. Meanwhile, occludin mRNA level was only significantly increased at 24 h PI. In common with the mRNA results, ZO-1 and occludin protein expressions significantly increased over time in the ICR mice but not in the B6 mice. Interestingly, while ZO-1 mRNA level was downregulated at 24 h PI, ZO-1 protein expression progressively increased. It is thought that this difference is related to the time required to synthesize proteins from mRNAs during the recovery process after TJ impairment. Similar to the ZO-1 results, occludin protein expression also showed an upward tendency at 24 h PI.

Even though some previous in vitro studies have shown that TJ mRNA and protein levels decrease after LPS injection [21, 25] , a compensatory mechanism could not be generally achieved in vitro than in vivo. In other words, even though the BBB structures were destroyed and its function was impaired, it could not be recovered in vitro system in plastic plate or flask, because there are no other kinds of cells or tissues which counteract and cooperate with epithelial cells with TJs. However, in vivo system, the compensatory reaction in the body could be activated by various cells and tissues for healing of impaired region. Therefore, we believe that the expressions of TJ-related mRNAs and proteins continuously increase until TJ structure and function are restored through physiological processes in animal models. In this study, B6 mice showed lower BBB penetrance of EB dye than that in ICR mice; moreover, in both mouse strains, the mRNA and protein expression results from both mice strains generally corresponded with EB assay results.

The study design's time schedule in the present study did not allow determination of the exact times when the mRNA and protein expressions were up-or downregulated; however, we speculate on some important clues in which some diverse cytokine actions, closely related with ZO-1 and occludin activity in cytoplasm, might be achieved by mouse strain-specific differences in signal transduction following LPS treatment. It appears that diverse cytokines produced in cytoplasm after LPS injection can break down the intracellular linkage between occludin and ZO-1. Damaged occludin and ZO-1 proteins are degraded by proteasomes and a recovery pathway is needed. In endothelial cells of the BBB in ICR mice, the impairment and recovery routes for occludin and ZO-1 might be time-consuming. As mentioned, the TJ molecular structure or the inflammatory and recovery pathways of BBB in B6 mice might be different from those in ICR mice. Considering the EB assay results demonstrating the disruption of TJs in BBB, we suggest two possibilities for the differences between B6 and ICR mice. First, impairment of ZO-1 and occludin protein of B6 mice might be less caused than those of ICR mice by an inflammatory activity suppressed by an unknown factor. Second, the structure and function of ZO-1 and occludin in B6 mice are recovered after 24 h PI. However, the hypothesis that there might be unknown factors in B6 mice which protects TJs from being destroyed by LPS or it requires time longer than 24 h to be destroyed and/or recovered should be investigated through further study.

In order to investigate the mouse strain-based differences in the participating BBB structural disruption mechanisms, further protein analyses such as those in proteomic studies should be considered. The relationships between mRNA levels and protein expressions of ZO-1 or occludin in ICR mice are approximated in panel C in Fig. 4 , which includes arrows showing translational streams. While ZO-1 mRNA levels in ICR BBB increased until 4 h PI and decreased until 24 h PI, the ZO-1 protein expression was continuously increased until 24 h PI. This time lag is thought to be caused by the process requirements for protein synthesis from mRNA and by the protein expression regulatory activity of the biological mechanism within brain epithelial cells. The expression of occludin mRNA and protein were rather more slowly increased than those of ZO-1. The location of proteins and accessibility of proteasomes for occludin as a transmembrane protein and for ZO-1 as an intracellular protein might be factors in the temporal differences between the two TJ proteins.

After considering the study's results, we have established a positive-control protocol for verification of vaccine safety to brain by measuring BBB disruption and restoration conditions via a molecular biological approach. We conclude that the ICR mouse strain is suitable for use in the protocol with TJ disruption of the BBB assessed after 4 h of exposure following systemic LPS administration at a dosage of 1 mg LPS(S) in 4 mL PBS per kilogram body weight. By comparing the TJ mRNA levels at 4 h PI between the LPS-injected group and the selected vaccine-injected group, it is possible to surmise the potential danger of damage to the BBB structure of the tested vaccine. For example, if TJ mRNA levels of vaccine-injected group are significantly higher than those of the LPS-injected group 4 h after injection, one should consider the vaccine to be unsafe to BBB structure or function at the injected concentration. Further testing may be done at other concentrations.

Further studies should be undertaken to identify the exact time PI at which TJ mRNA and protein levels are altered. This can be accomplished by narrowing the time intervals used in this study. Moreover, investigations into possible differences between male and female mice, and into establishment of double-check modules by applying other kinds of TJs including claudins or JAMs are needed. We are planning additional research to ensure the safety of various vaccines, including those already in the market, by using this novel positive-control protocol. We expect that the protocol will be very useful in determining brain safety of urgently developed new vaccines.

",0.6401566390055361
Structural breaks in international tourism demand: Are they caused by crises or disasters?,We study the number and date of structural breaks in international tourism demand. We use Bai and Perron (1998) structural break test for 25 countries and Madeira. We fill a gap in the literature regarding the ex-post detection of tourism crisis. We compare the date of tourism crises to the dating of these structural breaks. We observe those tourism crises are largely consistent with the dates of breaks.,"Tourism crises and disasters are not new phenomena. The media is replete with negative reports of well (and lesser)-known examples of events such terrorist attacks, natural disasters, political instability, outbreak of war, economic recession, biosecurity, disease threats and other negative events that threaten the tourism industry. As reported by Ritchie (2008) , whatever the source of these negative events, the subsequent decrease in international tourist arrivals may have a significant social and economic impact both for the destination and the wider global economy.

After the publication of Faulkner's (2001) seminal work on tourism crises and disasters, an increasing number of articles have been published on this subject, especially in crisis and disaster management. This is due to the fact that, as mentioned by Speakman and Sharpley (2012: 67) , even though the tourism industry continues to grow in scope and scale, such events appear, perhaps inevitably, to occur with increasing frequency, to the extent that ""tourism destinations in every corner of the globe face the virtual certainty of experiencing a disaster of one form or another at some point in their history"" (Faulkner, 2001, p. 142) .

Nevertheless, despite the growing number of studies in the tourism field on tourism crisis management there is no clear and commonly accepted definition and statistical methodology that allows us to define a crisis or disaster event in the tourism sector. In part this problem is explained by the fact that most of these studies may have simply adopted a qualitative case study research design that is by nature not quantitative. The majority of the studies on tourism crises and disasters appear to be concerned with prevention, planning, response and recovery of such events (see, e.g., Ritchie, 2008; Hall, 2010) . The only references in this respect are limited to the definition of the terms crisis and disaster 1 without defining a methodology for ex-post detection of these negative events.

The present paper differs from previous studies as we seek to fill a gap in the literature regarding the ex-post detection of tourism crises and disasters. Accordingly, given the nature of these events e ""a sudden unpredictable catastrophic change over which it has little control"" (Faulkner, 2001) , we propose the Perron (1998, 2003a, b) (hereafter BP) structural break method to estimate both the number and date of structural breaks in international tourism arrival series for 25 countries and Madeira Island. This method has been widely used in different sectors of activity in the determination of structural changes. After that, we compare the date of tourism crises and disasters to the dating of these structural breaks. We observe that tourism crises and disasters are largely consistent with the dates of breaks.

Therefore, this method contributes in two ways to the literature related to crisis or disaster events in the tourism sector. First, this method allows us to close a gap in the tourism industry related to the correct allocation of negative shocks in international tourism arrival demand to crisis or disaster phenomena. As stated by Intriligator, Bodkin, and Hsiao (1996: 579 ) ""a break in structure is always a serious matter because it implies that none of the statistical techniques (hypothesis testing, confidence intervals, etc.) is valid any longer. Moreover, the substantive conclusions about the system being modeled that one might wish to draw are also vitiated, to a greater or smaller extent"". Thus, we should be careful in interpreting the results of studies using international tourism demand data that did not take into account these structural breaks, to the extent that their results can be biased. Second, this method can be an important tool for monitoring the impact of a crisis event on the performance of the sector and to provide useful information to develop adequate policy responses to governments, destination managers and hotel managers. A crisis event that causes a structural break should require a specific policy and a greater allocation of resources by policymakers'. 2 The structure of the remaining paper is as follows. Section 2 provides some conceptual background and a review of previous studies about tourism crises and disasters and Bai and Perron's method and Section 3 the econometric procedure. Data is presented in Section 4, and Section 5 reports and discusses the results. The conclusion, limitations and future research appear in Section 6.

The important question we seek to discuss in this section is the definition of a criterion/detection methodology of the presence of a crisis or disaster event in the tourism sector. Scott and Laws (2005: 152) present a table with different definitions of the terms crisis and disaster. We present three of these definitions. Reilly (1993) defines crisis as ""a situation which is harmful and disruptive, of high magnitude and is outside the firm's typical operating frameworks"". Carter (1991, p. xxiii) defines a disaster as ""an event, natural or man-made, sudden or progressive, which impacts with such severity that the affected community has to respond by taking exceptional measures"". Finally, the Pacific Asia Travel Association (PATA) (1991) describes a tourism crisis as ""a disaster, whether natural or man-made, that has the potential to totally disrupt the tourism industry"". As Faulkner (2001: 137) emphasizes, all these definitions have transformational connotations, with each event having potentially negative outcomes.

The question is how to know when there is ""in practice"" a tourism crisis or disaster. Hall (2010: 403) refers that ""any period where international tourism numbers only increase by 2% or less often appears to be described as a crisis for the industry"". We are of the opinion that this is a simplistic definition that may explain the high number of researches on crises in the tourism sector. In fact, the author emphasizes a substantial number of publications that have looked at tourism crises. He reports 103,000 publications whose abstracts refer to crises, tourism crises and different types of crises. However, Brecher (1978) highlights the danger of oversimplification and the importance of recognizing that the crisis can be a complex phenomenon, given the chaotic nature of crises and disasters and the uncertainty that surrounds them. Given that crises and disasters can lead to chaotic situations and complex interrelations between human and natural systems, the comprehension of the relationship between cause and effect and the implications of decisions and actions is a complicated process (Faulkner, 2001) . In this sense Brecher (1978) highlights the seven aspects of the crisis that must be analyzed e source, gravity, complexity, time, communication patterns, outcomes and potential intensity.

The study of crisis events and disasters in the tourism sector, in particular the impacts of such events on the tourism industry and the responses of industry and the relevant government agencies are extremely important (Faulkner, 2001) . The author states that tourism, as an area of human activity is no less prone to disasters than any other. However, the recovery of this industry from a crisis is far more complicated than for other industries (Cavlek, 2002) . A very strong partnership and coordinated work among the government, national tourism organizations, foreign tour operators, local travel organizers, and local hospitality officials are essential, according to Cavlek (2002: 487) . Each needs to participate to an important degree in order to secure the fulfilment of several important actions. These include successful rebuilding of the destination image, overcoming any adverse policy resulting from the crisis, short-term restoration and long-term reconstruction of the damaged tourism facilities and infrastructures, effective management of media coverage, reduction of barriers and travel facilitation, and business and consumer regulation support and subsidies (Cavlek, 2002; Steiner et al. 2013) .

The past few years seem to confirm the scenario that tourism is no less prone to disasters than any other industry, by showing an increasing number of disaster and crisis which affect the tourism industry, ranging from human-to nature-influenced incidents. Are examples of these events the terrorist attacks, natural disasters, political instability, outbreak of war, economic recession, biosecurity and disease threats (for detailed information about the various studies carried out on tourism crises and disasters, see, e.g., Ritchie, 2008; Hall, 2010) .

Finally, although various studies have extensively investigated crisis phenomena by using quantitative approaches in order to identify disruptions in tourist arrivals (e.g. Cavlek, 2002; Enders, Sandler, & Parise, 1992; Steiner et al., 2013) , none presents a methodology for identifying tourism crises or disasters. The identification of a ""real crisis"" is often a complex issue for several reasons. Sometimes countries suffer damage caused by a ""ripple"" effect that goes far beyond the destinations and parties directly 1 Scott and Laws (2005) involved in the incident that may result in a crisis (Cavlek, 2002, p. 481) . As shown by Young and Montgomery (1998:4) ""… a crisis has the potential to be detrimental to the marketability of any tourist destination, particularly if it is dramatised and distorted through rumours and the media"". On the other hand, as stated by Faulkner (2001:142) , ""the effectiveness with which the tourism industry in a disaster area handles a crisis, and therefore the degree to which it is prepared for it"", could explain why potentially serious crisis events for the industry did not bring about negative results. For this reason, we propose the application of Bai and Perron's (1998) method to identify structural changes in the international tourism arrival series. 3 This method also allows us to examine whether the dates defined by the method correspond to crisis or disaster events cited among other authors in Hall (2010) and Ritchie (2008) . Given that the BP method determines endogenously the structural break dates that we do not know in practice, it allows us to see which crisis events were truly harmful to the tourism industry, i.e., the real crisis events. We are of the opinion that it is the best method to identify an expost tourism crisis or disaster since this method determines endogenously structural breaks and therefore ""a situation which is harmful and disruptive"". Subsequently, in the explanation of structural breaks, we can relate them to crisis and disaster events referred to in researches and in the media.

We use the Perron (1998, 2003a, b) structural break method to test for infrequent structural breaks in international tourism arrivals. We use the BP method for three reasons. First, the method can deal with multiple structural breaks. We are not interested in models that can only detect a single structural break. Second, the method assumes that potential structural breakpoints are unknown and determine endogenously the structural break dates that we do not know in practice. Finally, the BP method is also revealed to be appropriate for small samples (see Jones & Olken, 2008) .

The BP method has been widely used in different sectors of activity, in the determination of structural changes. This method appears in the top 1‰ ranking of research items by number of citations. 4 Its scope ranges from the energy sector e e.g. Aruga (2016) and Wakamatsu and Aruga (2013) use the BP method to identify the break data related to the shale gas revolution and Noguera (2013) in oil prices; sports e e.g. Groothuis, Rotthoff, and Strazicich (2015) use time-series tests with structural breaks in Major Baseball performance; legal studies e e.g. Vuji c, Commandeur, and Koopman (2016) apply the BP method to investigate the monthly crime rates in Virginia and Narayan (2004, 2006) examine structural changes in the level of consensus on the high court of Australia and on the U.S. supreme court, respectively; fisheries and farming e e.g. Wakamatsu and Miyata (2015) ; Gutierrez, Westerlund, and Erickson (2007) and Kristofersson and Anderson (2006) use it on the economic and financial sector (with the highest number of papers) e (i) interest rates e e.g. Caporale and Grier (2000) and Rapach and Wohar (2005) , (ii) inflation rate e e.g. Gadea, Sabat e, and Serrano (2004) and Hassler and Meller (2014) , (iii) foreign exchange e e.g. Kellard, Jiang, and Wohar (2015) and Frankel and Xie (2010) , (iv) government debt e e.g. Cuestas, Gil-Alana, and Staehr (2014) and Tamakoshi and Hamori (2014) , (v) stock market e e.g. Bekiros, Gupta, and Kyei (2016) , (vi) nominal wage e e.g. Gottschalk (2005) , (vii) commodity prices e e.g. Enders and Holt (2012) , (viii) productivity growth e e.g. Fernald (2007) , (ix) output growth e e.g.

Cori c and Pugh (2013) and (x) real estate market e e.g. Martins, Serra, Martins, and Stevenson (2015) . Nevertheless, we do not find any application of the BP method in the tourism sector, although this sector often uses time series.

The BP method is applied by us in a series of international tourism demand to identify possible structural changes in country tourism demand and to check whether this break is caused by a crisis or disaster in the tourism sector. In this manner, we are only interested in the structural breaks that led to a decrease in the number of international tourists, seeking to identify the tourism crises or disasters that are at its origin. As is pointed out by Pesaran and Timmermann (2004) , the majority of time series are subject to occasional structural breaks and it can be very costly to ignore breaks because the results will be biased and forecasts inconsistent. Given that the majority of researches using international tourism arrival data do not analyze the existence of breaks in tourism demand series, the quality of the results is not guaranteed, neither is the correct identification of crisis and disaster phenomena in the tourism sector. In this sense, the BP method is used by us in this study in order to analyze the linkage between structural changes and crisis and disaster phenomena in the tourism sector. In the next section, the BP method is explained in detail. Perron (1998, 2003a, b) provide theoretical and computational testing procedures by which we can identify unknown breakpoints given T observations and m potential breaks (producing mþ1 regimes) by least squares. Following among others, Caporale and Grier (2000) and Rapach and Wohar (2005) , we regress international tourism arrival series on a constant and test for structural breaks in the constant. The model is:

(1) for j ¼ 1, …, m þ 1, where y t is the international tourism arrival series in period t and b j (j ¼ 1, …, m þ 1) is the mean of a dependent variable in the jth regime. The m-partition, (T 1 , …, T m ), represents the breakpoints for the different regimes (by convention, T 0 ¼ 0 and T mþ1 ¼ T). We can identify multiple structural changes in the following manner: For each m-partition, (T 1 , …,T m ) the leastsquares estimates of b j are obtained by minimizing the sum of squared residuals,

where, S T represents the sum of squared residuals in m-partition.

Being the regression coefficient estimates based on a given m- 

where S T ðT 1 ; …; T m Þ denotes the sum of squared residuals. After estimating the breakpoint, it is straightforward to compute the corresponding least-squares regression parameter estimates as algorithm for the minimization problem in Equation (3) based on the principle of dynamic programming. Bai and Perron (1998) suggest the application of the three following tests to determine the existence of structural changes and to identify the number of breaks: (i) SupF T ðmÞ test; (ii) ""double maximum"" statistics given by UD max ¼ max 1 m M SupF T ðmÞ and UD max ¼ max 1 m M W m SupF T ðmÞ;

(iii) the sequential SupF T (m þ 1\m) test. According to the authors, the procedure to determine the existence of structural changes and to identify the number of breaks is as follows. First, let SupF T ðmÞ indicate the Fstatistic for testing the null hypothesis of no structural breaks against the alternative hypothesis that there are m ¼ b breaks, which are selected according to (3). Second, let UD max ¼ max 1 m M SupF T ðmÞ and UD max ¼ max 1 m M W m SupF T ðmÞ denote the two ""double maximum"" statistics, to test the null hypothesis of no structural breaks against the alternative hypothesis of an unknown number of breaks given an upper bound, M. Third, let SupF T (m þ 1\m) denote the F-statistic to test the null hypothesis of m breaks against the alternative hypothesis of mþ1 breaks. Fourth, see the UD max or WD max tests to know if at least one break is present. Finally, see the sequential SupF T (m þ 1\m) statistics and define the number of breaks at the 5% level.

To study the presence of structural changes in international tourism demand, we collect data on international tourism arrivals for 25 countries between 1995 and 2014 from the World Bank. 5 The first countries to be analyzed are the Maldives, Thailand, Sri Lanka, Indonesia, Singapore, Taiwan, Vietnam, Hong Kong, China and Japan. For these countries, we want to check if structural breaks are linked to any of these crises identified in tourism literature e Asian financial crisis, tsunami and earthquakes and Severe Acute Respiratory Syndrome (SARS) and avian flu. Next, we analyze Syria, Tunisia, Egypt and Israel to check if structural breaks are linked with the Gulf War, 9/11 attacks and the Arab Spring. 6 We also analyze the tourism demand of New Zealand and Haiti to study the impact of earthquakes in these two countries. It is followed by the analysis of the impact of the foot and mouth disease disaster, 9/11 attacks and AH1N1 influenza crisis in the United Kingdom, USA and Mexico, respectively. Finally, we analyze the effects of the recent financial and economic crises in Ireland and in five southern European countries e Greece, Cyprus, Portugal, Spain and Italy.

Additionally it is for us examined the case of Madeira Island, where we intend to determine the existence of structural breaks and check if any of the dates of the breaks correspond with the storms of October 1993 and February 2010. In this case we collect data about ""guest arrivals in hotel establishments"" between 1991 and 2014 from Tourism Statistics of the Autonomous Region of Madeira. 7

The structural break estimation results through EVIEWS 8 are presented in Table 1 . We only report the structural breaks that led to a decrease in the number of international tourists. Posteriorly, we compare the dates of tourism crises and disasters to the dating of these structural breaks. We observe that tourism crises and disasters are largely consistent with the dates of breaks.

The results reveal the existence of structural breaks in the ten Asian countries analyzed. In the case of China, Hong Kong, Japan, Singapore, Taiwan and Vietnam, the BP method reveals the existence of a structural break in 2003, which can be explained by SARS and avian flu. In this regard, among others, Wen, Huimin, and Kavanaugh (2005) and Zeng, Carter, and Lacy (2005) , in the case of China; Au, Ramasamy, and Yeung (2005) , in the case of Hong Kong; Henderson and Ng (2004) , in the case of Singapore; and McKercher and Chon (2004) and Kuo, Chen, Tseng, Ju, and Huang (2008) for the generality of Asian countries, highlight the negative effects of SARS on the tourism sector in these countries. Our results also show that the tsunami of 26 December 2004 in the Indian Ocean had a negative effect on the tourism industry of the Maldives, Sri Lanka, Thailand and Indonesia. In this latter country the Bali bombings have also contributed to poor performance of the tourism sector. The structural breaks identified in 2005 are in line with the devastation of the tourism industry verified in these countries, as is indicated by Sharpley (2005) . Finally, we detected the existence of three more structural breaks -in 2007 and 2008 in Hong Kong and Sri Lanka and in 2011 in Japan. In the latter case, we believe that structural change is due to the earthquake, tsunami, and nuclear accident that had occurred consecutively in Fukushima, Japan, in March 2011 (in this regard see, e.g., Kim & Park, 2016) . The breaks identified in Hong Kong and Sri Lanka in 2007 and 2008, were probably due to the impact of financial and economic crisis. Song and Lin (2010) argue that financial and economic crisis have a negative impact on both inbound and outbound tourism in Asia.

The results of the BP structural break method for the four countries of MENA analyzed show that the 9/11 attacks and the Arab Spring are responsible for the fall of the flow of tourists registered in 2001 and 2011, in most of these countries. With regard to the 9/11 attacks, Avraham (2015: 226) states that ""tourism flow to these countries has been affected not only by events that occurred there but also by events that happened elsewhere and involved Arab citizens, as is the case of 9/11 attacks"". The author further notes that in the case of the Arab Spring the negative effect on tourism on the Middle East is due to the intensive coverage that the Arab Spring uprisings got in the media. Finally, we also verified the existence of a structural break in 2008 in Israel, which we believe is a result of the financial crisis that has affected all countries in a generalized way, as shown by Papatheodorou, Rossell o, and Xiao (2010) .

With regard to Ireland and the five countries of southern Europe analyzed, the results reveal the existence of structural breaks in four of them e Cyprus, Greece, Ireland and Spain -and the absence of any structural break in Portugal and Italy. Costa, Gomes, and Montenegro (2014) and Cellini and Cuccia (2015) maintain that the tourism industry of Portugal and Italy, respectively, as a whole has been more resilient to economic crisis than the other industries. In the case of Italy, the authors show that the ""tourism industry has been able to substitute domestic demand with foreign demand, thus limiting the negative effect of severe domestic crisis"".

For the other four countries, the financial crisis is responsible for the observed structural breaks. In the case of Cyprus, we identify two structural breaks e in 2002 and 2009 e that can be explained according to Boukas and Ziakas (2013) , by the financial crisis. The authors argue that ""the overdependence of this island on tertiary sector/tourism and on the foreign capital makes it susceptible to global crises"". Finally, we identify one structural break in Greece, Ireland and Spain that is due to the impact of the financial and economic crisis, as emphasized by O'Brien (2012), Alonso-Almeida and Bremser (2013), Smeral (2009 Smeral ( , 2010 and Papatheodorou et al. (2010) .

Haiti and New Zealand both recently suffered an earthquake. In the case of Haiti, the earthquake of January 2010 caused heavy damage to the tourism sector, so the structural break identified in 2010 was to be expected. In the case of New Zealand, the earthquake of February 2011 caused damage mainly around Christchurch and we have not identified any structural break in the international tourism arrival series. As stated by Brecher (1978) , the gravity and potential intensity are two important factors for the beginning of a crisis that probably not have occurred in the case of New Zealand. The same idea is present in the two structural breaks identified in Madeira. In both cases, natural disasters are at their source. As noted by Machado (2012) , the ""Island of Madeira was not prepared for a catastrophe such as the one that occurred in February 2010, and its consequence has been a huge drop in the tourism industry"". Finally, we identify four structural breaks for Mexico, the UK and the USA. The structural break that we find in Mexico is due to the AH1N1 influenza outbreak that has occurred in the country in 2009. As underlined by Speakman and Sharpley (2012: 72) , soon as the ""situation dominate the world's media, international visitor numbers to Mexico decreased rapidly with the news of the influenza outbreak"". In the case of the UK the structural break identified is related to the foot and mouth disease, which was studied by, among others, Baxter and Bowen (2004) . The two structural breaks identified for the US are related to the 9/11 attacks and the financial crisis. The negative impact of the 9/11 attacks on US tourism and air travel is highlighted by Goodrich (2002) and Blunk, Clark, and McGibany (2006) . The recent economic and financial crisis seems to explain the structural break in 2009, as stated by Ritchie, Molinar, and Frechtling (2010) .

In this article, we use the Perron (1998, 2003a, b) multiple regime shift procedure to identify the exact number and dates of breakpoints in international tourism arrival series of 25 countries and in Madeira Island. Our results suggest that this method is shown to be reliable in ex-post detection of negative impacts of tourism crises and disasters related to natural disasters, terrorist attacks, political instability, economic and financial crisis and Table 1 Structural Break Estimation Results and its Rationale. This table presents for each country analyzed the results of the Perron (1998, 2003a, b) structural break method to test for infrequent structural breaks in international tourism arrivals. For the identified structural break dates, we seek to identify their rational, i.e. the existence of a crisis or disaster in the tourism sector of the country. In this manner, we are only interested in the structural breaks that led to a decrease in the number of international tourists, seeking to identify the tourism crises or disasters that are at its origin. * Significant at the 0.05 level. ** Bai and Perron (2003b) biosecurity and disease threats. The Bai and Perron's method has the advantage of determining endogenously structural breaks and identifying the respective dates, which allows a relationship between these dates and crisis and disaster events. Therefore, this method contributes in two ways to the literature related to crisis or disaster events in the tourism sector. First, this method allows us to solve a gap in the tourism industry related to the correct allocation of negative shocks in international tourism arrival demand to crisis or disaster phenomena, ensuring that the estimated results are not biased. Second, this method can be an important tool for monitoring the impact of a crisis event on the performance of the sector and for providing useful information to develop adequate policy responses to governments, destination managers and hotel managers. A crisis event that causes a structural break should require a specific policy and a greater allocation of resources by policymakers'.

While our research has valuable contributions, it also has some limitations. The first limitation is related to the fact that different crises may occur simultaneously in a given country or region. Given that Bai and Perron's method only determines endogenously the structural breaks and identifies their dates, it is impossible to know in these cases if only one or both of these crises are responsible for the structural break in demand for international tourism.

Secondly, given the ex-post detection of structural breaks, it can be argued that the intended monitoring of statistical data for management purposes seems to be solely possible within certain time lag. This is an extremely important point and has been one of the highlights of the current review of the European Union Solidarity Fund (EUSF) Regulation that provides financial support to a Member State in the event of a major natural disaster. 8 The recommendations for improving the EUSF include (i) budgetary discipline; (ii) reduction of the time taken to provide aid and (iii) a greater clarity as regards scope and definitions. The authors of the document consider that the time currently taken to mobilise the fund is unacceptably long, introducing the possibility of paying advances as soon as the affected state has applied for assistance and the aid would have to be returned to the Union budget in the event that the application was not accepted. For this purpose, it is necessary to define clearly and simply what a disaster is. Given that in developed countries it is possible to incorporate forecasts in international tourism demand series based on travel and accommodation booking requests, it may be possible to detect structural breaks in a short period of time. In the presence of a tourism crisis, the cancellation of travel and hotel reservations often occurs, a situation that tends to be rapidly incorporated into the estimates made for the series analyzed and captured by the BP method. In the short time that elapsed between the occurrence of the crisis and its detection by the BP method, the mechanism of pay advances mentioned above can be used.

Finally, the BP method should be applied in other countries or regions not included in the present study where crises or disasters have occurred in order to test the robustness of the present method in the detection of structural breaks.

",0.6398272428438665
Emerging tick-borne infections in mainland China: an increasing public health threat HHS Public Access,"Since the beginning of the 1980s, 33 emerging tick-borne agents have been identified in mainland China, including eight species of spotted fever group rickettsiae, seven species in the family Anaplasmataceae, six genospecies in the complex Borrelia burgdorferi sensu lato, 11 species of Babesia, and the virus causing severe fever with thrombocytopenia syndrome. In this Review we have mapped the geographical distributions of human cases of infection. 15 of the 33 emerging tick-borne agents have been reported to cause human disease, and their clinical characteristics have been described. The non-specific clinical manifestations caused by tick-borne pathogens present a major diagnostic challenge and most physicians are unfamiliar with the many tick-borne diseases that present with non-specific symptoms in the early stages of the illness. Advances in and application of modern molecular techniques should help with identification of emerging tickborne pathogens and improve laboratory diagnosis of human infections. We expect that more novel tick-borne infections in ticks and animals will be identified and additional emerging tickborne diseases in human beings will be discovered. Among the 33 newly recognised tick-associated agents, 15 have been reported to cause human infection, including: four species of SFGR; 11,35,46,48 an Ehrlichia species, 53 two Anaplasma species, 17,53 and Candidatus N mikurensis; 18 three genospecies of B burgdorferi sensu lato; 19,21,78 three species of Babesia; 33,104,108 and SFTSV (table 2). 34 Six of the tickborne pathogens were first identified in febrile patients (Candidatus Rickettsia tarasevichiae, Candidatus N mikurensis, Borrelia garinii, Borrelia afzelii, Babesia venatorum [not yet","Ticks were the first arthropods to be recognised as vectors that can transmit pathogens to human beings and are second only to mosquitoes as vectors of infectious diseases in the world. 1 Tick-borne infections are zoonoses with pathogens maintained in natural cycles involving tick vectors and animal hosts. Human beings are occasional hosts for ticks and are usually viewed as dead-end hosts that have no role in maintaining tick-borne agents in nature. 2 Different tick species favour distinct biotopes or environments, which define their geographical distribution and, consequently, the areas of risk for human tick-borne infections. In the past three decades, tick-borne pathogens have emerged worldwide and become a great threat to human health. 1, 3 China is the largest developing country in the world and has made tremendous progress in the control and prevention of infectious diseases; however, emerging infectious diseases are the new challenge now facing China. 4 Although outbreaks of severe acute respiratory syndrome and H5N1 and H7N9 avian influenza virus infections have attracted great attention, emerging tick-borne diseases have generally been neglected by primary healthcare providers and clinicians. Although an increasing number of tick-borne infections have been reported in mainland China, no comprehensive review of this substantial public health problem has been undertaken. We will provide an overview of the type and distribution of emerging tick-borne infections in tick vectors, animal hosts, and human beings. We will describe the clinical characteristics of human tick-borne diseases, and discuss possible factors contributing to their emergence.

Since 1982, 33 emerging tick-associated agents have been identified in mainland China, including eight species of spotted fever group rickettsiae (SFGR); 5-11 three species of Ehrlichia, [12] [13] [14] three species of Anaplasma, [15] [16] [17] and Candidatus Neoehrlichia mikurensis 18 in the family Anaplasmataceae; six genospecies in the complex Borrelia burgdorferi sensu lato; [19] [20] [21] [22] [23] 11 species of Babesia; [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] and severe fever with thrombocytopenia syndrome virus (SFTSV). 34 The location and year in which each emerging tick-borne agent was first identified are shown in the appendix. Most (19 of 33) were initially detected in ticks; however, six were identified in domestic animals (sheep, goat, buffalo, and dog), two in wild animals (Chinese white-bellied rat and Chinese hare), and six in people. Detailed information regarding the identification of these emerging tick-borne infections is summarised in table 1.

shown to infect human beings (Rickettsia heilongjiangiensis, Rickettsia sibirica sp BJ-90, Rickettsia raoultii, Ehrlichia chaffeensis, Anaplasma phagocytophilum, Anaplasma capra, Borrelia valaisiana-related genospecies, Babesia microti, and Babesia divergens). Among these 15 emerging tick-borne diseases, severe fever with thrombocytopenia syndrome (SFTS) was first identified in mainland China 34 and subsequently reported in South Korea and Japan. 134, 135 A disease similar to SFTS has been reported in the USA. 136 Human infections with R sibirica sp BJ-90, Candidatus R tarasevichiae, A capra, and B valaisianarelated genospecies have been exclusively diagnosed in mainland China. 11, 17, 46, 78 Human infections of Candidatus N mikurensis, Ba venatorum, and R raoultii, which have been detected in China, were first identified in Europe. 18, 33, 48 Five tick-borne pathogens have been detected in ticks or reservoir hosts, or both, but have not yet been reported to cause infection in human beings in China. They include Rickettsia monacensis, Rickettsia slovaca, Rickettsia sibirica sp mongolotimonae, B valaisiana, and B burgdorferi sensu stricto. [137] [138] [139] [140] [141] Additionally, a Rickettsia species (Candidatus Rickettsia hebeiii), two Ehrlichia species (Ehrlichia canis and Ehrlichia sp Tibet), an Anaplasma species (Anaplasma platys), a genospecies of B burgdorferi sensu lato (Borrelia sinica), and eight Babesia species (Babesia ovis, Babesia ovata, Babesia orientalis, Babesia major, Babesia motasi, Babesia caballi, Babesia sp Kashi, and Babesia sp Xinjiang) have been identified in ticks or animals, but their pathogenicity to human beings is unknown.

Eight novel species of SFGR have been recorded in mainland China since 1982 (table  1) . [5] [6] [7] [8] [9] [10] [11] These species are mainly distributed in northern China (north of 36° north latitude).

The eight emerging species and uncharacterised species of SFGR have been shown to be associated with 16 tick species (figure 1). R heilongjiangiensis has been proven to infect a range of tick species, three rodent species, and goats (appendix). In northeastern China, R heilongjiangiensis was detected in nine tick species and two rodent species from Heilongjiang Province, [36] [37] [38] and in Haemaphysalis spp ticks from Jilin Province. 38 In Inner Mongolia, R heilongjiangiensis has been reported in Haemaphysalis verticalis, Detmacentor niveus, Hyalomma asiaticum kozlovi, Rhipicephalus pumilio, and Detmacentor nuttalli ticks. 39 In northwestern China, R heilongjiangiensis has been detected in Dermacentor silvarum ticks from Qinghai Province. 40 In southern China, R heilongjiangiensis has been detected in Haemaphysalis longicornis ticks from Guangdong Province, 41 in goats from regarding infected ticks and animal hosts are shown in figure 1 and the appendix. [7] [8] [9] [10] 47, 52, 54, 119, [142] [143] [144] [145] [146] [147] 

Four species of emerging SFGR have been reported to infect human beings, including R heilongjiangiensis, Candidatus R tarasevichiae, R sibirica sp BJ-90, and R raoultii (table 2; figure 2 ). Of 34 people infected with R heilongjiangiensis, 19 were diagnosed in forested areas of northeastern China in the 1990s. 35, 45 The other 15 cases of infection were reported on the island of Hainan Province in 2008. 43 In 2012, we undertook an active surveillance for human SFGR infections at the Mudanjiang Forestry Central Hospital of Heilongjiang Province in northeastern China, and eight cases of SFGR infection (including five infected by Candidatus R tarasevichiae, one by R sibirica sp BJ-90, and two by R raoultii) were identified by molecular detection and sequence determination. The presence of antibody against SFGR antigen in serum samples detected by immunofluorescence assay lent support to the diagnosis of SFGR. 46, 48 Additionally, 37 patients were reported to have been infected with uncharacterised species of SFGR, including one in Inner Mongolia, one in Xinjiang Autonomous Region, 29 in Hainan Province, and six in Heilongjiang Province ( figure  2 ). [119] [120] [121] [122] [123] [124] The clinical manifestations of SFGR infections mainly include fever, eschar, headache, malaise, asthenia, anorexia, nausea, and lymphadenopathy. A few patients had rash and neurological manifestations such as coma, neck stiffness, and Kernig's sign. 11, 35, 43, 45, 46, 48, [119] [120] [121] [122] [123] [124] Detailed information about the clinical and laboratory characteristics of patients infected with each of the emerging SFGR species are summarised in the appendix.

Seven species in the family Anaplasmataceae have been identified in mainland China, including E chaffeensis, E canis, Ehrlichia sp Tibet, A phagocytophilum, A capra, A platys, and Candidatus N mikurensis (table 1) .

The seven emerging agents in the family Anaplasmataceae are known to be associated with 17 tick species (figure 1). E chaffeensis was first detected in Amblyomma testudinarium ticks from Yunnan Province in 1996. 12 Several surveys report that it infects various tick species, and is widely dispersed in mainland China. 31, [54] [55] [56] [57] [58] Furthermore, E chaffeensis was detected in dogs from Shandong Province, in long-tailed ground squirrels (Citellus undulates Pallas) and gerbils from Xinjiang Autonomous Region, in the striped field mouse (Apodemus agrarius) from Heilongjiang Province, and in rodents (Rattus norvegicus, Rattus losea, Rattus flavipectus, Rattus niviventer, Mus musculus, Niviventer confucianus, and Rattus edwardsi) and hares (Lepus sinensis) from Fujian and Zhejiang provinces (appendix). 36, 55, [59] [60] [61] [62] A phagocytophilum was first detected in Ixodes persulcatus from Heilongjiang Province in 1997, 15 and is now the most widely encountered species in the family Anaplasmataceae over broad areas where multifarious tick species are vectors (figure 1). Additionally, A phagocytophilum infects domestic and wild animals, including cattle, (appendix). 36, 51, 54, 55, 62, [64] [65] [66] [67] [68] [69] [70] [71] [72] A capra was first recognised in goats (Capra aegagrus hircus) and provisionally named by our group, but has since been detected in I persulcatus ticks in Heilongjiang Province. 17 Although Candidatus N mikurensis was initially identified in I persulcatus and Haemaphysalis concinna ticks, 18 it was subsequently detected in D silvarum, H longicornis, and various rodents from many areas of mainland China. 18, 76, 77 Three other species, including E canis, Ehrlichia sp Tibet, and A platys, have not been proven to infect human beings. Data regarding infected ticks and animal hosts are shown in figure 1 and the appendix. 13, 14, 16, 57, 63 

Four species of Anaplasmataceae have been identified to cause human infections in mainland China: E chaffeensis, A phagocytophilum, A capra, and Candidatus N mikurensis.

The first human case of monocytic ehrlichiosis was diagnosed in a forested area of Inner Mongolia in 1999. 53 Of the 12 human cases of monocytic ehrlichiosis reported, four were recorded in Inner Mongolia, two in Beijing, two in Tianjin, and four in Shandong Province (figure 2). 53,55 A cluster of ten cases of human granulocytic anaplasmosis caused by nosocomial transmission was identified in Anhui Province in 2006. 73 An additional 94 cases of human granulocytic anaplasmosis have been reported, including 33 in Beijing, six in Tianjin, 41 in Shandong Province, one in Anhui Province, and four to five each in Henan and Hubei provinces and Inner Mongolia (figure 2). 53, 55, 64, 74, 75 In 2015, 28 patients were reported to be infected with A capra in Heilongjiang Province. 17 Seven cases of Candidatus N mikurensis infection were identified from 622 febrile patients in the same location in 2010. 18 Patients with infections caused by species from the Anaplasmataceae family showed undifferentiated clinical manifestations, mainly including fever, malaise, myalgia, arthralgia, and gastrointestinal symptoms (diarrhoea, nausea, vomiting, and anorexia). Laboratory abnormalities included leucopenia, thrombocytopenia, and raised hepatic aminopherase, lactate dehydrogenase, and blood urea nitrogen (appendix). 17, 18, 53, 55, 64, [73] [74] [75] Emerging infections with B burgdorferi sensu lato B burgdorferi sensu lato was first detected in human beings and I persulcatus in China (Heilongjiang Province) in 1986. 19 This isolate was later classified as B garinii by molecular biological methods in our laboratory. Five other genospecies were subsequently identified, including B valaisiana, B sinica, B afzelii, B valaisiana-related genospecies, and B burgdorferi sensu stricto (table 1) .

Over a wide geographical distribution of 25 provinces in mainland China, 26 tick species have been shown to carry B burgdorferi sensu lato (figure 1). B garinii is the most common genospecies and has been identified in many tick species (figure 1). 54,79-86 B garinii has been detected in rodents from many endemic areas, 23, 36, 79, 85, 87, 88 in dogs from Yunnan Province, in sheep keds (Melophagus ovinus) from Tibet, and in hares from Gansu Province (appendix). 89 B afzelii is the second most common genospecies, and has been identified in the same ticks and rodents as B garinii with a similar distribution. 36,79,81,83-85 B valaisianarelated genospecies (a genetically related but distinct genospecies of B valaisiana) was detected in Ixodes granulatus and H longicornis from Guizhou Province, and in rodents from Guizhou and Zhejiang provinces. 22, 83 Three other genospecies that have not been reported to infect human beings include B valaisiana, B sinica, and B burgdorferi sensu stricto. Data regarding infected ticks and animal hosts are shown in figure 1 and the appendix. 20, 23, 86, 92, [148] [149] [150] [151] [152] [153] [154] [155] [156] 

After the identification of B burgdorferi sensu lato in ticks and animals, human cases of B burgdorferi sensu lato infection have been reported frequently in almost all provinces in mainland China, except for Tibet and Shanghai metropolis ( figure 2) . B garinii, B afzelii, and B valaisiana-related genospecies have been reported to cause human infections (table   2) . 19, 21, 23, 78, 90, 91, 125 The genospecies causing thousands of cases of Lyme disease in mainland China have not been characterised. [126] [127] [128] [129] [130] [131] All the uncharacterised human cases in figure 2 were classified as B burgdorferi sensu lato infections irrespective of their actual genospecies. Infections with different genospecies of B burgdorferi sensu lato might result in slightly different clinical manifestations, including erythema migrans, arthritis or arthralgia, fever, headache, and fatigue. 19 

The transmission of babesia is associated with 13 tick species (figure 1). Ba microti, Ba divergens, and Ba venatorum have been proven to infect human beings. Ba microti was identified in I persulcatus and H concinna ticks and in striped field mice and reed voles (Microtus fortis) from forested areas of Heilongjiang Province, in H longicornis ticks and dogs from Henan Province, and in rodents from Fujian, Zhejiang, Henan, and Heilongjiang provinces. 32, 36, 58, 105, 106 Ba divergens was detected in I persulcatus, H concinna, and Haemaphysalis japonica ticks and striped field mice in several areas of Heilongjiang Province. 32, 36 Ba venatorum was reported in I persulcatus ticks from forested areas of northeastern China. 109 Other species of Babesia that have not been shown to infect human beings include Ba ovis, Ba major, Ba ovata, Ba orientalis, Ba motasi, Ba caballi, Babesia sp Kashi, and Babesia sp Xinjiang. Data regarding their infected ticks and animals are shown in figure 1 and the appendix. [24] [25] [26] [27] [28] [29] [30] [31] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] 

Ba microti, Ba divergens, and Ba venatorum have been reported to cause human infections in mainland China (figure 2). A patient was diagnosed with Ba microti infection by peripheral blood and bone marrow smears and PCR assay in Zhejiang Province in 2011. 104 By use of PCR, Ba microti infections were identified in an additional ten patients among 449 febrile patients with malaria-like symptoms in Yunnan Province during 2012-13. 107 One patient was co-infected with Plasmodium vivax and another was co-infected with Plasmodium falciparum. Two cases of Ba divergens infection were detected from 377 patients with anaemia in Shandong Province in 2009. 108 A case of babesiosis caused by Ba venatorum was reported in a child from Xinjiang Autonomous Region. 33 Between 2011 and 2014, 48 cases of Ba venatorum infection were reported through our active surveillance at a sentinel hospital in forested areas of northeastern China. Among them, 32 were confirmed cases, and 16 were probable cases. 109 These infections were the first report of endemic human Ba venatorum disease anywhere in the world. Additionally, two cases of babesiosis caused by uncharacterised Babesia species were reported in Yunnan Province of southwestern China in 1982 and one in 2008 (figure 2). 132, 133 Clinical manifestations for patients with Babesia species infections included fever, fatigue, anaemia, chills, and high levels of hepatic aminopherase and C-reactive protein. 33, 108, 104, 109, 107, 132, 133 Detailed information about the clinical manifestations of each of these species of Babesia infections is summarised in the appendix.

SFTSV is a novel member of the genus Phlebovirus in the Bunyaviridae family, and was first identified in China. The identification of SFTSV infections was made possible by enhanced active surveillance in selected provinces of China. 34 Since a systematic review on the epidemiology, clinical signs, pathogenesis, diagnosis, treatment, and prevention of human infection with SFTSV has been published, 157 we provide only a brief description and some new information about the infection.

SFTSV have been reported to infect H longicornis and Rhipicephalus microplus ticks, with a higher prevalence of infection in H longicornis (4-9%) compared with in R microplus (0-6%). 34, 110 Various animals, including goats, cattle, dogs, pigs, rodents, chickens, geese, and hedgehogs, might be infected by this pathogen. 110, 111 Studies in Shandong, Jiangsu, and Hubei provinces showed that goats and cattle had the highest seropositive rate. Furthermore, big animals usually had higher seropositive rates than small animals. 111, 112 

As of 2013, 2543 human SFTS infections have been reported to the China Center For Disease Control And Prevention, including 154 deaths. 34, [113] [114] [115] [116] [117] [118] Incidence hotspots are located in Henan, Hubei, Anhui, and Shandong provinces of middle-eastern China, in addition to Liaoning Province of northeastern China (appendix). Clinical manifestations in patients with SFTS are non-specific with major symptoms including respiratory tract symptoms, sudden onset of fever up to 38-41°C, headache, fatigue, myalgia, and gastrointestinal symptoms (loss of appetite, nausea, vomiting, and diarrhoea). Multiple organ failure develops rapidly in most patients (with raised concentrations of serum alanine aminotransferase, aspartate aminotransferase, creatine kinase, and lactate dehydrogenase; and proteinuria and haematuria), and is usually accompanied by thrombocytopenia, leucocytopenia, and lymphadenopathy. 34, 117 Factors contributing to the emergence of tick-borne infections Beyond doubt, advances in and application of molecular technologies have resulted in the discovery of novel agents and helped to identify human infections caused by agents previously detected in ticks. Thus, to some extent, the emergence of tick-borne infections has resulted from the discovery of novel pathogens through the use of more sensitive and reliable detection methods. However, various biological factors, such as tick and host population dynamics, which have caused an increase in the transmission and dissemination of tick-borne zoonotic diseases, are probably the main reasons for the emergence of these infections. 3 Changes in land use have affected the emergence of tick-borne zoonotic diseases by altering the interactions and abundance of ticks, wild and domestic hosts, and human exposure to pathogens. 158 An example is the emergence of Lyme disease in northeastern USA. Reforestation of this region during the 20th century is thought to have increased the population of white-tailed deer, which greatly amplified the number of Ixodes scapularis.

Consequentially, vector tick densities grew and expanded, contributing substantially to the emergence of Lyme disease in the USA. 159 Fragmentation of forests in eastern regions of Canada and the USA might have increased the relative abundance of small mammals because of a reduction in predator communities, leading to an increase in B burgdorferi sensu lato infection rates in I scapularis nymphs. Ultimately, people in these areas were confronted with a higher risk of Lyme disease. 160, 161 Since the mid-1990s, the Chinese central government has initiated the Greening Program to regain forests and grasslands from former agricultural lands. 162 Reforestation and grass replanting with high-quality vegetative cover could have increased the abundance and diversity of ticks and animal hosts, and favoured the re-establishment of pre-existing tick vector enzootic cycles in these areas. One example described in our study was in the most severely endemic region of SFTS. Our findings showed that the incidence of SFTS is significantly associated with vegetation-rich lands. A 10% increase in shrub, forest, and rainfed cropland areas resulted in increased human SFTS incidence rates of 51%, 51%, and 90%, respectively. 163 Additionally, urbanisation has affected the emergence and increasing incidence of tick-borne diseases. Studies in Europe suggest that encroachment into forested and uncultivated areas, and protection of existing green spaces in the process of urbanisation, have created opportunities for ticks to survive in urban and especially suburban environments. The presence of many pets and domestic animals, which can serve as tick hosts and pathogen reservoirs, might help tick transmission of various human and animal pathogens. 164, 165 In the past three decades, China has gone through the most rapid urbanisation in its history. This rapid urbanisation, followed by widespread rural-to-urban migration of the human population, intensive long-distance trade, and explosive short-term travel for shopping, has led to substantial health risks including air pollution, occupational and traffic hazards, and altered diets and activity. 166 All of these changes in human activity, together with increased contact between human beings and their pets and nature, have probably contributed to the increasing abundance of tick exposure, as reported in other developed countries. 164 Further investigation is needed to show the relation between emerging tick-borne diseases and urbanisation in mainland China.

The effect of worldwide climate change on the emergence of most vector-borne zoonotic diseases (including tick-borne diseases) is thought to be less important than changes in land use, animal host communities, human living conditions, and societal factors. 3 Although the effects of climate on transmission of infectious diseases are thought to be non-linear and act in opposing directions in different climate regions, the seasonal dynamics of tick vectors are largely defined by climate conditions, which might further affect the seasonal pattern of tickborne diseases. An increase in winter temperatures is known to cause the northward extension and increased abundance of Ixodes ricinus, which subsequently raises the risk of tick-borne disease dissemination. [167] [168] [169] Although this topic is still under debate, climatic change could have a role in the emergence of tick-borne diseases in China. Further studies are needed to better elucidate this issue.

Ticks are thought to be second only to mosquitoes as worldwide vectors of human infectious diseases. 170 Up to now, more than 120 tick species have been identified in China, including over 100 species in the Ixodidae (so-called hard tick) family and 19 species in the Argasidae (soft tick) family. 171 They transmit several pathogens when feeding on a range of animals, including human beings as accidental hosts. About 30 tick species are reported to feed on human beings. 172, 173 The diverse geographical distribution and abundance of ticks are dependent on optimum environmental conditions and biotopes for each tick species, which define the risk areas for corresponding tick-borne zoonoses. 3 In addition to the emerging tick-borne infections, previously documented and well established tick-borne diseases are a continuing threat to human health, including tick-borne encephalitis in northeastern China, Crimean-Congo haemorrhagic fever in northwestern China, tularaemia and north-Asia tickborne spotted fever in northern China, and Q fever, which is widely distributed throughout China. [174] [175] [176] [177] [178] [179] [180] [181] [182] [183] The non-specific clinical manifestations caused by tick-borne pathogens, such as SFGR, Anaplasma species, B burgdorferi sensu lato, Babesia species, and SFTSV, present a major diagnostic challenge. Most physicians are unfamiliar with the many tick-borne diseases that present with non-specific symptoms in the early stages of the illness. The wide distributions of the 33 emerging tick-associated agents and their tick vectors, in addition to the diversity of tick species throughout mainland China, imply that reported cases of infection might be only the tip of the iceberg in regard to the actual number of tick-borne diseases. To develop a reasonable differential diagnosis and identify a specific pathogen, laboratory diagnostic methods that are rapid, convenient, and practical are urgently needed for these emerging tick-borne diseases. Although great progress has been made in mainland China in the detection and identification of various tick-borne pathogens and diagnosis of the infections they cause, the necessary technologies are still unavailable in most general hospitals. Unfortunately, the importance of tick-borne diseases to human and animal health has not been sufficiently recognised as a result of inaccessibility to laboratory tests for aetiological diagnosis and inadequate surveillance activities.

In China, 18 tick-borne agents have been detected in ticks or animals, including four Rickettsia species, two Ehrlichia species, an Anaplasma species, three genospecies of B burgdorferi sensu lato, and eight Babesia species; however, the full effect of their pathogenicity to human beings is still to be established. Despite increasing knowledge about their geographical distribution, the natural cycle of these agents and the natural history of their infection in tick vectors and animal hosts have yet to be elucidated. Identifying human infections after potentially pathogenic agents are identified in ticks could take many years, in part because the microbial loads are much lower in human blood than in arthropods. 184 For example, R sibirica sp BJ-90 was first identified in D sinicus ticks in China in 1990, 7 but was not discovered to infect human beings until 22 years later. 46 Similarly, A phagocytophilum was initially detected in China in 1997; 15 however, nosocomial transmission of human granulocytic anaplasmosis was not recognised until 10 years later. 73 As such, the 18 tick-associated agents that have been recognised in China in either ticks or animals represent potential candidates for new tick-borne human diseases. Likewise, a search for potential new pathogens in ticks is essential for the discovery of emerging tickborne diseases in human beings.

33 tick species belonging to six genuses have been reported to be naturally infected with the emerging pathogenic agents described in this Review (figure 1). Except for eight species (Dermacentor abaensis, Dermacentor daghestanicus, Haemaphysalis bispinosa, H flava, Haemaphysalis sinensis, Haemaphysalis yeni, Ixodes myospalacis, and Rhipicephalus turanicus), most species carry two or more agents. Conversely, some emerging agents such as R heilongjiangensis, E chaffeensis, A phagocytophilum, and B garinii can infect several tick species. Additionally, tick-borne pathogens that infect domestic animals might eventually cause human disease. A representative example is B divergens, which has long been recognised to cause cattle babesiosis in Europe, and was subsequently identified as a human pathogen. 185 In China, a wide range of emerging tick-borne agents are known to infect various domestic animals (appendix), and their potential capability to infect humans warrants great attention and further investigation. With the continued emergence of tickborne diseases in mainland China, improving our understanding of the existence and health burden of these diseases is essential for China and for the rest of the world.

Refer to Web version on PubMed Central for supplementary material. Human cases of SFGR infections are shown in blue; patients infected with agents in the family Anaplasmataceae in green; patients infected with agents in the complex Borrelia burgdorferi sensu lato in purple; and patients infected with agents in the genus Babesia in black. SFGR=spotted fever group rickettsiae. *Not yet formally described. 

",0.6393631913841572
Orphan drugs: Unmet societal need for non-profitable privately supplied new products,"Due to the severity of rare diseases, the societal need for biopharmaceutical treatments for these diseases is high, despite low numbers of patients. Therefore, we investigated the barriers currently hindering the willingness to develop orphan drugs in the Netherlands. To this end, a robust, small sample, exploratory analysis of Dutch multi-actor development of orphan drugs was performed. Various factors that were expected to stimulate the adoption of orphan drug development were found to be important barriers. Concerted actions of producers, users, and especially regulators are necessary to overcome these barriers, but the prerequisite of a shared problem definition is lacking.","In various instances, market conditions fail to produce incentives for the development and production of products (goods and/or services) that are demanded in Western societies. If such market imperfections occur with respect to generally desired products, such as security, healthcare, housing and environment, then governments often develop policies and measures to sustain the supply of these products. These governmental policies and measures are developed using juridical rules, taxation, subsidies, and public financing, and aim to increase the accessibility of these products for the general public. Decision-making concerning such gov-ernmental policies and measures can be analyzed using public-choice theory (Arrow, 1963; Olson, 1966) . The dilemma to be solved by public-choice theory is between the efficacy of reaching those societal groups that need but cannot afford these products and the efficiency of putting a (financial) burden on those groups that do not need or want these products in order to produce them. This dilemma becomes more complicated if there is only a (very) small societal need for such products but a large societal moral pressure to provide them, for example, clean air in isolated, heavily polluted industrial areas, and healthcare for rare diseases. This is particularly true when these products must be provided by private enterprises at large costs within networks in which many other interested groups are involved, such as research institutes, governmental agencies, and pressure groups. In these cases, firm behavior is not only dependent on external incentives, such as governmental policies and measures, but is also dependent on the characteristics of the network in which these firms operate. In addition, if new products are needed, then the functioning of the network -the product-related innovation system -in which the firms participate becomes particularly important. Also, not only juridical and financial measures determine the functioning of that innovation system but also knowledge production and diffusion, learning, coordination of activities, and demand articulation by organizing consumer demand. Consequently, the successful operation of such an innovation system depends on various characteristics of the actors involved and their linkages. However, all these aspects may also contribute to the barriers that prevent an innovation system from becoming successful in providing publicly desired and privately produced new products.

To identify and analyze such innovation barriers, an exploratory empirical study of orphan drug development in the Netherlands has been conducted. More specifically, this article aims to provide insight into the innovation barriers that influence the dynamics of orphan drug development. Accordingly, the central research question is: Which barriers in the innovation trajectories of orphan drugs are currently present and hinder the development of orphan drugs in the Netherlands?

This study is limited to orphan drug development in the Netherlands. The reasons for this empirical limitation are two-fold.

First, during the late 1990s, biotechnology was identified by the Dutch government as an important enabling technology for the development of a national knowledgebased economy, which is regarded as a prerequisite for sustaining societal wealth and welfare in the Netherlands in the future. In 1998, an analysis by the Ministry of Economic Affairs showed that the Dutch biopharmaceutical sector was lagging behind neighboring countries, the main restrictions being that Dutch knowledge institutes lacked a business culture, the results of scientific research were seldom commercialized, there was not enough venture capital available for new life science companies, and there was a shortage of facilities, such as office and laboratory space (Ministry of Economic Affairs, 1999) . Therefore, in 2000, the BioPartner Network program was started to stimulate the start-up of biotech firms in the Netherlands. Today, the Dutch life sciences sector is still in an early stage of development and consists of about 160 primarily small, privately held, often lossmaking, entrepreneurial companies (BioPartner, 2005) . These start-up companies often begin as 'offspring' of Dutch academic research groups trying to commercialize new research findings in the biotechnological field. Dutch policy makers also realize that the failure rate of start-up firms is high, especially in a new technological field like biotechnology. But, it is expected that the surviving start-up firms will be able to grow and together contribute to the development of a new high-technology, knowledge-based industry within the Netherlands. Biopharmaceutical companies receive special attention in this policy because of their large knowledge content.

Second, only a small number of orphan drugs are developed in the Netherlands, some of them being granted EU orphan designation by the European Medicines Evaluation Agency (EMEA), and none of them have yet left the experimental or clinical phases of drug development and received market approval. 1 But, market approval of orphan drugs is a necessary condition for biopharmaceutical firms to expand their operations. Therefore, insight into the factors and conditions that hinder the development of orphan drugs in the Netherlands is important, especially from the industry policy perspective described above.

As only a small number of orphan drugs have been developed within the Netherlands, and have not yet received market approval, research into their development would only reveal partial information on current barriers to innovation. In particular, information about postponed and suspended decision-making and decision-making with negative outcomes that prevent the development of other orphan drugs or stop further development of orphan drugs will not be gathered. Furthermore, information about barriers to commercialization and use of orphan drugs is missing. Therefore, another approach is adopted in this study by identifying the small number of important, knowledgeable actors involved in the Dutch field of orphan drugs and interviewing them on relevant issues concerning the whole trajectory of orphan drug development.

This article is organized as follows. Section 2 discusses the process of (orphan) drug development and the actors and institutional structures involved in this process. Some of them provide incentives that affect the willingness to develop orphan drugs. Section 3 presents a conceptual model of these incentives and other factors affecting the adoption of orphan drug development. To investigate causal effects in the conceptual model, Section 4 describes the applied research methodology. Interviews with knowledgeable actors in the field of orphan drug development provided a rather small dataset for analysis. To test the presumed causal effects in the conceptual model, this dataset has been analyzed by means of robust small sample statistical methods. Section 5 presents the results of these analyses together with their managerial and policy implications. These results are discussed in Section 6. Section 7 presents the conclusions drawn from these results, as an answer to the research question stated above.

It is estimated that there are between 5000 and 8000 identified rare diseases, which together affect about 55 million people in Europe and the USA alone (Binns and Driscoll, 2000; Rinaldi, 2005) . However, each of these diseases affects relatively few people. A rare disease is, according to European definition, a life-threatening or chronically debilitating condition from which not more than one affected person per 2000 citizens in the European Union suffer. As only small numbers of people are affected, there are only small markets for drugs, and the high costs associated with drug development generally makes it unprofitable for pharmaceutical companies to develop drugs for these diseases. The medicinal products intended for the diagnosis, prevention or treatment of rare disorders are commonly known as 'orphan drugs'. Several rare diseases are well described, such as cystic fibrosis, hemophilia, amyotrophic lateral sclerosis (ALS), Huntington's chorea, phenylketonuria (PKU), and severe acute respiratory syndrome (SARS). Box 1

gives an example of a well-known orphan disease, hereditary angioedema (HAE), and the developed orphan drug recombinant human C1 inhibitor. 2 Examples of general not well-described disorders include primary ciliary dyskinesia, Darier disease, Usher syndrome, and alkaptonuria. Examples of groups of rare diseases are neuromuscular diseases, inborn errors of metabolism (such as lysosomal storage disorders, such as Gaucher, Pompe and Fabry), several chromosomal disorders and rare forms of cancer (van Weely and Leufkens, 2004; Orphanet, 2006) .

The rapid growth of molecular biological knowledge in the past few years and the recent insights from the Human Genome Project have led to new delineations Box 1. Pharming and the development of C1 inhibitor orphan drug for hereditary angioedema treatment Background and prevalence: Hereditary angioedema (HAE) is a genetic disorder that causes shortage of a protein called C1 inhibitor. Patients suffer from recurrent attacks of oedema, causing swellings in the soft tissues of the body. This swelling can be extremely painful. When misdiagnosed it can lead to unnecessary surgical interference and costly hospital care. The disease can even be lethal if attacks in the throat area lead to asphyxiation. The prevalence of HAE is from 1 in 10,000 to 1 in 150,000, with an average of 1 in 30,000, which means that in the Western world some 22,000 patients suffer from it. Treatment: Androgens can help to prevent attacks of HAE but may cause serious side effects, such as excessive hair growth in women, menstrual disorders, infertility, hepatitis, and, in rare cases, liver cancer. Attacks are also treated with anti-fibrinolytics or plasma C1 inhibitor. Anti-fibrinolytics are associated with side effects such as nausea, abdominal pain, diarrhea, and even thrombosis. In addition, the presently available C1 inhibitor protein is obtained from blood plasma. This implies a risk of transmission of pathogenic viruses and its availability is limited since human donor blood is scarce.

Pharming has developed a recombinant human C1 inhibitor (rhC1INH) that is obtained from the milk of transgenic rabbits. Pharming is a Dutch life sciences company developing innovative protein therapeutics for unmet medical needs. The company's products include potential treatments for genetic disorders, medical, and specialty products for surgical indications, and intermediates for various applications.

The transgenic rabbit DNA contains an extra bovine-milk-specific promoter sequence (alpha-S1 casein) functionally linked to the gene encoding human C1 inhibitor. This makes them produce rhC1INH in their milk. This is a relative easy, quick and clean production method of human C1 inhibitor in large quantities. Project status: Pharming is nearing the end of the development program for rhC1INH. Almost all safety tests in laboratory animals have been completed, and control tests have been developed. The product is now in phase III of clinical testing in humans. Submission for market approval is expected in 2005.

To enhance development speed, Pharming had a patient register developed, which contains unique patient data, enabling faster recruitment of patients for clinical trials and facilitating various opportunities for further scientific research. Once marketing approval has been obtained, the register ensures faster market penetration of the product. Orphan drug designation: Pharming obtained an orphan drug designation for rhC1INH in both prophylaxis and acute treatment of hereditary and acquired angioedema from the US Food and Drug Administration (FDA). In addition, the EMEA granted an Orphan Medicinal Product status for prophylactic and acute treatment. These designations provide several advantages like certain market exclusivity, various financial incentives, and a well-defined regulatory approval path.

Sources: Pharming (2005) and Lareb (2005) . and classifications of disorders and diseases. At present, about 7000 genetic disorders are known to be the result of a mistake in the coding of one gene. Many of these genetic disorders are categorized as rare diseases.

About 80% of the identified rare diseases have a genetic origin (Rinaldi, 2005) . Another category of rare diseases include rare variants of non-rare disorders, such as heart and vascular diseases, lung diseases, rheumatism, cancer, and psychiatric disorders. Owing to increased understanding of these disorders and improved DNA techniques and DNA-scanning methods, the pos-sibilities for diagnosis and therapy may be expected to improve considerably in the future. 3 If it becomes possible to diagnose many hundreds of genetic disorders, then further development of biopharmaceutical orphan drugs becomes feasible (Meijer et al., 2001; Zitter, 2005) .

Despite all the progress made over the past decades, for many of the rare diseases there is still no effective and safe treatment available. Various barriers related to the development of orphan drugs can be identified, such as the small number of patients, the debate about choosing appropriate trial methodology and outcome parameters, and a lack of knowledge about the natural course of the disease, all of which hamper positioning of orphan drugs in clinical practices (Haffner et al., 2002; van Weely and Leufkens, 2004) .

Affordability of orphan drugs has become a major issue and creates tensions between the various stakeholders involved (van Weely and Leufkens, 2004; Stolk et al., 2005) . Owing to the severe symptoms of rare diseases, it is hard to accept that, in general, only few drugs for rare diseases are going to be further developed or brought onto the market. The development of many orphan drugs often entered the preclinical stage, after which there was no clear further development towards registration and market introduction. The availability of orphan drugs is limited by their weak economic incentive and lack of commercial value. Nevertheless, rare conditions such as hereditary angioedema (HAE), amyotrophic lateral sclerosis (ALS), Gaucher's disease, and Huntington's chorea affect thousands of people worldwide.

Several regulatory instruments have been developed to encourage the development of orphan drugs. The first of these instruments was the Orphan Drug Act (ODA) that was passed in the USA in 1983. The ODA offers significant incentives for (bio)pharmaceutical firms to develop drugs for rare diseases. These benefits include quick review by the US Food and Drug Administration (FDA), a short approval time, tax credits, 7 years of market exclusivity for orphan indications of approved products and exemptions from drug registration fees for a company willing to develop an orphan drug. Furthermore, the FDA Office of Orphan Products Development (OOPD) has awarded research grants to both private and public parties to support clinical trials of orphan drugs. To qualify for these incentives, potential medicines first receive 'orphan designation' from the OOPD, and then go through the normal evaluation process for safety and efficacy (Haffner et al., 2002; Maeder, 2003) . Since the ODA came into effect, its impact on the development of orphan drugs has been tremendous. From 1983 to April 2005, a total of 269 orphan drugs received market approval in the USA and 1449 orphan designations were accepted (FDA, 2005) . Together, these 269 new drugs now provide help for more than 11 million people in the USA alone (Rinaldi, 2005 ). An EU legislative framework did not follow until December 1999, when the European Orphan Drug Regulation (EODR) was approved by the European Parliament (EU Regulation 141/2000; Binns and Driscoll, 2000; Sinnema, 2001) . The EODR incentives include 10 years' market exclusivity to orphan drugs, exemption from registration fees, technical assistance for development of the application file, and accelerated marketing procedures. Other provisions, such as tax breaks and grants towards clinical research costs were largely delegated to the member states (Rinaldi, 2005) . So far, approximately 270 drugs have been granted EU orphan designation, and about 23 of these have reached the market in 2005 (Pharmacos, 2005) .

Since orphan drug regulations have been in place in various countries (including USA, EU, Japan, Australia, Singapore, and Canada), patients, clinicians, policy makers, and industry have recognized their huge impact on the development of new drugs (Bosanquet et al., 2003; Stolk et al., 2005) . There are, however, two main concerns regarding orphan drugs. First, many orphan drugs are (very) expensive. Second, certain drugs that were originally approved in the USA as orphan drugs later became top sellers, either because the once rare condi-tion they were intended to treat increased in frequency, such as AZT to block HIV replication, or because the drugs, such as epoetin-alfa (EPO), proved effective against more common disorders (Lawton, 1992; Maeder, 2003) .

Drug development takes, on average, 10-12 years and involves various stages (Fig. 1 ). The first stage in the development of medicinal drugs consists of research into the mechanism and the pathogenesis of the particular disease. This information is needed as a basis for the development of therapeutic substances. Therefore, the lack of knowledge about the pathogenesis of orphan diseases is a big problem in the development of orphan drugs. Basic molecular understanding of the disease and identification of possible pharmacological targets (valid biomarkers) is often lacking for many of the rare mono-and polygenic disorders (Maeder, 2003; Stolk et al., 2005) .

The next stage in the development of a drug begins with the design of the substance. When a promising (guiding) substance is found (by screening, molecular modification, rational design, or serendipity), it is tested for its quality, safety, and efficacy. The first series of the tests involves animal experiments. Animal-experimental models are used to replicate a disease or a partial aspect of a disease in humans (Drews, 1999) . Animal models for rare diseases, however, are often absent or rarely available because only little research has been carried out on the pathogenesis of rare diseases and valid biomarkers are often lacking.

Clinical experiments form the next series of tests, which can be categorized into three phases of testing. In clinical phase I the substance is administered to a small number of healthy volunteers. The purpose of these experiments is to collect data on the safety of the drug in humans. In experiments conducted during phase II of the clinical research only patients are involved.

Furthermore, the experiments involve larger groups of subjects. The main goal of this research is to establish the short-term single-dose efficacy. In orphan drug development, phase I and II studies are often combined, and the registration authorities have accepted these exceptional circumstances.

Phase III studies involve large groups of patients in randomized, controlled, double-blind trials to establish both the short-and long-term efficacy. But in the case of rare diseases, it is difficult to recruit sufficient patients for these clinical trials. In some cases, all patients in one or even several countries have to be included to carry out a statistically reliable experiment. Only when phases I-III have successfully been concluded, the pharmaceutical substance becomes eligible for market registration. The pharmaceutical firm has to submit a registration dossier containing adequate data on the safety, efficacy, and quality of the drug. When the drug is approved, it can be introduced to the market. After registration and market introduction of a drug, the pharmaceutical firm is responsible for the monitoring and evaluation of its prolonged effects on patients (so-called post-marketing surveillance) (Buurma et al., 1999) .

Various actors have important roles in the development of (orphan) drugs, namely universities and research institutes, academic hospitals, pharmaceutical companies, patients' organizations, and governmental institutions (the 'government' in Fig. 2 ). Universities and research institutes have important roles in the first stage of drug development because they perform the research into the mechanisms underlying a rare disease. Various university groups, including (bio)chemical, pharmaceutical, and molecular biology departments, are involved in the drug discovery processes. Research collaboration with academic hospitals, which can provide clinical data, is frequent. Relative to other countries, the Netherlands has shown ample scientific interest in rare diseases, the underlying molecular mechanisms, and possible therapeutic approaches. In particular, the Dutch research community has gained international recognition in the field of lysosomal storage disorders (Gaucher, Pompe, Fabry), other inheritable diseases (such as hemophilia), rare disorders in children (such as neurological diseases), and neonatal diseases (Leufkens and . University departments often cooperate with pharmaceutical companies by providing data on the pathogenesis of the disease, while leaving the actual development process of the drug to the pharmaceutical industry. Sometimes, universities have a role in subsequent stages of the drug development process, but this is mostly as a subcontractor conducting, for example, animal experiments for pharmaceutical firms. Furthermore, university medical centers often cooperate with academic hospitals in clinical trials. Thus, the first stages of orphan drugs development often take place within universities and academic hospitals.

Academic hospitals also have important roles in the drug development process via their study of the pathogenesis of the rare disorder and providing the clinical data. Furthermore, academic hospitals have important roles in the clinical trials. These are often carried out in cooperation with pharmaceutical companies due to the high costs associated with testing large groups of patients and expensive registration procedures. In the Netherlands, more than 10 different research laboratories, university departments, and academic hospitals are active in orphan drug development (Orphanet, 2006). 4 Pharmaceutical companies have the most important roles in the development of medicinal drugs, often participating in every stage of the development process. Some companies study the pathogenesis of diseases themselves, but most firms obtain this knowledge from research institutes, university research, and academic hospitals, as this type of fundamental research is not very profitable for pharmaceutical firms. Most firms focus only on the development of specific pharmaceutical substances. In the next phase of drug development, some firms perform their own animal experiments, although it is not uncommon for firms to subcontract these experiments to universities and research institutes. Generally, phase I and II clinical trials are contracted out. For orphan drug development, phase I and II clinical trials often take place in academic hospitals because the hospitals are more familiar with the patients, often via specialized rare diseases centers. Many pharmaceutical firms also increasingly cooperate with patients' organizations (Meijer et al., 2001) . The pharmaceutical firms then use the know-how of these clinical centers and patient organizations for further development of the orphan drugs. So far, only two Dutch orphan drug projects have been granted an orphan drug designation, one project of Pharming Technologies (see Box 1) and one project of AMT, a ""spin-off"" company of the Academic Medical Center Amsterdam. 5 Patients' organizations are particularly important in increasing the awareness of a certain disease, especially in the case of orphan diseases (Lang and Wood, 1999 ). An inventory done by patients' organizations within the Netherlands in 1994 led to the preparation of an extensive list of orphan diseases. Furthermore, these organizations indicated the importance of further development of diagnostics, the collection of reliable data on the prevalence of orphan diseases, the further development of treatment methods based on genetic knowledge, and the mobilization of sufficiently large groups of patient in studies to establish the efficacy of orphan drugs. In addition, these organizations collect information on regulation as well as sources of financial support. They also have important roles in initiating and stimulating the cooperation between scientific researchers and the industry (RGO, 1998) . Furthermore, patients' organizations promote and stimulate the development of orphan drugs, and they are continuously trying to persuade the government and pharmaceutical companies to invest in orphan drug development. The Vereniging Samenwerkende Ouder-en Patientorganisaties (VSOP) is the Dutch Genetic Alliance, and is an umbrella organization of about 60 national, disease-linked, parent and patient organizations, most of which are concerned with genetic and/or congenital disorders. Rare disorders is a central field of interest (VSOP, 2005) . Examples of Dutch orphan-disease-related patient organizations are the Dutch Cystic Fibrosis Foundation (Nederlandse Cystische Fibrosis Stichting), the Dutch Fabry Support and Information Group (Fabry Support & Informatie Groep Nederland), the Dutch Gaucher Association (Gaucher Vereniging Nederland), and the Dutch Muscular Diseases Association (Vereniging Spierziekten Nederland). The European Platform for Patients' Organizations, Science and Industry (EPPOSI, 2005) is an EU patient-led partnership between patients, industry, and academic science institutes focusing on the treatment and prevention of serious diseases.

The government has a role in the process by which the developed medicinal drug gains authorization via the Dutch Medicine Evaluation Board (College ter Beoordeling van Geneesmiddelen [CBG] ) and the EMEA. Furthermore, the government is responsible for policies regarding medical knowledge, industry, and public health (RGO, 1998) . The government can influence the development of orphan drugs in several ways by applying the European Orphan Drugs Regulation (EODR), which allows the provision of market exclusivity, scientific advice, and regulatory guidance (Binns and Driscoll, 2000; Maeder, 2003) .

In the Netherlands, several initiatives have been undertaken to encourage scientists in public knowledge institutes (such as universities and academic hospitals) to start up their own biotechnology company, for example, the above-mentioned BioPartner programme initiated by the Ministry of Economic Affairs (BioPartner, 2005) . However, at the end of 2004, only three small companies with a business plan for treatment of a specific rare disease had started in this way.

The European Parliament and the Council of the European Union have also shown a clear interest in orphan diseases during the past few years. The adop-tion of the Programme of Community Action on Rare Diseases (1999 Diseases ( -2003 (COMP) was created as part of the EMEA, supporting the registration procedure of orphan drugs. In 1997, the former Minister of Health, Welfare and Sports (VWS) asked the Dutch Advisory Council on Health Research (RGO) to provide advice regarding the coordination, priority, and stimulation of research in the Netherlands with respect to orphan drugs. In 1998, the RGO advised the building of a national structure on orphan drugs. Therefore, the Steering Committee on Orphan Drugs (Stuurgroep Weesgeneesmiddelen) was appointed by the Minister of VWS in April 2001 for a period of at least 4 years. This committee is an independent organization and consists of 10 members and one observer. The members are representatives of patient organizations, pharmaceutical companies, physicians, hospital pharmacists, scientists, a representative of the Dutch Medicine Evaluation Board and a representative of the Dutch Health Insurance Board (WGM, 2005) . In January 2003, the Dutch Health Care Insurance Board (CVZ) compiled requirements regarding the reimbursement for orphan drugs (CVZ, 2003) . In the Netherlands, the Minister of Health, Welfare and Sports, advised by the Health Care Insurance Board (CVZ), decides whether a new product will be included in the National Health Insurance System. New drugs aiming for intramural treatment (treatment within the hospital), such as most orphan drugs, should be paid from the hospitals budgets. Orphan drugs often do not fit within these budgets, being expensive, often biotechnologically made products. Therefore, two Dutch policy measures have been introduced for the intramural use of drugs (one for expensive drugs and one for orphan drugs). In the case of expensive drugs in hospitals, 80% of the drug costs will be reimbursed. In the case of orphan drugs in academic hospitals, 95% of the drug costs will be reimbursed. When the Minister of VWS comes to decide on these policy measures, the Dutch 'College Tarieven Gezondheidszorg/Zorgauthoriteit i.o.' (CTG/Zaio) is the executive organization responsible for the policy measures for expensive drugs and orphan drugs. 6 Drugs for treatment outside the hospital (extramural) can be reimbursed by means of the Dutch Drugs Reimbursement System, called the 'Geneesmiddelenvergoedingssysteem' (GVS). Since 1 January 2006, a subsidy of 11 million euros has been allocated for the costs of European registered extramural orphan drugs for use in the Netherlands (CTG/Zaio, 2006) . 7 Despite the measures described above, the current situation in the Netherlands is that, although orphan drug development in the preclinical stage has increased, only the Dutch firm Pharming has a drug with orphan designation in the last phase of clinical trials (see Box 1). Phase III of clinical trials is still a barrier to market introduction for many orphan drugs. This is due to the small number of patients involved and the fact that most orphan drugs are developed by start-up firms. The small number of patients involved requires the cooperation of patients in many countries in a phase III clinical trial. But start-up firms are not very well connected to the international market and often experience severe difficulties organizing phase III clinical trials. Consequently, the development of orphan drugs can get stuck at the stage of phase III clinical trials. One solution to this problem is for the start-up firms to cooperate with large (foreign) pharmaceutical firms, which already have access to the international market. But such cooperation is not free and often results in mergers and take-overs of start-up firms by large pharmaceutical firms. Loss of independence and identity are then the unattractive consequences for the start-up firm.

To avoid these problems with phase III clinical trials in orphan drug development, which also threaten the development of a high-technology, knowledge-based biopharmaceutical industry within the Netherlands, the Dutch government should take a more active role in organizing access to international patient groups with rare diseases via the European Union or even the World Health Organization.

To acquire a more comprehensive insight into the barriers to orphan drug development, the next section presents some theoretical notions leading to a conceptual model of the willingness to develop orphan drugs. Later, this model will be investigated empirically.

In general, drug development represents a sciencebased innovation trajectory, carried out by a network of 7 The annual cost of treatment of orphan diseases (D 6000 to >D 300,000) is beyond the possibilities of most households (Alcimed, 2004) . interrelated actors, such as research institutes, producers, government, and patient organizations, and based on demand-driven conditions, such as unmet medical needs (Tidd et al., 2001) . These conditions are reflected in expectations about potential customers (i.e. patients) and drug innovation, adoption, and diffusion. If large-scale drug adoption by patients is expected to be high, then the willingness to develop such a drug will be large for reasons of competitive advantage and returns on investments. Consequently, we apply the conceptual model of innovation adoption and diffusion developed in various successive innovation studies (Tornatzky and Klein, 1982; Moore and Benbasat, 1991; Rogers, 1995; Tidd et al., 2001) . For the purpose of this study, actual innovation adoption and diffusion in this model has been replaced by expected 8 innovation adoption and diffusion among customers, which is reflected in the producer's willingness to develop orphan drugs. By analyzing this conceptual model for various actor groups involved in different stages of orphan drug development, a multiactor perspective is chosen. In orphan drug development, three important actor groups can be identified: producers, regulators, and consumers.

The actor group of producers comprises organizations that are directly involved in orphan drug development, namely pharmaceutical companies in collaboration with research institutes, academic hospitals, and universities, such as the Dutch firms Pharming and AMT. The actor group of regulators includes various public and private medical and health organizations, including the Dutch Ministry of Health, Welfare and Sports, the Medicine Evaluation Board, the Health Care Insurance Board, and the CTG/Zaio, which represent the institutional infrastructure of orphan drugs and some of whom are involved in the reimbursement of treatment of orphan diseases. The actor group of consumers represents patients, patient groups, and also physicians and (hospital) pharmacists who prescribe the orphan drugs.

The willingness to develop orphan drugs and its causal factors apply mainly to the actor group of producers. In addition, the influence of both other actor groups on the adoption of research and development activities into orphan drugs consists of affecting its causal factors. By focusing on the perceptions and behaviors of the three actor groups identified, the structuralistic and functionalistic approaches of (innovation) systems can be integrated into a system dynamics framework. The structuralistic approach considers system dynamics as a function of the structure of the system, which is made up of its institutional environment and (the perceptions of) their regulations and rules imposed on it. The functionalistic approach of systems conceives system dynamics as a function of interactions among its constituent subsystems and system elements. By linking the perceptions of regulations, rules, and other causes of (adoption) behavior to interactions among the three actor groups, the structuralistic and functionalistic perspectives on system dynamics become integrated. Effective system dynamics regarding the adoption of R&D activities necessary for orphan drug development can then be seen to depend on the degree of concerted actions among the three actor groups involved, which in turn depends on a shared view regarding how to develop orphan drugs. The shared view consists of the degree of agreement among the three actor groups about whether to develop orphan drugs and the extent to which the causal factors have a positive or negative effect. Concerted action then consists of adaptations of current behaviors of the actor groups to stimulate the willingness to develop orphan drugs of the producers. However, disagreement among the actor groups involved over the effects and relations of the causal factors will be conceived as innovation barriers in the development of orphan drugs. To identify such barriers to orphan drug development, these causal factors and their effects and relations are briefly described below.

The clinical success of a drug is measured in terms of the adoption rate. Adoption is defined as ""a decision to make full use of an innovation as the best course of action available"" (Rogers, 1995) . According to Rogers' diffusion theory, the adoption of innovations, in this case orphan drugs, by individuals or social entities takes place through the innovation-decision process. The different stages of this process are dominated by the goal to reduce uncertainty about the attributes of a new product through information gathering. Rogers (1995) mentions several key innovation characteristics relevant to the process of (expected) adoption (so called 'adoption factors') or, in the context of this study, the degree of willingness to develop orphan drugs: (1) relative advantage; (2) compatibility; (3) complexity; (4) trialability; and (5) observability.

Relative advantage is the degree to which an innovation is perceived as better than its predecessor. This attribute depends on the number of alternatives and the added value of the new drug compared with the standard therapy. The added value is measured in terms of economic benefits, including financial risk and non-financial benefits (for example, business factors and legislative measures), and social prestige (image).

In general, drug development is associated with several uncertainties. A new drug has to be evidence-based, therapeutically needed and financially profitable to be developed. Orphan drugs are therapeutically necessary. Pharmaceutical companies have to cope with considerable financial uncertainties about whether they will receive sufficient revenues after market introduction to cover the initial R&D costs of orphan drug development. A government can try to reduce these financial uncertainties by imposing tax credits, clinical research assistance, and market exclusivity. This uncertainty is thus highly correlated with financial risk. Pharmaceutical companies are no different from other firms in trying to avoid such risk as much as possible. Consequently, they are inclined to select (the most) profitable opportunities. In this study, the relative advantage is not relative to the predecessor, but relative to the development and adoption of conventional drugs. The most important non-financial benefits are provided by legislative measures for orphan drug development, which are expected to have a stimulating effect on the willingness to develop such drugs. Both the American Orphan Drug Act and the European Orphan Drug Regulation are examples of legislative measures aimed at increasing the financial benefits of orphan drug development (for example, market exclusivity, scientific advice, and regulatory guidance). 9 For some pharmaceutical companies, an orphan drug designation may provide a powerful incentive for image improvement, particularly when proper funding and a profound knowledge base are lacking. For dedicated life sciences companies, the small production scale for orphan drugs can be attractive. Consequently, legislative measures are conceived to be contained in the concept of non-financial benefits. In addition to relative advantage, we included the independent variable financial risk in our conceptual model and expected it to have a negative effect on the willingness of producers, and more specifically of pharmaceutical companies, to develop orphan drugs.

Social prestige and image are closely related. Image is defined as ""the degree to which use of an innovation is perceived to enhance one's image or status in one's social system"" (Moore and Benbasat, 1991) . The image of developing an orphan drug (that is, helping the 'orphans') would be expected to have a positive influence on the willingness to develop such a drug.

Compatibility is the degree to which an innovation is perceived as being consistent with the existing values, past experiences, and needs of potential adopters 9 Legislative measures are correlated with risk via financial aspects. (Rogers, 1995; Moore and Benbasat, 1991) . Compatibility is high when the research or development involved fit the company or research institute well. In principle, the development trajectory of orphan drugs is no different from the development of conventional drugs, as it involves preclinical, clinical, and registration phases. Therefore, compatibility is not considered to be a large barrier in the development of orphan drugs and is therefore excluded from our conceptual model.

Complexity is defined as ""the degree to which an innovation is perceived as being difficult to use"" (Moore and Benbasat, 1991) . As mentioned in Section 2, orphan drug development is relatively complex compared with the development of conventional drugs due to a lack of mechanistic knowledge about the disease, a lack of valid biomarkers, inappropriate diagnostics, and small patient groups for clinical trials. Rare diseases often have a genetic element, lack suitable animal models, and require more collaboration with other institutes than conventional drug development. Together, these factors indicate highly complex development processes and form a barrier to orphan drug development.

Trialability is defined as ""the degree to which an innovation may be experimented with before adoption"" (Moore and Benbasat, 1991) . Regarding drug adoption, the trialability of a drug depends on the overall clinical experience with the specific drug. The trialability of orphan drugs suffers from the difficulties that arise during the clinical trials stage, when sufficient patients with a particular orphan disease are needed for testing. Patients' organizations can then be very helpful in finding suitable patients. We expect a positive effect of trialability on the willingness to develop orphan drugs.

Observability is defined as ""the degree to which the results of an innovation are observable to others"" (Rogers, 1995; Moore and Benbasat, 1991) . It could be stated that the advancement of research into orphan drugs is less observable, due to the lack of information at each stage of development. This is due to the complexity of orphan drug development. These variables are therefore closely related, which makes it difficult to distinguish their effects on the willingness to develop orphan drugs. Therefore, only one of them is included in the conceptual model, namely, complexity leaving out observability as the associated dimension.

In summary, the following conceptual model has been derived, which indicates a set of factors influencing the willingness to develop orphan drugs. These factors can be conceived as stimuli but also as barriers to innovation (Fig. 3) .

The following propositions about the effects of the independent concepts on the willingness to develop orphan drugs can now be formulated:

1. If the financial risk decreases, then the willingness to develop orphan drugs will increase. 2. If the non-financial benefits are perceived to be lower, then the willingness to develop orphan drugs will decrease. 3. If image is an important factor for a research institute or pharmaceutical company, then the willingness to develop orphan drugs will increase. 4. If research and development of orphan drugs is more complex, then the willingness to develop orphan drugs will decrease. 5. If the trialability of developing orphan drugs increases, then the willingness to develop orphan drugs will increase.

These propositions apply especially to the actor group of producers. From their perspective, high financial risk, low non-financial benefits, poor image, high complexity, and low trialability represent important barriers to orphan drug development. From the multi-actor perspective of orphan drug development described above, the regulator and consumer actor groups will affect the independent concepts in Fig. 3 , and may try to lower these barriers in the following ways. Regulators can influence the prevailing values of financial risk, non-financial benefits, complexity, and trialability by taking (enhanced) financial, legislative, and regulatory measures, for example, based on the European Orphan Drug Regulation (EODR). Consumers can improve the prevailing values of complexity and trialability by participating in the study of rare diseases and the development process of orphan drugs (such as diagnosis, pathogenesis (animal) models, ways of treatment, and number of patients involved in clinical trials). But regulators and consumers will only be inclined to contribute to lowering the barriers to orphan drug development if they are aware of these barriers, and only when all the three groups have a shared view on the barriers and their mutual relations in the orphan drug development trajectory. Then, the willingness to conduct concerted actions may be expected to increase to facilitate orphan drug development. This is the key theme to be elaborated in order to lower the innovation system barriers to orphan drug development. For the assessment and comparison of the perceptions of these barriers among producers, regulators, and consumers, the conceptual model in Fig. 3 will be operationalized in the next section.

The concepts in Fig. 3 have been operationalized according to their empirical facets discussed in Section 3.

Financial risk applies to producers, which is related to their uncertainty about the economic profitability of orphan drug development (Rogers, 1995) . This uncertainty arises from the huge costs of orphan drug development and the relative small target population to which the drug can be marketed, thereby creating revenues. Therefore, financial risk is indicated by the costs of orphan drug development and their expected revenues.

Non-financial benefits are indicated by variables that provide incentives for orphan drug development relative to conventional drug development by reducing the costs and improving the expected revenues from orphan drug development. These incentives comprise the small scale of production and marketing needed, and the governmental measures that provide juridical benefits and financial support allowed by the EODR via the orphan drug designation, providing scientific advice in clinical trials, regulatory guidance at registration and granting market exclusivity. In this study, non-financial benefits (for example, business factors and legislative measures), are expected to reduce the financial risk (Rogers, 1995) .

Image is indicated by actual orphan drug development by pharmaceutical companies and cooperation in orphan drug development with academic hospitals, patients' organizations and the regulatory authorities. This operationalization refers to the efforts of pharmaceutical companies to satisfy societal need to tackle (particular) orphan diseases. By doing so, the social prestige of these companies increases, thereby increasing the likelihood of the public to favor other products in their portfolios (Rogers, 1995) .

Complexity is indicated by the degree of available knowledge about the pathogenesis of orphan diseases, available animal models, measures for financial support and registration procedures, together with the degree of shared knowledge concerning these issues among the producer, regulator and consumer actor groups. Little knowledge of each of these issues and a lack of shared knowledge increases the complexity of orphan drug development considerably (Rogers, 1995) .

Trialability refers to the possibility of testing an innovative product in reality (Rogers, 1995) . Here, it is measured by the ability to conduct clinical trials and the cooperation of patients' organizations in these trials.

Willingness to develop orphan drugs is indicated by the various stages in the orphan drug development policy process, namely nothing done, policy goals defined, policy measures planned, policy instruments developed, and policy instruments implemented.

The indicators are measured on the perceptions of their relative importance held by selected key actors in the field of orphan drug development within the Netherlands, and are expressed on a five-point Likert scale (1-no importance/strongly negative, 5-most important/strongly positive). The key actors have been selected according to the following criteria. Academic experts in the field of rare diseases and biopharmaceuticals at Dutch universities were asked to suggest knowledgeable fieldworkers. Individuals who were mentioned at least twice by different academic experts were invited for an interview. Therefore, the selection of respondents in this study is based on their reputation in the field of rare diseases and orphan drug development in the Netherlands.

The key actors within each main actor group were interviewed in September 2001 (Kleijwegt and Schuttelaar, 2001; van Mastbergen and van der Valk, 2001; Otten and Vermeulen, 2001) . These key actors included:

(1) For the producer group, a university professor who specializes in biopharmaceuticals and pharmacotherapy concerning orphan diseases, the head of a research team at an academic children's hospital who is responsible for managing research into the pathogenesis of orphan diseases, the chairman and secretary of the scientific council on orphan drug development, the director and managers of three (bio)pharmaceutical companies, and a member of the organization of pharmaceutical enterprises (N = 7). (2) For the regulator group, two representatives of the ministries responsible for public health and economic affairs, a member of the Medicines Evaluation Board, a member of the organization for health insurance companies, and a representative of the advisory council on public health (N = 5).

(3) For the consumer group, three representatives of patients' organizations on orphan diseases, a member of the committee for orphan medicinal products, the secretary of the scientific council on orphan drug development, two medical specialists working at an orphan disease department of an academic children's hospital, and a head pharmacist in a hospital (N = 8).

As these key actors are well informed about orphan drug development they were interviewed to improve the reliability of the data analyzed. Because orphan diseases are not common diseases and are also markedly different from each other with respect to their pathogenesis, the actual number of people in each actor group involved is limited. Consequently, statistical analyses based on random samples of respondents from the three actor groups are not feasible.

Statistical analysis of data obtained from only five to eight respondents within each actor group will result in inherently unreliable results due to the effects of outliers in small samples. Consequently, the challenge of the current multi-actor research is to develop a robust method of data analysis for small samples. These small samples are due to the relative young age of the technological field of biopharmaceuticals and the rareness of the area of orphan diseases, which together result in only a limited number of specialists and patients involved and only a few well-informed individuals.

The method of data analysis that was developed and applied is based on the combination of majority counts and average scores, thereby eliminating the effects of outliers in the data on the results.

Case 1: A Likert scale in which ""1"" represents ""strongly negative"" and ""5"" represents ""strongly positive"". If the majority and the average of the scores for each indicator are below or above the neutral scale value of 3, then the coincidence of the indicator with the concept it represents is interpreted as negative or positive, respectively. The neutral value of 3 is assigned 0 for this coincidence. In this case, the association between an indicator and the concept its represents is indicated as −, 0, or +. Case 2: A Likert scale in which ""1"" represents ""no influence"" and ""5"" represents a ""very strong influence"". If the majority and the average of the scores are between 0 and 1.5, 1.5 and 3.5, or 3.5 and 5, then the coincidence of the indicator with the concept it represents is interpreted as non-existent, weakly positive, or very positive, respectively. In this case, the association between an indicator and the concept its represents is indicated as 0, +, or ++.

In both cases, if the average score of an indicator does not coincide with the majority of all scores, then the coincidence of that indicator with the concept it represents is interpreted as non-existent because it is generated by outliers. Such non-existence of the coincidence of an indicator with the concept it represents is also assigned 0 for their relationship.

To investigate the nature of the relation among the independent concepts, positive, negative or no correlations among their indicators are assessed as follows. The sign of the correlation of each pair of indicators is derived from their combination of categories of coincidence with the concepts represented by them.

If one indicator coincides positively with its concept, which implies that the overall score of the respondents on that indicator is relatively high (see above), and another indicator coincides negatively with its concept, which implies that the overall score of the respondents is relatively low (see above), then the indicators are considered to be negatively associated and their concepts also. If two indicators coincide negatively with the concepts they represent, then the indicators are considered to be positively associated and accordingly, the concepts are also considered to be positively associated. If one indicator of each pair of indicators does not coincide with the concept it represents, then the association between both indicators and, accordingly, between the concepts represented by them, is considered to be zero. All possible associations between the coincidences of two indicators with the concepts Table 1 Perceived coincidences of empirical indicators with the concepts they represent

Indicator ( Willingness to contribute to orphan drug development (0/++) + + 0 they represent derived in this study are shown in Fig. 4 .

The methods of analysis have been applied to the data obtained on the observed indicators as described in the previous section. These methods produce positive, negative, and non-existent coincidences of observed indicators with the concepts represented by them for each of the investigated actor groups involved in orphan drug development ( Table 1) . Comparison of the reported coincidences of an indicator with the concept it represents between the actor groups involved provides insight into the degree of consensus about the prevalence of the coincidence of that indicator with the concept it represents. After making such comparisons for all observed indicators, it is possible to determine the shared view of all actor groups to the barriers in the development of orphan drugs.

In theory (Fig. 3) , the willingness to contribute to orphan drug development is expected to be influenced positively by non-financial benefits, image, and trialability of orphan drug development. The financial risks and complexity of orphan drug development are expected to negatively influence this willingness. Consequently, these five causal concepts become barriers to orphan drug development if non-financial benefits, image, and trialability have relatively low scores and financial risk and complexity have relatively high scores. Owing to the multi-faceted empirical appearances of these causal concepts depending on the context of technology or technological field investigated, these five concepts have been operationalized in this study according to the 20 empirical indicators reported in Table 1 . Consequently, 20 potential barriers are indicated in this study of orphan drug development in the Netherlands.

As can be seen in Table 1 , all actor groups share only four perceived barriers to orphan drug development: facilitating registration, granting market exclusivity, knowledge of pathogenesis, and clinical trials. However, they still differ in the importance assigned to most of these shared barriers. Regarding the sixteen other barriers, the three groups do not have a shared view on their existence. In addition, with respect to the knowledge-sharing barrier there is a marked difference in perceptions of its existence and effects on the complexity of orphan drug development between the producers and consumers on one hand and the regulators on the other hand. A more detailed account of these results is given below.

The lack of a shared view on 80% of the empirical barriers to orphan drug development represents a superseding barrier for this willingness to develop orphan drugs. Without a shared view on (the majority of) the barriers for increasing the willingness to contribute to orphan drug development, concerted actions to overcome these barriers by the three actor groups are unlikely to be jointly undertaken. This makes a further discussion of the effectiveness of such concerted actions to diminish the barriers obsolete.

The willingness to contribute to orphan drug development within the producer and regulator groups is just a little positive reflecting intentions rather than actions. Within the consumer group there is disagreement about the willingness to contribute to orphan drug development.

The regulators and consumers consider the financial risk of orphan drug development to be an enormous amount of the development costs. However, producers think quite different: they are acquainted with the relatively large costs of any drug development, but the small revenues that are expected from orphan drugs is more of a concern.

The non-financial benefits of orphan drug development are considered to be negatively affected by the relatively small scale of production and the extra efforts needed to service its small market niches by the producers. Consumers think that the small scale of production improves the non-financial benefits of orphan drug development because they will mostly be produced by small biopharmaceutical enterprises. But most of these enterprises lack the financial resources to boost production and to develop market niches to a maximum to generate profitable economies of scale.

Both producers and consumers stress the importance of getting help from governmental agencies in the form of achieving orphan drug status under the EODR and subsequently facilitating registration of and granting market exclusivity for orphan drugs.

The regulators agree with the significance of the latter two measures but consider their contribution to be less important. This may be attributed to the fact that the regulators do not fully recognize the urgency of achieving the orphan drug status, which is perceived by producers as a necessary condition for developing orphan drugs, as a crucial non-financial benefit stimulating the willingness to develop orphan drugs.

The improvement of the image of pharmaceutical companies due to actual orphan drug development and their cooperation with patients' organizations and medi-cal (research) institutions to gain societal support for burden sharing are perceived as important by the producers and consumers of orphan drugs but not by the regulators.

The complexity of orphan drug development is rather high according to the producers and regulators owing to a lack of knowledge about their pathogenesis and a lack of suitable animal models. Consumers think less negatively about this complexity because they can contribute actively to the knowledge-base concerning orphan diseases.

The regulators think that the available knowledge about financial support measures and registration procedures are widespread and shared, although the producers are less optimistic about this.

With respect to knowledge sharing regarding orphan drug development, producers and consumers both think that this happens on a moderate but increasing scale, indicating the complexity of orphan drug development. Producers and consumers already participating in the development trajectory of orphan drugs also perceive knowledge sharing to increase the problem of complexity. Probably they foresee that knowledge sharing will further complicate the organization of orphan drug development with a negative marginal improvement of the development trajectory. Regulators think quite differently about this: they perceive a lack of knowledge about orphan diseases and orphan drug development within pharmaceutical companies and think that knowledge sharing needs to be stimulated to reduce the complexity of orphan drug development. As if those companies are not already cooperating and sharing knowledge with academic hospitals and research institutes.

All actor groups involved in orphan drug development perceive the trialability of orphan drugs to depend on the possibilities of conducting clinical trials. Additionally, producers stress the importance of the participation of patients' organizations in these trials. But, consumers do not yet consider the importance of such a role for patients' organizations in orphan drug development.

What can now be concluded about the perceptions of barriers to orphan drug development in the Netherlands that are held and shared among the three actor groups involved? Barriers perceived by the producers are: little expected revenues from an orphan drug due to serving only market niches by means of small-scale production; little support from the regulators in realizing these small expected revenues by helping the producers with achieving the orphan drug status, thereby subsidizing research and development costs, facilitating the registration of the drug, granting market exclusivity for the drug and subsi-dizing the use of the drug; little image improvement due to the small number of orphan drug approvals by 2005; little knowledge of the pathogenesis of orphan diseases and, subsequently, of suitable animal models in clinical trials; and low participation of patients and patients' organizations in clinical trials. The barrier of the financial risk of non-profitable orphan drugs is also recognized by the regulators as being due to little expected revenues of an orphan drug. The regulator-related barrier of producers requiring help to secure the small revenues from an orphan drug is not recognized as such by the regulators. In addition, the regulator-related barrier of image improvement due to the approval of orphan drugs is not sufficiently recognized as a barrier by the regulators. With respect to the knowledge-and testing-related barriers, all actor groups agree on their existence and the need for increased patient participation to lower these barriers. But regulators and consumers are not yet convinced that patients' organizations have a crucial role in promoting and continuing patient participation in knowledge creation and drug testing.

In summary, the presented results indicate that especially lowering the barriers related to the regulators and consumers could help to secure and improve expected revenues from orphan drug development by producers.

The derived causal order between the indicators of the innovation barriers derived from the adoption factors related to orphan drug development as argued above could be tested. This is done by assessing the correlations between these indicators according to the derived causal order as path coefficients in the causal sense. The assessment of correlations between indicators has been carried out as described in Section 4.3. The resulting path coefficients are presented in Fig. 5 as causal effects indicated by thin arrows accompanied by their sign. Via their coherence with the concept they represent, which is indicated by the thick arrows and the associated signs in Fig. 5 , the causal effects of the indicators of innovation barriers on the willingness to develop orphan drugs can be assessed. In addition, the actor groups most concerned with particular subsets of barriers are also indicated in Fig. 5 . Although it was not investigated as an indicator of one of the concepts, the approval of a drug is included as an indicator to Fig. 5 . It is included because it is the outcome of registration procedures based on the results of clinical trials and represents a necessary condition for generating revenue from drug development.

A few other coincidences of indicators with the concepts they represent (as mentioned in Table 1 ) and associated non-zero correlations among indicators are not presented in Fig. 5 . These concern the non-coincidence of a prolonged patent period and the non-financial benefits for producers, the positive coincidences of image and cooperation with medical institutions and public health organizations, and the coincidence of knowledge sharing and complexity for which there was disagreement. The cooperation between producers and medical institutions is implicitly taken into account by putting medical institutions into the category of producers due to their tight relationships with universities, research institutes, and pharmaceutical companies. The cooperation between knowledge sharing and complexity is not taken into account because the regulator-driven effects in Fig. 5 imply this cooperation.

The results shown in Fig. 5 support the derived causal ordering of the barriers to increasing willingness to develop orphan drugs from good intentions to actions, namely:

(1) A high financial risk due to large development costs, small expected revenues and no realized approval of orphan drugs. The costs of orphan drug development are large due to the small scale of production, small market niches to be served and little subsidiary payments of research costs by the regulators (i.e. the government). Small expected revenues are perceived to be due to uncertainty about granting market exclusivity and the absence of an agreement until 2003 about subsidiary payments by the regulators (i.e. insurance companies) to consumers to reduce the costs of use. 10 Furthermore, until 2004, only a few orphan drugs had been approved by public health organizations. This is mostly due to problems concerning their registration.

(2) The perception of small non-financial benefits from developing orphan drugs because of a required scale of production that is smaller than and different from conventional drug production, the need to serve market niches, which differs from serving a large consumer market for conventional drugs, and the prevailing uncertainties concerning registration, market exclusivity, and subsidiary payments for research. (Pharmacos, 2005) .

(4) A high degree of complexity concerning orphan drug development owing to a lack of knowledge about the pathogenesis of orphan diseases, suitable animal models and patient participation in clinical trials (e.g. 10 versus 10,000 patients). Furthermore, orphan drug development takes place in an international setting, due to the relatively few internationally oriented pharmaceutical companies and start-ups actually involved in orphan drugs development (e.g. Genzyme (USA), Novartis (Switzerland), Bayer Healthcare (Germany), Amgen (USA), Pfizer (USA), Actelion (UK)), thereby further increasing the complexity of orphan drug development. (5) A low level of trialability resulting from insufficient possibilities to conduct statistically valid clinical trials, which in turn result from the conditions mentioned above.

Concerted actions to improve these unfavorable conditions and subsequently to increase the willingness to contribute to orphan drug development remain unlikely as long as there is no shared view of the importance of the regulator-and consumer-related barriers among all three actor groups involved.

With regard to the content of the tentative results presented, some interesting options for facilitating orphan drug development can be derived. From Fig. 5 it becomes clear that the regulator actor group may affect the concepts influencing the willingness to develop orphan drugs directly or indirectly and to a larger extent than with conventional drug development. The EODR provides the legislative foundation for this to occur.

If the measures related to the orphan designation granted to pharmaceutical firms are supported by research subsidiary payments by the government and reimbursement by insurance companies, the barriers to orphan drug development and market approval may be overcome. However, this requires the regulators to adopt another focus on orphan drug development. The current focus of the regulators of orphan drug development is on sharing knowledge about orphan diseases and orphan drug development among the various actor groups to promote learning and cooperation between them. The producer and consumer actor groups do not support this action. Due to the poor regulatory infrastructure, they stress the importance of support from governmental agencies in the form of achieving the orphan designation under the EODR, and subsequently facilitating registration of and granting market exclusivity for orphan drugs. They require them to assume a more active role in orphan drug development by promoting and facilitating achievement of the orphan designation by pharmaceutical enterprises and to support this achievement with subsidiary payments of (clinical) research into orphan drugs, which in turn would lead to more approved orphan drugs within the European Union. This implies that the regulators should also help with organizing phase III clinical trials of orphan drugs by mobilizing patients in other countries to participate in these trials via the European Union or the World Health Organization. This should be achieved by governmental agencies in cooperation with patient organizations. It should be a priority for governmental agencies to adopt such a role, to stimulate coordination and planning of the actions of the various actor groups involved in orphan drug development. This should lead to a shared vision and, consequently, more concerted action programs for specific orphan drug development in which the contributions and liabilities of each actor are matched and defined. In this respect, international programs such as the Sixth and Seventh Framework Programs of the European Union may have important roles.

The results presented in Fig. 5 on the variables and concepts influencing the willingness to develop orphan drugs in the Netherlands are new but should be considered tentative. This is due to the current nature of orphan drug development and the limited opportunities to conduct empirical research. As most biopharmaceutical orphan drug development is based on only recently acquired insights into the human genome and related DNA technology, the field is relatively new, which leads to three problems.

First, only a few distinct orphan drugs have been developed and approved so far. So, a comparison between successful and unsuccessful orphan drug development trajectories is not yet possible. For this reason, the concept of actual orphan drug development has been replaced by the concept of the willingness to develop orphan drugs. This conceptualization turns out to be very useful in identifying barriers in the innovation system of orphan drug development. The results obtained from the interviews support this conceptualization.

Second, identification of good indicators of the concepts defined in the conceptual model is problematic due to large varieties in a small number of contexts wherein the phenomenon can be studied. Fortunately, as expected from the conceptual model, the indicators applied in this study are mutually associated, implying a high degree of construct validity. However, some indicators, for example, '(knowledge of) subsidiary payments for research and use' and '(knowledge of) facilitating registration' contain multiple observable aspects. These indicators should be further refined in future research to improve the validity (cf. Riley, 1963) .

Third, only small numbers of people are involved in orphan drug development and even fewer are well informed and knowledgeable in this field. This excludes the possibilities of large-scale research and the use of appropriate statistical tests based on probability theory of the formulated hypotheses. To overcome this problem, some robust, small sample, statistical measures of association have been developed in this study. The results obtained provide good estimates of the associations between indicators, but the confidence intervals cannot be estimated. This implies that the estimated associations between indicators hold for the interviewed respondents but that they cannot yet be generalized.

In summary, when biopharmaceuticals in general and orphan drugs in particular are produced and used on a larger scale in the future, research into orphan drug development can be further improved as there will then be a history on which the research can be based. At present, this is not yet an option.

This study attempted to identify innovation system barriers to orphan drug development within the Netherlands. On the basis of the conceptual model of the adoption of innovations (Tornatzky and Klein, 1982; Rogers, 1995; Tidd et al., 2001) and by operationalizing the concepts in this model for three different actor groups involved in orphan drug development (producers, regulators, and consumers), a multi-actor approach of analysis is adopted in this study. The data derived from interviews with key actors in each actor group have been analyzed by means of simple descriptive statistics, due to the few respondents involved. The results of the analyses highlight the empirical relationships among the indicators and between the indicators and the concepts they represent (Fig. 5) , and also indicate the conditions via the (ordinal) values of their indicators that hamper the willingness to develop orphan drugs to rise from the level of good intentions to the level of active involvement. These conditions are thresholds in the innovation system of orphan drug development, and include: omissions in the knowledge about the pathogenesis of orphan diseases; a lack of suitable animal models; problems with conducting sufficient clinical trials; the uncertainty about achieving orphan designation; problems with the registration of orphan drugs; the enormous costs of orphan drug development; and the expected small revenues from orphan drug use.

To improve these conditions, concerted actions by the identified actor groups of producers, regulators, and consumers are needed. This requires the regulators (especially governmental agencies) to take a leading role in planning and coordinating these actions. However, as long as the actor groups do not have a shared view on the importance of these conditions and the urgency to improve them, the willingness of producers to develop orphan drugs within the Netherlands will not increase.

This shows that cooperation on innovation in networks of actors with complementary but different competences, responsibilities and interests within and between organizations is only a necessary condition for innovation. Cooperation may take various forms, not all of which guarantee successful coordinated actions of the actors involved and resulting in successful product innovations. To initiate orphan drug development, which is a prerequisite for a biopharmaceutical industry to develop within the Netherlands, governmental agencies in particular must take a leading role in coordinating, not steering the development of at least three additional conditions:

(1) Develop a shared definition of the innovation problem that takes into account all individual actors' competencies, responsibilities, and interests. (2) Develop an agreed plan of concerted action in time, for example, a road map of innovation policies, management and activities to be pursued by the various actors involved. (3) Increase the mobilization of patients with rare diseases within and outside the Netherlands for participation in clinical trials and future market access.

The results presented in this study should be regarded as only tentative due to the exploratory nature of the research carried out. Further research should be conducted along two lines of investigation. First, the same research can be conducted in other Western countries. The results obtained from these investigations could be combined with those from the Netherlands to carry out a statistically more reliable international comparative study. Second, if in the future various orphan drugs are introduced to the international market, orphan drug development trajectories can be compared and the barriers to the trajectories can be statistically tested. Hence, statistically better grounded confirmatory research of barriers in orphan drug development can be conducted in the next future.

",0.639091798347829
Public Health Network Structure and Collaboration Effectiveness during the 2015 MERS Outbreak in South Korea: An Institutional Collective Action Framework,"Following the 2015 Middle East Respiratory Syndrome (MERS) outbreak in South Korea, this research aims to examine the structural effect of public health network explaining collaboration effectiveness, which is defined as joint efforts to improve quality of service provision, cost savings, and coordination. We tested the bonding and bridging effects on collaboration effectiveness during the MERS outbreak response by utilizing an institutional collective action framework. The analysis results of 114 organizations responding during the crisis show a significant association between the bonding effect and the effectiveness of collaboration, as well as a positive association between risk communication in disseminating public health information and the effectiveness of collaboration.","The effectiveness of collaboration reflects a successful implementation of planned actions and procedures to coordinate join activities among seemingly independent set of private and public agencies. An effective collaboration captures not only meaningful interactions previously developed by agencies but also a set of planned actions and procedures contributing to an effective cooperation among various levels of public agencies and private organizations during a crisis [1, 2] . An effective collaborative effort can be examined through three dimensions: the expectation that a collaborative effort is contributed by: (1) improved quality of services provision and production, (2) cost savings, and (3) coordination [3] [4] [5] .

However, current literatures on public health emergency response have been particularly silent on the relationship between social positions of organizations and the tangible or intangible advantages that individual organizations can gain from the positions [6] . There are at least two competing perspectives on mechanisms explaining what organizations can expect to gain from their interactions, i.e., as to what type of connections should be sought by organizations in order to reap the advantage of social interactions [7, 8] .

According to the bonding effect, individual organizations can benefit from their closed-knit social structure when they share risks and gain access to quality information. The bridging perspective suggests public health emergency response requires rapidity and agility of responders to act. The benefits of interaction among first responders are assumed to reduce the cost of response when they can allocate risks through diversification of partners and take advantage of various expertise and skills within their networks [2, 6] .

This study aims to develop theories to understand a self-organized mechanism to mitigate collective action dilemmas derived from public health network responding to an epidemic event in South Korea, which is known to have a strong hierarchically coordination. The research question guiding this study is what communication strategies governmental agencies take to enhance the effectiveness of intergovernmental collaboration during public health emergency response. This research argues that governmental agencies take strategic communication actions to gain through interactions with other agencies to improve public health service provision. The other argument is that the more positive the opinion an organization has on the flow/channel of information dissemination concerning an epidemic event, the more confident the organization has on the successful collaboration effort. A positive disposition implies favorable perception on successful collaboration even though organizations may have different experiences about how information is shared with the public, which in turns influences the opinion of first responders regarding the effectiveness of collaboration.

This research contributes to a growing body of literature on interorganizational collaboration by focusing on public health emergency response in the arena of communicable diseases. While much of the current research on the application of the Institutional Collective Action (ICA) framework tends to focus on policy arenas in the realm of water conservation, public good markets, and local sustainable policy [1, 9, 10] , this paper examines the advantages of collaboration in the area of public health emergency response. In addition to the application of the ICA framework to the area of public health network, this research extends the application of the framework to an international setting. Although the ICA dilemma is expected to vary across countries [1] , the ICA framework has been rarely applied in other governmental settings (except [6, 11, 12] ).

Previous research applying the ICA framework explained that, individual organizations do better than other organizations given the nature of their interactions [13] [14] [15] [16] [17] [18] . Public officials representing their respective organization developed relationships with their partnered organizations. In order to get the work done, they are motivated to seek information on available resources and reached out to individuals across organizational boundaries. According to the ICA framework, through repeated interactions, local governments developed a certain level of trust, commitment, and shared vision. That, interorganizational collaboration is a function of social interactions that can be studied as a social structure. The structure facilitates certain tangible outcomes to an organization (see [1, 9] ). However, the ICA framework also posits that intergovernmental collaboration involves issues of coordination, negotiation, and monitoring [1, 6, 9] . Governmental agencies can encounter an issue of coordination in performing interjurisdictional activities. While coordination is critical for provision of public services requiring interconnected efforts across jurisdictions, the risk of coordination failure can be high if the collective action involves a set of diverse actors and activities. A linkage with a central actor that has critical information may help resolve a problem of incoordination [1] .

Moreover, another challenge is that institutional actors may confront bargaining problems or effective monitoring mechanism if they do not have perfect information about other actors' behaviors [6, 9] . Governmental agencies may lack the ability to negotiate division of benefits and costs without knowing about costs each actor is willing to pay for [1] . Each actor has an incentive to reap a greater benefit from the join actions but spend less costs on the join action. If an actor perceived that the other actor does not bear the same burden of the actions, the actor will anticipate higher costs of negotiation and deliberation. Under such situation, the actor will perceive a higher level of uncertainty and thus experiencing difficulties in implementing join activities [9] .

Monitoring an agreement is another challenge to intergovernmental cooperation [1, 9, 19] . Individual actors can show opportunistic behaviors following their interest to contribute less to the join efforts. The risk of opportunism can be higher if partners have limited information about their partners' behaviors. Without a mechanism to ensuring the credibility of commitment to the collective action, institutional actors may be reluctant to be involved in collaborative efforts. For example, cities in Georgia were less likely to enter interlocal service agreements when the difficulties measuring the services increase beyond a certain point [16] .

While there are a few empirical studies applying the ICA framework [10, 18, 20] , few have examined ICA problems at the international setting. Even fewer has examined the ICA problems during public health emergency response. Most studies tend to focus on policy arenas in the realm of water conservation, public good markets, and local sustainable policy. In the next section, this paper examines the advantages of collaboration in the area of public health emergency response by integrating Burt's structural hole theory in testing the ICA framework.

What explains the effectiveness of collaboration? The effectiveness of collaboration during public health emergency response can be partly affected by access to quality information. Being informed about important developments and situational awareness can save the organization valuable resources, money and the time to act during a crisis [6] . Working closely together as a collective suggests risks are shared among various organizations; and that, the by-product of such densely connected networks provides individuals with group benefits, i.e., they gain access to information and redundancy of shared resources [9] .

Densely interconnected individuals-which facilitates the bonding effect-can also generate intangible benefits particularly to an individual organization [7] . Here, the commitment of an individual organization to work jointly during a crisis illustrates the advantage of bonding when risks of inaction by its members can be minimized through sanctions and reputational effects [2] . During public health emergency responses, where risks of failure are shared, effective sanction and monitoring mechanisms become key to guide the collective action, i.e., acceptable and professional behaviors [21] . Reputations of an organization (as well as the collective) are at stake when responding to a crisis suggesting there is a strong incentive for organizations to observe the actions or inaction of another [22, 23] . The expectation will motivate each organization to act accordingly in order to maintain their reputation within the network and thus ensuring collaborative efforts are a success [22] . The failure of one affects the ability of others to get the job done.

However, in the field of public health emergency response, the empirical studies on the effect of bonding have been sketchy. The logistic nightmare aside, the flexibility of relationship requires trust. In presence of a crisis, the risk of failure of individual responders can contribute to the difficulty to make appropriate choices and set priorities. For example, second hand information (especially to evaluate the situation) often leaves out contextual situation influencing outcomes [24] . There are bias about information, which influence the perceived effectiveness of collaboration [25] . For instance, the evaluation of success is shaped by group discussion/interactions [26] . If closed-knit groups have a positive view of the outcome, the likelihood of the individuals in the group to have a positive predisposition about the join efforts would be high. A positive outcome gets repeated because it is appealing to all actors in the networks (rather than trustfulness). According to Burt, a positive image of outcomes can lead to certain individuals to have favorable opinion about outcomes [27] .

The explanations for a network centrality-as a proxy for the bonding effect-on the level of perceived effectiveness of collaboration are as follows: network centrality, which is measured in terms of network size, provides an opportunity for an actor to access information and resources in a timely way from multiple actors and improve their task performance [28] [29] [30] [31] . The focal actor is located at the central location of network can improve its reputation from others and can ensure cooperation from their partners [29] . The prominent actors can access timely information from their partners and better exploit information. The prominent actor also can develop capacity to use information and resources from their partners by expanding expericence of creating and maintaing linkage with other actors [30] . The focal actor that has actual interaction with other actors and has better understandings about the availability of information and resources from their partners, develop innovative solutions, and improve its performance.

A public agency that has linkage with multiple organizations is better able to access core timely information because the focal agency can receive information directly from them [32] . In other words, socialization leads to consistency of opinion. The agency can develop better understanding about a crisis by communicating with multiple organizations and provide better information to their partner agencies. The centralization of the focal agency can improve the effectiveness of the collaborative efforts with other organizations in preventing further crisis to occur. Hypothesis 1. The higher the level of network centrality of an organization, the higher the perceived level of collaboration effectiveness will be during a crisis.

The quality of information deteriorates as more intermediaries are involved, however. An organization that is constrained by partners that are directly connected is more likely to receive redundant information from their partners [33] . The actor may not obtain innovative solutions from their homogenous partners. Organizations are constrained if they are positioned within a close-knit network. On the other hand, an organization that exchanges information with other organizations that are connected to each other will gain redundant information rather than novel information, which is essential for identifying the path of the transmission path [34] . The organizations that have high constraints or are densely connected to other organizations may not be able to access new risk information and may not collaborate effectively to prevent the further transmission of infectious disease. In other words, Burt's logic of network constraint explains that the higher the level of constraints, the lower the perceived level of collaboration effectiveness during the crisis [31] .

In the context of interorganizational collaboration, the next best option for the organization is to interact with organizations in other groups. This will allow the organization to gain information that is not available from the closely connected organizations [31, 33, 34] . For example, Burt suggests that network brokerage enables individuals to access new ideas and develop innovative solutions [35] . He finds that managers that interact with other groups are better able to obtain alternative ideas and develop creative solutions [35] . The managers that arbitrage ideas with their partners that do not talk to each other are more likely to express innovative ideas and received tangible rewards for their performance.

Similarly, a public agency should be able to obtain core information that describes the nature of the problem and invest the given time and resources in responding to the immediate needs [36] . This is important especially when the response agency is able to access information from multiple jurisdictions in responding to a virus outbreak. The agency needs to understand the path of infectious disease transmission, which has a transboundary nature [37] . The agency will also be better able to access the transmission information and improve the effectiveness of interagency collaboration when the agency interacts with organizations in other regions.

The lower the level of constraints, the higher the perceived level of collaboration effectiveness will be during a crisis.

A public agency may choose an efficient mode of communication strategies by exchanging information only with key agencies. Network hierarchy refers to a closed form of network structure organized around a few prominent alters and allows a focal actor to leverage on others through the central actors [31, 33, 38] . A higher level of network hierarchy means less redundant ties and a more efficient communication ties to access new information.

The argument is a direct criticism of the bonding effect, i.e., when the agency is surrounded with redundant ties, the agency can access novel information [8, 33] . However, the bonding effect fails to take into account the idea that an organization may be advantaged if it can communicate with an influential actor within a network, i.e., it is able to access information from all others without investing time and resource. Instead, the organization can create a communication linkage with the higher-level agencies instead of all organizations in the network [39] . The linkage with the central actors, therefore, enables the focal actor to obtain diverse information from the prominent actor that received information from heterogeneous set of other actors and thus allowing the organization to save time and resources.

The network hierarchy captures the logic of the bridging effect. In other words, an organization will have a higher score on a network hierarchy suggests that the organization communicates with a higher-level agency or a prominent agency that already have information from higher level agencies. Interacting with these agencies not only give certain advantages to the organization, i.e., save time and resources for creating and maintaining the linkage with other multiple agencies, but also access to comprehensive information about the crisis. Considering the transboundary nature of an epidemic event, communication with a national or a regional agency will help the public agency at the local level to understand the nature of an epidemic event. Local agencies will be better able to use limited resource for collaborating with other agencies in preventing secondary effect of hazard more efficiently.

The higher the level of network hierarchy, the higher the perceived level of collaboration effectiveness during the crisis.

How does effective dissemination of information affect interorganizational collaboration? The ability of various organizations to disseminate information regarding a crisis also creates conditions that can promote interorganizational collaboration during responses [40, 41] . Collaboration among public agencies facilitates a culture of responsiveness among first responders and responsible agencies. Shared values and concerns for the public are important elements during public health emergency response [42] . Such a culture provides a condition that allows a highly productive team to work together in order to develop a cohesive response.

Scholars have argued that online information channels can expand ways to disseminate timely information to the target population as a new epidemic situation develops [43] [44] [45] [46] [47] . Social media also enables members of the public to communicate their situations easily with government agencies and improve situational awareness of public health emergency response organizations [45] . Social media is also powerful means to involve members of the public in volunteering and donation for disaster response [44] . For instance, following the Red River flooding of 2009, a Facebook group helped the city of Fargo (ND, USA) to recruit 5 percent of the total population when the city suffered from low response capabilities.

The literature on risk communication provides clues on the mechanisms to enhance the effectiveness of intergovernmental collaboration. Risk communication refers to an action of ""exchanging information about health and/or environments between interested parties"" [47] . Such information includes not only factors of health and environment risk itself but also government decision for controlling and managing risks. Effective risk communication is critical for informing the public of the very nature of hazards and governmental actions to avoid inaccurate perception of the hazard risk and unnecessary sufferings from the lack of knowledge [48] . Timely and reciprocal information provision can improve trust on governmental actions and help them follow guidance from governmental agencies while high levels of trust in governmental actions can discourage individuals to take personal protections [43, 49, 50] .

The more an organization employed the social media to disseminate information to the public, the higher the level of perceived effectiveness of interorganizational collaboration.

South Korea suffered from a MERS outbreak in 2015. One argument suggests that the country failed to control the transmission of the infectious disease from the first patient, who travelled internationally to the Middle East. MERS was a novel pathogen at the time and most cases had been found in Saudi Arabia [51] . The disease is caused by a virus called by MERS Coronavirus, and the common symptoms include shortness of breath, cough, and fever [52, 53] . Three in ten infected patients tend to die. Although the virus is thought to be transmitted through close contact with infected persons, there is no precise understanding of the mode(s) of transmission. There is currently no recommended vaccine or specific treatment for the virus infection. Most medical doctors and the national public health authority did not have knowledge about the nature or transmissibility of the virus. The first-MERS patient who had traveled to Middle East found that he had fever and visited five hospitals but could not find the cause of the symptom in the first four hospitals [54] . Eventually, the last hospital he visited identified that he had become infected with MERS. The national public health authority was not also effective in controlling the transmission of infectious disease from the confirmation of the first case on 20 May 2015. Epidemic investigators from the South Korea Center for Disease Control (CDC) set a narrow quarantine range and missed other clinicians and family members who had come into contact with the patient [55] .

The failure of the initial response led to further transmission that resulted in 186 confirmed cases, the death of 38 and the quarantine of about 17,000 people [56] . The mortality rate was the second highest in the world. People who were under quarantine were to stay at home for fourteen days to prevent potential transmission to others. The outbreak came under control on 15 June 2015 and no further transmission was declared on 27 July 2015 [57] . In addition, the outbreak had other societal impacts, such as the disruption of education and other economic activities. When there was great anxiety of infection among people, about two thousand schools canceled the classes and people avoided visiting hospitals, shopping malls, and amusement parks [58] . Not only the high-risk patients and the isolated resident but also parents who had children with cancelled classes had difficulties to continue their existing economic activities [59] . The outbreak, especially, damaged tourism industries and the country had two million tourists less than expected during the year of the outbreak [60] .

Public agencies had several challenges in seeking risk information to prevent the further transmission during the outbreak response. The national public health authority disclosed information about hospitals with MERS cases two weeks after the first patient was identified and was blamed for the further transmission [61] . Residents who had not known about the hospitals and visited the facilities became infected. The Seoul metropolitan government criticized the national government regarding the transparency regarding the hospitals with MERS infection [62] . Public health agencies also had difficulties in using the National Public Health Information System, which was supposed to facilitate the exchange of information among agencies. Because the information system did not allow local public health agencies to input their local risk information instantly, the system was not able to integrate the local information and failed to provide up-to-date transmission information for the users [63] . It is evident that South Korea public agencies were desperate to gain information about MERS in order to prevent the further transmission of infectious disease in their jurisdictions. The MERS crisis reflects challenges in interorganizational collaboration, especially the effectiveness of collaboration among public agencies. Along with information sharing, national, regional, and local public health agencies had challenges in cooperation for resource exchange. For instance, one local public health agency of Seoul requested equipment (e.g., ambulance) and personnel to the regional agency to tackle with high risk patients in the local jurisdiction but the upper level agency did not respond to the low-level agency's demand immediately. The slow response was not beneficial for governmental agencies to prevent further transmission of the infectious disease [55] .

The data were collected in the Seoul metropolitan region and 25 other local jurisdictions with reported MERS cases in February 2016. The survey was administered to three national ministries, 10 regional governments/agencies, 50 local public health agencies, 58 local police agencies, and 48 local fire agencies between January and March 2016. The regional and local agencies represent Seoul Metropolitan areas and 25 other local areas that had hospitals with MERS cases. All local public agencies in the Seoul Metropolitan areas were selected because 64 percent of local jurisdictions had more than one MERS case and residents have high mobility within the region.

Local agencies in non-Seoul metropolitan areas were identified from a list of affected areas from the Ministry of Welfare and Health. We received complete responses from 120 local agencies responding to the crisis including three regional agencies, 38 local public health agencies, 47 police agencies, and 32 local fire agencies. The response rate was 71 percent. Local jurisdictions with the responding local public health agencies do not have significant differences with non-responding agencies in term of socio-demographic characteristics, such as population density and elderly population (see Table 1 ). Although only 120 public agencies provided completed surveys, we utilized 19 other governmental agencies that provided only network information but not complete information, including three national agencies and one regional agency. Their responses were used to provide network information in order to construct the risk communication network. 

We measured interagency collaboration effectiveness based on three survey items: improved quality of services provision and production, costs savings, and coordination. Public officials from the selected agencies were asked to judge interagency effectiveness in the tree dimensions using the following survey items:

Quality of service and production: ""Please rate how much did interagency collaboration help your agency improve the service quality to address the needs of high risk patients and suspected residents during the first month of the outbreak response (20 May-14 June 2015)?"" Cost savings: ""Please rate how much did interagency collaboration help your agency to save costs regarding service quality to address the needs of high risk patients and suspected residents during the first month of the outbreak response (20 May-14 June 2015)?"" Coordination: ""Please rate how much did interagency collaboration help your agency to coordinate efforts to address the needs of high risk patients and suspected residents during the first month of the outbreak response (20 May-14 June 2015)?""

The response on each item is rating of each dimension and ranges from 0 (not at all) to 10 (very much helpful). The response categories are ratings of the responses and are summed as one index variable (Cronbach's alpha = 0.9102). As shown by the descriptive statistics in Table 2 , the average value is 21.44 and the standard error is 6.65.

Although there are various ways to measure interorganizational collaboration, we used two survey items asking the following question to measure collaboration:

Sending information: ""To which organization did your organization send information regarding MERS during the outbreak directly (information about high risk patient, hospital, or response information).""

Receiving information: ""To which organization did your organization receive information regarding MERS during the outbreak directly (information about high risk patient, hospital, or response information). "" The data were managed as a non-directed matrix after we merged the survey response in the same direction to make the most use of information about interagency communication. Social Network Analysis was employed to analyze the data to determine interorganizational communication linkage [2, 6, 12] . Table 3 presents a summary of the network statistics. The number of agencies in the network is 154, with ties of 425. On average, each agency has 2.76 ties with other agencies. The average length of the shortest path is 2.97 and the distance-based cohesion is 0.31. 

The network centrality is operationalized by calculating the normalized degree centrality score, which is the number of nodes adjacent to the focal node (i.e., degree) [64, 65] . The literature used degree centrality to measure the prominence of an actor in an interorganizational network. UCINET 6 (version 6.012, Analytic Technologies, Harvard, MA, USA) was employed to calculate the normalized score, which is degree divided by maximum possible number of nodes [64] .

Network constraint is used to measure bridging effects that allow the focal actor to access heterogeneous information through directly connected alters [66] . Network constraint is the extent to which the focal node is connected to others that are connected to each other [29, 35] . A high level of network constraint means few structure holes and decreases the likelihood of a local bridging role between directly connected alters while a low level of the constraint means many structure holes increase the likelihood of local bridging role. UCINET 6 was employed to calculate the constraint score based on the undirected matrix [64] .

Network hierarchy measured as the extent which total constraints are located at the focal node's single other [33] . When an actor has a high hierarchy score, the actor has an alter that interact with multiple partners and can help the actor to access new information efficiently [39] . We calculated the hierarchy score using UCINET 6 [64] .

We measured the online channel effectiveness as an index summing the response to the three survey items rating the effectiveness of the three types of online channel to disseminate risk information to the public: agency website, social media page (e.g., Facebook, Twitter), and agency blogs. The response categories range from 0 (not used at all) to 10 (very much effective). The summative index has high reliability (Cronbach alpha = 0.990).

A set of control variables was also included at the organizational level, i.e., organizational types and socio-demographic characteristics of community served by the organizations. First, two dummy variables for organizational types were created for a local police agency and fire agencies, which support local public health agencies, regional and national governmental agencies. It was expected that the two types of local agencies may have differences with governmental agencies that led the outbreak response. We also added two socio-demographic variables: population density and elderly population. Population density was measured as the total population divided by the land area (km 2 ) and elderly population was measured as the proportion of elderly population to total population. It was also expected that local agencies that served densely populated area or a large proportion of senior population will face challenges when responding to the outbreak because citizens are more vulnerable to the virus transmission via contact.

An ordinary least square (OLS) regressions was used to test the effects of the three types of communication strategies and online channel on interagency collaboration effectiveness. OLS regressions were applied to similar research that examine the relationship between network effects and outcomes [2, 6, 12] . Six models was estimated including three models for all agencies and three other models for local public health agencies. Although all the models do not have multicollinearity issues (average variance inflation factor < 2), Breusch-Pagan tests show that the first models had issues of heteroscedasticity. We employed robust regressions to address the problem in estimating the first three models.

Tables 4 and 5 present the OLS regression results testing the effects of network position and communication technology on the effectiveness of interagency collaboration. Table 4 is testing for all types of organization and Table 5 is for local public health agencies. The regression results do not support the first hypothesis that the greater the network centrality, the higher the perceived level of collaboration effectiveness during the crisis. Although the coefficients of the variable are positive (0.362 and 0.146) in the Models 1 and 4, network centrality does not have a significant relationship with the effectiveness of interagency collaboration at the level of 0.1. The benefits of accessing essential risk information directly from multiple agencies may not outweigh the costs of creating and maintaining linkages with diverse agencies during the outbreak response. Especially, different types of public agencies have different culture and information standards, which create challenges in expanding communication with multiple agencies [67, 68] . An interview with one local public health official supports the quantitative findings: ""One of the challenge for interagency collaboration was a difference in information standards across different types of agencies. My local public health agency had confusion in sharing information about period and termination date of quarantines with police or fire agencies because the supporting agencies did not have the same information standard for reporting the quarantine start and termination dates."" Model 2 results confirm the second hypothesis that the lower the level of constraints, the higher the perceived level of collaboration effectiveness during the crisis ( Table 4 ). The coefficient of network constraint is −6.862 and is significant at the levels of 0.01 in Model 2. A public agency that communicates with agencies in other regions can have a better understanding of the infectious disease transmission and may be better able to collaborate with other agencies to address the immediate response needs. High mobility of residents may increase the importance of communication across regions to accurately identify the transmission path to prevent further transmission in their jurisdictions. For instance, two residents lived in a region about 250 miles away from the hospital with tens of MERS cases and the two of them became infected. On the other hand, Model 5 does not support the hypothesized effects of network constraints on perceived levels of interagency collaboration effectiveness. Although the coefficient of network constraint is −0.038 in the model, the coefficient is not significant at the level of 0.1. Most local public health agencies that led the outbreak response in their regions communicated with agencies in other jurisdictions or high-level agencies, so network constraints do not have differentiated effects on the levels of interagency collaboration effectiveness.

Model 3 result does not support the third hypothesis that the higher the level of network hierarchy, the higher the perceived level of collaboration effectiveness during the crisis. Although the coefficient of network hierarchy is positive (β = 0.615), the coefficient is not statistically significant at the level of 0.1. Communication with a few prominent agencies may not necessarily help public agencies better understand the nature of the outbreak and improve the effectiveness of interagency collaboration effectiveness. On the other hand, Model 6 confirms the hypothesized effects of network hierarchy on interagency collaboration effectiveness among local public health agencies. The coefficient of network hierarchy is 13.53 and significant at the level of 0.05. When local public health agencies that represent their local jurisdictions communicate with national, regional, or other prominent agencies have better understanding about the infectious disease transmission and can develop better strategies to coordinate interagency efforts in preventing the further transmission in the jurisdictions.

The first three model results support the fourth hypothesis that risk communication is positively associated with the perceived levels of interagency collaboration effectiveness. The coefficients of the three models are each 0.042, 0.046, and 0.043 and are statistically significant at the level of 0.01. A public agency that used internet, social media and blogs can expand their communication channels to inform the public about the outbreak. They can respond and improve interagency efforts to prevent further transmission of the virus within their local jurisdictions. Online communication channels also helped improve the awareness of the public and contribute to interorganizational efforts toward the prevention of the transmission.

Additionally, the results of the model show that organizations, locations, and demographic characteristics of local organizations do not have a significant effect on the perceived levels of interagency collaboration effectiveness. Although local police agencies or fire agencies tend to have a lower level of perceived interagency collaboration effectiveness than other agencies, most coefficients in the models are not statistically significant at the level of 0.1. Elderly population served by local agencies has a positive effect on the dependent variable but the coefficient is not statistically significant. An agency serving densely populated residents, on average, reported to have favorable view on interagency collaboration (Model 6). This finding is unexpected, i.e., public health agencies with populous local jurisdictions tend to have high levels of interagency collaboration effectiveness. Public agencies in the populous jurisdictions may be sensitive to potential widespread transmission and choose to collaborate better with other agencies than those jurisdictions that are serving less densely populated residents.

This study suggests that interagency risk communication strategies and online risk communication are important means to improve interagency effort toward a virus outbreak. Public agencies need to take adequate interagency risk communication strategies to access core information to prevent the transmission of an infectious disease that has a transboundary nature. Bridging strategies are useful for public agencies to access new risk information from other regions, understand the dissemination path of the disease, and exert better collective efforts toward the prevention of transmission. Efficiency-based risk communication helps local public health agencies to obtain critical risk information and save time and resource for productive activities to prevent the further transmission of an infectious disease. Finally, when public agencies use online media, such as websites and social media, the agencies can expand the channel of public risk communication. The expanded communication channels help improve the public awareness of the outbreak response, individual protection actions, and contribute to interagency collaboration.

The findings have several practical implications for effective interagency communication and collaboration in response to epidemics that has a transboundary nature. First, public agencies should be able to exchange risk information with agencies in other regions or jurisdictions. Bridging strategies enable a public agency to access information, which is not available within or nearby local jurisdictions to understand better the path of infectious disease transmission. When the agency can receive risk information from other regions where their residents travel frequently, the agency can identify the path of transmission to their residents and take response actions to prevent further transmission.

Second, communication with a national, regional agency, or other prominent agency can help a public health agency to save time and resource for improving the outbreak response in the jurisdictions. Although a public agency that leads the outbreak response in their local jurisdiction can create linkages with multiple linkage with other agencies and obtain information from them, the agency should divert resource and time for the actual response to prevent the further transmission to their local population. Instead of creating multiple communication linkage, communication with the prominent agencies that exchange information with other regional or local agencies can obtain comprehensive information about the transmission path.

Finally, expansion of communication channel is helpful for a public agency to raise the situational awareness of the public regarding local response status and improve interagency efforts to prevent the further transmission of the infectious disease. Online media, such as websites, blogs, and other social media allows the public agency to disseminate up-to-date risk information to residents and help residents to understand how the individuals can take protective actions and contribute to interagency efforts toward the control of the transmission. When members of the public as co-producer of public services have better understandings of the response [69] , they can be better able to contribute to the control of the transmission.

The study has several limitations in testing the effects of the three communication strategies and online channel use on intergovernmental collaboration effectiveness. First, the study does not use objective measures to assess the outcome variables. Because there is no archival data that assess agency or collaboration performance, this study used the perception of public officials who are knowledgeable about their agency response. Even though it is hard to establish objective indicators for collaboration performance, future studies may develop objective measures for public health emergencies and compare the measure with the subjective measures. Second, the study might not be generalizable to all regions or local areas. This study focused on Seoul metropolitan areas and other local areas that had MERS cases due to time and costs. This study did not investigate other local areas or regions that were affected by the outbreak although the areas did not have confirmed cases.

",0.6381490162330875
The Oakville Oil Refinery Closure and Its Influence on Local Hospitalizations: A Natural Experiment on Sulfur Dioxide,"Background: An oil refinery in Oakville, Canada, closed over 2004Canada, closed over -2005 an opportunity for a natural experiment to examine the effects on oil refinery-related air pollution and residents' health. Methods: Environmental and health data were collected for the 16 years around the refinery closure. Toronto (2.5 million persons) and the Greater Toronto Area (GTA, 6.3 million persons) were used as control and reference populations, respectively, for Oakville (160,000 persons). We compared sulfur dioxide and age-and season-standardized hospitalizations, considering potential factors such as changes in demographics, socio-economics, drug prescriptions, and environmental variables. Results: The closure of the refinery eliminated 6000 tons/year of SO 2 emissions, with an observed reduction of 20% in wind direction-adjusted ambient concentrations in Oakville. After accounting for trends, a decrease in cold-season peak-centered respiratory hospitalizations was observed for Oakville (reduction of 2.2 cases/1000 persons per year, p = 0.0006) but not in Toronto (p = 0.856) and the GTA (p = 0.334). The reduction of respiratory hospitalizations in Oakville post closure appeared to have no observed link to known confounders or effect modifiers. Conclusion: The refinery closure allowed an assessment of the change in community health. This natural experiment provides evidence that a reduction in emissions was associated with improvements in population health. This study design addresses the impact of a removed source of air pollution.","A very large number of population-based observational studies have reported a positive association between ambient concentrations of air pollution and hospitalizations for respiratory and cardiac disease; however, there exist few studies assessing the health benefit of an intervention which reduces community exposure to air pollution. Improvements in both air quality and health effects have been documented following a number of large-scale changes in local air pollution emissions, including copper smelters in the Southwestern United States [1] , bitumous coal burning in Dublin, Ireland [2] , lowered sulfur content for fuel oil and gasoline in Hong Kong [3] , closure of pollution-emitting industries in East Germany [4] , reduction of traffic during the 1996 Summer Olympic Games in Atlanta [5] , and reduction of traffic during the Summer Asian Games in Busan, South Korea [6] . Studies have also been done following the closure of the Utah Valley Steel Mill [7, 8] and on the impact of the air pollution-lowering steps adopted prior to the 2008 Beijing Olympics [9, 10] . In many of these studies the intervention led to significant reductions in negative health effect metrics for the affected populations.

The present study assesses the health impact of the closure of an oil refinery in Oakville, Canada on residents' hospitalizations. The refinery was designed for production of consumer-grade gasoline and by-products such as asphalt and kerosene. It was located near the shore of Lake Ontario and produced 13,200 cubic meters of gasoline per day (approximately 90,000 barrels) [11] . The refinery was built in 1958 [12] and closed and decommissioned over a six-month period from October 2004 to March 2005.

There remains a small-scale oil terminal at the location, but air pollution emissions, in particular SO 2 , have been almost completely eliminated after the refinery closure. Ambient measurements of air pollution for the city were more complete than site-specific emissions data for the study period, as the site-specific monitor was removed before the refinery closure. Considering the relatively small population size of Oakville, we examined respiratory-related morbidity, as there were more hospital admissions than deaths, providing more statistical power. Thus, in this study we focused on the changes in ambient air pollution in relation to those in morbidity records to measure the impact of the refinery closure on public health.

All data referenced in the following section were recorded in, and around, the city of Oakville, Ontario (ON), Canada. Oakville sits on the northwestern shore of Lake Ontario, one of the five great lakes, and experiences a humid continental climate, with cold winters and warm summers. Typical monthly average temperatures range from −5 • C to +26 • C. The city itself is largely suburban, and major pollutant sources nearby include the city of Hamilton, ON (which has a large industrial region) and prevailing winds carrying pollution from the Ohio River valley of the United States up through Southwestern Ontario. No major changes to pollution sources in the local region occurred during the study period aside from the closure of the refinery described in the following, excepting only the national regulatory change (enacted in [2002] [2003] which lowered the sulfur content of gasoline: by early 2003 this change was fully in effect. For comparisons with neighbor cities during the study period, City of Toronto (Toronto) was selected as a control city and the Greater Toronto Area (GTA) was used as a population demographic reference to more accurately represent the demographic composition of Oakville.

To address the emission reduction from closure of the refinery, we obtained site-specific emissions data from the National Pollutant Release Inventory (NPRI) program, run by Environment and Climate Change Canada (ECCC). This program collects information from industrial, commercial, and institutional facilities that meet certain minimum emission requirements [13, 14] . Data were obtained for air emissions from the refinery of SO 2 , CO, NO 2 , total PM, PM 10 , and PM 2.5 for 2002-2005 as well as volatile organic compounds (VOCs) [13] for 2002-2012 from both the refinery and the storage facility that replaced it in 2006.

To assess the impact of refinery closure on ambient air pollution, we collected data from two National Air Pollution Surveillance (NAPS) air quality monitoring stations: Station 61602 (approximately 1 km from the refinery site, hereafter called the ""Refinery station"") and Station 61603 (approximately 10 km north-east of the refinery, hereafter called the ""Oakville station"") . Daily values  of sulfur dioxide, the indicator pollutant for the oil refinery emissions, were available for 1996-2002  at the Refinery station, and for 2003-2007 at the Oakville station. The Refinery station was closed in 2002 and the Oakville station was opened as a replacement in 2003. Figure 1 shows the two designated sites in the refinery region for the time period of interest, as well as a few stations from surrounding areas, including two NAPS stations (Stations 60430 and 60433) in Toronto and one station (63001) to the southwest in the city of Burlington, ON. The Refinery and Oakville stations in Figure 1 recorded air pollutant monitoring data for some portion of the study period (1996) (1997) (1998) (1999) (2000) (2001) (2002) (2003) (2004) (2005) (2006) (2007) (2008) (2009) (2010) (2011) (2012) , although no one station has complete data for the entire period. Table 1 gives an overview of the available data from the two designated stations. Table 1 . National Air Pollution Survey (NAPS) data availability from monitoring stations near the refinery (61602 and 61603) in Figure 1 . Figure 1 shows the two designated sites in the refinery region for the time period of interest, as well as a few stations from surrounding areas, including two NAPS stations (Stations 60430 and 60433) in Toronto and one station (63001) to the southwest in the city of Burlington, ON. The Refinery and Oakville stations in Figure 1 recorded air pollutant monitoring data for some portion of the study period (1996-2012), although no one station has complete data for the entire period. Table 1 gives an overview of the available data from the two designated stations. Table 1 . National Air Pollution Survey (NAPS) data availability from monitoring stations near the refinery (61602 and 61603) in Figure 1 . 

Hourly temperature and climate records were obtained from the Climate Data records of ECCC for the study period (1996-2012). These records are almost complete (greater than 99% availability) for two weather stations in the geographic vicinity of the refinery location: one located at Lester B. 

Hourly temperature and climate records were obtained from the Climate Data records of ECCC for the study period (1996) (1997) (1998) (1999) (2000) (2001) (2002) (2003) (2004) (2005) (2006) (2007) (2008) (2009) (2010) (2011) (2012) . These records are almost complete (greater than 99% availability) for two weather stations in the geographic vicinity of the refinery location: one located at Lester B. Pearson International Airport (station 5097 approximately 20 km northeast (NE) of the refinery) and the second located at the Burlington Piers (station 7868, approximately 10 km southwest (SW) of the refinery, as shown in Figure 1 ). These hourly records allowed us to categorize the prevailing wind patterns through Oakville for the study period.

Daily morbidity (hospitalization) counts for Oakville, Toronto and the GTA were collected for the study period. The International Classification of Diseases 10th Revision (ICD-10) [15] was used to identify circulatory (I00-I99) and respiratory (J00-J99) diagnoses. Ontario hospitals were transitioning between the ICD-9 and ICD-10 classifications over the period 2001-2004, so data recorded under ICD-9 were converted to ICD-10 based on the conversion table provided by the Canadian Institute for Health Information (CIHI).

To investigate potential confounders or effect modifiers related to morbidity we obtained age, sex, marital status, household type, and income from Statistics Canada [16, 17] at the census division (CD) level, measured every five years. Standardization was done against the census results of 2006, as a convenient center point of the analysis. Seasonal traffic patterns for the study period on the major highways running through Oakville were also collected to examine their effects on ambient air pollution concentrations and hospitalizations [18] .

In this study, it was necessary to control for seasonal and demographic changes in hospitalization counts when attempting to identify any possible association between reductions in ambient air pollution and hospitalizations for Oakville. For this purpose, a standardized hospitalization rate (SHR) was applied to standardize the number of hospitalizations in Oakville (""observed"") relative to the number of hospitalizations expected for a population with demographic distribution of the GTA in 2006 (""expected""). Seasonal (four 3-month blocks, seasons 1 to 4) aggregate counts were standardized against a distribution of six age categories, as described below, using GTA demographic data for the year 2006 (a census year in Canada). The GTA had a population of 6.3 million people in 2006, and encompasses Oakville, which comprises only a small portion of the overall population (approximately 3%).

Multiple standardization methods are possible [19] [20] [21] , and direct age-standardization against the reference population of the GTA in 2006, a census year, was selected as the most appropriate method for our available data and demographics. We used six age categories (years): 1-5, 6-20, 21-65, 66-75, 76-85, and 86 or older. We explored variations on this choice of stratification and found that the choice did not affect the analysis; the specific categories chosen here are a convenient choice commonly used at Health Canada for other studies and analyses.

In the following equation, the SHRs, (m t ), are aggregated by block as follows:

where t is the time index, k the age category (k = 1, . . . , 6), n k,t the city-specific age category health count at given time t, P c the city-specific age category population for the given time block, P s the reference population for the same age category for the reference time block, and P the total reference population (GTA) for the same. In implementation, the standardization was done by blocks (e.g., 3 months), multiplying the resulting seasonal totals by 4000 to become standardized cases per thousand persons per year.

In addition to the season-and age-standardized SHRs described above, which use fixed seasonal time periods (e.g., January to March), we implemented an alternative, dynamic method, which allows one block per year to float temporally according to annual variations. As the yearly cold and respiratory virus epidemic has variable start and termination dates year-by-year and, additionally, correlates with the maximum respiratory hospitalization rates, it is possible to locate the maximum weekly rate of respiratory hospitalization (between September and July, typically occurring in February or March), and then take a 15-week block centered on that maximum week (i.e., 7 weeks prior to, 7 weeks following, and the maximum week). These 15 weeks of age-standardized rates can then be aggregated and scaled to cases per thousand persons per year. We explore this in the following.

The proposed dynamic approach has value for our analysis over the traditional seasonal approach with a pre-determined fixed time period for two reasons. First, epidemic-related unusual hospitalizations can be accounted for if they exist. Second, off-season epidemic episodes can be accounted for as the dynamic approach puts weight on exact episode occurrence time rather than on calendar time. We use this cold-season peak-centered morbidity method to compare Oakville to Toronto. These results, aggregated and simplified to a single count (as a rate per thousand persons per year), were then fit using a simple segmented regression model as follows:

where R t is the yearly rate, t a time variable (year), and I t an indicator variable for the intervention, with the β elements coefficients of the same. This model allows for a long-term trend to be fit to the data while simultaneously modeling any step-function behaviour as delineated by the intervention variable (I t ). This is a particularly simple implementation of interrupted time-series modeling. Table 2 lists the reported total yearly emissions (in metric tons) for the Petro-Canada Refinery Table 2 indicates emissions below the required reporting thresholds. In addition to the reported reduction in annual emissions, we observed ambient SO 2 concentrations as decreasing across the refinery closure period. The SO 2 fuel regulations initiated in 2003 can be observed in the reduction in annual average hourly ambient SO 2 for Oakville of 39% (from 4.30 ppb to 2.62 ppb between 2001-2002 and 2003-2004) before the refinery closed. In an attempt at identifying any further reduction that might plausibly be related to the refinery closure in 2004, we compared two time periods, 2003-2004 versus 2005-2007 , restricting attention to wind direction from the south-west to allow for winds to carry refinery emissions to station 61603. For this data, the wind direction-restricted average hourly ambient SO 2 fell further by 20% after the refinery closure (from 4.92 ppb to 3.94 ppb). Against this, the ambient, unrestricted ambient concentration dropped 0.4 ppb, from 2.62 ppb to 2.26 ppb. This larger reduction for the sub-case of wind from the south-west (10% of observed hourly samples) may reflect the impact of the refinery closure. In addition, it appears that the variability of the exposure in both stations decreased, indicating that extreme events were less likely to occur post-closure. Table 3 gives means (µ) and standard deviations (s) for N available hourly observations of both all cases and wind-direction-restricted data (winds originating from the southwest) for the two periods. 

The unstandardized (whether by age or by season) hospital admission data showed little annual daily change in non-accidental all-cause, circulatory and respiratory gross number of hospitalizations over the study period (1996) (1997) (1998) (1999) (2000) (2001) (2002) (2003) (2004) (2005) (2006) (2007) (2008) (2009) (2010) (2011) (2012) in Oakville (Table 4 ). However, the unstandardized hospital admissions in Table 4 do not reflect demographic (e.g., changes in age distribution and population) or environmental changes (e.g., seasonal variations). Accordingly, we age-standardized the seasonal counts for Oakville against the GTA as a 6.3-million-person demographic reference population, from the national census in 2006 (see Online Supplement, Table S1 for details on the GTA records). As the population of Oakville increased from 132,234 to 190,902 (44%) [22] during the study period, to account for shifting demographics over time we employed age-standardized SHRs to detect any changes in the number of hospitalizations after the refinery closure. In Oakville, respiratory hospitalization rates were up to 50% greater in the cold season (winter, January-March, and fall, October-December, in white dots, Figure 2 ) than in the warm season (spring, April-June, and summer, July-September, in dark dots, Figure 2 ), and thus we examined seasonal-level age-standardized SHRs.

There was a sharp decline in Oakville seasonal age-standardized SHRs of respiratory hospitalizations after the closure of the refinery (Figure 2 , to the right of two vertical dashed lines). The 2003 rates appeared to be outliers, which could be explained by the severe acute respiratory syndrome (SARS) epidemic [23] in Ontario when patients were advised to stay away from hospitals. Apart from this specific cold season in 2003, there were approximately 6.5 fall/winter hospitalizations per thousand persons per year before the refinery closure (with the pattern going back as far as 1996, the first year of available data), dropping to a rate of 4.5 immediately following the closure, a 31% decline in respiratory SHRs (mean before to mean after).

To examine the standardization methodology, we explored a number of standardization approaches, including population standardization (year-by-year evolving reference) and static reference population (presented in this paper, e.g., Figure 2 ). In all cases, the decrease in Oakville stood out as being unusual among nearby cities, and against the demographic reference population of the GTA in 2006. Selected figures from this exploration for Oakville, the City of Toronto, and for the entire GTA are available in the Online Supplement for this paper as Figures S1 through S9. 132,234 to 190,902 (44%) [22] during the study period, to account for shifting demographics over time we employed age-standardized SHRs to detect any changes in the number of hospitalizations after the refinery closure. In Oakville, respiratory hospitalization rates were up to 50% greater in the cold season (winter, January-March, and fall, October-December, in white dots, Figure 2 ) than in the warm season (spring, April-June, and summer, July-September, in dark dots, Figure 2 ), and thus we examined seasonal-level age-standardized SHRs. There was a sharp decline in Oakville seasonal age-standardized SHRs of respiratory hospitalizations after the closure of the refinery (Figure 2 , to the right of two vertical dashed lines). The 2003 rates appeared to be outliers, which could be explained by the severe acute respiratory syndrome (SARS) epidemic [23] in Ontario when patients were advised to stay away from hospitals. Apart from this specific cold season in 2003, there were approximately 6.5 fall/winter hospitalizations per thousand persons per year before the refinery closure (with the pattern going back as far as 1996, the first year of available data), dropping to a rate of 4.5 immediately following the closure, a 31% decline in respiratory SHRs (mean before to mean after).

To examine the standardization methodology, we explored a number of standardization approaches, including population standardization (year-by-year evolving reference) and static reference population (presented in this paper, e.g., Figure 2 ). In all cases, the decrease in Oakville stood out as being unusual among nearby cities, and against the demographic reference population 

The peak of the respiratory-related hospitalization rates typically occurs during the yearly epidemic of cold and respiratory virus infections, which has variable timing and therefore is inconsistent from city to city across different hospitalization-cause categories. We thus considered a dynamic approach of taking 15-week blocks centered on the peak week of respiratory hospitalizations for a given cold season (October-March) and city. Eliminating 2003 as a clear outlier (attributed to the SARS epidemic) and excluding 2005 as the pivot year for refinery closure, we found a significant (p = 0.0006) mean difference between the period before the closure (1996) (1997) (1998) (1999) (2000) (2001) (2002) (2003) (2004) and afterwards (2006-2012) using a simple segmented linear regression model, as detailed in Equation ( 2) (details in Table 5 and Figure 3 ). We found no significant differences in all-cause (non-accidental), circulatory and non-cardiorespiratory related hospitalizations before and after the refinery closure (not shown here). Note that when data from 2003 and 2005 were included in the analysis, although the exact values changed, the conclusions did not; similarly, allowing 2005 to be considered as ""after"" closure did not change the conclusions. This result indicates that our finding on the Oakville respiratory hospitalizations is insensitive to the inclusion of the two specific years in question. In summary, cold-season peak-centered respiratory hospitalizations in Oakville fell by 2.2 cases per thousand persons per year (approximately 180 total hospitalizations for the year) from before the refinery closure to after, excluding the two outlier years. Across the same period, no immediate decrease (after accounting for trend) was found in Toronto.

To test the sensitivity of the results to the time of analysis, we repeated the above analysis by 

Other potentially confounding factors include changes in socioeconomic variables, temperature and traffic density occurring in the Oakville area over the same time as the refinery closure. However, using census data from 2001, 2006 and 2011 [24] , we found no corresponding change in median income, sex, education status, labor force status, or industry of main employment, nor was any such change reported by community organizations [22, 25] . For vehicular traffic, the time period of interest shows a steady increase in traffic volume and density, which we cannot plausibly link to a sudden decrease in respiratory-related hospitalizations. 

In this study, we found that the closure of the oil refinery located in the city of Oakville, Ontario was associated with measurable reductions in wind-direction-adjusted ambient SO2 in Oakville. We additionally found that there was a significant reduction in respiratory-related hospitalizations in Oakville, with the reduction occurring sharply after the refinery closure and persisting. Moreover, we found no other explanatory or confounding factors which influenced the reduction in hospitalizations for Oakville, such as changes in socio-economic status, traffic counts, or traffic-related air pollutants such as NO2. Taking these findings together, the reduction in SO2 emissions from the Oakville refinery closure and the subsequent decrease in SO2 concentrations in 

In this study, we found that the closure of the oil refinery located in the city of Oakville, Ontario was associated with measurable reductions in wind-direction-adjusted ambient SO 2 in Oakville. We additionally found that there was a significant reduction in respiratory-related hospitalizations in Oakville, with the reduction occurring sharply after the refinery closure and persisting. Moreover, we found no other explanatory or confounding factors which influenced the reduction in hospitalizations for Oakville, such as changes in socio-economic status, traffic counts, or traffic-related air pollutants such as NO 2 . Taking these findings together, the reduction in SO 2 emissions from the Oakville refinery closure and the subsequent decrease in SO 2 concentrations in ambient air concentrations appear to have occurred simultaneously with a significant reduction in cold-season respiratory hospitalizations in Oakville.

Natural experiments represent a powerful tool in establishing a causal link between exposure and response. In many cases, the experiment acts as an intervention, with correspondingly large changes: the Utah Valley steel mill closures [7] being one of the most clear and famous examples. In our study the evidence suggests that even closures of smaller-scale industrial sites like the Oakville refinery, if the sites are emitters of pollutants associated with health effects, can have immediate and measurable effects on the health of the surrounding community.

There were several limitations of our study. Data were lacking from the Refinery Station (61602), which was closed at the end of 2002, and the replacement Oakville station (61603) did not begin operation until April 2003, although data from the NPRI indicated that there was little change in the emissions from the refinery up to 2003, so the 1996-2002 span of available data from 61602 should give reasonably accurate estimates of the ambient air quality before closure. Data from Oakville station provided us a view into ambient air quality after closure. However, no SO 2 data were available after 2007 for the Oakville Station. In addition, VOC data (which would normally be of great interest when examining the emissions from a refinery) were sparse or unavailable for the period of the study.

The NPRI data used as a gauge of pollution emissions from the refinery are not entirely reliable, as the data are self-reported by industry, rather than observed by a third party. In addition, regulations only require coarse aggregate measures. Self-reported data on air pollution emissions are available from 2002-2014, and emissions of specific chemicals (e.g., toluene) are available back to 1994. As well, yearly NPRI reporting requirements, including addition/removal of substances, reporting thresholds and reporting exemptions for specific industrial sectors have changed over time [14, 26] . In 2001, new release groupings were created, including on-site pollutant release to air, and 7 criteria air contaminants were added in 2002, including SO 2 , NOx, CO, and VOCs.

There are no limitations to the health-related data, as they are complete due to mandatory reporting regulations, with complete morbidity data available back to 1996. The seasonal age-standardized SHRs based on the GTA should account for seasonal, demographic and population changes through the study period, and as demonstrated in the Online Supplement, are resistant to variation in parameter choices for the SHR.

By contrast, our study has several strengths. Use of SO 2 as a proxy for overall refinery emissions is a sensible choice, as SO 2 is known to be related to morbidity (hospitalizations), both in general [27] [28] [29] [30] [31] [32] and specifically for the Canadian population [33, 34] . While associations between SO 2 and morbidity are positive and significant, they are small in magnitude, so any association between refinery closure and health outcomes cannot be necessarily directly attributed to this pollutant, however use of SO 2 as a representative proxy of the whole seems reasonable. At the same time, no other ambient air pollutants with available data showed statistically significant reductions of step-function nature after the refinery closure, despite the clear reduction (6000 tons per year for SO 2 , Table 2 ) in emissions, so again, the use of SO 2 is justified.

The closure of the refinery coincided with a measurable reduction in hourly ambient SO 2 levels in the city of Oakville-a reduction that was greater than what would be expected from the refinery closure, which we estimated through evaluation of similar monitoring stations in the neighboring city of Toronto using wind-direction-restricted analyses. Comparing the before-and after-closure periods, we found a statistically significant (step-function) reduction of ambient SO 2 concentration levels in Oakville only, not in Toronto. This is another strength of the study, as with the local nature of SO 2 ambient concentration we would not expect to see step-function reductions of SO 2 concentration outside of the immediate region surrounding the refinery.

The closure of the refinery came soon after the introduction of a stringent sulfur content regulation for gasoline in Ontario: as of 1 January 2005, all gasoline sold in Ontario was limited to an annual average sulfur level of 30 ppm (30,000 ppb) or 30 milligrams per-kilogram (mg/kg), with a never-to-be-exceeded limit of 80 ppm (Government of Canada, 1999) . From July 2002 to 2005, there was an interim annual average sulfur content of 150 ppm. The ambient concentration reduction happened quite sharply in 2002-2003, despite the regulation rolling out in phases, as most fuel manufacturers moved their facilities to the new standard in time for the initiation of the regulation. Similar regulatory changes were implemented in June 2006 for diesel fuel, with a reduction from 500 to 15 ppm [35] . These changes, together with more restrictions on power plants and industry, led to a decrease in the Ontario average annual ambient concentration of sulfur dioxide (SO 2 ) from approximately 6 ppb to 2.2 ppb across the 2000-2013 period [36, 37] . The step-wise decrease in ambient SO 2 levels observed in Oakville appears to be in addition to the background decrease in SO 2 levels associated with the reduction in allowable sulfur content in gasoline, as seen in ambient concentrations of nearby municipalities.

Regarding other commonly available air pollutant measures, NO 2 concentrations have also been previously linked to morbidity [33, 34] ; however, examination of the present data revealed neither a significant decrease in 2001-2003 such as was observed in SO 2 with the roll-out of the Canadian Environmental Protection Act (1999) [35] regulations, nor any sudden decrease in Oakville levels at or around the closure of the refinery. As NO 2 is most commonly linked to automobile traffic, this fits available evidence, supporting the NPRI records which indicate that NO 2 was not a significant emission from the refinery.

While the Census Division of Halton (which contains CSD Oakville) experienced rapid population growth through the 2001-2011-time period, much of the growth occurred in newly-built subdivisions well north of Lake Ontario, away from the refinery and north of the highway system [22, 25] . The area directly co-located with the refinery saw very little population growth (0-3%) during this time. In addition, the age distribution skewed toward retirees through this period, with the demographic profile aging over the decade; much of this aging should have been compensated for by the standardization of hospitalization records. Any residual effect not compensated for by this modeling (due to population demographics being temporally coarse owing to census patterns) should have been in the direction of an aging population, which should result in increased levels of hospitalizations rather than the opposite.

In conclusion, despite limitations in data availability for the region and the study, we have demonstrated measurable reductions in ambient SO 2 levels as would be experienced by the residents of Oakville. We similarly demonstrated a sharp decrease in cold-season age-standardized hospitalization rates, which occurred immediately following the refinery closure. No other contributory factor was found which could explain this decrease. Thus, we present in this study another piece of evidence in the field, showing concurrent measurable reductions in both SO 2 concentrations and respiratory-related hospitalizations when a local emission source was removed.

This study demonstrated improved air quality and population health ameliorated in relation with the closure of an oil refinery. As a natural experiment, it provides evidence supporting a link between refinery emissions and adverse health effects.

Supplementary Materials: The following are available online at http://www.mdpi.com/1660-4601/15/9/2029/ s1, Figure S1 : Oakville seasonal (3-month blocks) non-age-standardized total respiratory hospitalizations from 1996 through 2010; Figure S2 : Oakville seasonal (3-month blocks) year-by-year age-standardized total respiratory hospitalization rate from 1996 through 2010, in 1000-persons per year. Standardization done against Oakville population for given year (evolving over time); Figure S3 : Oakville seasonal (3-month blocks) population age-standardized total respiratory hospitalization rate from 1996 through 2010, in 1000-persons per year.

Standardization done against a fixed population distribution for the GTA in 2006, a census year; Figure S4 : Toronto seasonal (3-month blocks) total non-age-standardized total respiratory hospitalizations from 1996 through 2010; Figure S5 : Toronto seasonal (3-month blocks) year-by-year age-standardized total respiratory hospitalization rate from 1996 through 2010, in 1000-persons per year. Normalization done against Toronto population for given year (evolving over time); Figure S6 : Toronto seasonal (3-month blocks) population age-standardized total respiratory hospitalization rate from 1996 through 2010, in 1000-persons per year. Standardization done against a fixed population distribution for the GTA in 2006, a census year; Figure S7 : GTA seasonal (3-month blocks) non-age-standardized total respiratory hospitalizations from 1996 through 2010; Figure S8 : GTA seasonal (3-month blocks) year-by-year age-standardized total respiratory hospitalization rate from 1996 through 2010, in 1000-persons per year. Standardization done against the current GTA population for given year (evolving over time); Figure S9 : GTA seasonal (3-month blocks) age-standardized total respiratory hospitalization rate from 1996 through 2010, in 1000-persons per year. Standardization done against a fixed population distribution for the GTA in 2006, a census year; Table S1 : yearly mean, standard deviation and maximum counts (with daily resolution) for all-cause, circulatory and respiratory hospitalizations for the GTA, a region encompassing Toronto, Ontario as well as a number of other co-located cities. 

",0.6375757882813171
Pandemic influenza control in Europe and the constraints resulting from incoherent public health laws,"Background: With the emergence of influenza H1N1v the world is facing its first 21 st century global pandemic. Severe Acute Respiratory Syndrome (SARS) and avian influenza H5N1 prompted development of pandemic preparedness plans. National systems of public health law are essential for public health stewardship and for the implementation of public health policy [1] . International coherence will contribute to effective regional and global responses. However little research has been undertaken on how law works as a tool for disease control in Europe. With co-funding from the European Union, we investigated the extent to which laws across Europe support or constrain pandemic preparedness planning, and whether national differences are likely to constrain control efforts.","Emerging infectious diseases pose global challenges to human health protection. SARS and the emergence of avian influenza H5N1 galvanized political and public health communities to strengthen international, national and local preparedness and response capacities, and the emergence of H1N1 influenza has tested those responses. The International Health Regulations 2005 (IHR) represent an important international commitment to strengthening global capacity and acknowledge that law is part of the public health armamentarium underpinning cooperative national and international responses [2] . Whilst global disease surveillance capacity, in particular, has been considerably strengthened through the IHR, management of disease outbreaks, including pandemics, remains grounded in notions of national sovereignty. The same can be noted with regard to European co-operation, where regulations concerning surveillance and early warning are drafted with full respect for domestic law [3] [4] [5] [6] . Harmonization of public health laws is not considered to be within the competence of the EU [7] . National systems of public health law are essential for influenza pandemic control, and international coherence will contribute to effective regional and global responses [1] .

We report here results from a three year study analysing whether public health laws across the European Union, Croatia, Turkey, Iceland, Liechtenstein and Norway are 'fit for purpose', whether they are coherent with states' strategic national preparedness plans, ways in which laws differ and whether these differences are likely to be important from a public health perspective.

The research focus was all European Union countries and the neighbouring countries of Croatia, Turkey, Iceland, Liechtenstein and Norway.

A scoping exercise identified thematic issues. We conducted a literature review including reports from pandemic influenza simulation exercises, national strategic and operational plans [8, 9] , regional preparedness documents and research publications on public health law in Europe. The review was limited to documents in French and English that were available in the public domain between 2003 and 2007. With support from a panel of public health and legal experts key intervention themes were identified that were linked to World Health Organization (WHO) phases (table 1) [10] .

Data collection was two staged:

In consultation with public health and legal experts and WHO, a self-administered questionnaire was developed, piloted and finalised. The questionnaire addressed the temporal phases and public health interventions that were identified through the scoping exercise. It consisted of 114 open-ended and closed-ended questions that were framed around an emergent influenza pandemic scenario (see additional file 1). Respondents from each country with legal and public health expertise were identified through WHO, ministries of public health, screening of pertinent publications, and through the European Public Health Law Network website developed within the project to facilitate communication between experts with an interest in public health law http://www.ephln.org. Questionnaires were sent by email with follow up reminders to ensure a high response rate. On receipt of the completed questionnaires, and where state pandemic plans were available in English or French, we compared the questionnaire results of each state with measures proposed in the pandemic preparedness plan of that state.

Identification of country experts Participants with expertise in law and public health were identified through the membership base of the project network, European Public Health Law Network http://www.ephln.org. In addition, participants were identified through contact with ministries of health, academic institutions, professional networks, announcements at conferences, and searching journals to identify authors with relevant expertise. The persons who completed the questionnaire were in all cases except one, the persons who attended the review workshops. Workshops Four workshops were held (London, Toulouse, Prague and Lisbon), with a participant from each state attending one workshop. Building upon information provided through the questionnaires, we explored how national state laws might assist or constrain public health interventions in the context of the influenza pandemic scenarios previously described through the survey instrument and also, with the emergence and global spread of H1N1v in early 2009, how contemporary national responses were impacted by current national laws. Respondents then analysed how differences between national legal systems and between specific laws might support or hinder regional responses to pandemic influenza, and the impact of national laws in the following areas:

• the movements of information, goods, services and people across borders in a time of pandemic;

• the capacity for surveillance, case detection, case management and community control;

• the deployment of strategies of prevention, containment, mitigation and recovery;

• the identification of commonalities and disconnects across states.

Workshops were conducted under 'Chatham House rules' to encourage openness and the sharing of information between speakers whilst preserving anonymity in the reporting of the results. Workshops were audiorecorded and transcripts were subsequently sent back to participants for verification.

As noted, we identified a priori emergent themes through the scoping exercise. Data from questionnaires and workshop reviews were organised according to the analytical categories. We adopted a 'framework' approach to analysis [11] , consisting of five interconnected stages: familiarisation; identification of a thematic framework; indexing; charting and mapping; and interpretation.

Not required.

The questionnaire was completed by participants from 23 states (Austria, Belgium, Bulgaria, Croatia, Cyprus, Estonia, Finland, France, Germany, Hungary, Iceland, Ireland, Latvia, Lithuania, Malta, the Netherlands, Norway, Poland, Portugal, Slovakia, Slovenia, Sweden and Turkey). All states have laws addressing the prevention and control of communicable diseases. In nine states there are also emergency powers provided in legislation that include a pandemic in the definition of an emergency, whilst fourteen states will limit interventions in a pandemic to those provided for by public health laws. Six of the Schengen states' plans consider a pandemic to constitute a serious threat to public policy or internal security to justify reintroduction of internal border controls. The Schengen Treaties (1985) on free traffic of persons, since 1997 incorporated into EU Law, are applicable in 26 countries, including the non-EU countries Norway, Iceland and Switzerland, but excluding the UK, Ireland, Bulgaria, Cyprus and Romania.

Ten states have laws that would authorise border closure in a pandemic. Table 2 provides some examples of the range of measures provided across those states for which we have completed questionnaire results:

The organizational structure of governments differs across European states as a consequence of cultural factors and differences in legal system. Seven state rapporteurs in the project responded that their state operated on the basis of a federal or quasi-federal system: Austria, Belgium, Germany, Portugal, Spain, Sweden and the United Kingdom. See Table 3 .

In follow up research, the national pandemic plans of the five federal or quasi federal states where plans were available in English were analyzed, examining the distribution of pandemic planning roles and responsibilities between national and regional governments. See Table 4 .

We were able to undertake a preliminary analysis to compare laws and plans in 11 states where plans were available in English or French. In only two of these states were all the measures proposed in plans supported by specific legal authorisation. In some states such as Belgium and France, however, there are general powers enabling any proportionate measure for a public health purpose. In other states, such as Ireland, there is a broad power to make regulations to prevent the spread of infectious disease and to treat people suffering from infectious disease that could authorise measures such as compulsory examination and treatment. Examples of proposed pandemic measures not supported by law included the use of unlicensed drugs, vaccines and health workers, closure of borders, control of media information and reporting, obligation of workers to work where there was risk to health and safety, compulsory isolation, quarantine, vaccination and prioritisation of people in access to vaccines and antivirals. Plans of some states, for example Estonia, recognised the need to ensure a legal framework for measures while plans of most states made no mention of legal underpinning.

Twenty-four countries were represented at review workshops (Austria, Belgium, Bulgaria, Croatia, Cyprus, Czech Republic, Estonia, Finland, France, Germany, Hungary, Iceland, Ireland, Latvia, Lithuania, Malta, the Netherlands, Compulsory treatment 17

Provision of healthcare to an EU national resident in their state 18

Provision of healthcare to a visitor from an EU member state 18

Provision of healthcare to a visitor from outside Europe 16

Authorise unlicensed staff to be requisitioned to perform medical acts in a pandemic 7

Obligation of a worker to work in a pandemic 7

Requisition of goods 14

Compensation authorised for requisition of premises 10

Prohibition of mass gatherings 20 *This figure includes those states that intend to use this measure for 'listed' diseases. Even where influenza is currently not in the 'listed' category, the process of listing is simple and the intention is for the power to apply to pandemic influenza, once listed. [12] , and this was explored in the workshop discussion. Some states have laws, such as those in England and Wales and in Estonia, that are detailed and prescriptive. These laws tend to rely on lists of notifiable diseases, and public health measures can only be undertaken in relation to listed diseases. In other states public health laws are broadly framed allowing for the significant exercise of discretion. In Cyprus there are no specific laws governing measures such as quarantine but the Council of Ministers has powers to take all necessary measures. New laws in France allow the Minister of Health to take any measures that are proportionate to protect the public health.

Where laws give broad discretion to act, principles such as those of proportionality or precaution are likely to form part of the law. In Slovenia the principle of proportionality has been written into the Constitution and has since been developed by the Constitutional court. The 2004/2005 Constitution in France incorporates the precautionary principle. The new legislation of the Netherlands is built upon the principle of precaution requiring a risk assessment before measures are taken.

States are at different stages of development of communicable disease legislation. Some states are reliant on nineteenth century laws that have undergone some updating for IHR compliance. Other states have new Mixed -response between National and Regional * The rapporteur representing the United Kingdom did not return the completed questionnaire, so questionnaire results do not include information on devolution of powers in the UK. We did however include the UK in our own follow-up research.

laws designed with pandemic diseases in mind. Some states operate their public health laws at national level while in others with devolved systems regional laws are more important. In most states, laws relevant to pandemic influenza are contained in dedicated public health laws, but in some states other legislation and case law must also be included in the body of disease control law. In some states (for example, Ireland and Belgium) the Constitution limits measures that can be taken, providing greater opportunities for individuals to challenge public health interventions. In all states the European Convention on Human Rights (ECHR) is considered to limit the measures that can be taken under public health legislation, though not necessarily measures taken under emergency powers legislation. Across states there are differing approaches to the status of pandemic preparedness plans. In Belgium and Slovenia, pandemic preparedness plans have the force of law in terms of both public perception and practice, whereas in other states measures proposed in the plan depend on legal underpinning. The relationship between laws and pandemic plans was not clear in many states. Comments by participants included, 'the (survey) questions were difficult to answer because of the lack of clarity between laws/decrees and the national preparedness plan'; 'the national preparedness plan was drafted without legal implications in mind at first'; and 'day to day practice is not aligned with the legal framework'; 'there is a gap between theory and practice. If there were a challenge to an exercise of a power, the court is likely to rule that it was up to the government to decide...'; 'There is a lack of clarity on competences and responsibilities, and the relevant bodies in charge.' In some states it was considered that legislation was more developed than the preparedness plan, and in others that the plan was well defined but that underpinning public health legislation was not sufficiently developed.

There are also differences in the extent to which emergency powers can be drawn upon in a pandemic. While emergency powers in France and Belgium apply only to war, in Slovenia, Hungary, Latvia and Lithuania emergency legislation specifically provides for communicable diseases and there are also emergency powers provisions in the contagious diseases legislation. In the Netherlands there is emergency powers legislation applicable to a pandemic, in exceptional circumstances and on a decision of the cabinet. In Estonia, two statutes provide emergency powers. Both apply to a pandemic and it is envisaged that they will be used. In the UK civil contingency powers play a role in pandemic strategy. The intention to use emergency powers is significant in that emergency powers may allow for greater intervention, with possible derogation from the European Convention on Human Rights [13] .

Discussion in the workshops was facilitated in accordance with the predetermined themes.

There is no common approach across the states represented as to the control of movement of people. Some states, such as Sweden and Ireland, envisage no restrictions. States such as Cyprus that are not signatories to the Schengen Agreement have the option to close borders against travellers from within Europe, and other states such as Slovenia and Estonia have emergency powers applicable to pandemic disease which might authorise border closures.

There are laws in some states to authorise restrictions on movement of goods, mostly on incoming rather than outgoing goods. Legislation in Malta and Cyprus allows for the stopping of movement of goods in and out of the country on public health grounds.

In relation to information, most states have incorporated the EU legislation on Data Protection [14] . Some states, Sweden for example, have passed laws to authorise the provision of public health data to public health authorities both within the state and to the EU and WHO.

In relation to surveillance of influenza, there are variations in duties of notification across states. Most states have laws imposing on health professionals duties to disclose to specified public health authorities information on suspected or confirmed cases of disease. However in some states these laws may only come into play where the disease is listed as notifiable (for example in England and Wales, Ireland and Estonia).

There are differences in powers of compulsory screening and medical examination across states. While some states' laws authorise powers of compulsory treatment and compulsory vaccination, these measures are prohibited by laws in other states. In most states there is a capacity for community control such as prohibition of gatherings and school closures, specifically provided by communicable disease or other legislation such as education laws, or possibly at the discretion of a public health authority or by means of exercise of emergency powers.

There are differences in accessibility to healthcare resources across European states, with some states prepared to provide healthcare to non-citizens, such as Lithuania, where undocumented immigrants have access to free necessary healthcare. While some states, such as Malta, intend to continue to allocate healthcare on clinical grounds, where resources are limited states may use emergency powers to control movements of goods and people so as to prioritise state citizens in the allocation of health benefits.

Pandemic plans set out strategies of prevention, containment, mitigation and recovery but workshop participants noted an insufficient link between pandemic plans and public health laws, potentially limiting the effectiveness of those strategies. Interventions proposed in some plans lack legal underpinning. Comments of workshop participants included, 'the preparedness plan mainly addresses health services. What is lacking is a link with the public authorities responsible for handling the pandemic'; 'public health authorities responsible for handling an outbreak have little knowledge about their role and how to prepare for it. There is confusion about who is competent'; 'there is a gap between the content of the preparedness plan and their awareness of their responsibilities'; 'there is a gap in planning in relation to coordination. For example people are not clear on their role in quarantine'; 'it is not clear how to implement some compulsory measures decreed by the Minister such as mask wearing in public. How do we oblige people to respect and comply with these measures?' and 'the infectious disease legislation gives some powers to the Minister to enforce the regulations but this is not actually done.' It was noted in relation to one state where laws gave powers to a public health authority: 'but this is premised on the assumption that they know what to do. It would be better if there were some directions/legal framework'.

Even where plans and laws are in place, clarity is lacking regarding which body is responsible for specific interventions and as to the organisation and management of pandemic planning. It was commented in the workshops that 'preparedness plans seem to assume that powers already exist', and 'it seems to have been assumed that the IHR 2005 would have direct effect, like a treaty. Some officials think the IHR 2005 are sufficient on their own to be considered as law, which is obviously not the case.' Several participants identified problems arising from a lack of expertise in public health law in their state, inhibiting implementation of pandemic planning. The lack of public health law expertise was even more significant in states where public health powers were not detailed in legislation. It was agreed by workshop participants that there is a need for education, training and research on the role of law in public health in Europe. One workshop participant commented, 'There is a definite need for public health law training for medical personnel and there are not enough courses available', and another, 'Health professionals do not have a sufficient understanding of how government departments such as the Ministry of Health are organised.' Other comments included, 'There is a need for more public health law expertise, as this issue is not well addressed within Europe'; 'There is little training in public health law. Law faculties and medical faculties keep very separate'; and 'Public health practitioners are not aware of public health laws.'

Disease notification duties were generally common across states. Although not all states specifically required notification of influenza, there were powers to make influenza notifiable. Commonality in notification duties is not surprising, despite the differing systems of disease notification across states, as the IHR along with WHO and EU surveillance systems require harmonised reporting of pandemic disease notification. There was some commonality across states in powers of social distancing, although again these were not always specified in laws and in some states would require authorisation by a political or public health authority. There was less commonality in relation to powers of compulsory screening, examination, vaccination and treatment. While some states' laws authorise compulsory vaccination and treatment, other states' laws prevent vaccination or treatment without consent. In Lithuania a person can only be detained for the purposes of treatment, with a focus on individual benefit rather than public health, while in other states, such as England and Wales, a person can be detained but cannot be compulsorily treated. There were also differences in access to healthcare and in the obligation of healthcare workers to work in a pandemic. Project participants were concerned by the consequences of these differences for the movement of persons across Europe.

A majority of participants suggested a need for greater guidance in the management of a disease pandemic. It was commented that 'a European response would be much more practical and easier, rather than states making their own decisions'; 'it would be good to have uniform guidelines to avoid medical tourism, for example', and 'if you have designated points of entry under the WHO, it would be helpful to have EU advice and commonality on the understanding of what is a designated point of entry. If people are coming from outside the EU and then moving around within the EU, it would help to have some commonality of rules'. One participant noted, 'there are economic considerations. Individual states are reluctant to be the first to take measures such as contact tracing, for economic reasons. It would be better if states decided together'. Others noted, '-It would be better to have laws harmonised across Europe. There ought to be better co-ordination of plans, however, countries should not be forced to follow others' plans'; and 'Politically the EU is trying not to be too interventionist. But we do need some coordination as there are no border controls. We also need coordination on issues of distribution of resources across Europe'; 'The EU should keep states posted on changes in the legal environment'; and 'There is a need for coordination and guidance from the EU in terms of management, but this may be difficult, thinking of refugees'. But further EU involvement would not be useful for all states: 'We have no expertise in public health law. But the public health specialists are knowledgeable on the relevant law. As to EU involvement, WHO and ECDC are more helpful... than the EU'.

In a democratic system that recognises international and European human rights conventions, interventions that infringe liberties must be enshrined in law. While public health policy and pandemic planning might propose measures beneficial to the public health, those measures cannot be applied without legal underpinning. The results of this study show differences across European states in the extent to which national pandemic policy and pandemic plans have been integrated with public health laws. There are differences in the legal status of pandemic plans; in some states plans have for all practical purposes the status of law, and in others plans have no legal authority.

A consequence of the disconnect between plans and laws is lack of clarity as to the responsibility and competence of public health authorities. Only in two states were lines of command thought to be clear, coordinated and detailed. One role of law is to provide an inviolable framework for policy and the application of powers. Where pandemic plans have been prepared independently, and in ignorance or neglectful, of law, and where the legal framework has not been updated to reflect plans, then confusion is inevitable. A common theme in workshop discussions was the lack of clarity within states and across states as to the authority responsible for the management of different aspects of disease control. Authorities in one state are not always clear which body is their equivalent in other states, and whom to contact. This is a particular problem in states where powers are devolved to regional or local levels. For example in Sweden, where the system is decentralised and law places responsibility at regional/local level, there are 21 regional preparedness plans with their own responsible public health bodies. Further work needs to be undertaken on making clear pandemic responsibilities and competences. This is an issue of concern, given the lead-in time we have been given for a pandemic resulting from SARS and avian influenza H5N1. It is hoped that the H1N1v pandemic, a pandemic associated with limited morbidity and mortality, will provide an opportunity to establish responsibilities and hierarchies in management.

While some states have passed new public health legislation addressing contemporary understanding of public health risks, many states have public health laws that originate in the nineteenth century. In some cases attempts have been made to amend laws in recognition of IHR obligations and pandemic planning, without addressing the outdated science and jurisprudence that underlay old legislation, resulting in an inaccessible collection of uncoordinated and unconsolidated laws. Lack of public health legal expertise across Europe compounds this inaccessibility. While rapporteurs reported an intention by their states to make national laws compliant with the IHR by 2012, there appears to be insufficient understanding in many states of the role of the IHR, of what is required in the way of compliance in Annex 1 of the IHR and of the relationship of the IHR to state plans and laws, despite guidance on legislative compliance provided by the WHO[15]. Our results suggest that lack of understanding of laws by persons working in public health has contributed to lack of coherence between the IHR, plans and laws in some states. As has been noted elsewhere[16] the IHR (2005) like its predecessor regulations, relies on nonbinding recommendations and guidance ('soft law') rather than on legal obligations, such that compliance by signatory states cannot be assured.

In some states laws that might be needed in a pandemic are not yet drafted, and the intention is to draft these laws when they are needed. It is essential that laws authorising public health powers be passed and disseminated in advance of a pandemic and that reactive laws are not drafted in times of crisis. In states where public health law is broadly framed leaving much to the discretion of public health authorities, and in states where plans rely on the exercise of emergency powers, hurriedly drafted laws might be difficult to challenge on constitutional and human rights grounds.

Workshop discussion suggested that there is an argument for greater involvement of the European Union in the management of pandemic disease, in the form of recommendations and guidelines. Participants argued for more input on disease management from the EU. There was concern that with disparate laws, states will respond differently in their preparedness to carry out measures such as contact tracing or the passing on of information on travelers. There is also discrepancy in healthcare resources across states within Europe, and a lack of clarity on strategies to cope with consequent movements of populations seeking care. Different approaches to access to healthcare across Europe might result in movement of populations.

The project, scenario and questionnaire were designed with H5N1 influenza in mind, and the first two workshops considered plans and laws on this basis. However by the time of the two final workshops, the H1N1 pandemic was well established, altering the focus of some aspects of the discussion. For example in the second two workshops, participants were concerned about the feasibility of legal reporting obligations where disease was so widespread, and possible conflict between national laws on disease reporting and WHO advice. The emergency of pandemic H1N1 halfway through the study provided useful data on the practical application of plans and laws and on amendments to plans and laws necessitated by the experience of a pandemic. Discussants in the final two workshops had the advantage of assessing the application of plans and laws in that context.

While some of the experts representing states were government appointed or recommended, some were selfselected. The appropriateness of these experts was verified by means of their publications and their professional history. The challenge of finding experts is in itself an important finding of the study, highlighting the need for capacity development and coordination in this field.

Some workshop participants were trained in law, some in public health and some in both. The differing language of the disciplines of law and public health resulted in different interpretations of the questionnaire questions and different understandings of some legal terminology.

We were unable to recruit participants from seven states, primarily because we were unable to identify persons with appropriate public health law expertise in those states.

We were reliant on the expertise and knowledge of the state rapporteurs for information on their states. Where possible we cross checked responses but in many instances this was not possible as laws and pandemic plans were in languages in which we have no expertise.

The scope of the project focused on the extent to which national laws supported or constrained the implementation of pandemic preparedness plans. It was not an objective of the project to evaluate national public health legislation against the International Health Regulations (2005), although issues in relation to IHR compliance emerged in workshop discussion. Such an evaluation would be a worthwhile area of further research.

There are significant differences in legislation and in the legitimacy of conduct across states in Europe. In some states pandemic plans are part of the law and in others not. In some states disease measures are clearly specified in advance, and in others measures are to be determined where the need arises. States differ in the range and the nature of intervention measures authorized by law, the extent to which borders will be closed to movement of persons and goods during a pandemic, and access to healthcare of non-resident persons. Some states propose use of emergency powers that might potentially override human rights protections while other states propose to limit interventions to those authorized by public health laws. These differences could create a problem for European strategies.

Differences across Europe in legal systems, in the breadth in which public health law is framed, in the level of discretion given to ministries and public health authorities and the extent to which emergency powers are to be used in a pandemic, mean that comparison and evaluation of efficacy of laws across European states is a difficult task. The results of our research suggest that states would welcome further guidance from the EU on management of a pandemic, and guidance to assist in greater commonality of legal approaches across states. There will be ramifications of incoherence of laws across states for movement of populations, transportation of drugs, access to healthcare and for human rights and data protection. There is a need for further analyses to determine the public health implications of differences in laws, and whether regions beyond Europe are more coherent in their legal responses to pandemic influenza.

There is a dearth of expertise and training in public health law across Europe. Most pandemic planning in Europe is undertaken by public health practitioners with no input from persons with expertise in law, and workshop results suggest that there is limited understanding of the relationship between law and public health practice in the management of disease prevention and control. This suggests an urgent need for improved training in public health law in both the law and healthcare sectors.

PHLawFlu was co-funded by the European Union. The funder had no role in the design, analysis, or interpretation of the study. The views expressed are those of the authors and do not necessarily reflect the position of the funding body.

Additional file 1: Questionnaire consisting of open-ended and closed-ended questions addressing national laws in relation to public health interventions in an influenza pandemic.

",0.6372452458413056
"Capacity of Public Health Surveillance to Comply with Revised International Health Regulations, USA Author affi liations: Centers for Disease Control and Prevention","Public health surveillance is essential for detecting and responding to infectious diseases and necessary for compliance with the revised International Health Regulations (IHR) 2005. To assess reporting capacities and compliance with IHR of all 50 states and Washington, DC, we sent a questionnaire to respective epidemiologists; 47 of 51 responded. Overall reporting capacity was high. Eighty-one percent of respondents reported being able to transmit notifi cations about unknown or unexpected events to the Centers for Disease Control and Prevention (CDC) daily. Additionally, 80% of respondents reported use of a risk assessment tool to determine whether CDC should be notifi ed of possible public health emergencies. These fi ndings suggest that most states have systems in place to ensure compliance with IHR. However, full state-level compliance will require additional efforts.","T he 2005 revisions to the International Health Regulations (IHR 2005) were a major global policy achievement to ensure international recognition and notifi cation of unusual public health events. These regulations are an international legal instrument that binds 194 countries (World Health Organization [WHO] member states). The goal of IHR 2005 is to help the international community prevent or respond to acute public health risks that have the potential to cross borders and threaten the global population. As seen with the emergence of pandemic (H1N1) 2009, diseases have the potential to spread quickly around the globe through international travel and trade (1) . Member states are required to report certain diseases and public health events to WHO. Furthermore, the rights and obligations of member states are defi ned to establish procedures that WHO must follow to uphold global public health security (2) .

The 2005 revision was the fi rst major update to the IHR since 1969 and was designed to refl ect trends in disease emergence and spread over the past several decades. The 2005 revision also was meant to unify the considerable changes in communication capacity, disease surveillance, and investigation infrastructure. Member states must report potential public health emergencies of international concern (PHEIC), including those of biologic, chemical, radionuclear, or unknown origin, to WHO. A common decision matrix that focuses national reporting around a risk assessment process is used rather than sole reliance on reporting of specifi c diseases or incidents (3). This method of reporting requires all member states to develop, strengthen, and maintain a core set of public health surveillance and response capacities at the local, intermediate, and national levels (4).

After IHR revision in 2005, member states were provided a 2-year window in which to assess their surveillance and response capacities, focusing on 4 necessary characteristics of surveillance systems: timeliness, sensitivity, stability, and usefulness (3) . Each of the 194 member states also was required to designate a National Focal Point that would assess any event within 48 hours. After the assessment specifi ed in Appendix 2 of IHR 2005, each member state must notify WHO of any potential PHEIC. Therefore, core public health surveillance systems at local and national levels must be capable of ensuring national awareness of incidents in a timely manner.

Surveillance systems exist at many levels: clinics; hospitals; and local, state, national, regional, and global levels. To be effective, these different levels must be well integrated. Literature on surveillance systems often examines each level separately or, at most, the links between 2 adjacent levels, describing the importance of integrating each system so that communication between levels is more effective (3, (5) (6) (7) . State-to-national notifi cation is a key aspect of federalist systems and has been viewed as a key challenge for countries with this type of government structure (3); several authors have noted the political and practical diffi culties these surveillance systems may face and the various ways national disease surveillance can be facilitated (8) .

In the United States, reporting of nationally notifi able diseases to the Centers for Disease Control and Prevention (CDC) by states is voluntary. Public health surveillance takes place within a state on the basis of reports received from a variety of sources, typically local. Reporting is mandated by state legislation or regulation. States then determine whether CDC should be notifi ed. Notifi cations from states, territories, and the District of Columbia are collected and analyzed by the National Notifi able Diseases Surveillance System. A 2004 review of this system showed that for meningococcal disease laboratory results, local entities reported to their states and then states notifi ed NNDSS within 2-117 days (5) . More recently, 60% of meningococcal diagnosis reports were received by states within 1 day after diagnosis (9) .

The literature identifi es essential elements that surveillance systems need to meet IHR 2005 criteria, including electronic information systems and supportive infrastructure, to ensure timely reporting to the National Focal Point (6, 7) . In addition, intergovernment cooperation with both formal and informal communication, from the local to the international level, are essential aspects of successfully functioning public health surveillance systems (3, 6, 8) and can ensure rapid reporting of incidents before laboratory confi rmation is received (3, 6) . Overall, key aspects of successful surveillance systems identifi ed in the literature align closely with the requirements of IHR 2005. This alignment suggests that systems built in accordance with the agreement will provide successful global coverage.

Since the new regulations took effect, no studies have been done to determine the timeliness of reporting conditions specifi ed in IHR 2005. In addition, no reports exist that discuss timeliness of notifi cation to CDC about unusual cases or outbreaks of unknown cause. To address these gaps and to determine the ability of states to comply with IHR 2005, we assessed state surveillance capacities through surveys completed by the Council of State and Territorial Epidemiologists (CSTE). This assessment examined several key requirements that are necessary to effectively meet national responsibilities and ensure compliance with IHR 2005.

In February 2009, CSTE electronically distributed a structured, self-administered questionnaire to state epidemiologists in all 50 states and Washington, DC. Responses were made anonymous at the time of data analysis. The questionnaire was designed to address the following questions: 1) How are states able to determine the status of potential public health emergencies? 2) Are local health departments able to report in a timely manner to the correct point-of-contact (POC) in their state? 3) Are states able to notify CDC of public health emergencies in a timely manner (i.e., within 24 hours)? 4) Do states support or implement other control measures (i.e., collaboration with other departments or cross-jurisdiction)? Frequencies and percentages were used to describe the results.

The CSTE State Reportable Conditions Assessment, completed by state epidemiologists, also was used to assess state reporting practices. Information was obtained from the 2007 Assessment, which is available for review by all states and territories (10) .

A total of 47 (92%) of the 51 eligible jurisdictions responded to the questionnaire. Eighty percent of respondents reported the use of risk assessments to determine the necessity of notifying CDC about unusual or unexpected events ( Figure 1 ). Of those who used risk assessments, ≈50% used them to initiate formal investigations. About 50% of respondents reported the use of risk assessments to evaluate whether notifi cation to the state health offi cer (51%), chief emergency response/management offi ce (47%), and CDC or other federal entities (51%), respectively, was necessary. Twentyeight percent reported use of a state-based algorithm; 25% reported use of the WHO algorithm for risk assessments. The remainder of respondents used another algorithm (excluding a state-based algorithm or the WHO algorithm) or were not sure of the algorithm used in their state.

More than 90% of jurisdictions required reports of suspected and probable cases of the 4 immediately notifi able IHR 2005 conditions (i.e., smallpox, poliomyelitis caused by wild-type poliovirus, human infl uenza caused by a new subtype, and severe acute respiratory syndrome [SARS]) within 24 hours after diagnosis (Table 1 ). In addition, 96% of states reported that they would notify CDC of suspected and probable cases of IHR 2005 conditions within 24 hours (Table 1) . Eighty-one percent of respondents reported having the capacity to transmit daily notifi cations to CDC. Of those unable to transmit daily notifi cations, 5 indicated that they would be able to provide daily electronic data to CDC within <1 year All respondents reported they would either always or sometimes notify CDC of an unusual or unexpected case or outbreak; 60% reported they would always notify CDC within 24 hours (Figure 2 ). Among the respondents, 30% would sometimes notify CDC within 24 hours. According to the 2007 State Reportable Conditions Assessment, 48 of the jurisdictions included unusual or unexpected events on their reportable conditions lists. Furthermore, most states could identify a specifi c POC in their state for reporting various public health events and emergencies (Figure 3 ). Ninety-one percent of states reported having a designated POC for zoonotic, foodborne, and infectious events; for chemical and radiologic events, 84% and 86% of states, respectively, reported having a designated POC.

More than 50% of respondents reported having formal information-sharing systems or mechanisms pertaining to emergencies or outbreaks with state law enforcement, emergency management and homeland security, agriculture, environmental protection, and fi sh and wildlife agencies, excluding the state departments of transportation (37%) ( Table 2 ). Fifty-one percent reported participating in cross-jurisdiction electronic surveillance and having reporting systems for foodborne and infectious diseases with neighboring states.

Most states can successfully conduct public health surveillance in compliance with IHR 2005. However, full state-level capacity for compliance was not found on any assessment response. Additional efforts are needed to ensure the ability of the United States to meet its IHR 2005 obligations.

Most states reported use of risk assessments to determine the need to notify CDC about unusual or unexpected events. In addition, ≈50% of states reported the use of risk assessment when initiating a formal investigation. IHR 2005 emphasizes the use of risk assessments to notify WHO about public health emergencies, rather than about specifi c events. The use of risk assessments in notifying CDC can help determine whether notifi cation is necessary and ensure timely notifi cation without waiting for laboratory confi rmation.

Most (>80%) states reported having 1 POC for reporting chemical, radiologic, foodborne, infectious, and zoonotic events. All states should have a POC to facilitate prompt assessment and appropriate reporting. Such contacts also can assist in assessing events outside their areas of expertise or with unknown cause.

Reports of capacity to transmit daily notifi cations to CDC suggest that data collection and transmission capacity has improved substantially in recent years. The ability to transmit reports to CDC is a critical function, which allows for national situational awareness in high-profi le events and public health emergencies.

Not all respondents included unusual or unexpected events on their state's reportable conditions lists. Adding this criterion would help ensure that conditions having the potential to become public health emergencies can be recognized and reported in a timely manner.

Internal and external relationships of each state can play a role in reporting. Most states have formal information-sharing systems with other agencies within their state. Such dissemination of information within a state increases the likelihood that IHR reportable events are appropriately evaluated and reported. Other agencies may have knowledge that could be incorporated into a risk assessment.

About 50% of respondents reported participation in cross-jurisdictional electronic surveillance and having reporting systems for foodborne and infectious diseases. Public health surveillance across state jurisdictions is as important as the sharing of information within a state. Neighboring states can be alerted to public health emergencies that have the potential to become widespread or even global. Our assessment has several limitations. First, only states were assessed; neither territories nor local health departments were included. Circumstances are suffi ciently different in the territories; no conclusions about their capacity should be drawn because they are likely to have different reporting practices and capacities. Future assessments should focus on identifying and documenting capacities for IHR 2005 compliance in at least a sample of jurisdictions at the local or county level. Second, our assessment relied on self-reporting, which in some instances may have led to inaccuracies or bias. Data were made anonymous before analysis so that states would not be singled out as having suboptimal reporting practices or capacities. Therefore, we could not inquire about potential discrepancies.

To enable the United States to fully meet its IHR 2005 obligations, all states should include unusual or unexpected events or outbreaks on their state lists of reportable conditions. In addition, states and CDC should work toward further development of the nationally notifi able conditions list and the timeframes for reporting. The capacity to transmit records to CDC on a daily basis is key to full compliance with IHR 2005. Risk assessments of unusual or unexpected events should be performed to determine whether they meet requirements for notifi cation to CDC as a potential PHEIC. Performing such risk assessments will enable timely notifi cation to CDC, even before laboratory confi rmation. Furthermore, state POCs are likely to facilitate recognition and reporting of potential public health emergencies within their respective states.

State health departments should work to ensure that their counterparts in state government and in local health departments understand the requirements of IHR 2005; reporting exercises may help accomplish this goal. All health offi cials, particularly those at the state level, should have a basic understanding of these international regulations, especially the reporting and notifi cation timeframes and practices. Reporting exercises would give state and local health offi cials the ability to assess potential public health emergencies in a practice environment and allow a broader perspective of when notifi cation is necessary.

Expansion of cross-jurisdictional surveillance and reporting systems also would benefi t national recognition and investigation of public health emergencies, especially for foodborne illness and infectious diseases. Such systems are not explicitly required by IHR 2005 but would assist states in the assessment and timely reporting of public health emergencies, both of which are necessary for compliance. National, state, and local government agencies should assist states in implementing these practices and developing appropriate infrastructures. Emerging Infectious Diseases • www.cdc.gov/eid • Vol. 16, No. 5, May 2010 

",0.6371779786046818
Cost-effective length and timing of school closure during an influenza pandemic depend on the severity,"Background: There has been a variation in published opinions toward the effectiveness of school closure which is implemented reactively when substantial influenza transmissions are seen at schools. Parameterizing an age-structured epidemic model using published estimates of the pandemic H1N1-2009 and accounting for the cost effectiveness, we examined if the timing and length of school closure could be optimized. Methods: Age-structured renewal equation was employed to describe the epidemic dynamics of an influenza pandemic. School closure was assumed to take place only once during the course of the pandemic, abruptly reducing child-to-child transmission for a fixed length of time and also influencing the transmission between children and adults. Public health effectiveness was measured by reduction in the cumulative incidence, and cost effectiveness was also examined by calculating the incremental cost effectiveness ratio and adopting a threshold of 1.0 × 10 7 Japanese Yen/life-year.","Results: School closure at the epidemic peak appeared to yield the largest reduction in the final size, while the time of epidemic peak was shown to depend on the transmissibility. As the length of school closure was extended, we observed larger reduction in the cumulative incidence. Nevertheless, the cost effectiveness analysis showed that the cost of our school closure scenario with the parameters derived from H1N1-2009 was not justifiable. If the risk of death is three times or greater than that of H1N1-2009, the school closure could be regarded as cost effective. Conclusions: There is no fixed timing and duration of school closure that can be recommended as universal guideline for different types of influenza viruses. The effectiveness of school closure depends on the transmission dynamics of a particular influenza virus strain, especially the virulence (i.e. the infection fatality risk).

School closure is one of important non-pharmaceutical countermeasures against influenza pandemic [1] . Among various types of school closure, the so-called ""proactive closure"", i.e., the closure of schools before observing substantial transmissions among school children [1] , was conducted in Japan during the early stage of H1N1-2009 pandemic [2] , and micro-clade of the viruses that caused the earliest clusters is known to have declined to extinction [3] . Japanese experience demonstrated that the proactive closure as part of concerted effort of containment measure can be very helpful in achieving the local extinction.

However, there has been a variation in published opinions toward another type of closure, ""the reactive closure"", i.e., the closure of schools when many children, staff or both are experiencing illness, as part of mitigation strategy. Published studies have empirically explored the impact of reactive school closure on an influenza epidemic or pandemic, and some of the studies demonstrated substantial reduction in the rate of transmission among school children during the closure [4] [5] [6] [7] . However, others emphasized that the community impact, e.g. reduction in the demand of healthcare service including hospitalization of severe cases, is likely very limited [8] [9] [10] . Elucidating the details of school transmission mechanism has been ongoing (see Discussion) , and there has been no simple policy (e.g. the timing and duration) to implement the closure in the reactive manner during the course of a pandemic.

If we have a clear quantitative guideline for the reactive closure (e.g. provision of public health conditions at which the closure can be justified and decided), that could greatly benefit public health policymakers. Fundamental insights into the effectiveness of school closure can be gained from a parsimonious mathematical model, exploring possible answers to such key policy questions using simplistic modelling approaches. In the present study, our questions are two-folds. First, we examine when one should close the school during the course of a pandemic. Second, we explore how long the closure should be implemented. Parameterizing the model using published epidemiological estimates of the pandemic H1N1-2009 and accounting for the cost effectiveness of closure, we discuss if the timing and length of school closure could be optimized.

In the present study, the cost effectiveness of school closure is examined using a singlelayer epidemic model. Specifically, we consider an age-structured epidemic model that describes the time-and age-dependent transmission dynamics of influenza [11] . Let j a (t) be the incidence (i.e. the number of new infections) of influenza in age-group a at calendar time t. The renewal process is modelled as

where s a (t) is the fraction of susceptible individuals of age-group a at time t, and A ab (s) stands for the rate of secondary transmission from a single infected individual in agegroup b to susceptibles in age-group a at the infection-age (i.e. the time since infection) s, which may be decomposed as

where R ab represents the average number of secondary cases in age-group a generated by single infected individual in age-group b, constituting a single element of the so-called age-dependent ""next-generation matrix"". g b (s) is the probability density function of the generation time, assumed as dependent on the age-group of primary case. This model can be interpreted as a general representation of the socalled Susceptible-Exposed-Infected-Removed (SEIR) model and its variants in continuous time with a discrete age-structure (e.g. [7, 9] ). Susceptible individuals are depleted as:

where N a represents the population size of age-group a.

Let K be the age-dependent next-generation matrix, [R ab ] which scales the secondary transmission in (2) . In the present study, we consider 3 × 3 matrix, describing within and between group transmissions between/among children, young adults and elderly.

That is, we have

where subscripts 1, 2 and 3 represent children, young adults and elderly, respectively. The basic reproduction number, R 0 , representing the average number of secondary cases produced by a single 'typical' primary case in a fully susceptible population is computed as the largest eigenvalue of the next-generation matrix (4) . During the course of a pandemic, the matrix which describes the age-dependent net reproduction would be scaled by the remaining fraction of susceptibles, s a (t), i.e., K t ð Þ ¼

We assume that K(t) is decomposed into biological part (e.g. those characterizing susceptibility or infectivity) and contact part (i.e. those associated with contact), i.e.,

where α i is a relative susceptibility of age group i, M ij represents the number of contacts that an individual in age group i experiences with individuals in age group j per unit time, and Γ i is the integral of the survival function of infectious period of age group i (Γ i = 1/γ i if the infectious period is exponentially distributed with the mean 1/γ i days). Hereafter, the matrix M = [M ij ] is referred to as the contact matrix.

In the event of school closure, we assume that the net reproduction matrix is further scaled by a function ε(t). If the closure influences only the contact among children, we would simplify the issue by rescaling (1,1)-element, i.e.,

where ε(t) may be modelled as

where q represents the relative risk of secondary transmissions during the closure, t 0 represents the starting time of closure, and τ stands for the total length of closure. In the existing guideline in Japan, τ is suggested to be on the order of 7 days [12] which we regard as our baseline, and examine the possible length up to 50 days. As can be understood from (8), our study considers an epidemic scenario in which the school closure takes place only once. If we additionally account for a compensation of contact with young adults, we assume that the net reproduction matrix is rescaled as

where ϕ(t) represents the relative increase in the reproduction number between children and young adults due to compensatory behaviour of children with young adults during the closure. For clarity of modelling and due to shortage of scientific evidence, we ignore the influence of compensation on other elements of the contact matrix (e.g. we assume that the contacts within young adults and between children and elderly are not influenced by closure). Further mathematical details of the compensatory contact are described in Appendix.

Let z a represent the final size (i.e. the cumulative incidence) of age group a, i.e.,

Although mitigation strategy including school closure involves multiple public health objectives (e.g. delaying peak and reducing the height of peak prevalence), the present study focuses on the cumulative risk of infection. Concentrating on this aspect, we explore the possible optimal timing and duration of school closure and examine the cost effectiveness of this countermeasure. Since the average life expectancies are different between age-groups, the loss of life-years, L, due to the pandemic is measured by employing age-dependent weighting function, w a .

w a is given as the product of the infection fatality risk (IFR; i.e., the risk of death given infection with influenza virus) and life-expectancy of age group a, assumed to be 65, 45 and 15 years for children, young adults and elderly, respectively [13] . The age-specific estimates of IFR are extracted from empirical study in Hong Kong, assumed as 1, 10 and 500 deaths per 100,000 infections for children, young adults and elderly, respectively [14] . To measure the effectiveness of school closure, we compare the absolute difference of L between two scenarios, i.e. with and without school closure, L 1 and L 0 (i.e., L 0 -L 1 ), yielding the life-years saved by the school closure. School closure also involves the cost of forced annual leave among parents during the closure. The frequency of such annual leave among Japanese businessmen has been surveyed by Mizumoto et al. [15] , showing that up to 16% of daylight time workers were influenced by the school closure and were compelled to take at least an annual leave for half a day. Especially, the parental absenteeism was as high as 26.7% among households with at least one infected child, and this proportion was 10.4% otherwise [15] . Let c be the average daily rate of cost induced by annual leave per single young adult. The gross social cost of such parental absenteeism, G, is calculated as

where u 1 and u 0 are the proportions of households in which either father or mother has to take annual leave, with or without an infected child in the household, respectively (i.e., u 1 = 0.267 and u 0 = 0.104) and N 2 is the population size of adults. For simplicity, we ignore small fractions of childless couples and unmarried adults among the total young adults. The cost of u 1 is multiplied to the cumulative incidence of children during the closure, ignoring multiple infections in a household (i.e. we ignore brother(s) or sister(s) who are infected at the same time in a household). The population size to multiply u 0 is calculated by subtracting the cumulative incidence from N 2 /2, because it is usually the case that either father or mother is absent from work during closure (and one of them continues to work). According to a white paper of an economic study, c is given as the product of hourly wage, the average working hours (per day) and the cost to be paid for workers during school closure [16] , yielding c = 10,019.52 Japanese Yen (approximately 100 US Dollars) per day. Since we consider the short (and realistic) lengths of closure (e.g. 7, 14 or 21 days), we account for only individual impact of closure and ignore other indirect social cost such as the loss of business opportunities due to extended period of closure or stagnation of overall economic activity. Restricting our scenarios to those reducing epidemic peak by the school closure, we measure the average cost that is required to save a single life year as

It should be noted that the quantity Y is theoretically equivalent to the incremental cost effectiveness ratio (ICER) in the cost effectiveness analysis (CEA) studies. We aim to identify reasonable combination of τ and t 0 that minimizes our objective function Y. In the United States and United Kingdom, the acceptable threshold of ICER tends to lie around 100,000 US dollars and 30,000 British Pound per life year, respectively. Accordingly, we assume that the corresponding threshold lies in the range from 5.0 × 10 6 to 1.0 × 10 7 Japanese Yen (and draw a line for the latter threshold in all associated figures).

We consider an epidemic in a population of 1 million. The size of each age group is assumed as proportional to age-specific population sizes of those aged from 0-19, 20-59 and 60 years and over for the entire Japan, i.e., (N 1 , N 2 , N 3 ) = (177207, 500957,  321836) . The parameters of contact matrix are derived from previous study with an identical age categorization [17] which essentially assumed that the age-specific contact pattern in Japan is not different from that in England [18] . In our scenario, an epidemic takes place with an introduction of single infected child j 1 (0) = 1 (while j 2 (0) = j 3 (0) = 0), and all residents are assumed as initially susceptible, i.e., (s 1 (0), s 2 (0), s 3 (0)) = (1,1,1) . Nevertheless, their relative susceptibility per contact (e.g. the probability of successful transmission per contact) depends on age, i.e., (α 1 , α 2 , α 3 ) = (1.000,0.370,0.059) as empirical evidence suggests [11, 19] . Mean generation times of secondary transmission caused by children and young adults or elderly are assumed to be 2.2 days and 2.7 days, respectively [20] , and we assume that the generation time follows an exponential distribution for mathematical convenience (so that the model (1) can also be written as ordinary differential equations using the next-generation matrix parameterized by (6)). The basic reproduction number, R 0 , is used for scaling the next-generation matrix and is assumed to be 1.4 [21, 22] .

With respect to the protective effect of intervention, the relative reduction in the reproduction number during school closure (hereafter referred to as the ""efficacy"" of school closure) has been empirically estimated in limited number of settings [4, 5, 9, 23] . As our baseline, we assume that there is a 70% decline in child-to-child transmission during the closure and thus q = 0.3 [5] . In a Japanese survey [15] , no apparent increase in the frequency of child-to-adult contact was observed during the closure, and we set the proportion of child contacts compensated (π; see Appendix) as 0 at the baseline and then vary it from 0 to 0.5 (where 0.5 means that 50% of intervened within-child contacts are alternatively made with young adults).

The above-mentioned parameters correspond to empirically measured results from pandemic H1N1-2009 which is known to have been very mild [24] . Thus, we also measure the sensitivity of ICER to different levels of transmission potential and risks of death, varying R 0 from 1.2 to 1.8 and elevating the relative risk of death from 1 to 100 (using H1N1-2009 as the reference).

Epidemic dynamics and school closure Figure 1 illustrates a single scenario of influenza pandemic with school closure. At a specified epidemic day, we assumed that school closure is initiated, abruptly reducing the number of child-to-child secondary transmissions by a constant factor for a fixed length of time, and the transmission potential recovers when the school is reopened ( Figure 1A) . A sharp decline in the child incidence was observed, and it also influenced the transmission dynamics of adults and elderly. In the absence of school closure, the cumulative incidence of children, young adults and elderly were 103242 (58.2%), 66156 (13.2%) and 3142 (0.9%), respectively, yielding the final size of 17.3% for the entire population. These figures were in line with the result from seroepidemiological survey of H1N1-2009 in Japan [25] . The highest incidence was observed at Day 61. When the school closure is implemented at Day 50 for 7 days, the cumulative incidence of children, young adults and elderly were 95795 (54.1%), 61967 (12.4%) and 2930 (0.9%), respectively, yielding the final size of 16.1% for the entire population. The highest incidence was observed at Day 75. Figure 2 shows the sensitivity of the cumulative incidence to variable timing and lengths of school closure. The cumulative incidence was minimized if the closure was started at Day 61. Namely, the largest reduction of final size was observed by implementing the closure at the peak of the epidemic (Figure 2A) . Moreover, as the length of school closure was extended, the cumulative incidence was decreased ( Figure 2B ). Based on our univariate sensitivity analysis, these findings were only marginally influenced by different efficacy of school closure (i.e. variations in the relative reduction in the child-to-child secondary transmissions had little impact on the cumulative incidence) (Figures 2C and 2D ) and variable compensatory behaviour of children with young adults (Figures 2E and 2F) .

Cost effectiveness analysis of school closure Figure 3 shows ICER values with various timing and lengths of school closure. As there was an optimal timing to minimize the final size in Figure 2 , the ICER also took the minimum value when the closure is implemented at the epidemic peak ( Figures 3A and  3C) . Nevertheless, it is noteworthy that the ICER remained to be above the acceptable threshold for all the assumed parameter space for H1N1-2009 (even when ICER took the minimum value). As for the length of closure, the ICER appeared to be a monotonically increasing function of the length of closure. For both the timing and length, the variations in the efficacy of closure and proportion of contacts compensated had only marginal impact on ICER. Figure 4 examines the ICER by varying two additional epidemiological variables, i.e., the basic reproduction number ( Figure 4A ) and the relative risk of death given infection ( Figure 4B ). The timing of epidemic peak varied with R 0 , and thus, the time for ICER to take the minimum value also greatly varied with R 0 . Greater R 0 yielded the minimum ICER value at earlier epidemic time. Nevertheless, again all the ICER values were above the acceptable threshold in Figure 4A . When the infection fatality risk was proportionally magnified, we found that the ICER fall below acceptable threshold. That is, when the risk of death was three times or greater than that of H1N1-2009, all scenarios that we examined appeared to be cost effective. Panels A, C and E examine the sensitivity of the cumulative incidence (i.e. final size) to different timing of school closure. Length of school closure in these panels is set at 7 days. Similarly, Panels B, D and F explore the sensitivity of the cumulative incidence to different lengths of school closure. Timing of school closure in these panels is set at Day 50. The basic reproduction number is set at 1.4. Panels A and B compare the final size among children to that of the entire population. Panels C and D vary the efficacy of school closure (i.e. the relative reduction in the child-to-child secondary transmissions) from 50% to 90%. No compensatory contact with adults is assumed in these panels. Panels E and F vary the proportion of child contact compensated with young adults, assuming that the compensation occurs for 0 to 50% of intervened within-child contacts. In these panels, the efficacy of school closure is set at 70%.

The present study examined the public health effectiveness and cost effectiveness of school closure which was assumed to be implemented only once during the course of a pandemic. The model was parameterized with reference to empirical estimates of the pandemic H1N1-2009. School closure at the epidemic peak appeared to minimize the cumulative incidence, but the time of epidemic peak was shown to depend on R 0 . As the duration of school closure was extended, we observed a larger reduction in the cumulative incidence. Strikingly, the cost effectiveness analysis showed that our school closure scenario with parameters derived from the pandemic H1N1-2009 was not cost effective. Nevertheless, if the virulence is three times or greater than that of H1N1-2009, the cost of closure could be justified. These findings were not very sensitive to the efficacy of school closure and compensatory contact behaviour among children.

There are three important learning points from the present study. First, we have shown that there is no fixed timing and duration of school closure that can be recommended as universal for different types of influenza viruses. It is natural that the effectiveness of school closure depends on the underlying transmission dynamics, Figure 3 Cost effectiveness of school closure with different timing and lengths. Incremental cost effectiveness ratio (ICER), expressed as Japanese Yen per single life-year saved, is computed. Horizontal dashed grey line represents the threshold value of ICER, 1.0 × 10 7 Yen/life-year, below which one may regard the intervention as cost-effective. Panels A and C examine the sensitivity of ICER to different timing of school closure. Length of school closure in these panels is set at 7 days. Similarly, Panels B and D explore the sensitivity of ICER to different lengths of school closure. Timing of school closure in these panels is set at Day 50. The basic reproduction number is set at 1.4. Panels A and B vary the efficacy of school closure (i.e. the relative reduction in the child-to-child secondary transmissions) from 50% to 90%. No compensatory contact with adults is assumed in these panels. Panels C and D vary the proportion of child contact compensated with young adults, assuming that the compensation occurs for 0 to 50% of intervened within-child contacts. In these panels, the efficacy of school closure is set at 70%. and the absence of simple universal guideline should be explained and communicated to non-experts. In fact, the dependence of the effectiveness of school closure on the transmission dynamics could explain variations in published opinions toward the epidemiological performance of the school closure as an option of mitigation strategy. Second, school closure during the pandemic H1N1-2009 appeared not to be cost effective even when the ICER took the minimum value. It implies that the cost-effective intervention against mild pandemic strain such as H1N1-2009 is different from that of virulent strains. This echoes the finding by Halder et al. [26, 27] based on a simulation approach. Third, if a particular strain is virulent, school closure could be cost effective. Namely, given that the virulent strain widely spreads in the community, yielding high disease burden, school closure intervention, the cost of which is regarded as small for the high disease burden, should be implemented to reduce the disaster size as much as possible. In addition, since the equation (12) involves the wage of parents, it is important to remember that the cost-effectiveness may also depend on an economic standard of a country (e.g. school closure of pandemic H1N1-2009 may even be justified in a country with much smaller salary than that of Japan).

As a policy implication at minimum, one should remember that optimal school closure depends on the severity of pandemic, characterized by the transmissibility and virulence. Especially, the school closure is likely cost effective for virulent influenza strains. One should also know that the cost-effective interventions of a particular influenza strain are different from those for other strains. In addition, rather than industrialized countries, it may be easier to justify the cost of school closure in developing countries where the parental impact is likely smaller.

Three technical limitations should be noted. First, our model assumes that the transmission from child to child is homogeneous. More rigorous network model has shown that such random mixing assumption could overestimate the effectiveness of school closure [28] . Second, the impact of school closure on social mixing patterns should ideally be based on more realistically socially structured (layered) modelling approach [29] , perhaps classifying transmissions into those occurring in households, schools and community [30] . Third, more precise features of child contact, including weekend contact and the impact of illness on the contact, are recently shown to have a substantial impact on the effectiveness of intervention [31, 32] .

Unfortunately, school closure during H1N1-2009 may not be fully justified when it comes to the cost. Nevertheless, the cost effectiveness should be regarded as merely a single aspect of the impact of this intervention to help policymaking. Perhaps, rather than focusing more on the cost, one should carefully reconsider public health objectives of this intervention, e.g. delaying epidemic peak, reducing the height of peak prevalence, or reducing the overall epidemic size, and decide what we would expect from this intervention more in detail. Expecting the effectiveness in all these aspects may not be feasible [33] . Despite the presence of numerous tasks to guide school closure in the next pandemics, our study has at least shown that one can examine the potential performance of school closure using the proposed simplistic modelling approach.

The present study examined the public health effectiveness and cost effectiveness of school closure. The effectiveness of school closure depends on the transmission dynamics of a particular influenza virus strain, especially the virulence. School closure in our scenario with parameters derived from the pandemic H1N1-2009 appeared not to be cost effective. There is no fixed timing and duration of school closure that can be recommended as universal guideline for different types of influenza viruses.

Sum of elements in a single row i or single column i represents the total number of contacts (per day) made by all of those in age-group i. We assume that the sum of each column is decomposed as the product of average contact per person k i and the population size N i , so that the sum can be rewritten as k i N i . In the presence of compensatory behaviour, we assume that the total number of contact, k i N i , made by age-group i is partially maintained even during the school closure. Let p 1 and p 2 be the proportion of child contacts spent for children and young adults, then the first row (or first column) of M * should read (p 1 k 1 N 1 , p 2 k 1 N 1 , (1-p 1 -p 2 )k 1 N 1 ).

Similarly, let p 3 be the proportion of young adult contacts for other young adults or elderly that are spent for young adults. We get

Thus, the contact matrix M is given by

In the event of school closure, the contact rate among school children is expected to decrease. If the impact of school closure were seen only among children, we would have the following contact matrix during the closure, M*, as

where, as in the main text, ε(t) may be modelled as ε t ð Þ ¼ 1 for t < t 0 or t ≥ t 0 þ τ; q for t 0 ≤ t < t 0 þ τ:

Nevertheless, school closure could, in theory, influence the contacts with other agegroups too. That is, school children are likely to stay in the home during the closure and the contact rate between children and adults may be increased in an indirect manner, e.g., parents may have to spend longer time with children than usual. If the proportion π of reduced child-to-child contacts is maintained and compensated by child-to-young adult contacts, the contact matrix during the closure may read

It should be noted that the increase in child contact with elderly is ignored, assuming that mostly parents, not elderly, have to take care of children during the school closure [15] . It should also be noted that other contacts were assumed not to have been influenced by the compensation, although, in theory, the maintenance of contacts should influence all other elements (so that the total number of contacts per day remain constant for all age-groups); we ignore this mathematical issue for simplicity.

",0.6371772094095715
BMC Public Health Influenza pandemic preparedness: motivation for protection among small and medium businesses in Australia,"Background: Community-wide preparedness for pandemic influenza is an issue that has featured prominently in the recent news media, and is currently a priority for health authorities in many countries. The small and medium business sector is a major provider of private sector employment in Australia, yet we have little information about the preparedness of this sector for pandemic influenza. This study aimed to investigate the association between individual perceptions and preparedness for pandemic influenza among small and medium business owners and managers.","Since late 2005 the risk of pandemic influenza and the need for preparedness have featured reasonably frequently in the news media in Australia, often associated with overseas reports of large outbreaks of infection among birds or small clusters of infection among humans. Strategic plans have been prepared for an outbreak of pandemic influenza associated with human avian influenza infection at national and global levels [1, 2] . Many of these plans include mechanisms to facilitate and manage community-wide responses in recognition of the likelihood that pandemic response requirements will exceed the response capacity of health authorities and governments.

Preparation by the business community for an influenza pandemic is encouraged by governments, but much of the onus is on businesses to inform themselves about the threat posed by pandemic influenza and develop their own plans. As such, large corporations often have detailed plans, but less is known about pandemic preparedness in small and medium sized businesses. Small and medium businesses are a major employer in Australia, accounting for approximately half of all private sector employees [3] . Small businesses, which include businesses with less than 20 employees, were alone estimated to employ almost 3.6 million people in Australia in 2001 [3] . Pandemic influenza is likely to have a major impact on businesses, yet little is known about the needs and preparedness of small and medium sized businesses.

Government and health authorities in Australia and overseas have recommended that businesses, community organisations and individuals implement a range of strategies to prepare for pandemic influenza, and an increasing number of resources are being produced to provide guidance on pandemic preparedness and business continuity planning [4] [5] [6] [7] . A resource specific to pandemic preparedness planning among small businesses in Australia is also available [8] .

Pandemic planning resources generally describe the nature of the anticipated threat, highlight the role of government and health authorities, outline essential business continuity planning and response requirements, and describe specific measures that may be implemented to limit or prevent disease spread. Specific strategies recommended to limit disease spread within the workplace include promoting improved hygiene and infection control practices, using social distancing measures and flexible work arrangements to minimise contact between individuals within the workplace and the community, using personal protective equipment, restricting workplace entry and isolating individuals who may be infectious [4] .

A greater understanding of the factors associated with planning for pandemic influenza among small and medium businesses is required to inform communication strategies that promote improved preparedness for a pandemic. Protection motivation theory [9] is a commonly used framework for fear-appeal research [10] . Protection motivation theory conceptualises an individual's acceptance of advice on how to protect themselves from a health threat as primarily a function of four specific beliefs: the perceived severity or seriousness of the threat and the likelihood of the threat occurring (which together constitute 'threat appraisal'); and the perceived effectiveness of actions to avoid the threat and the individual's perceived self-capacity to implement those actions (which together constitute 'coping appraisal'). If a sufficient level of threat is perceived to exist, and coping appraisal is high, then the individual will take appropriate action. However, where a threat appraisal is high but coping appraisal is low, the individual is unlikely to take appropriate action. Protection motivation theory suggests that campaigns using threats must include information about how to avert the threat, and ensure that members of the target audience have the skills and resources necessary to adopt the recommended actions.

Investigations of the effectiveness of health threat communications are supportive of the protection motivation theory framework [11] , finding that communication effectiveness is associated with the extent to which the communications present real but controllable threats [12] . The health belief model [13, 14] also conceptualises behaviour as dependent upon individual perceptions including the perceived likelihood and severity of the potential health threat, and the perceived effectiveness of responses to the threat. Similarly, research has supported the importance of health belief model constructs in behaviour change [15] , particularly where illness avoidance and perceived threat are of central importance [16, 17] .

Among health behaviour theories that identify similar constructs as important determinants of health behaviour, current research provides no clear indication of the superiority of any single approach [17] . Guided by the concepts considered to be of importance in these health behaviour theories, and the protection motivation theory framework in particular, we aimed to investigate the association between selected beliefs and preparedness for pandemic influenza among small and medium business owners and managers.

Between May and July 2006, structured face-to-face interviews were conducted with either the owners or managers of 201 small or medium businesses in Australia. Eligible businesses were defined as businesses which have less than 200 employees [3] . Participating businesses were recruited from New South Wales (101) and Western Australia (100), with approximately 70 per cent of the sample in each state being selected from businesses located in the capital cities (n = 140), 20 per cent being recruited from large satellite cities (n = 41), and the remaining 10 per cent from rural centres (n = 20).

Participating businesses in Western Australia were randomly sampled from a membership database of businesses obtained from the Local Chambers of Commerce and Industry. In New South Wales businesses were randomly sampled from a purchased list of 1500 businesses stratified by industry type. In both instances the lists of businesses were checked to ensure that the sampling frames included only businesses which operated in the eligible study areas prior to recruitment. A flow chart was used to guide the recruitment of interviewees in both states to ensure recruitment processes were standardised, including gaining confirmation that the business had less than 200 employees, ensuring that a minimum of three attempts were made to establish contact with each business to be recruited, and ensuring that an appropriate person was interviewed.

Face to face interviews with business owners or senior managers were administered by trained interviewers from a market research firm in New South Wales, and by trained interviewers contracted by the Local Chambers of Commerce and Industry in Western Australia. Prior to the study interview verbal consent to participate was obtained following the provision of, and discussion of, a study information sheet. Study procedures were approved by the Human Research Ethics Committee of Curtin University of Technology.

Focus group discussions with business owners and managers in Perth and Sydney were used to inform and develop the structured interview schedule. The interview schedule was pre-tested among a small sample of business owners to ensure the questions were acceptable, understandable, unambiguous, and that open ended questions elicited the expected type of response.

Basic characteristics of the participating businesses assessed included the job classification of the interviewee (owner, chief executive officer/managing director, senior manager), main business location (capital city, satellite city, rural centre), industry type, business size (number of employees), average number of customers per day, and the educational level of employees (proportion of employees who attended university). The response categories for industry type were pre-coded based on the divisions in the 1993 Australian and New Zealand Standard Industrial Classification [18] . For analysis purposes, businesses operating in the primary and secondary industry sectors (i.e., businesses engaged in production and manufacturing) were aggregated; and classifications for tertiary industry businesses (i.e., businesses operating within the service sector) were aggregated according to the following three industry type categories: property and business services; retail trade; and other tertiary.

The following beliefs about pandemic influenza were each assessed by a single question: the perceived severity of the threat; the risk of the threat; and the ability to respond effectively to the threat. The general belief about the severity of the threat was operationalised as the perceived proportion of people that could become sick if pandemic influenza were to affect the local community. The perceived risk of the threat was operationalised as the likelihood that pandemic influenza would become a significant health issue in Australia in the near future, and assessed on a four point scale (very unlikely, unlikely, likely, very likely). An additional business-specific indicator of risk, the perceived level of risk that pandemic influenza poses to the interviewee's business, was also rated on a four-point scale (no risk, some risk, moderate risk, high risk). A dichotomous indicator of coping appraisal was derived from the open-ended question: ""Can you think of any steps you can take to protect your business from pandemic influenza?"" Responses were independently reviewed by two coders, and participants who were unable to identify any potentially useful steps that could be taken to protect their business or limit disease spread were classified as having low coping appraisal.

A small proportion of participants nominated the responses 'don't know' and 'no idea' to represent their beliefs about the risk and severity of pandemic influenza (Table 1) . When dichotomous indicators of risk and severity were used in the analysis, these responses were aggregated with the other low risk or low severity responses for analysis purposes on the basis that these responses indicated an absence of perceptions of high risk or high severity. This coding did not significantly affect the findings of the analysis.

Three dependent variables in the analysis provide different indicators of engagement in adaptive processes associated with the threat of pandemic influenza. Participants were asked ""Before being contacted about this study, how much have you thought about the impact of pandemic influenza on your business?"" (not at all, a little, a lot). The need for help with planning for pandemic influenza, which can be considered an indicator of an adaptive response to the threat of pandemic influenza, was assessed using the following open-ended question: ""Is there anything you need to help you prepare for pandemic influenza?"" Responses were dichotomised into a variable which indicated whether help was or was not required. Lastly, the presence of a plan for pandemic influenza was assessed by the single question ""Has your business made any specific plans should pandemic influenza arise?"" (yes, no, unsure).

The chi-square test of independence was used to test for associations between categorical study variables, and the independent samples t-test was used to test for differences between groups on continuous variables. Phi, which is a measure based on the chi-square test of association, is used to assess the strength of association between two dichotomous variables, and indicates the amount of total variance explained by the association between the variables.

Binomial logistic regression analysis was used to identify the significant independent predictors of the health behaviour theory-based belief variables and the three main dependent variables: having considered the impact of, hav- ing a plan for, and needing help to prepare for pandemic influenza.

Dependent variables were dichotomised for analysis due to skewed distributions and the small sample size. Initial model development included entry of variables into a forward stepwise model, with the probability criterion for entry set at 0.05 and exit at 0.10. The final models were developed manually to allow exploration of alternative model forms. A main effects model was initially determined. Effect modification was also explored, and the inclusion of interactions was determined by the significance of the change in log likelihood of the model. Crude odds ratios (COR), adjusted odds ratios (AOR) adjusted for the other variables in each model, and 95% confidence intervals (95%CI) are used to summarise the magnitude of association found between variables. All analyses were performed using SPSS version 13.0 (SPSS Inc., 2004) and the significance level was set at p ≤ 0.05.

In total, 832 eligible businesses were contacted and 201 interviews were completed, producing an overall response rate of 24 per cent. The response rate of 15 per cent (101/ 660) for New South Wales (NSW) was considerably lower than the 58 per cent (100/172) achieved for Western Australia (WA), but consistent with the different sampling methods used. There was no significant difference between participating and non-participating businesses in WA according to business size (p = 0.3) or industry type (p = 0.09). Similar data on the characteristics of non-participating businesses in NSW were not available for analysis. Nonparticipation was most frequently associated with the business owner or manager being either too busy or unavailable during the interview period, explaining 83 per cent and 85 per cent of refusals in the WA and NSW samples respectively.

The characteristics of participating businesses are summarized in Tables 1 and 2 by state. Most participating busi-nesses had less than 20 employees and more than half of the individuals interviewed were business owners. Business owners were significantly more likely to be interviewed in WA than NSW ( Table 1) . Most of the participating businesses operated within the tertiary or service sector. The representation of businesses from different industry types was significantly different by state, with the WA sample having a higher proportion of retailers (Table 1) and reporting a significantly lower proportion of university educated staff compared with the NSW sample (Table 2) .

Approximately 40 per cent of participants believed that pandemic influenza was likely or very likely to become a significant health issue in Australia in the near future (Table 1) , and, on average, participants believed 35 per cent of people in affected communities would become sick ( Table 2 ). Around 40 per cent of participants reported that they had not spent any time thinking about the impact of pandemic influenza on their business, and over 40 per cent could not identify any steps that they could take to protect their business (Table 1 ). Only 6 per cent of participants reported having a pandemic influenza plan for their business (3 per cent were unsure), and over 60 per cent of participants reported needing help to prepare for pandemic influenza (Table 1) .

Beliefs about the risk and severity of pandemic influenza and the amount of time spent considering the impact of pandemic influenza on the business did not differ significantly between states. Beliefs about being able to respond to the threat and perceptions about the need for help did differ between states, with businesses in WA having a significantly lower level of response efficacy and being less likely to need help to prepare than businesses in NSW (Table 1) . Businesses in NSW were also significantly more likely to have a plan for pandemic influenza than businesses in WA (Table 1) ; however, the difference in response rate for the two states renders the generalisability of such differences as tenuous. 

Beliefs about the perceived severity of pandemic influenza and the perceived risk of pandemic influenza to the business were not significantly associated with any business characteristics. Business characteristics which were significant predictors of beliefs about the perceived risk of pandemic influenza and coping appraisal are summarized in Table 3 . The perceived risk of apandemic in Australia was significantly associated with the role of the person interviewed, with non-owners being about twice as likely to consider pandemic influenza as a likely or very likely risk than business owners. Both state and the role of the individual interviewed were significantly associated with response efficacy, with businesses in NSW and non-owners being about twice as likely to be able to identify actions which could protect their business in the event of a pandemic than businesses in WA and owners. Each of the significant predictors of beliefs identified only explained a small proportion (less than 5 per cent) of the overall variance associated with the belief variables.

Bivariate associations between beliefs and the dependent variables (Table 4) indicate that almost all beliefs and dependent variables assessed were significantly associated. The high correlation between the general belief about the risk of pandemic influenza and the specific belief about the risk of pandemic influenza to the business, which explained over 37 per cent of the total variance in responses (equivalent to a Pearson correlation coefficient of approximately 0.6), was among the strongest associations found. There was no significant association between having a plan and the need for help, and coping appraisal was independent of perceptions of severity.

Logistic regression models were used to determine the significant independent predictors of having considered the impact of, having a plan for, and needing help to prepare for a pandemic. All models were tested for interaction terms and no significant effect modification was found.

The significant independent predictors of dependent variables, based on the inclusion of both belief variables and business characteristics, are summarised in Table 5 .

Thinking a lot (versus a little or not at all) about the impact of pandemic influenza on the business was most strongly associated with the perceived risk of pandemic influenza, with participants who perceived a pandemic as likely or very likely to be a health issue in Australia in the near future being approximately 5 times more likely to have reported thinking a lot about the impact of a pandemic on their business. Businesses that were located in the capital city were about three times more likely to have spent a lot of time thinking about the impact of a pandemic compared with businesses in satellite city or rural locations. These same factors were also significant predictors of having considered the impact of a pandemic on the business when this variable was dichotomised as thought at all (a little or a lot) versus not at all.

The perceived need for help was most strongly associated with the perceived risk of pandemic influenza to the business, with participants who perceived the risk of a pandemic to the business as moderate or high being approximately 3 times more likely to report needing help to prepare. State was a significant independent predictor of the perceived need for help, with businesses in NSW more likely to report needing help than those in WA. The perceived need for help was also significantly associated with industry type, with businesses in the property and business services and retail trade sectors being significantly less likely to need help than other service sector businesses. Industry type was not significantly associated with perceptions about the risk or severity of a pandemic, but was significantly associated with coping appraisal (χ 2 = 8.4, p = 0.04), with 58 per cent of retailers unable to think of steps to protect their business as opposed to 35 per cent of other service sector businesses and 30 per cent of production and manufacturing businesses. The presence of a specific plan for pandemic influenza was significantly and independently associated with both perceived severity of a pandemic and coping appraisal. Participants who believed that 30 per cent or more of the local community would become sick were over 9 times more likely to have a plan, and participants who were able to identify steps that could be taken to protect their business were over 8 times more likely to have a plan for pandemic influenza.

There is a lack of empirical data to inform public health response strategies for pandemic influenza. To our knowledge this study provides the first systematically collected information on preparedness among small and medium businesses in Australia, and is among only a few studies in the field worldwide. We found that only a small proportion of businesses studied had thought a lot about how pandemic influenza may impact on their business, that few had made any specific plans to protect their staff or their business in the event of pandemic influenza, and that over 60 per cent state they need help to prepare for pandemic influenza. These findings suggest that additional strategies are required to promote increased awareness of the threat of pandemic influenza in the community, to promote the resources available to assist with preparedness, and to facilitate engagement in preparedness planning.

Behaviour change is a process, and time is required to initiate and establish new behaviours. According to the protection motivation theory, coping appraisal responses which lead to the establishment of protection motivation occur after the threat-appraisal process, as a threat needs to be identified before coping options can be evaluated [11] . As such, and as has already been highlighted by others, occasional media reports are insufficient to adequately inform individuals about pandemic influenza, and interventions are required before a pandemic occurs to improve public awareness, build mutual trust, promote effective coping responses and assist in the successful implementation of plans when they are required [19] .

National influenza plans require collective communitywide efforts for an effective response to pandemic influenza. However, they lack information relating to strategies to enable the effective dissemination of this information beyond the availability of these plans on websites [19] . Given that the strategy for response to pandemic influenza in Australia is based on containment and reducing transmission of the virus [2] , and that key response strategies such as isolation, social distancing, and improved personal hygiene which have been supported by mathematical modelling studies [20] depend on community-wide behaviour modification, additional strategies are required to enable an effective shared response.

Our findings suggest that the beliefs of small and medium business owners and managers are likely to have important consequences for preparedness. Beliefs about the risk of and severity of pandemic influenza were the most important independent predictors of having thought about, and having a plan for pandemic influenza respectively. The perceived risk of pandemic influenza to the business was also the most important predictor of needing help to prepare. These findings are consistent with the relationships proposed by prominent theories of health behaviour, including the protection motivation theory [9, 11] , and suggest that these theories provide a useful model for understanding preparedness behaviours among small and medium businesses in Australia and elsewhere.

Protection motivation theory and health belief model concepts have been found to be valuable for understanding and promoting a variety of health-related behaviours [10, 12, 21, 22] , including the performance of protective behaviours during the outbreak of the severe acute respiratory syndrome in Hong Kong [16] . The importance of perceptions about risk and severity in understanding preparedness behaviour suggests that health behaviour theories provide a useful framework for the design of communication strategies that aim to promote preparedness for pandemic influenza among the business community. Based on the temporal relations identified in these theoretical frameworks, our results suggest that communications containing information about risk and severity are likely to promote both threat appraisal and coping appraisal processes, and can motivate protective behaviours given a perceived ability to implement recommended actions. Promotion of the ability to respond effectively to the threat of pandemic influenza appears to be an important factor associated with protective responses to the threat of pandemic influenza. This finding is consistent with research findings based on other health threats which indicates that low levels of self efficacy and response efficacy provide a barrier to action [11, 12] . The high proportion of participants reporting needing help with preparation indicates that self efficacy may be an important factor limiting planning for pandemic influenza, which is consistent with the findings of recent research in Europe and Asia [23] .

Individual business characteristics were relatively unimportant among the predictors of having thought about or planned for pandemic influenza. Apart from beliefs about risk, the only other significant predictor of having considered the impact of pandemic influenza on the business was whether the business operated within or outside a capital city. It is possible that this association reflects a factor which can modify the perceived threat of pandemic influenza based on understandings about population density and the probability of exposure to infection.

In contrast, individual business characteristics were more important predictors of needing help to prepare, with industry type and state being significant predictors in addition to beliefs about risk. Retail traders and businesses that provide property and business services were less likely to report the need for help. Differences in the need for help by industry type, given the significant association between industry type and coping appraisal, suggests that some businesses may have difficulty identifying effective protection strategies that are appropriate for specific high-risk business environments, such as retail outlets. This finding highlights the importance of providing support to identify effective response strategies and overcome response difficulties within all business environments. Furthermore, our finding that the need for help was not significantly related to whether a plan for pandemic influenza exists appears to highlight the difficulties associated with planning for pandemic influenza, even among those businesses that have already made specific plans for pandemic influenza.

Our finding of a difference in the need for help by state is likely to be associated with the different sampling and recruitment processes used in the two study locations. In WA the Local Chambers of Commerce was directly contracted to supply the business contact details and conduct the interviews. Thus, the existing relationship with the businesses sampled is likely to explain the higher response rate in WA, why a higher proportion of owners were interviewed, and provide a sample which may be less biased in terms of either having a specific interest in pandemic influenza or time or resource pressures than the NSW sample. Selection bias associated with the different recruitment strategies may explain why participants from NSW were more likely to have a plan, were more likely to need help and reported lower response efficacy. Alternatively, these findings may be due to real differences in beliefs and behaviour between states, which may for example be associated with differences in media exposure or other local influences. Regardless of the cause, these differences did not significantly influence the associations found between beliefs and preparedness.

Due to the cross-sectional study design we are limited in the type of conclusions that we can draw about causality based on the associations observed. For example, having prepared a pandemic influenza plan is likely to result in improved levels of coping appraisal. However, experimental research [24] has provided support for the impact of beliefs on protection motivation and current behaviour. The associations found in this study explained a low proportion of variance in preparedness behaviour, although the magnitude of the associations found is similar to those reported for protection motivation theory concepts and other health-related behaviours [24, 25] . Several factors could have contributed to the low explanatory power in the present study, including the assessment of a limited number of theory-based belief constructs, the use of single-item and thus limited operationalisations of the key belief and outcome variables which have unknown reliability, and the use of dichotomous indicators due to the small sample size. Also we did not assess behavioural intentions. Further work is required to extend the scope of this study and considered other relevant constructs including social norms and response costs.

The non-random nature of the sampling frames used to recruit study participants and the small scale of the study limits the generalisability of the study findings. It is also likely that response bias associated with the low response rate may have resulted in an overestimation of the proportion of businesses that have a plan for pandemic influenza, particularly in NSW. The use of financial or other incentives for participation is recommended in future studies to facilitate improved response rates, particularly where industry partners are not used. The findings of this study may also be limited in that self-report methods were used to assess whether the business had a pandemic influenza plan. Responses may have been biased in favour of reporting the presence of a plan or having considered the impact of pandemic influenza on the business associated with social desirability bias.

There is a shortage of data available to guide public health policy and practice in pandemic influenza planning and response [6] . Current guidance for pandemic influenza preparedness appears to have had little impact on preparedness among the small and medium business sectors in Australia. Our findings suggest that further investment by governments is required to improve both the specification of and utilisation of available planning resources, as has been highlighted previously [26] . Further work is required to underpin both the design of communication strategies to promote behavioural change, as well as the feasibility and effectiveness of strategies for disease control, which also support beliefs about being able to respond effectively to the threat of pandemic influenza.

The findings of this study should be interpreted alongside more in-depth knowledge about the beliefs of business owners and managers that underlie the protection motivation theory constructs, as has been illustrated elsewhere [27] . In this way, a greater understanding about beliefs to be reinforced or changed, and responses to specific strategies can be gained, helping to promote improved effectiveness of the communication strategies developed. Also, particularly in the small and medium business sectors that may have significant resource constraints, the presence of alternative adaptive responses to the threat of pandemic influenza require further investigation.

We found that only a small proportion of small and medium sized businesses in Australia have made formal plans to guide their response in the event of pandemic influenza. Effective communication strategies and support structures to promote preparedness for pandemic influenza are essential to facilitate large-scale community involvement in response efforts. Findings from this study provide knowledge which can be used in the preparation of strategies to enable the effective delivery of information on preparedness for businesses. Our results indicate that to motivate improved planning among the small and medium business sector, campaigns targeting small and medium businesses should emphasise the severity of the consequences to their businesses if a pandemic were to occur, and, at the same time, reassure them that there are effective strategies capable of being implemented by small and medium businesses to deal with a pandemic.

",0.6371221418090394
BMC Public Health Establishing a nationwide emergency department-based syndromic surveillance system for better public health responses in Taiwan,"Background: With international concern over emerging infectious diseases (EID) and bioterrorist attacks, public health is being required to have early outbreak detection systems. A disease surveillance team was organized to establish a hospital emergency department-based syndromic surveillance system (ED-SSS) capable of automatically transmitting patient data electronically from the hospitals responsible for emergency care throughout the country to the Centers for Disease Control in Taiwan (Taiwan-CDC) starting March, 2004. This report describes the challenges and steps involved in developing ED-SSS and the timely information it provides to improve in public health decision-making.","groups starting in November 2004 and two clear spikes in enterovirus-like infections coinciding with the two school semesters. Using ED-SSS for surveillance of influenza-like illnesses and enteroviruses-related infections has improved Taiwan's pandemic flu preparedness and disease control capabilities.

Taiwan's ED-SSS represents the first nationwide real-time syndromic surveillance system ever established in Asia. The experiences reported herein can encourage other countries to develop their own surveillance systems. The system can be adapted to other cultural and language environments for better global surveillance of infectious diseases and international collaboration.

With the recent global concern over emerging infectious diseases (EID) and the challenges of the 2003 SARS epidemics, government health officials in SARS-affected countries have begun to consider various measures of improving their infectious disease surveillance systems [1] [2] [3] [4] . Infectious disease epidemiologists and several leading public health administrators at the Centers for Disease Control in Taiwan (Taiwan-CDC) becoming aware of the importance of early detection of EID or bioterrorism, started developing an automatic alert system. Therefore, the Automatic Syndromic Surveillance Planning Task Force Committee was created and recruited infection physicians, epidemiologists, biostatisticians, and information technology (IT) experts in July 2003 to oversee the initiation and development of Taiwan's first medical informatics-based emergency department syndromic surveillance system (ED-SSS).

To prepare for this project, we reviewed the syndromic surveillance systems of other countries and officials of health informatics at Taiwan-CDC started collaborating with the Real-time Outbreak and Disease Surveillance (RODS) Laboratory at the University of Pittsburgh to develop a real-time syndromic surveillance system for Taiwan in August 2003 [1, [4] [5] [6] [7] [8] . RODS, used during the 2002 Olympic Winter Games, is the first commonly used syndromic surveillance system in the United States and has been found to efficiently process and analyze data in a timely manner [9] [10] [11] . Together, the task force and the RODS group aimed to establish a nationwide syndromic surveillance system within six months to meet the challenges of potential avian flu outbreaks for up-coming winter seasons and other future EIDs. To gain more operational level experiences, we also visited the Department of Health in New York City, where syndromic surveillance system was established and has been in daily operation since 2001 [12] . There, the task force members observed routine workflow processes and became familiar with other practical concerns of operating an ED-SSS on a daily basis. Based on these experiences and high population density in Taiwan, we decided to create a nationwide surveillance system. To this nationwide ED-SSS, we added geographical information system (GIS) technology, meant to facilitate epidemiological investigation and feedback between data providers and decision-makers [13] .

Using the electronic data from the health information systems already in place in about eighty percent of the hospitals in Taiwan required by the National Health Insurance Payment Program and the technical support of the RODS Laboratory at the University of Pittsburgh, Taiwan's ED-SSS has been in operation since March, 2004 [14] . It is the first time in Taiwan that information technology and timely data directly from hospitals has been used with systematic approaches to facilitate public health surveillance. This report shares our experience of establishing an ED-SSS in a non-English-speaking country. It covers the process of taking into account the various needs at different levels of hospitals, discusses the stages of developing the system, and highlights the characteristics of ED-SSS data collected during the first year. The experiences reported here may benefit other countries seeking to establish or improve their own surveillance systems for infectious disease.

Initially, two Taipei City Municipal Hospitals that kept electronic files of their emergency department patients' medical information were selected as pilot sites. From these two pilot hospitals, we had learned work flow involved in the process of data transfer, format of Chinese chief complaints, practical concern of ED (such as heavy workload etc.) during and after the 2003 outbreak of SARS, and available electronic ED information from nationwide emergency care hospitals. To obtain more representative data from various geographical areas, we gradually enlisted the cooperation of 189 hospitals nationwide, all offering emergency healthcare. Because many outbreaks of EIDs require emergency health care, these emergency care-designated hospitals were required transmit their ED triage and patient data to the Taiwan-CDC electronically on a daily basis ( Figure 1 ).

Nurses at the triage stations at all participating hospitals generated the bulk of the information needed for syndromic surveillance. That information included time and date of admission, date of birth, gender, home zip-code, body temperature, triage categories and chief complaints for patients admitted to their EDs. Because the National Health Insurance requires hospitals to keep clinical data written in ICD-9 codes based on the criteria of the international classification of diseases, 9th revision, clinical modification (ICD-9-CM) for billing purposes, we were also able to collect clinical data from initial assessment of each case by an ED physician. These fundamental data were provided in either the health level-seven protocol (HL-7) format or extensible Markup Language (XML) format, if HL-7 was not used by a hospital during the time of the study. Thus, ED-SSS is capable of accepting these data of above-mentioned variables (Table 1) , including the patients' clinical and demographical information, and hospital identification numbers, in either format.

All the sentinel hospitals recruited into our system had independent MySQL servers on which their data were saved, plus a remote connecting program for automatic transfer of data. Data files generated by the 189 hospitals, including data from their triage classification systems, hospital information systems (HIS) and clinical information systems (CIS), were firstly de-identified and then transferred hourly to a Microsoft SQL Server 2000 at the Taiwan-CDC. Hyper text transfer protocol over secured sockets layer (HTTPS) or secured file transfer protocol (SFTP) was used in this process. All communication histories were recorded in a log file in the SQL server at the Taiwan-CDC and monitored daily by health informatics personnel. A program was written into the system so that each transfer attempt to the Taiwan-CDC would automatically generate an e-mail to the hospital notifying them whether the transfer had been successful or not.

Three different data storage tables were designed to process the data in the Taiwan-CDC's syndromic surveillance database. All information received is initially fed into the first table, with a serial number generated in an additional column for each case. The system picks up the data from the first table every five minutes and moves it into a second temporary table for a logic check and data cleansing. At this point, the system checks for unambiguously erroneous data, e.g., a birth date later than the admission date or other variables such as body temperatures that fall outside of reasonable ranges. The data cleansing work is accomplished through a system algorithm written with SQL commands. The cleaned-up data are transferred to the third table for further epidemiological analysis, aberration detection, and then sent them to related local public health agencies.

Although only a few variables were collected from each hospital on consecutive days, one major difficulty we had was the data presented by discontinuous data, i.e. data that sometimes be there and sometimes not. Sometimes data were repeated. To handle this problem, specific criteria of data cleansing were used for different variables, including the logic checks described above and double checks for possible presence of duplicate patient records. If data in the chief complaint field was written as ""test"" or the field was left empty or if the ICD-9 field was written as ""test"" or left empty, they were deleted before data analysis. Hospital ID, date of birth, admission time, gender, and home zip-code were used as key indicators of whether a listing is a duplicate listing and be deleted as repeated data. The system was capable of performing frequent and rapid checks of any subject of hospital identification code and time format of all time fields. It was capable of moving erroneous data to an ""error table"" for storage. Incorrectly formatted ICD-9-CM data were also moved to the error table. All deletion and removal operations were recorded in the log file for monitoring. In certain situa-tions in case possible systematic errors were found (i.e., aberrant number of ED visits on certain days or occasionally inconsistent formats of ICD-9 codes), the data examiner would contact the medical informatics officers of those specific hospitals to discuss improving data entry.

After the data cleansing, we categorized ED visits into 11 different syndromic groups important in Taiwan. There were: (1) fever, (2) respiratory, (3) skin, (4) neurological, (5) upper gastrointestinal (GI), (6) lower GI, (7) haemorrhagic, (8) influenza-like illness (ILI), (9) asthma, (10) enterovirus-related infection (EVI) syndrome, and (11) syndrome for severe illness or death. Since only about 25% of all chief complaints were written fully in English and the grouping of syndromes by chief-complaints due to Chinese language barriers would have effects on the outbreak detection ability, we first analyzed our data according to the ICD-9 coded syndrome groups [15] . Definitions of clinical syndromes were based on two different sources: (1) those associated with bioterrorism-related agents as announced by the Centers for Disease Control and Prevention (CDC) in the U.S.; and (2) those identified as important by the ED-SSS Advisory Committee in Taiwan, whose members include infectious disease physicians, emergency doctors, pediatricians, and epidemiologists [16] . For example, because the epidemics of enterovirus 71 caused severe fatal cases in Taiwan in the years of 1998, 2000 and 2001, the EVI syndrome group was considered as an important syndrome group locally (ICD-9 codes listed in Table 2 ) [17] . All patient information was de-identified and only aggregated data was used for data analysis. The protocol for this study was approved by the Research Ethical Committee (Institutional Review Board) of National Taiwan University. 20 Coxsackie carditis, unspecified 074. 21 Coxsackie pericarditis 074. 22 Coxsackie endocarditis 074. 23 Coxsackie myocarditis, Aseptic myocarditis of newborn 074. 3 Hand, foot, and mouth disease Vesicular stomatitis and exanthem 074. 8 Other specified diseases due to Coxsackie virus, Acute lymphonodular pharyngitis Although the ED-SSS data started transferring on March 10, 2004, we confined our analysis to data collected between April 1, 2004 (when the data became more stabilized) and March 31, 2005 . Data were organized using statistical programs to perform a descriptive analysis of the daily and weekly plots of different syndrome cases and obtain a baseline pattern for each syndrome in Taiwan. We initially generated the SQL commands for data querying and data grouping into the 11 different syndromic groups. To increase the sensitivity of this ED-SSS in monitoring regional patterns of these 11 syndrome groups, we categorized ED-SSS data by four different geographical areas (northern, central, southern and eastern Taiwan), based on major regional variations in the types of infec-tious diseases. In analyzing the seasonal patterns of ED visits, the correlation between the ILI syndrome and respiratory or asthma syndrome was assessed by the value of Pearson's coefficient (R). the Taiwan-CDC on a daily basis. The greatest challenge as began to develop this system was communication with different hospitals. Because different hospitals were using different information systems and inputting different data with various formats, it took long time to agree which variables and their data format should be collected. At the very beginning to build the ED-SSS, we had at first intended to collect information on a large set of epidemiologically useful variables, including occupation, travel history, family clustering, other exposure-related factors, and address for each patient to help detect possible zoonosis. However, such data were not collected during routine medical examination and care. Finally, we decided to capture only parameters usually collected by the hospitals during examination, intake and care.

During the planning phase, it was necessary to gain a full understanding of what not only just public health personnel expected but also what the medical staff at participating hospitals expected from the ED-SSS. Public health officials tended to prefer a timely and sensitive surveillance system able to detect all possible outbreaks of emerging or known infectious diseases. Mostly concerned over the limited public health resources, they wanted more evidence to prove the cost-effectiveness of ED-SSS and fewer false positive signals from pilot studies before integrating the system into routine public health surveillance workflow. On the other hand, the hospitals and their medical staff had three major expectations. First, the hospitals expected an easily operated feedback mechanism and quick feedback of useful information for better decision-making. Those who had experienced nosocomial infection of SARS during the 2003 SARS outbreak were particularly interested having analyzed information, based on their own hospital or regional/national hospital data, quickly fed back them. They believed that this would provide incentive for them to share hospital data and routinely maintain the high quality of their data for public health usage. Second, the hospitals anticipated two-way communication with public health agencies, as they frequently been requested or even forced to send data on short notice when they were too busy or too involved in emergency care. What made matters worse, despite their compliance; they had difficulty in obtaining useful feedback information from the public health agencies so that they could improve their care of patients at the time of an outbreak crisis. Third, the hospital decision-makers wanted immediate firsthand feedback, particularly with regard to control of nosocomial infection and hospital management in order that their health-care workers could be protected during regional outbreaks. Considering the expectations of both public health agencies and hospitals, we learned that the syndromic surveillance system should provide efficient means of feedback and effective two-way communication.

From April 1, 2004 to March 31, 2005 , data transmitted from the 189 hospitals on 2,692,325 ED visits were collected and stored in the Taiwan-CDC database. Initially, we appointed two computer engineers to cleanse the data by checking the log files and inform the hospitals by telephone on weekdays to correct errors or provide missing data. Then, these cleaned data on daily counts of ED visits collected from ED-SSS were analyzed. The time series plot of rough data on daily numbers of ED visits in our nationwide ED-SSS during the study period is shown in Figure 3 .

The computer system shut down twice during this period. The first time occurred from August 8 th to August 9 th , when no daily procedure was installed to monitor the quality of uploaded data. To help both hospitals and public health agencies perform routine data quality checks, we installed a computer program having check-up procedures of data quality after each data transfer from the hospital to the Taiwan-CDC for automatic quality control of data. This program records all the logs of each data sending from the participated hospitals. If Taiwan-CDC doesn't receive data from hospitals, program will send e-mail automatically to inform the personnel of health informatics in that hospital about failure sending. System maintenance personnel need to check the log daily and make a phone call to the hospital to verify successful data transfer and quality of data if there are no data transfers in two consecutive days.

Only 5.04% of hospitals failed to send ICD-9-CM data, but almost half (47.4%) failed to send chief complaints. For example, of the 239,617 sets of cleaned data received in July 2005, about 7.1% of ICD-9-CM information was lacking or filled out as 'null' and 54.82% of cases did not include chief complaints. Additionally, certain hospitals transmitted the patients' chief complaints in Chinese, further complicating the analysis. Because of these difficulties, this report focuses on the data based on ICD-9-CM diagnostic codes only. The time-series plots of the 11 syndrome groups (Figure 4 , 5) that may correlate to infectious diseases and important health problems in Taiwan (e.g., asthma has become an important pediatric problem in recent years) were analyzed.

Understanding the characteristics and patterns of numbers of ED visits over time from our established ED-SSS in Taiwan is very crucial before we set up appropriate threshold levels of different syndrome groups for outbreak detection, There was a significant difference in daily counts between weekdays and weekends, which occurred on a weekly basis. ED visits were 1.288-fold-higher on weekends than on weekdays (p < 0.001), while national During this first year study, the ED-SSS found patients throughout Taiwan were more likely to seek emergency medical services at medical centers than at district or local hospitals. Most patients visiting ED were 60 years old or older (21.46%) or below the age of 10 years old (18.55%). These two groups were also at greatest risk for various infectious diseases, especially influenza (Table 3) . Young adults between 20 to 39 years old ranked the third most frequent visitors to ED (17.43%), though traffic accidents, not infectious diseases, were the main reasons for their visits. These age distributions suggest that the newly established ED-SSS was capable of providing information for the age groups most at risk for severe cases of infectious diseases in Taiwan. Male ED patients slightly outnumbered female ED patients (male : female = 1.12:1), which approximates the general distribution of gender in Taiwan (male/female ratio = 1.10). For elderly ED patients (age > 65 years old), the male/female ratio in our sample was the same as general population (male: female = 1.10:1).

To understand the epidemiological characteristics of the eleven important syndrome groups and asthma syndrome in ED-SSS, data of their daily and weekly counts were plotted and shown in Figure 4 and 5, respectively. As can been seen in time series plots in Figure 6 , the seasonal patterns for ED visits due to respiratory syndrome and ILI syndrome were quite similar with high correlation (R = 0.98), while that for asthma syndrome, which had distinct peaks, was nonetheless not highly correlated with ILI syndrome (R = 0.78) nor respiratory syndrome (R = 0.77). Importantly, the ED-SSS was able to detect peaks of these respiratory-related syndromes even occurred in or around the summer season (July to September), though their most noticeable peaks were found during the Chinese new year holidays ( Figure 5B, 5C, 5D ). Like respiratory infection syndrome, visits due to the fever syndrome also showed another peak during the summer (July to September) ( Figure 4A ). ED visits for respiratory syndrome peaked earlier (mid-September) than those visits for ILI (mid-October) and asthma syndrome (end of September, e.g. the transition period between summer and autumn). Visits for pediatric asthma syndrome for children 12 years old and below peaked in mid-autumn, around mid-October (data not shown). Later during winter season (between November and February), there was an increase 

Respiratory Syndrom Asthma Visits In gastrointestinal (GI) syndromes, visits due to upper GI or lower GI or total GI started increasing November 2004 and peaked during the Chinese New Year holidays ( Figure  4F, 4G, 4H) . Interestingly, cases of hemorrhagic syndrome also increased slightly in the winter season ( Figure 4I ).

For those syndrome groups with severe symptoms, including skin rash, neurological symptoms and death/ coma that might be related to bioterrorism attacks, there was no significantly cyclic or seasonal patterns ( Figure 4J , 4K and 4L). One spike of syndrome cases with clinical severity (severe syndrome) appeared in mid-May, but that occurred as a result of one hospital sending duplicate data that escaped from our check algorithm ( Figure 4L ).

Taiwan has very high population density (Taiwan, 632.23 persons/km 2 ; Taipei, 9662.53 persons/km 2 ), which increases the spread of many human-to-human infectious diseases [19] . This makes the surveillance of infectious diseases very important for this island. Taiwan's ED-SSS is the first syndromic surveillance system to be implemented in Asia. It also represents the first time that Taiwan's public health agencies have attempted active nationwide surveillance. Its automatic data collection mechanism is capable of capturing comprehensive population-based health information and providing important details on current disease epidemics at the community level. The information it provides can also be used as community baseline data for further infectious disease modeling and can also improve the detection of emerging infectious diseases.

In addition to the information that the ED-SSS can provide for disease control, it can open avenues for further investigation. For example, in addition to the neurological syndrome, asthma syndrome, and syndrome for severe symptoms, there were clear and consistent weekend and holiday increases in the visits of other nine syndrome groups. Because cost of ED visits in Taiwan is not as expensive as it is in other countries, especially the United States, it is very likely that many patients seek ED medical care when local clinics are closed on weekends and holidays. It is also possible that the gathering of people on the holidays would increase the transmission of certain pathogens, particularly on cold days in closed spaces where respiratory viruses including influenza virus are easily transmitted. Therefore, future research might want to investigate the effect of holidays on the aberration detection of outbreaks and prediction of number of cases for certain infectious diseases using Taiwan's ED-SSS.

We also found differences in seasonal trends in visits due to symptoms/signs related to respiratory, influenza-like illness and asthma syndromes. Our ED-SSS found a sum-mer peak in visits for cases with influenza-like illnesses in 2004. This has seldom been found by the previous passive surveillance systems used in Taiwan. These summer cases of influenza-like illness occurred before annual vaccinations usually done in October or November. Therefore, a further longitudinal analysis of influenza-like syndrome patterns is needed to formulate the best vaccination policy on human influenza.

Another epidemiological finding from our ED-SSS was an increasing trend in visits due to gastrointestinal syndrome starting late autumn 2004. Such trends have not been detected by other infectious disease surveillance systems in Taiwan. There are two possible explanations for this finding. One reason for the increases might be related to the increased activity of certain pathogens, including rotavirus or norovirus, during winter season, as was found during the winter of 2006 in both Japan and Taiwan [20] [21] [22] [23] [24] [25] [26] [27] . Another reason might be the social habits of Taiwanese who like to dip raw meats and seafood into boiling water fondues and eat from the chafing pot during the winter season. This would increase the change that inexperienced or careless diners would consume undercook seafood or use chopsticks contaminated by raw seafood.

The findings of our ED-SSS, the first time in Taiwan to use daily rather than weekly data, suggest further directions for research into GI syndrome and many other diseases of significant interest to public health. For example, during the 1998, 2000 and 2001 enterovirus 71 epidemics, children aged 3 years and younger who were at higher risk of severe or fatal cases of the disease were identified for more effective prevention only after the occurrence of several cases of sudden deaths from weekly sentinel physician surveillance and later retrospective epidemiological data analysis on those cases when sample size became larger. Therefore, prospective monitoring of daily ongoing data of EVI syndrome in this high risk age group and early ED-SSS detection of enterovirus activities by local public health personnel might help minimize social panic among parents. Furthermore, the results on seasonal pattern of enterovirus-like infection in our ED-SSS was consistent with the previous epidemic patterns in Taiwan, again demonstrating the usefulness of ED-SSS to avoid future large-scale or severe epidemics caused by enteroviruses [18] . In summary, these initial findings suggest that it is necessary to develop algorithms capable of detecting aberrations for different syndrome groups from patients in different geographical areas of Taiwan, taking into account variations in the levels of medical care and the effect of weekends and holidays on ER visit.

The ED-SSS did not, however, reveal obvious trends in all syndrome groups. For example, it was hard to find seasonal patterns or secular trends in cases of coma/death, skin rash, or neurological symptoms -the three syndrome groups that might be useful in the detection of severe outbreaks caused by bioterrorism, e.g., anthrax, during the study period without bioterrorism attacks [28, 29] . Certainly, continuous monitoring for these syndrome groups at both local and national levels will be very helpful in detecting possible bioterrorism or EIDs in future years. Using those trends in coma/death and other syndrome groups of clinical severity or unexpected symptoms/signs, our ED-SSS data have provided directions for further research in the areas of pathogen detection, epidemiological clues, and improvement in public health policies. Therefore, future investigations have to control the weekend and holiday effects of ED visits for better aberration detection even during long holidays.

In daily public health practice to monitor the data of ED-SSS, careful verification and systematic management is needed once the aberration signals are detected. The server needs an automatic error feedback system function instead of the original use of engineers to double check for data errors would increase the efficiency and completeness of surveillance. Future efforts require closer collaboration between computer-science professionals and medical informatics personnel at the Taiwan-CDC to establish a system with the standard operating procedures (SOP) for database maintenance and to provide more continuous on-job training for both hospital users and local and central public health agencies [30] .

The major difficulty in developing our ED-SSS was diverse formats for different types of data, including categories of chief complaints, the ways to fill out ICD-9-CM codes, and even the different number of digits used in home zip codes in different participated hospitals. For example, most Taiwan hospital ED physicians/nurses EDs only write down one chief complaint, which is very different from the ED reports made by most U.S. hospitals which list all possible complaints (with text format) in English. Several participating hospitals only had a paper system for recording triage chief complaint data. A standard format for select syndromes and variables needs to be established and continuously reevaluated to improve data quality and stability of data transmission. There are needs to have more research into the chief complaints with Chinese styles, the suitability of chief complaints vs. ICD-9 codes, how to combine symptoms/signs and link data to improve sensitivity.

With regard to the current epidemics of avian influenza H5N1 in China and many other southeast Asian countries, an ED-SSS like the one we developed in Taiwan may play an important role in detecting an outbreak possibly caused by human-to-human transmission even when cluster size is small [31] [32] [33] [34] . Through early detection, ED-SSS may help minimize the adaptation of avian influenza virus to human populations. Because of the large volume of business traffic, international travelers, and workers from Southeast Asia coming to Taiwan, it has previously been difficult to do real-time surveillance for imported infectious diseases, including dengue, malaria, acquired immunodeficiency syndrome (AIDS) and SARS. However, using the ED-SSS to monitor health status at the community level may help public health decision-makers handle unexpected health threats. Because countries are so interconnected today, it is imperative that we share our health information and experiences with other countries if international health is to be guarded. Our ED-SSS has equipped Taiwan the ability to closely monitor avian influenza and other potential EIDs in Asia and worldwide. We hope that by sharing our experiences developing ED-SSS, other countries can be encouraged to develop and improve their own surveillance systems for infectious disease.

The author(s) declare that they have no competing interests.

TSJW was in charge of epidemiological data analysis, improvement of the ED -Syndromic Surveillance System, and manuscript writing. FYFS initiated the thoughts on Syndromic Surveillance for detecting emerging infectious diseases in 2003 and contributed to syndrome groupings, selection of variables for ED-SSS, and system improvement based on clinical data analysis. MYY contributed to syndrome groupings, initiating the standard format for collecting Chinese Chief-Complaints in our ED-SSS, and system improvement with regard to clinical aspects. JSJW initiated the thoughts on Syndromic Surveillance for Emerging Infectious Disease in 2003. SWL worked on computer programming on ED-SSS and health informatics for surveillance systems of infectious disease at Taiwan-CDC. KCMC was a leader and coordinator on health informatics for surveillance systems of infectious diseases at Taiwan-CDC and gave the most administrative support on systematic improvement in health informatics. CH provided statistical consultation and chose the best statistical modeling method for outbreak detection in the initiation stage of establishing the ED-SSS. JHC was the Deputy Director at Taiwan-CDC in charge of improving surveillance of infectious diseases, coordinated the 189 hospitals designated for emergency health care to participate the ED-SSS, and provided strong administrative support on ED-SSS. YTC was a research assistant of ED-SSS in charge of the data analysis and project administrative assistance. HC was the Director of Department of Health in Taipei City and participated in the task force meetings from planning to implementation of ED-SSS in the perspective of the local government. CHC was the Section Chief of Department of Health in Taipei City in charge of surveillance, prevention and control of infectious diseases in Taipei City and gave suggestions health informatics and practical concerns from the viewpoints of local government. FCRT helped set up the Real-time Outbreak and Disease Surveillance (RODS) system at Taiwan-CDC. MMW introduced our public health officials and scholars in Taiwan to the practical applications of RODS in the USA and informed us of recent progress of RODS. IJS, the former director of Taiwan-CDC, had the vision to invite scholars to discuss the improvement of infectious surveillance system in Taiwan right after the 2003 outbreak of SARS. CCK, involved in the improvement of infectious disease surveillance in Taiwan for more than twelve years, initiated research on syndromic surveillance, coordinated each trouble-shooting step as the ED-SSS was developed and implemented, and was involved the revision of the manuscript. All authors read and approved the final manuscript.

Publish with Bio Med Central and every scientist can read your work free of charge 

",0.6370788432925083
Which Kind of Provider's Operation Volumes Matters? Associations between CABG Surgical Site Infection Risk and Hospital and Surgeon Operation Volumes among Medical Centers in Taiwan,"Volume-infection relationships have been examined for high-risk surgical procedures, but the conclusions remain controversial. The inconsistency might be due to inaccurate identification of cases of infection and different methods of categorizing service volumes. This study takes coronary artery bypass graft (CABG) surgical site infections (SSIs) as an example to examine whether a relationship exists between operation volumes and SSIs, when different SSIs case identification, definitions and categorization methods of operation volumes were implemented.","data, which should use hierarchical models, may result in biased estimation of the variation and also lead to incorrect conclusions.

SSIs following coronary artery bypass graft (CABG) procedures place a heavy burden on patients and healthcare systems. The total length of stay and expenditure for patients with SSIs after CABG surgery is significantly longer and higher than those without SSIs. [20, 21] In 2008, the Centers for Medicare & Medicaid of the United States of America implemented the ""Never Event"" policy, where hospitals would no longer receive higher payments for the additional costs associated with treating patients for certain healthcare-acquired infections, including those related to CABG.

In view of the accuracy of SSIs identification and the heterogeneity of definition and categorization methods, no existing studies have used different infection case identification nor definitions and categorization methods of operation volume simultaneously to explore the relationship between operation volumes and infection. The current study takes CABG SSIs as an example to examine whether a relationship exists between operation volumes and SSIs, given different SSI cases identification, operation volume definitions and categorization methods.

This retrospective and cross-sectional study adopted a multilevel design to examine the relationships between provider volumes and SSIs after adjusting for patient-, surgeon-, and hospital-level covariates.

We used data from the Taiwan National Health Insurance Research Database (NHIRD) from 2005 and 2008. The NHIRD, published by the Taiwan National Health Research Institute, includes all the original claims data and registration files for beneficiaries enrolled under the National Health Insurance (NHI) program. The database covers the 23 million Taiwanese enrollees (approximately 98% of the population) in the NHI program. It is a de-identified secondary database containing patient-level demographic and administrative information; however, treatment items are aggregated and without time-related and clinical information. The data is released for research purposes.

The protocol for the study was approved by the Institutional Review Board of the National Taiwan University Hospital (protocol #201001027R). The dataset we used in this study was secondary data; all information was de-identified by data owners.

In this study, we adopted the ICD-9-CM SSI codes (hereafter referred to as the ICD-9-CM based model) and the Classification and Regression Trees (CART) model, which was developed in our previous work [11] to identify SSI cases. As we mentioned above, the ICD-9-CM SSI codes were the most popular tool to identify the SSI cases in claims data. In the ICD-9-CM based model, SSI cases were divided into two categories: index hospitalization events and post-discharge events (i.e., SSIs that occurred within 1 year after discharge and required readmission to a hospital and/ or the use of ambulatory services). Following Wu et al [13] , this study adopted the secondary ICD-9-CM diagnosis codes for index hospitalization events (ICD-9-CM code: 996.03, 996.61, 996.72, and 998.5), and the primary and secondary diagnosis codes for post-discharge events (ICD-9-CM code: 038.0-038. 4 ) as the criteria for SSI identification, in order to avoid cases in which infection existed prior to hospitalization. If a case had an index hospitalization event or a post-discharge event, then he/ she will be identified as SSIs by the ICD-9-CM based model. In the CART model, we adopted the type of antibiotics, dose of cefazolin, length of stay, and number of vessels obstructed (as a proxy indicator of duration of operation) as the parameters to identify the SSIs, according to our previous findings. [11] In our previous work, we used the 2005-2008 National Health Insurance claims data and healthcare-associated infection surveillance data from two medical centers for model development and model verification. Infection cases based on surveillance were identified by infection control personnel if the patient met the Taiwan CDC's criteria, which are the same as those adopted in the U.S. CDC. They manually review medical records of all patients at risk for the specified healthcare-associated infection.

The classification algorithms, the multivariable regression model, and the data mining model were adopted to develop alternative models based on surrogate indicators to identify cases of CABG SSIs and to compare the performance among these models and the ICD-9-CMbased model. For the classification algorithms, researchers build up several criteria, and if a case satisfies (or exceeds) a specific number of criteria, then it will be identified as a case of infection. For the multivariable regression model, researchers usually calculated a risk score by the logistic regression model, and the optimal cutoff point was determined according to the resulting receiver operating characteristic curve.

Concerning the data mining approach, which is widely used for predicting and classifying objects, the characteristics are: automatic discovery of patterns, prediction of likely outcomes, creation of actionable information, and focus on large data sets and databases. The classification and regression tree (CART) model, which is the most popular approach as applied in our work, and the growing, stopping, and pruning of the tree were determined by Gini improvement measures. [22, 23] After referring to the literature and conferring with infectious disease specialists, we adopted the following seven parameters: type of antibiotic, doses of antibiotic, doses of cefazolin, use of second-line antibiotics, length of stay, and number of vessels obstructed. Additionally, cross-validation was also employed, where data from one medical center was used for model development, and another one was used for model validation.

The results of our previous work revealed that the CART model offered better performance than that of the other identification models or the ICD-9-CM based model, especially in the positive predictive value (>70%), which was only found to be 20% in the ICD-9-CM based model. (Table 1 ) The findings also implied that the CART was a decidedly better tool for identifying cases of SSI in the Taiwan National Health Insurance database. Therefore, this study also adopted the CART model for identifying CABG SSIs.

To ensure homogeneity, current study analyzed 7,007 patients from 19 medical centers in Taiwan who underwent CABG surgery (ICD-9-CM procedure codes 36.1x-36.2x) between 2006 and 2008. CABG patients under the age of 18 years or over 85 years were excluded in this study. A total of 302 cases were identified as SSIs by ICD-9-CM based model, and a total of 107 cases were identified as SSIs by CART model.

In this study, we used the following two definitions to define operation volumes: (1) the cumulative operation volumes by each surgeon and hospital within the study period, which was the most common definition in the literature; and (2) following Yasunaga et al.'s study, [24] cumulative operation volumes by each surgeon and hospital in the previous one year for each surgery. However, our data was skewed, which did not follow a normal distribution. Therefore, we conducted the log transformations on operation volumes.

The current work treated operation volumes in three different ways: (1) a continuous variable; (2) a categorical variable based on the first and the third quartile as cutoff points (the most common method to categorize service/ operation volumes) [25] [26] [27] [28] ; and (3) a data-driven categorical variable based on k-means clustering algorithm. This study categorized surgeon and hospital volumes into low, medium, and high volume groups by quartile method and kmeans clustering algorithm.

In the quartile method, the cut-off value (transformed by logarithm) of the first quartile (<25%) for hospital volumes was 5.65, and the third quartile (>75%) was 6.43. In terms of surgeon volumes, the first quartile was 4.38, and the third was 5.35, when we used the cumulative operation volumes within the study period as the definition. While the definition changed, first quartile (<25%) for hospital volumes was 4.66, and the third quartile (>75%) was 5.31. In terms of surgeon volumes, the first quartile was 3.40, and the third was 4.32.

K-means clustering is an unsupervised machine-learning algorithm introduced by MacQueen in 1960s. This method is not only a simple and very reliable method in categorization/ classification, but is also recognized as one of the top 10 algorithms in data mining. [29] This method has often been applied in many fields. [30] [31] [32] Yu and his colleagues even applied it to define the quality of CABG care, and to explore the relationship among patient's income status, the level of quality of care, and inpatient mortality. [33] The main idea of this method is to partition observed data points into k non-overlapping clusters by minimizing the within-group sum of squares. Each point is assigned to the mean of its cluster using the Euclidian distance. Firstly, k cluster centers were randomly generated. Previous studies usually divided surgeons and hospitals into low-, medium-, and high-volume groups; therefore, we also predetermined the surgeon and hospital service volumes into 3 groups (k = 3). Then, participants were assigned to the cluster with the shortest distance to these cluster centers. Finally, the cluster centers were recomputed using the new cluster assignment and these steps would be iterated until convergence was achieved. [34] The cut-off values of hospital volumes were 5.21 and 5.69, and for surgeon's volumes were 2.40 and 4.38 respectively, when cumulative operation volumes within the study period was used as the definition. Likewise, when cumulative operation volumes before each surgery was used as definition, the cut-off values were 4.11 and 4.89 for hospital volumes, and 2.64 and 3.91 for surgeon's volumes. All cutoff values were transformed by logarithm. The results of k-means clustering are demonstrated in Figs 1-4. As the results show, the operation volumes were divided into three groups separately. In addition to surgeon and hospital volumes and SSI, we collected patient-, surgeon-, and hospital-level data. Firstly, patient-level variables included age, gender, length of ICU stay, number of vessels obstructed that were involved in the surgical operation, and the presence of important underlying diseases (e.g. diabetes mellitus, chronic obstructive pulmonary disease (COPD), heart failure, renal failure and renal insufficiency, which were associated with SSI).

[13] Secondly, the surgeon-level variables included age and gender. Thirdly, the hospital-level variables included hospital ownership and geographic location.

All statistical analyses of volume-infection relationship were performed using SAS (version 9.2, SAS Institution Inc., Cary, NC, USA). In statistical testing, a two-sided p value 0.05 was considered statistically significant. The distributional properties of continuous variables were expressed by mean ± standard deviation (SD), whereas categorical variables were presented by frequency and percentage. In univariate analysis, the potential three-level predictors of SSI were examined using chi-square test or two-sample t-test as appropriate. Next, to account for the correlations within surgeon (level-2) and hospital (level-3), multivariate analysis was conducted by fitting mixed-effects logistic regression models to each patient's data for estimating the effects of three-level predictors on the probability of post-operational SSI. Furthermore, subgroup analysis for comorbidities was also conducted. Table 2 shows that there were 7,007 patients with CABG performed by 199 surgeons in 19 hospitals during 2006-2008 in Taiwan. The majority of patients were male (77.5%), and the mean age of patients was 65.3 years. The average ICU stay was 6.05 days, the mean level of number of vessels obstructed was around 1.6, while 51.8% of patients had diabetes mellitus, 33.3% had heart failure, 14.1% had renal failure and renal insufficiency, and 22.0% had COPD. Three hundred and two patients (4.31%) were identified as having the ICD-9-CM SSI codes. However, identification by the CART model only revealed 107 infection cases, and 94 cases were identified in both models. Most cases received CABG surgery by male surgeons, with a mean age of 45.0 years, and the surgeon's average operation volumes within the study period was 151.64, while the average operation volumes before surgery was 52.18. More than half of the cases were performed with CABG in not-for-profit hospitals, and the hospitals' average operation volumes within the study period was 473.60, while the average operation volumes before each surgery was 158.79. Moreover, most of patients received their surgeries by high-volume surgeons and hospitals, when k-means algorithm was used for categorization, regardless of which definition of operation volumes were used. Table 3 shows the results of multilevel mixed-effect models, with the SSIs being identified by ICD-9-CM codes, and the operation volumes defined as the cumulative volumes within the study period. The results of Model 1 (continuous) reveal that the surgeon's volumes were negatively associated with SSIs, while hospital's volumes were not associated with surgical site infection SSIs. Model 2 (quartile) suggests that low-volume surgeons had higher SSI risk (OR = 2.220, p-value = 0.022) than high-volume surgeons. There were also no associations between hospital's operation volumes and SSIs. Model 3 (k-means) shows that the association did not exist between hospital's/ surgeon's volumes and SSIs. Table 4 displays the results of multilevel mixed-effect models, in which the SSIs were identified by the CART model, and the operation volumes were also defined as the cumulative volumes within the study period. Model 1 again indicated a negative association between surgeon's volumes and SSIs, and hospital's volumes were not found to be associated with SSIs. In Model 2, the results showed that the relationship between hospital's/ surgeon's volumes and SSIs did not exist. In Model 3, results revealed low-volume surgeons had higher risk (OR = 1.691, p = 0.002) than high-volume surgeons. Table 5 displays the results of multilevel mixed-effect models, in which the SSIs were identified by ICD-9-CM codes, but the operation volumes were defined as the cumulative volume in the previous one year for each surgery. Model 1 also indicated a negative association between surgeon's volumes and SSIs, and hospital's volumes were not found to be associated with SSIs. In Model 2, the results showed that the relationship between hospital's/ surgeon's volumes and SSIs did not exist. In Model 3, results also revealed low-volume surgeons had higher risk (OR = 1.642, p = 0.040) than high-volume surgeons. Table 6 displays the results of multilevel mixed-effect models, in which the SSIs were identified by the CART model, and the operation volumes were also defined as the cumulative volume in previous one year for each surgery. In Model 1, different to the above findings, there was no association between hospital's/ surgeon's volumes and SSIs. In Model 2, the results showed that the relationship between hospital's/ surgeon's volumes and SSIs did not exist. In Model 3, results also revealed low-volume surgeons had higher risk (OR = 1.163, p = 0.020) than high-volume surgeons.

We further examined the associations of surgeon and hospital volumes with SSIs in stratification analyses by underlying diseases. When the operation volumes were defined as the cumulative operation volume within the study period, no relationships existed between hospital/ surgeon operation volumes and SSIs. (Table 7 ) However, when the operation volumes were defined as the cumulative operation volumes in the previous one year for each surgery, the results suggested that there was a negative association between surgeon volumes and SSIs in the diabetes group, except that the volumes were treated as continuous variable and the infection cases were identified by ICD-9 codes. In terms of hospital operation volumes, the association did not exist. (Table 8 ) 

No studies have evaluated how different service/ operation volumes definitions and categorization methods affect volume-infection relationships. Moreover, several studies have pointed out the inappropriateness of identifying infection cases using the ICD-9-CM codes in claims data. Given these reasons, this study adopted two approaches to identifying SSIs, two definitions of operation volumes, and three methods for categorizing operation volumes to examine the relationships between operation volumes and SSIs. Our findings showed that the relationships between hospital volumes and SSIs did not exist, no matter which definitions, categorization mehods, or SSIs case identification approaches were used. On the contrary, the relationships between surgeon volumes and SSIs were not robust in our data. It might be affected by different definitions and categorization methods of operation volumes, and also by different SSI cases identification approaches. In summary, most of the models demonstrated that the low-volume surgeons had higher risk than high-volume surgeons, and they also showed the risks were similar between medium-volume and high-volume surgeons. However, why did surgeon volume relate to SSIs, but hospital volume did not? Except for those issues we were concerned about in this study, there are some disagreements in the literature. Such as ""Does provider volume really represent quality of care?"" [12, 35] Or ""Is provider volume the only one predictor for outcome of care?"" [35, 36] These issues are worthy of further discussion, but are out of the scope of this study.

Service/ operation volumes are treated as a proxy indicator for experiences; previous studies used it to examine whether practice makes perfect or not. But, except for provider's experiences, SSIs are also impacted by many factors, such as environmental and clinical factors. Wu et al once used Taiwan 2001 NHI claims data to explore the relationship between provider CABG operation volumes and SSIs. [13] They found that hospital volumes had a greater effect than surgeon volumes and claimed that this may imply that hospital teamwork is more important than individual surgeon. However, our findings demonstrated that there was no relationship between hospital volumes and SSIs. Wu et al. adopted the cumulative operation volumes within the study period as the definition, and identified SSIs by ICD-9-CM codes. Except, there were two differences between our work and Wu et al., which were the length and year of the data; our data was longer and more updated than theirs. Moreover, it is worth noting that there was an outbreak of severe acute respiratory syndrome (SARS) in Taiwan in 2003, after which the hospital infection control system in Taiwan was reviewed and re-designed. Wu et al data was before SARS, so these efforts may also have improved the level of SSIs control in hospitals, leading to different findings in this study.

In addition, although most models revealed that there were negative relationships between surgeon's volumes and surgical site infection, the relationships were not robust. The results varied between different definitions and categorization method of operation volumes, and between SSIs identification approaches. Researchers need to consider how to identify SSIs correctly, how to choose optimal cut-off values, and how to decide on which definition is appropriate.

Finally, the results of stratification analyses showed that low-volume surgeon had higher risk than high-volume surgeon in the diabetes mellitus group, when the cumulative operation in the previous one year before surgery was used as definition. A large number of studies have indicated diabetes mellitus is associated with a higher risk of SSIs, [37] [38] [39] and the findings of this study suggest that CABG patients with diabetes mellitus should be cared for by experienced surgeons.

A multilevel analysis was applied to manage the nested factors, and two definitions of operation volume along with three different operation volume categorization methods were adopted to examine the relationship between volume and SSIs under two kinds of SSIs identification approaches. Nevertheless, the study suffered from several major limitations. First, the accuracy of SSIs identification was still an issue. Although the performance of the CART model to identify CABG SSIs was better than ICD-9-CM codes in Taiwan NHI claims data, it did not reach the perfect scenario. The accuracy of SSIs identification was still a challenge in our work. The second limitation relates to unmeasured variables, such as length of stay before operation, infection condition, hair removal, clinical information (e.g. blood glucose level, causative microorganism), time-related information (e.g. the duration of operation), the environment, surgical skills, use of post-operative drains, number of operations involved, and surgical site and wound care, etc. [40] Furthermore, information about type (elective or urgent) and incision site for surgery was not available in the Taiwan NHI claims data.

In conclusion, the findings of this study suggest that different definitions and categorization methods of operation volumes, and different SSIs identification approaches might lead to different findings, although surgeon volumes were more important than hospital volumes in exploring the relationships between CABG operation volumes and SSIs in Taiwan, but they were still not robust. Definitions and categorization methods of operation volumes, and correct identification of SSIs are important issues for future research.

",0.636675042595518
