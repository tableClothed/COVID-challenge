title,abstract,text,similarity
Internet-based surveillance systems for monitoring emerging infectious diseases,"Emerging infectious diseases present a complex challenge to public health offi cials and governments; these challenges have been compounded by rapidly shifting patterns of human behaviour and globalisation. The increase in emerging infectious diseases has led to calls for new technologies and approaches for detection, tracking, reporting, and response. Internet-based surveillance systems off er a novel and developing means of monitoring conditions of public health concern, including emerging infectious diseases. We review studies that have exploited internet use and search trends to monitor two such diseases: infl uenza and dengue. Internet-based surveillance systems have good congruence with traditional surveillance approaches. Additionally, internet-based approaches are logistically and economically appealing. However, they do not have the capacity to replace traditional surveillance systems; they should not be viewed as an alternative, but rather an extension. Future research should focus on using data generated through internet-based surveillance and response systems to bolster the capacity of traditional surveillance systems for emerging infectious diseases.","Emerging infectious diseases are of particular concern to public health. Emergence is driven by sociocultural, environmental, and ecological factors. 1 The vulnerability of people to emerging infectious diseases has been shown by the emergence of AIDS in the late 1970s, severe acute respiratory syndrome (SARS) in 2003, pandemic infl uenza H1N1 in 2009, and multidrug-resistant nosocomial pathogens, as well as the re-emergence of dengue, chikungunya, and malaria. Traditionally, eff ective disease surveillance is expensive and needs a formal public health network. 2 Such systems are maintained by most countries, to varying degrees. Data sources, surveillance methods, analytical approaches, and factors aff ecting these systems are varied and have been reviewed in detail elsewhere. 3, 4 Traditional, passive surveillance systems typically rely on data submitted to the relevant public health authority by physicians, laboratories, and other health-care providers; they provide information crucial to the eff ective functioning of health systems. 5 These systems can be com plex and expensive. Time and resource constraints, as well as a lack of operational knowledge of reporting systems, adversely aff ect the completeness of reporting, 6 resulting in an incomplete account of disease emergence. Furthermore, substantial lags between an event and its notifi cation are common; a result of late or failed reporting and the hierarchical structure of these systems. 7 The average delay from receipt to dissemination of data by traditional sentinel surveillance networks is roughly 2 weeks. 8 Internet availability and use has increased greatly in the past 10 years (fi gure 1). 9, 10 The availability of healthrelated information on the internet (of varying quality and legitimacy) has also changed how people seek information about health. 10, 11 These changes provide a new means to detect and monitor infectious diseases. The nature of emerging infectious diseases often limits the eff ectiveness of traditional surveillance systems. 12 Digital surveillance could improve both the sensitivity and timeliness of detection of health events. 13 We review recent studies that have exploited internet use and search trends to monitor two acute-onset viral illnesses of worldwide importance that have substantial seasonal and geographic variation: infl uenza and dengue. We critically analyse the eff ectiveness of monitoring internet data to track these diseases and discuss the advantages and limitations of this approach. Finally, we make recommendations for future research into these systems.

Digital surveillance attempts to provide knowledge of public health issues by analysis of health information stored digitally, as well as the distribution and patterns governing access to these data. Approaches to digital surveillance vary according to the media targeted. However, all exploit changes in behaviour related to information seeking, collection, storage, and communication pathways that have occurred with the development and increased availability of the internet and associated technologies.

Several surveillance systems use non-structured, eventbased, digital data. 14 The Global Public Health Intelligence Network (GPHIN)-developed by the Public Health Agency of Canada-automatically retrieves information about potential public health emergencies from news feed aggregators and distributes this information to public health agencies, including the WHO Global Outbreak Alert and Response Network. 2, 15 The eff ectiveness of this system was shown during the SARS outbreak; GPHIN detected SARS more than 2 months before the fi rst publications by the WHO. 2 Other systems-eg, HealthMap 16 and ProMED-mail 17, 18 -provide information about emerging public health problems by aggregating information about emerging diseases from various structured and non-structured data sources. These and other similar systems are reviewed elsewhere. 13, 14, 19 Internet use has increased consistently in almost every country. 20 Internet users in the USA alone generate 8 million queries for health-related information every day. 21 The increase in worldwide internet availability and use over the past 10 years, combined with these changes in health-seeking behaviour, has created new possibilities for the development of innovative surveillance systems. 9, 10, 22, 23 Although still very much in its infancy, analysis of digital data has been used to monitor communicable 2,24-33 and non-communicable diseases, 34, 35 as well as mental health, 36, 37 illegal drug use, 38 health policy impact, 39 and behaviours with potential health implications. 40 Numerous studies have sought to exploit online healthseeking behaviour to monitor disease incidence. Although these studies use diff erent data sources, they all rely on the premise that people who contract a disease will seek information about their condition from the internet and that incidence can be estimated by tracking changes in frequencies of searches for key terms. By monitoring search queries submitted to the search engine Yahoo!, Polgreen and colleagues 41 predicted increases in positive infl uenza cultures 1-3 weeks before their occurrence. Similarly, Hulth and co-workers 42 developed a model for estimating intensity and peak incidence of infl uenza in Sweden by monitoring queries submitted to the medical web site Vårdguiden. This model correlated closely with both data for infl uenza-like illness (R²=0·89) and laboratory-confi rmed cases of infl uenza (R²=0·90). A subsequent study showed this model to have good congruence with sentinel data over the course of the 2009 infl uenza H1N1 pandemic (r=0·88-0·90). 43 More recently, infl uenza incidence in China was estimated by assessment of searches submitted to Baidu (the most commonly used search engine in China). 44 This study reported a correlation of R²=0·96 between a composite search index (eight terms) and monthly Ministry of Health infl uenza reports. Furthermore, using a combination of Ministry of Health and Baidu data, the researchers produced accurate estimates (R²=0·95) of incidence 1-2 weeks before of Ministry of Health reports.

Google search queries also correlate highly with disease incidence. Historical logs of aggregated Google search queries-presented as normalised time seriesare publically available through Google Trends from Jan 1, 2004. These data are available by country, state, and city in the USA, but only at country-level for many other regions (especially low-income countries). Previously, Google off ered two user interfaces to access search reports: Google Trends and Google Insights for Search, which were merged in September, 2012. 45 Carneiro and Mylonakis 24 used Google Trends to analyse worldwide search frequency for the term ""bird fl u"". They reported an increase in search frequency between 2005, and 2006, coinciding with the spread of avian infl uenza from China to Turkey. Other studies reported the use of Google search data to monitor the frequency of searches for manually selected terms related to infl uenza in Chinese, 46 Spanish, 31 and French. 28 These studies reported high degrees of correlation, which shows the potential application of this technology in languages other than English. Ginsberg and colleagues 47 used an automated approach to select search terms from Google search logs with the greatest correlation with the US Infl uenza Sentinel Provider Surveillance Network of the US Centers for Disease Control and Prevention (CDC). The terms were then used to develop a model for monitoring infl uenza activity. Estimates from the model correlated highly with regional Centers for Disease Control and Prevention data (r=0·80-0·96, nine regions) and accurately estimate incidence of infl uenza-like illness 1-2 weeks before surveillance reports. 47 An online infl uenza surveillance tool-Google Flu Trends-is based on the model of Ginsberg and coworkers and now includes 29 countries.

To date, two publications have reported the use of internet search data to estimate incidence of dengue. Chan and colleagues 48 used a similar method to Ginsberg and coworkers 47 to create models of dengue transmission for Bolivia, Brazil, India, Indonesia, and Singapore. Correlations between model estimates and holdout surveillance data (data excluded from the model, used for validation) were high for all countries (r=0·83-0·99). These models have been used to develop a free, publically available online resource for dengue surveillance: Google Dengue Trends. Althouse and colleagues 49 used Google Insights for Search to monitor searches for dengue-related terms and applied these results to step-down linear regression models for Bangkok (Thailand) and Singapore. Both models showed a high degree of correlation with surveillance data (R²=0·95 for Bangkok and 0·94 for Singapore). Additionally, this study developed support vector machine 50 and logistic regression models to predict periods of high dengue incidence. Area-under-the-receiveroperating-characteristic-curve, using the 75th percentile, was 0·960 for Bangkok and 0·906 for Singapore according 

to the support vector machine model compared with 0·960 and 0·896 respectively for the logistic regression model.

Several studies have compared the performance of Google Flu Trends with national data for infl uenza-like illness. Google Flu Trends was visually compared with surveillance data for Australia 51 53 These fi ndings accord with the results of Hulth and colleagues 43 and Valdivia and colleagues 54 who showed that Google Flu Trends correlated strongly with estimates of infl uenza incidence and peak incidences produced from data collected by sentinel physician networks throughout Europe. Finally, Google Flu Trends correlated highly with the Electronic Surveillance System for the Early Notifi cation of Community-Based Epidemics (ESSENCE; a syndromic surveillance system run by the US Department of Defence; r=0·88) 55 and with infl uenzalike illness estimates produced for Flanders, Belgium, by the Great Infl uenza Survey (a weekly, online infl uenza survey; r=0·62-0·94). 56 Traditional infl uenza surveillance systems commonly monitor incidence with virological data, rather than infl uenza-like illness. Data from Google Flu Trends was highly correlated with data from the US Infl uenza Virologic Surveillance System (r=0·72); 57 however, this correlation was lower than that reported for Google Flu Trends and CDC infl uenza-like illness data (r=0·94). Google Flu Trends estimates have been reported to correlate highly with laboratory-confi rmed infl uenza at a provincial and city level. Malik and colleagues 12 reported the correlation between weekly counts of laboratoryconfi rmed H1N1 infl uenza cases in Manitoba, Canada, and Google Flu Trends data during the 2009 infl uenza pandemic (R²=0·69; 2 week lag). Similarly, Google Flu Trends had a high level of congruence with virology data from a Baltimore hospital (adult r=0·88; paediatric r=0·72). 58 Dugas and coworkers 58 noted that Google Flu Trends correlated well with paediatric emergency department crowding measures, leading them to suggest that Google Flu Trends could be used for strategic management of emergency department resources. The potential applications of Google Flu Trends data to strategic allocation of resources and priority setting is further shown by Patwardhan and Bilkovski, 59 who compared sales of four drugs commonly prescribed for treatment of infl uenza with Google Flu Trends and CDC ILINet data; aggregate correlation between Google Flu Trends and prescription sales was r=0·92 .

Any changes to the status quo of internet search behaviour could alter how well Google Flu Trends models actual infl uenza incidence. Loss of resolution might occur as a result of media-driven interest or through other events that change search behaviour. 43, 49, 57, 58, 60, 61 Google Flu Trends accounts for changing search behaviour by updating the model each year to best represent reference surveillance data. 62 Despite this precaution, a loss of resolution was reported to have occurred during the 2009 H1N1 infl uenza pandemic. 53 

Studies have predominantly focused on retrospective assessment of the performance of Google Flu Trends. However, almost real-time disease tracking can be done by application of Google Flu Trends data to a seasonspecifi c compartmental mathematical model. 63 Google Flu Trends data can also be used for early detection of epidemics. 64 Pervaiz and colleagues applied various algorithms to Google Flu Trends data to develop early epidemic detection systems capable of generating actionable alerts. Although this study did not identify a single best method, it showed the potential use of Google Flu Trends data in this manner. Zhou and coworkers 65 have developed a system to predict epidemic alert levels from daily Google Trends data. Hidden Markov modelbased methods predicted infl uenza alert levels in realtime with 97·7% accuracy and provided an indication of infl uenza activity up to 4 weeks ahead of the release of CDC reports. In another recent study, ensemble adjustment was used to assimilate Google Flu Trends data into a humidity-driven compartmental mathematical model, enabling real-time predictions of peaks to be made more than 7 weeks in advance of their occurrence. 66 Finally, using a negative binomial generalised autoregressive moving average model-which included Google Flu Trends data as a secondary variable-Dugas and colleagues 67 predicted weekly infl uenza cases at a medical centre with a high degree of accuracy (83% of estimates were within seven cases).

These models are promising and, overall, Google Flu Trends seems to provide timely and accurate estimates of infl uenza-like illness and laboratory-confi rmed infl uenza. However, methods to integrate this information into existing surveillance systems need to be developed. 55 Scarpino and colleagues 68 postulated that the predictive power of the Texas ILINet could be improved by use of a smaller set of carefully chosen sentinel providers. Additionally, they investigated the potential of incorporating Google Flu Trends data into the network as a virtual provider. Google Flu Trends was reported to have a high degree of correlation with the ILINet in Texas (R²=0·77 at a 0 week lag). It was the most informative provider, matching the predictive performance of an optimised network of 44 sentinel providers.

The power of social media as both a source of information and as a means of disseminating information is increasingly recognised in public health. 69, 70 Corley and colleagues 71 have proposed that infl uenza incidence could be estimated by tracking use of key terms in web and social media. They analysed the frequency of English language blog posts that contained the terms ""infl uenza"" or ""fl u"" and compared these with CDC ILINet data. Correlation was r=0·63 for this study and r=0·55 in a subsequent study with an extended dataset. 72 Microblogs (such as Twitter) were not included in these studies. Collier and coworkers 73 used supervised learning to categorise expressions from Twitter messages into fi ve infl uenza-related categories and correlated these expressions with CDC data for positive infl uenza A H1N1 tests. Correlations in this study (r=0·58-0·67) were similar to those for Corley and colleagues. 71, 72 Chew and Eysenbach 74 sorted Twitter posts containing terms related to infl uenza A H1N1 into groups describing ""personal experiences"" or ""concern"" and compared these with H1N1 incidence rates in the USA. Correlations were r=0·77 for ""personal experiences"" and r=0·66 for ""concern"". These correlations are not as high as those reported for approaches based on internet search queries. However, Lampos and Cristianini 75 reported correlations of up to r=0·933 for their analysis of infl uenza created with a supervised learning framework compared with infl uenza-like illnesses reported by the UK Health Protection Agency. They concluded that a supervised learning framework is a suitable method for selection of features for use in digital surveillance systems. Culotta 76 reported that the accuracy of estimates could be improved by use of a document classifi cation component; they reported correlations of up to r=0·97.

Google Flu Trends usually showed an increase of infl uenza incidence 0-2 weeks before traditional systems. Internet-based surveillance systems circumvent the bureaucratic structure of traditional systems. Furthermore, they target a diff erent section of the community to traditional surveillance systems. Zeng and Wagner's 77 model of patient behaviour during epidemics identifi es four phases in health-care seeking: recognition of symptoms, interpretation of symptoms, representation of illness, and seeking treatment. Traditional surveillance systems only source data from people seeking treatment. Internet-based surveillance systems access people from not just the fi nal phase, but also the earlier interpretation of symptoms and representation of illness phases. 48 However, internet-based surveillance systems are limited to people who seek health-related information on the internet (or proxies, such as parents or carers of sick children). Despite this limitation, they can capture many cases. Attrition during disease pathogenesis or healthseeking pathways is both high and cumulative-results of a study done in rural Cambodia showed that 67% of cases of haemorrhagic fever were treated at home, rather than in a health facility; thus, a health-care-based surveillance system would miss 67% of information before it even becomes accessible. 78 Systems that target points earlier in surveillance will produce more timely information. For an infl uenza epidemic with a 20% infection rate, 10% clinical attack rate, 2% case hospital admission rate, and 0·1% symptomatic case fatality rate, 79 the fraction of the population assessable by an internet-based surveillance system (7488 people per 100 000 patients) would be nearly ten-times that of a traditional system (750 people per 100 000 patients), for a population with the internet use of an average highincome country (76·8%; fi gure 2). 9 Internet-based surveillance systems work best for large populations 24 and their use can be limited by national infrastructure (fi gure 3). Although the fraction of people assessable with an internet-based system in the average low-income country (30·7% internet access) is only 2993 people per 100 000 patients (fi gure 2), this fraction still exceeds that of a traditional surveillance system (750 people per 100 000 patients). The number of people who have access to the internet is not the only relevant factor. Internet use and health-seeking behaviour vary between diff erent sectors of the community. 23, 80 The accuracy of national Google Flu Trends estimates is positively correlated with the proportion of the population who use the internet to obtain health-related information. 54 However, large discrepancies exist between availability and uptake of the internet, and seeking health-care information as a proxy for disease has biases stemming from unequal use and access. 20 The spatial resolution of Google Flu Trends (and Google Trends) is improving. At present, Google Flu Trends off ers some city-level estimates of infl uenza incidence in the USA but probably has neither the sensitivity nor spatial resolution necessary to detect small, localised outbreaks. 12 Spatial resolution is limited by the level of data aggregation and search volume; Review resolution should improve over time as overall internet access increases and the internet becomes more widely accepted as a source of health-based information. 54 Results of these systems should also be interpreted carefully. Although internet-based surveillance systems seem to have high correlation with traditional surveillance systems, overall correlations could hide short-term periods of high variance. 62 Translation of internet-based data into an accurate, meaningful, and useful format is a challenge. Bias introduced by self-reporting and media-driven interest might be the biggest confounder of internet-based surveillance systems. Targeting microblogs has the potential to track, not just disease activity, but also related community concerns and perceptions. 73 However, the frequency of posts on social media is generally accepted to be a function of personal experience and perception of what an individual believes the ir friends and followers would fi nd interesting, rather than a true refl ection of the occurrence of an event. 81 Similarly, the media drive search frequency. An increase in searches for ""bird fl u"" occurred in the USA between 2005 and 2006, despite no avian infl uenza being detected; this trend was attributed to media-driven interest about the infl uenza outbreak aff ecting Asia at the time. 24 A similar occurence was reported for dengue-related searches in India in 2006; an unusually large spike in searches was attributed to news that a member of the prime minister's family had been admitted to hosptial for dengue. 48 To reduce the eff ect of media-driven searches, the Google Dengue Trends model replaces spikes that exceed the mean of the previous 4 weeks by fi ve SDs with an imputed value. 48 Mediadriven behaviour does not exclusively aff ect internetbased surveillance systems. On April 26, 2009, the US CDC declared a national public-health emergency in response to the emerging H1N1 pandemic; the following week was termed fear week. 61 Despite state-wide viral surveillance data showing little infl uenza activity, emergency department patient volumes increased substantially. A similar trend occurred in a Baltimore paediatric emergency department. 58 Because changes in Google Flu Trends over this period correlated with the increase in emergency department patient volumes, the investigators suggested that Google Flu Trends could have a role in planning emergency department surge capacity; 58 rather than representing infl uenza incidence, Google Flu Trends identifi ed public perceptions of the threat of infl uenza and predicted the associated increase in health-care demand.

Unlike systems that rely on input from health-care practitioners or laboratories, internet-based surveillance instruments are unlikely to become overwhelmed during a pandemic and, because they are automated, are available year-round (contingent on suffi cient search volume), whereas traditional networks might only operate seasonally. 82 These internet-based systems could be of particular use in countries with poorly developed traditional surveillance systems. 52 However, implementation of such systems in these countries is fraught with diffi culties. Internet-based surveillance systems work on the premise that disease incidence correlates with frequency of information-seeking using specifi c terms. Textual information can be diffi cult to classify and interpret 83 and accuracy might be heavily aff ected by cultural nuances, language shifts, and use of colloquialisms or even memes. The model of Collier and coworkers 73 needed a fi lter to reduce the eff ect of terms such as ""Bieber fever"" (which refers to infatuation with Canadian pop musician Justin Bieber) on the keyword of interest, ""fever"". Changes to search behaviours and information-seeking practices will aff ect the performance of these models; 62 furthermore, such changes are unlikely to occur uniformly. The re-emergence of infectious diseases with similar clinical presentations-eg, chikungunya in dengue-endemic areas-also presents a diffi culty. 48 Models should be designed for a specifi c system (country or region) and be validated against reference data before they are used to guide health policy or action. As such, they cannot replace traditional surveillance. 42 The problem of privacy has been raised by several researchers. 2, 48, 83 For ethical reasons, data are de-identifi ed or-in the case of data from Google-aggregated before public release, precluding identifi cation of the source of specifi c posts or searches. Although not a problem in itself, this process could make interpretation diffi cult. Content cannot be connected with individuals and care should be taken not to commit an ecological fallacy-to make inferences about the characteristics of individuals based on aggregate data. 36 Finally, the security of health information is an imperative. 84 Google Flu Trends and Google Dengue Trends are operated by the philanthropic arm of Google, which is a publicly listed company. Although these services are freely available, Google does not release the search terms used in the algorithms; caution is urged in relying too heavily on closed-source data that are under the control of a multinational company.

Few studies have explored how to translate internetbased surveillance systems into a public health response. Search queries submitted to Vårdguiden have been used to develop an automated system for generation of reports about epidemiological trends. 85 GET WELL (Generating Epidemiological Trends from WEb Logs, Like) extracts search queries from Vårdguiden logs, aggregates the data (weekly), and produces time-series graphs. Additionally, the system enables custom statistical analyses to be integrated; this function is routinely used for norovirus and infl uenza. GET WELL is used by Swedish Institute for Infectious Disease Control in conjunction with traditional surveillance networks to identify emerging concerns and to focus epidemiological investigations.

The potential for internet-based surveillance systems to revolutionise emerging infectious disease surveillance was shown by Scarpino and colleagues. 68 They presented a method for optimisation of sentinel surveillance networks that enabled integration of Google Flu Trends into the network as a virtual provider (enabling it to function as a sentinal provider reporting infl uenza-like illness within the community). Google Flu Trends alone explained roughly 60% of infl uenza-associated hospital admissions in Texas; which is equivalent to the performance of an optimised sentinel network with 44 providers (R²=0·63). Furthermore, Google Flu Trends outperformed the 2008 Texas ILINet which drew information from 82 providers (R²=0·57). An optimised network of 82 providers outperformed Google Flu Trends (R²=0·77); however, the best predictive performance was achieved by optimised hybrid networks, which allowed use of Google Flu Trends as a virtual provider. Allowing Google Flu Trends as a virtual provider in a network of 82 providers increased predictive performance by a further 12·5% (R²=0·90). 68 These studies show potential applications of internet-based surveillance systems in bolstering traditional surveillance system capacity and guiding public health action. However, the routine integration of non-traditional, unstructured, internetbased data into existing surveillance systems will necessitate a change in the structure and rhetoric of units responsible for surveillance if it is to be eff ectively translated into public health action. 86

To date, most studies of internet-based surveillance systems are retrospective analyses of performance; the prospective performance of these systems needs to be assessed. Future studies should not only focus on development of new detection methods nor on application of these methods to new diseases, but they should also explore ways to integrate these approaches into existing systems. 55 In doing so, care must be taken to ensure that new systems add to the capacity of old ones. The potential application of internet-based surveillance systems is not restricted to surveillance. They can also be strategic instruments for resource management and allocation, 58, 59 which warrants further investigation. Finally, despite the potential of internet-based surveillance systems, they No data available <20% 20-40% 40-60% 60-80% >80%

have not been applied with a global focus. Strategies for surveillance of infectious diseases have been criticised for focusing too heavily on high-income countries. 87 New infectious diseases emerge all over the world and their emergence is aff ected by many sociocultural, economic, environmental, and ecological factors. 1 The international nature of emerging infectious diseases, combined with the globalisation of travel and trade, have increased the interconnectedness of all countries. Strategies to detect, monitor, and control emerging infectious diseases should recognise this change-these diseases are a global concern. The potential to develop global surveillance systems for emerging infectious diseases that use internet-based data should be explored.

Assessment of internet queries for surveillance of emerging infectious diseases is a new concept that has been applied with promising results. These systems are appealing from a logistical, economical, and epidemiological standpoint. Internet-based systems are intuitive, adaptable, operate in almost real-time and, once established, are cheap to operate and maintain. 12 Furthermore, these systems do not rely on the healthcare system to provide and analyse data, or a government to disseminate information and advise the international community of emerging concerns-all limitations of traditional surveillance systems. However, internet-based surveillance does not provide an alternative to traditional surveillance systems. Rather, these systems are an extension of traditional systems. The societal eff ect and extent of spread of infectious diseases within a community cannot be measured by any one surveillance system. 43 Surveillance systems should be fl exible, built with models that incorporate several means of collecting information, and integrate information from other sources to create a comprehensive understanding of and approach to addressing emerging problems. 86 Further more, addressing emerging infectious diseases is con tingent on their recognition as global, rather than regional, issues. A global response requires concerted international approaches to strengthen the capacity of emerging infectious diseases surveillance systems worldwide. Future research needs to focus on how to use internet-based surveillance systems to complement existing systems.

WH developed the original idea for this Review. The structure of the Review was developed by WH and GJM. GJM did the literature search, wrote the fi rst draft, and created tables and fi gures. WH produced the map. WH, ACAC, and GMW provided editorial advice on the report and the fi nal version was approved by all authors.

We declare that we have no confl icts of interest.

We searched Medline (via PubMed) and Web of Science with the following search terms: ""digital disease detection"" ""Google Flu Trends"", ""Google Insights"", ""Google Trends"", ""infodemiology"", ""infoveillance"", ""real-time disease surveillance"", and ""syndromic surveillance"". We also did searches with the terms ""dengue"", ""infectious"", OR ""infl uenza"" AND ""early warning"", ""Google"", ""internet"", ""search engine"", ""social media"", ""Twitter"", ""Facebook"", OR ""web"". Finally, we did searches for the terms ""internet"" OR ""web"" AND ""disease surveillance"" OR ""disease detection"". To be eligible for inclusion, studies needed to be peer reviewed, describe the use of internet-search metrics or social media data for surveillance of infl uenza or dengue, and assess performance of this surveillance approach by comparing it with data from traditional surveillance approaches. Results were restricted to those published in English between Jan 1, 2008, and June 30, 2013. The appendix shows the publications that fi t the inclusion criteria.

See Online for appendix

",0.8078308232755516
ScienceDirect Development of the electronic surveillance monitoring system on web applications,"This paper presents the electronic surveillance monitoring system (ESMS) via the web based application used especially for the health sector. The system is created for three purposes, the alert function for the surveillance and rapid response team (SRRT), the monitoring for inspection and evaluation, and the back-office report. With the import and export functions that are capable to retrieve electronic health record and edit the R506 and R507 data, the SRRT and local team can easily set up and manage their resources. The GPS function helps care takers to identify the geographical data for any countermeasure and action. The program is designed to submit reports automatically to the Ministry of Public Health (MOPH). The SRRT module can operate in both online and off-line conditions.","To achieve the goal for the total health care, World Health Organization (WHO) suggests four health services: the continuum of care, the health promotion, the disease prevention, and the palliative care service. These services are the main key success factor for the concept of health for all and all for health. The concept of health for all basically covers the primary health care and universal health coverage 1 . The ideas for responsiveness, efficiency, accessibility and availability are included. On the other hand, the concept of all for health is focused on health literacy, social and financial risk. WHO experts also recommended the care takers to concentrate their additional efforts on surveillance reports and epidemiological surveys in the community level, before moving to the hospital process 2,3 . Both health promotion and the disease prevention require the multidisciplinary team, which combines staffs such as physicians, epidemiologists, nurses, and village head. Stakeholders from hospital levels and community levels must operate together to overcome the diseases under surveillance 4 .

In additional to take a countermeasure from the cause of outbreak, the Ministry of Public Health (MOPH) regulates the protocols and establishes the surveillance and rapid response team (SRRT). SRRT consists of local staffs from MOPH, health volunteer, and community head. When the disease is found either in a hospital or a community, a SRRT team will go to investigate and report back to the hospital and the department of disease control (DDC) at MOPH. All activities, processes and evidences including the outcome must be measured and responded back to headquarter in the sub-region within 72 hours. Moreover according to the guideline, several reports from SRRT and teams must be submitted within 1-3 days to avoid the repeated cycle 5 .

Depending on the situation and disease, some surveillance reports are required to daily process from the SRRT to DDC, where some are weekly. For example the case report for diseases under surveillance 506 and 507 (R506 and R507) is daily requested. Serious epidemiology are required to monitor and response immediately such as Middle East respiratory syndrome coronavirus (MERS-CoV) and swine flu. All infected states must be presented to the centers disease control and prevention (CDC), WHO as agreement in MOU. However, these protocols and reports contains many different sub-system parts in which paper-based documents are basically used, and thus it becomes very difficult and complicated to process and share the data. Many have been informally done and handled carelessly, and thus no evidence could be found or confirmed.

As reports have to send to different departments and organizations, this is a difficult task for the SRRT and epidemiologist teams to catch up with their routine. Many times they fail to process due to the short of mandate. Therefore, for better prevalence and prevention for the surveillance team, this research aims to design the system that can monitor, track, and alert the SRRT team along with other necessary functions such as submitting reports in order to fill the gap and solve the above problems.

The ESMS is an electronic surveillance program designed by using web-based application to connect a SRRT, care providers and epidemiologist teams, and to process all protocols and documents with electronic forms. The concept of the program is to present four modules, the monitoring system, the alert and warning function, the job task system, and the report section. The existing processes and flows have been analyzed. The documents and files used for each process and the types of information are evaluated. The flow starts when the case is found either by a patient visit or a report. Warning and surveillance requests are issued from the epidemiologist team to three local organizations, SRRT, DCC, and municipality from Ministry of Interior (MOI). The case reports from sub-district and the field team are used to identify the priority of problem areas. If outbreak is contaminated, the final report will be sent to the provincial public health office and National Health Society Organization (NHSO). GIS and the webbased programming are added in order to determine the event location and abilities to retrieve the information.

As the design aims to replace the paper-based forms, new electronic record forms were designed for the SRRT and community health care team. All ESMS dataset of patients in the epidemic and surveillance services were simplified based on the process and data flow of R506 and R507 standard formats. All details were collected and analyzed in order to verify the problems and gaps between each informal process, and also to determine the informal documents such as Facebook, Line or Whatsapp. Moreover the connections between the data recorded in their processes were reorganized to minimize the under-coverage problems from the case report. Other problems such as when pandemic was subsided, OPD and IPD nurses only make a direct call to social medicine section for the patient that were sent for PCR were carefully considerate. The new dataset regarding to the new flow and stakeholder were listed as shown in Fig. 1 . Qualitative attributes such as simplicity, flexibility, and stability of the new system were verified and compared. In addition the stage, in which sample was send for the laboratory test, was modified to match the requirement from the MOPH headquarter. Instead of having only two laboratory stages, result positive and result negative, the result waiting was added as the third stage. This third stage simply put an ease to the care taker team as it took time for the department of medical science to analyze the simple being sent. Authority for each monitoring process were required as security. This program was designed to be able to import and export electronic health record (EHR) database module from hospital information system (HIS) and the surveillance documents R506 and R507. The new system flow was aimed to improve the completeness and quality of the report and information in the database.

The experiment results showed that the ESMS has abilities to help SRRT and the community team in the 8th health region. It successfully demonstrated four potentials for a better process as it reduces time and improves an overall performance by taking out the duplicated and informal documents. Many informal processes and documents such as using the social network were reduced. The results from SRRT and surveillance team indicated that with this system the contamination is easier to achieve. As the program allowed the team to manage and operate their work. With the data entry function that could select staff tasks along with their area of responsibility that matched the available resources, provincial public health staffs easily determine and set up the active field team according to the outbreak level. With the HIS-import function, the ESMS is able to fill out the patient forms in a shorter time and thus reduce the mandate and workload for the SRRT and back office.

Moreover, it showed the system effectiveness by warning the local team for data completion of the requested area, and reminding the DDC to analyze the laboratory test. The alert massage also set to pop-up when information was missing such as the number identification. This function helped a local team and care provider not to miss typing patient information. The GPS pointed out the demographical data in the event case. The program sent a report back to DDC and region health office in the csv format. Fig. 2 displays the items and functions in the ESMS report module as the dashboard style. All reports from each hospital using this program can successfully sent directly to the provincial public health office, but fail to deliver to National Health Society Organization (NHSO) due to the new data format. 

This ESMS can basically improve an overall performance by enhancing the electronic data into their protocol. The system reduced the human error and usage time from requiring and filling reports from different subsystems. The web application also provided a convenient way to access, edit, and modify data. This system checked null and error in the form before submitting a file which contains the surveillance reports from all hospitals in the 8 th health region to MOPH and other health organization. The reports generated from this program were mostly the csv format as agreement from the DDC. This saved the back-office team from the troubles as filling documents from the paperbased system was one of the toughest issues due to differences in types and formats. The simplified forms for SRRT and GPS helped the district health officer to manager their team and verify their performance.

",0.794128486283745
BMC Public Health Beyond traditional surveillance: applying syndromic surveillance to developing settings -opportunities and challenges,"Background: All countries need effective disease surveillance systems for early detection of outbreaks. The revised International Health Regulations [IHR], which entered into force for all 194 World Health Organization member states in 2007, have expanded traditional infectious disease notification to include surveillance for public health events of potential international importance, even if the causative agent is not yet known. However, there are no clearly established guidelines for how countries should conduct this surveillance, which types of emerging disease syndromes should be reported, nor any means for enforcement.","In this paper, we review examples of these novel applications in the detection of vector-borne diseases, foodborne illness, and sexually transmitted infections. We hope to demonstrate that syndromic surveillance in its basic version is a feasible and effective tool for surveillance in developing countries and may facilitate compliance with the new IHR guidelines.

All countries, whether high or low resourced, need sensitive disease surveillance systems for the early detection and monitoring of outbreaks. Syndromic surveillance, or the use of near ""real-time"" data and automated tools to detect and characterize unusual activity for further public health investigation, has been used in the United States and many other countries to augment traditional surveillance. For the purposes of this debate, we propose an expanded definition of syndromic surveillance to include the use of data on pre-diagnostic clinical syndromes rather than confirmed cases of specific diseases. The use of pre-diagnostic data and statistical algorithms aims to detect epidemics earlier than traditional surveillance based on reporting from laboratories and healthcare facilities, including atypical presentations of severe disease [1] . In 2003, over 100 different US health jurisdictions used syndromic surveillance to augment their public health surveillance [2] . In addition, several countries have used syndromic surveillance for the early detection and response to diseases of public health importance.

Despite this widespread use, syndromic surveillance is meant to enhance rather than replace traditional surveillance. An Institute of Medicine study concluded that a balance is needed between strengthening the proven approach of traditional surveillance and innovative surveillance systems [3] .

In many developing countries, surveillance is limited due to the lack of a robust public health or laboratory infrastructure; however, the revised International Health Regulations [IHR] , which entered into force for 194 World Health Organization [WHO] member states in 2007, require the reporting of public health emergencies of potential international concern even if the disease agent is unknown, such as for a previously unknown disease or a known disease presenting in more severe form [1, 4] . Syndromic surveillance, just as in the developed world, can augment traditional surveillance in developing countries [1] .

Syndromic surveillance often involves automated electronic reporting and statistical algorithms that sometimes require a complex information technology infrastructure. However, syndromic surveillance does not need to be highly computerized or technical; its tools can be simple, using few technological or human resources, and can complement existing surveillance programs [5] .

An early example of ""low technology"" syndromic surveillance is the use of acute flaccid paralysis (AFP) as the syndromic flag for poliomyelitis. The syndrome is infrequent and may detect an excess of cases of poliomyelitis in a timely fashion by comparing observed rates of AFP to expected rates [6, 7] . Nonetheless, syndromic surveillance can detect outbreaks of disease that do not fall into current WHO case classifications, which is particularly important for emerging diseases, or diseases with severe clinical presentations with undetermined diagnoses, such as the Severe Acute Respiratory Syndrome [SARS] outbreak of 2002-2003.

Since resources for surveillance are scarce in many countries, compounded by high rates of staff turnover and difficulties with Internet access and other communication tools, syndromic surveillance systems in low resourced countries need to be simple and build on prior work. The WHO open-source systems for surveillance are accessible to countries and technological assistance can be provided [5] . This paper will review the background for the IHRs and their application to syndromic surveillance, and review examples of syndromic surveillance programs that are currently being used in developing countries.

Given the recent concern for pandemics such as SARS and highly pathogenic avian influenza [H5N1], global and regional surveillance should be built on the concept of integrated surveillance. Prior to their revision, the IHR mandated reporting of only three diseases to the WHO: cholera, plague, and yellow fever. A revision of the IHR undertaken in 1995 was finalized in 2005 [4] . The revised IHR address the need for strengthening of disease surveillance by modifying disease lists to include syndromes for diseases of epidemic potential, and recommend the establishment of mechanisms for reporting outbreaks of major public health importance and the development of early warning surveillance systems. The IHR now include reporting by all countries for poliomyelitis, smallpox, human influenza caused by a new subtype, SARS, cholera, plague, yellow fever, viral hemorrhagic fevers, West Nile virus [WNV] , and other diseases of regional concern such as meningococcal disease and dengue [4, 8] .

Annex I, part A.4 (a) of the revised IHR states that ""state parties are to develop the capacity to detect events involving disease or death above expected levels for the particular time and place in all areas within the territory of the State Party,"" providing impetus for countries to improve their broad based public health surveillance infrastructure. IHR 2005 expands upon the previous IHR by broadening the scope of public health reporting, demanding improved surveillance and response at the country level, and strengthening core surveillance and outbreak response capacity [9] . Refer to Figure 1 Recommended syndromes for surveillance include hemorrhagic fever, acute respiratory syndrome, acute gastrointestinal syndrome, neurological syndrome, and a provision for severe infectious illness [10] . Although these syndromes are not officially part of the decision instrument, implementation of the syndromic approach complements the disease-specific approach with a precise definition for each syndrome, and was pilot-tested in 21 countries [8, 11] . Development and field testing of syndromic reporting initially identified 5 syndromes of potential public health importance. After the interim review, WHO concluded that syndromic reporting could be useful at the country level but was not feasible for the purposes of glo-bal public health reporting due to the challenges of field reporting of syndromes and inability to standardize rules for outbreak control for these syndromes [8, 11] . Baker and Fidler raised concerns that syndromic surveillance may not be effective in the timely detection of emerging diseases [9] . Although these critiques raised valid concerns, in some areas syndromic surveillance systems have detected outbreaks in a timely fashion, complementing traditional surveillance. The WHO also supported the use of syndromic surveillance at the national level during its interim review as part of strengthening core surveillance capacity. Many countries have already implemented surveillance systems to comply with the revised IHR, including surveillance for severe diarrhea, dengue fever [DF] and dengue hemorrhagic fever [DHF] , and acute flaccid paralysis [e.g., refs. [7, 8, 12] ].

A meeting of the Pan American Health Organization on the Surveillance Network for Emerging Infectious Diseases in Amazon countries in 2005 recommended the development of early warning systems, adopting the syndromic approach to surveillance to heighten the sensitiv- ity of disease detection and improve clinical management of cases, such as the febrile icteric syndrome for yellow fever. Although in 2005 there were few guidelines for syndromic surveillance in the region, a recommendation to disseminate protocols was made [13] , and new applications have since been developed, as we discuss below.

General Surveillance for Infectious Diseases Many syndromic surveillance systems detect general febrile illnesses such as malaria, dengue fever, other vector-borne diseases, and foodborne illness. Collaboration between countries with experience in syndromic surveillance and low resource nations has resulted in the introduction of syndromic surveillance to those countries.

[EWORS] is a collaboration between the Indonesian Ministry of Health and the US Naval Medical Research Unit-2 [NAMRU-2], in Jakarta, and was adopted by the Government of Indonesia as a national surveillance system in 2007 [1, 5, 14] . EWORS includes patients presenting to public hospitals and suspected of having an acute infection. Participating hospitals, clinics and emergency departments use a short, standardized questionnaire to collect demographic and clinical information. The questionnaire is filled out on a computer terminal with EWORS software and the data files are sent by email to the EWORS hub in the Ministry of Health for analysis daily. Counts of sign and symptom combinations that may reflect infectious diseases of national importance are compared to baseline counts by automated algorithms and monitored at reporting hospitals and Ministry of Health offices. If an outbreak is suspected, Ministry of Health staff may initiate investigation or control efforts. The EWORS hub also sends a monthly report to each participating site summarizing their surveillance data. System advantages include rapid data acquisition and interpretation by hospital operators, which may allow for earlier case finding. One limitation of EWORS has been the challenges with linking suspected outbreaks to response actions, which requires coordination of local bureaucracies. A second limitation is challenges with standardization of procedures at hubs, which can create alert uncertainty [15] . EWORS has now expanded to include other Southeast Asian nations, including Laos. In Peru, the Ministry of Health and US Naval Medical Research Center Detachment [NMRCD], in Lima, have collaborated to develop a similar system for early warning of dengue epidemics [15] . A second Peruvian system, Alerta, developed by Voxiva, the Peruvian Navy, and NMRCD, allows real time transmission via mobile telephone, text message, or Internet of health information from sailors and their families [1] . The system monitors all nationally notifiable diseases and syndromes, as well as other diseases of particular importance to the Peruvian Navy. [15] . Demographic and clinical data for suspected or confirmed cases of disease and syndromes is collected by the medical officer at each site, who spends ten to thirty minutes daily in medical record review for reporting. The data is then transmitted to the Alerta Disamar central hub by Internet, toll free telephone, or radio. Reporting frequency varies by disease and ranges from daily or twice weekly in batches for common syndromes such as diarrhea or respiratory illness. The hub uses Voxiva software to convert data reported by different communication methods to a common format. Staff review graphs of weekly counts automatically generated in Excel. Alerta has identified over 31 disease outbreaks [15, 16] and has facilitated investigation of diarrheal disease, malaria, and influenza as well as detected an outbreak of cyclosporiasis at a naval base in Lima, Peru. Alerta has been especially useful in helping the Peruvian Navy identify and respond to outbreaks at remote bases, which previously may have gone unreported or identified long after they began because usual reporting channels were slow [15, 17] .

Syndromic surveillance may provide a relatively inexpensive tool for early detection of malaria outbreaks in low resource countries. In Ethiopia, weekly malaria cases collected from health centers in 10 districts from 1990 to 2000 were reviewed [18] . Four types of alert threshold algorithms were compared by plotting a curve for each type of alert. The curve demonstrated potentially prevented disease cases versus the number of alerts over a decade. This study found that simple weekly percentile cutoffs were as good as the more complex algorithms for detection of malaria outbreaks [18] , exemplifying that syndromic surveillance can be basic while still providing useful information. WHO has advocated alerts when weekly cases exceed 75% of baseline [19] ; syndromic surveillance may be able to provide early alerts of this type that will allow timely spraying and mass drug administration. The use of these types of comparative statistics in surveillance is a novel method for evaluation of the performance of malaria early warning systems [18] .

In 2002, the Uganda Ministry of Health piloted a new district level monitoring system in the southwestern highlands. Incoming clinical data from health centers are collated and entered onto a district level computer and compared with a baseline of historical illness data. An anomaly measure is used to provide the index of deviation, followed by electronic reporting. This simple system detected two malaria outbreaks in Kabale, in 2005 and 2006, more than two weeks before case numbers began to peak [20] .

Some surveillance systems monitor climatic and environmental data to forecast infectious disease outbreaks. In some areas, climate variables monitored by satellite can provide a two to three month lead-time for malaria epidemics [21] . In Eritrea, monthly outpatient cases of malaria in 242 districts and NDVI and rainfall datasets showed strong correlation, but coverage of clinical data stations was considered too poor to be of use in epidemic control [21] . In Tanzania, analysis of two malaria seasons in the highlands showed an association between regional rainfall and malaria cases. An early warning system based on rainfall observations may thus be useful for malaria epidemic prediction in some areas [21, 22] . An example of such a system is the USAID Africa Data Dissemination website FEWS-NET, which uses rainfall-based indicators to predict change in malaria risk [23] . Similar dissemination websites could be employed in low to medium resource countries for malaria and other vector-borne diseases where remote Internet access is available.

Syndromic surveillance for malaria may enhance public health response. Since a locally transmitted case of malaria occurred in 2006 in Jamaica, active fever surveillance has been implemented for early detection at sentinel healthcare sites, airports and seaports [24] . Analysis occurs at the local level and then is transmitted centrally on a daily basis. The information is then used to conduct active door to door surveillance of fever cases if warranted.

Dengue surveillance is typically conducted by the passive notification of suspected or confirmed Dengue Fever (DF) or Dengue Hemorrhagic Fever (DHF) cases and deaths. These passive systems have low specificity due to infrequent laboratory confirmation but are still useful due to their simplicity and low use of resources. Unfortunately, waiting for reporting by clinicians may lead to delays in public health action and decrease efficacy of control measures. Active surveillance may include clinician sentinel networks, active fever surveillance by community health workers, and sentinel hospital systems. The first two monitor for nonspecific viral syndrome, which may also be useful for detecting outbreaks of other diseases such as influenza or malaria. A web-based reporting system may improve reporting completeness [25] . Hospital sentinel systems monitor for severe disease and death, with immediate investigation of all hemorrhagic fever. Such systems must be complemented by laboratory-based surveillance for trends and serotypes [26] .

Despite concerns with its specificity, surveillance of the fever syndrome may be useful given that fever is likely caused by dengue in endemic regions [27] . In addition to serologic surveillance, there has been great interest in syndromic surveillance for detection and control of dengue [28] .

As an example, in 2004 an early warning system, 2 SE FAG, was established in French Guiana with the goal of detecting outbreaks of febrile illness in French soldiers, including dengue [29] . In 2006 the system was expanded to include 25 civilian health centers that provide surveillance on sanitary conditions. Before 2006, the only data available for dengue surveillance in French Guiana was laboratory confirmed cases. They compared the frequency and timing of detected febrile cases with the traditional surveillance system for dengue, and the sensitivity was found to be high, but specificity was low [30] [31] [32] . For this system, data on all cases of fever, suspected and confirmed cases of dengue fever, and confirmed cases of malaria by syndromic surveillance syndrome definitions (fever plus headache, myalgia, arthralgia, or retro-orbital pain) are employed. Data is collected in real-time by a medical provider seeing a patient, and information is recorded on the available platform (PDA or a computer). Data is then reported to French health authorities in Cayenne. Syndromic surveillance data is converted to a weekly format and reported to the health authorities weekly in cases of normal operation, or immediately in the case of an alarm. Automated alarms are issued from the syndromic surveillance system based on current past experience graph (CPEG). Under ideal circumstances, there is a 60-minute delay between case presentation and detection by the system [30, 31, 33] . To evaluate the effectiveness of the system, during an outbreak of DEN-2 DF beginning in November 2006, data on confirmed dengue fever cases from a reference laboratory and data from 2SE FAG for the occurrence of undiagnosed fever associated with headaches, arthralgias, myalgias, or retro-orbital pain were compared. Levels of alarm and public health actions taken were also recorded. The syndromic surveillance pre-alarm activated 6 weeks prior to the full alarm in week 2 of 2006 and provided early warning for military personnel in comparison to the laboratory-based program [33] .

The system was able to detect 6900 suspected dengue cases compared to 800 laboratory-confirmed cases in 2006. Although the sensitivity was high, specificity was low. Other limitations included incomplete report forms and dengue fever reporting being 67% higher than with traditional reports, most likely because traditional surveillance is limited to confirmed cases [30] [31] [32] . Despite its sensitivity, there has been some concern with using a febrile syndrome for dengue surveillance, given that of 7195 febrile cases, only 8% were confirmed to be dengue [13] . As in other applications of syndromic surveillance, other methods of surveillance must also be in place. A lab-oratory-based system for dengue may be more useful; yet given limited laboratory capabilities in dengue endemic areas, syndromic surveillance may provide valuable information on population epidemiology prior to laboratory confirmation [13] . Vector surveillance and control provides the earliest opportunity to avert or contain dengue epidemics, but many dengue-endemic countries lack resources for launching these programs.

The IHR call for reporting of other vector borne diseases such as West Nile virus and Rift Valley fever. A surveillance system developed in the Netherlands for early detection of WNV focuses on cases presenting with neurologic diseases and includes the monitoring of hospital discharge data, trends in laboratory testing, and monitoring neurological diseases in horses [34] . Such a system could be applied to medium resource developing countries by monitoring the neurologic syndrome and neurologic disease in animals.

Similar to early warning systems for malaria, comparison of different prediction models for cutaneous leishmaniasis [CL] show a strong relationship with climatic variables and thus may be amenable to the development of an early warning system [35] . Models for CL incidence in Costa Rica may outperform models with no climatic indicators [36] . For vector-borne diseases with a clear relationship with the El Nino Southern Oscillation phenomenon [ENSO], models that use climate indicators to forecast disease risks are being developed [37] . In Australia, climate modeling has shown a sensitivity of up to 90% when combined with mosquito surveillance data to predict epidemics of Ross River virus disease [38] . However, many climate-based systems are not widely used due to the lack of published models outside testing areas. Disease modeling is often limited to discrete data sets for small areas [39] . Nonetheless, modeling of these data sets may be useful for selected syndromes in low resource regions. Monitoring of both climate triggers and vector-borne disease indicators together may increase sensitivity and specificity and also provide validation of data sources and backup for potential system failure.

Several recent emerging infectious disease outbreaks, such as SARS and highly pathogenic avian influenza [H5N1], arose in Asia. Surveillance for new respiratory illnesses is therefore crucial in this region. In many regions, electronic data exist that can assist with an automated system. In Taiwan, an emergency department based syndromic surveillance system for 189 hospitals automatically transmits electronic patient data to the Taiwan Centers for Disease Control. This system was built on existing work done in the United States in collaboration with the Realtime Outbreak and Disease Surveillance [RODS] Laboratory [40] . The goal of this system, among the first nationwide real time surveillance systems in Asia, is to detect winter and summer spikes in influenza-like illness, respiratory syndrome, and gastrointestinal illness [41] . Should another epidemic like SARS arise, this system may be able to provide early warning and notification, thus improving global surveillance of emerging infectious diseases. Such automated systems could be used in other medium resource regions for the detection of emerging viral illnesses.

Syndromic surveillance for foodborne illness is important given the globalization of the food supply and the morbidity caused by diarrhea in the developing world. Systems can monitor for gastrointestinal illnesses through the tracking of diarrhea and vomiting symptoms. In the United States, the RUSick 2 disease forum is a web-based forum that allows residents to report information on nausea, vomiting and diarrhea syndromes, including foods consumed, with the goal of identifying common food vehicles in gastrointestinal outbreaks. The goal of this system is to decrease the time delay with routine laboratory surveillance for food borne outbreaks. Completeness of the syndromic surveillance reports collected via the webbased forum has been found to be as effective as similar reports from phone calls to the health department [42, 43] . Poison Control Center data has also been used to detect foodborne outbreaks, and found to be useful in early detection where there is no confirmatory diagnostic information available [44] . Establishment of these types of networks may be useful in areas with medium resources and good communications infrastructure.

These types of surveillance can also be applied to the developing world. In the Pacific region, there are four distinct levels of foodborne disease surveillance: no formal surveillance, syndromic surveillance, laboratory or pathogen-specific methods, and integrated food chain surveillance [45] . Vanuatu and Solomon Islands primarily use syndromic surveillance. Few countries have specialized laboratory surveillance, and thus information on specific pathogens is limited. A regional approach under the Pacific Public Health Surveillance Network would include development of uniform case definitions for reporting as the basis for syndromic surveillance and facilitate centralized data collection and sharing [45] .

Incidence estimates of typhoid fever in Egypt have been derived recently from hospital-based syndromic surveillance along with lab-based surveillance. Although assisted by the syndromic surveillance system, the majority of patients were evaluated in the primary care system and would not have been detected by the hospital based syndromic surveillance. This situation emphasizes the ability of syndromic surveillance to augment but not replace traditional surveillance. Syndromic surveillance must be broad in scope in order to catch mild disease and expand surveillance beyond hospital data, which tends to capture more severe cases [46] . This type of surveillance may be difficult to enact in low resource regions with limited access to laboratory facilities.

As for vector-borne diseases, environmental parameters may be useful for early detection of food or waterborne disease outbreaks. One study found strong correlation between cases of Vibrio cholera O1 in children in Bangladesh and temperature and rainfall two to four months prior. This type of model could be a good model for a climate-based early warning system for cholera in this region [47] and could be implemented in low resource countries.

Syndromic surveillance systems have also been useful in detection of diarrheal outbreaks. EWORS facilitated the detection of a large cholera outbreak in Indonesia [48] .

The WHO recommends a global health sector strategy as part of a multisectoral approach to responding to epidemics of STIs, including a syndromic approach for the detection and management of abnormal vaginal discharge [49] . The WHO strategy aims to decrease the cost of testing and to improve treatment practices, with a certain minimum data required for surveillance. This plan mandates knowledge of the prevalence of specific agents and their susceptibilities, which necessitates at least periodic laboratory surveillance [50] . Vaginal discharge and urethritis are the most common syndromes [51] . Syndrome case reports may include genital ulcers, urethral discharge, and vaginal discharge [52] . Monitoring of these syndromes may allow improved public health response for countries with low resources, for which automated reporting is not easily implemented. Since the introduction of syndromic surveillance in 1996 in Burkino Faso, the prevalence of STIs such as gonorrhea, syphilis, and chlamydia has decreased [53] , providing an impetus to continue this surveillance. In Cote d'Ivoire, the national health information system collects data in all community-based public health clinics, which are located in 29 districts and 10 regions [54] .

In the United States, the Philadelphia Department of Public Health monitors chief complaint and discharge data from emergency departments containing reportable disease information to detect cases of syphilis and manage them according to CDC guidelines [55] . These types of systems could be implemented in the developing world where electronic data or computerized systems are available.

Syndromic surveillance is thought by many to be a high technology tool. But surveillance of syndromes is not a new phenomenon, with one of the earliest examples being acute flaccid paralysis for detection of poliomyelitis outbreaks [6] . Surveillance of influenza like illness worldwide is another example of syndrome rather than disease specific surveillance [56] . While syndromic surveillance is augmenting traditional surveillance in the developed world, it also has the potential to improve timely detection of infectious disease outbreaks in developing countries, most of which lack access to a strong public health infrastructure and specialized laboratories. The burden of public health surveillance in under-resourced and understaffed settings is a challenge. Despite this, there are several examples of low cost syndromic surveillance programs that may enhance global public health. For example, community based programs that employ volunteers may lessen the burden on hospital workers [15] . Increased use of automated reporting may decrease the burden on health care and public health workers and allow for more complete reporting of potential cases of public health importance.

Syndromic surveillance may be especially useful for early epidemic control of certain vector borne diseases as well as for diseases of public health importance that have the potential to cross international boundaries. Examples of current applications of syndromic surveillance in developing countries are summarized in Table 1 . The IHR mandated the reporting of diseases of international importance; surveillance for syndromes may facilitate compliance with this IHR requirement. There is currently no infrastructure in place to enforce these guidelines, and each country must design a national surveillance system that can allow for timely detection and notification of these disease outbreaks. Although WHO is required to assist countries in developing capacity to meet these requirements, no funding is allocated from WHO for this purpose. Thus, improving countries' national public health infrastructure and reporting capabilities will require large financial and technical support, likely from countries with existing automated reporting [9] . Improvements in the communication infrastructure, including Internet access, will need to occur to allow electronic communication and enhance the timeliness of reporting. Furthermore, investment in training in epidemiology, field investigation, and information technology are vital to the future success of broader surveillance activities.

Although syndromic surveillance can provide useful early warning of diseases such as malaria, there are no guidelines for what to do with the information provided. It is still necessary to have a robust public health infrastructure for investigation of cases and implementation of an effec- [54] tive control program as with any surveillance system. Simple monitoring tools can facilitate effective epidemic control, but require the translation of this early warning information into timely public health action.

Syndromic surveillance systems should build on existing public health surveillance infrastructure, as well as work that has been done in other regions. The collaboration between the Taiwan Centers for Disease Control and the RODS Laboratory is one such example [40, 41] . With increasing access to the Internet, and decreased cost and improved user friendliness of information technology in developing countries [1] , novel applications for syndromic surveillance are enhancing traditional surveillance and will hopefully continue to improve the detection of outbreaks worldwide, fulfilling the goals of the IHR. We hope this review demonstrates both the effectiveness and feasibility of ""low-tech"" syndromic surveillance in low resources countries, and can be the starting point for further development of guidelines for how to conduct syndromic surveillance in developing countries. 

",0.7933796202429038
Advanced Querying Features for Disease Surveillance Systems Advanced Querying Features for Disease Surveillance Systems,"Most automated disease surveillance systems notify users of increases in the prevalence of reports in syndrome categories and allow users to view patient level data related to those increases. Occasionally, a more dynamic level of control is required to properly detect an emerging disease in a community. Dynamic querying features are invaluable when using existing surveillance systems to investigate outbreaks of newly emergent diseases or to identify cases of reportable diseases within data being captured for surveillance. The objective of the Advance Querying Tool (AQT) is to build a more flexible query interface for most web-based disease surveillance systems. This interface allows users to define and build their query as if they were writing a logical expression for a mathematical computation. The AQT allows users to develop, investigate, save, and share complex case definitions. It provides a flexible interface that accommodates both advanced and novice users, checks the validity of the expression as it is built, and marks errors for users.","In its 2007 annual report, the World Health Organization warned of the increased rate at which diseases spread in a world where 2 billion people travel by air [1] . The early detection of known and emerging illnesses is becoming more important. Automated disease surveillance systems have been in existence for over 10 years [2] [3] [4] . Most of these systems analyze data by syndrome and search for disease outbreaks. A syndrome in this context is defined as a group of diseases related in some fashion, such as respiratory diseases. This level of investigation is often sufficient, but a more dynamic level of control may be required to understand an emerging illness in a community.

For example, during the 2002-2003 Severe Acute Respiratory Syndrome (SARS) disease epidemic [5] , the respiratory syndrome definition used by most automated disease surveillance systems was too broad to track SARS [6] . In this case, the users needed to create queries that looked for specific keywords in the patient chief complaint or specific combinations of ICD-9 codes [7] . A chief complaint is text entered by a triage professional in an emergency room or a clinic, based on a patient's description of their primary symptoms. Today's public health departments must deal with a multitude of data coming from a variety of sources. For example, Electronic Medical Record (EMR) data include sources such as radiology, laboratory, and pharmacy data. A more sophisticated querying tool is needed to assist investigators with creating inquiries across multiple data sources [8] [9] [10] .

Currently, there are surveillance systems, such as the Electronic Surveillance System for the Early Notification of Community-based Epidemics (ESSENCE) [11] , which provide limited dynamic querying capability. However, we wanted to design a flexible and simple graphical user interface (GUI) for this and other types of surveillance systems. Our prototype system, the Advanced Querying Tool (AQT), allows the investigators to handle complex cases where one can incorporate any data elements available in a disease surveillance system, then mix and match these data elements in order to define valid queries. Hence, this system removes the need for database administrators and application developers to define pre-packaged database queries and user interfaces every time a new and innovative query is written.

As an example, investigating a potential influenza outbreak in an adult population may require respiratory syndrome queries only, while investigating a similar outbreak in children under 4 years old may involve queries in both gastrointestinal and respiratory syndromes ( Figure  1 ). Table 1 provides examples of how a dynamic query tool exploits combinations of data elements available to disease surveillance systems. Most automated disease surveillance systems have a fixed number of predefined syndromes. These applications severely limit the surveillance system value for diseases that fall outside of its broad syndrome categories. The background noise level rises when all the chief complaints that potentially fall into a syndrome category are included, which in turn requires many more positive cases to identify an abnormal condition. Merely adding sub-syndrome categories, that are more granular than syndromes and cover a broader range of conditions than typical syndromic surveillance like injures and chronic disease [12] , provides the users with a more comprehensive means to filter the analysis window. If a disease surveillance system has 400 sub-syndromes, then taken singly the user has 400 additional choices; by combining two or three sub-syndromes, the analysis options are magnified to over ten million choices. Of course not all of these options are sensible, so the actual number of options is somewhat less. Even greater analytic flexibility is provided through the use of data elements contained within electronic medical records. The capability to select a combination of a microbiology laboratory result, radiology result, and ICD-9 code provides for a powerful tool that enables the public health community to rapidly identify specific high risk patients. 

The following objectives summarize the design features of the AQT: The tool's interface will help generate queries that can process any kind of data regardless of its source (e.g., emergency room visit, office visit, pharmacy, and laboratory). Unlike fixed-form query interfaces, AQT will not restrict users in what they can query. Instead, the user will be able to formulate ad-hoc queries across assorted data sources without the need to understand the underlying data models and the query languages associated with different systems. In addition, using this tool should save investigators' valuable time in obtaining the query results. Currently, if the surveillance system cannot generate the desired queries, the application developers and/or database administrators may have to create new interfaces or functionalities. The AQT, however, empowers the users to move forward with their research without waiting for developer or administrator modifications to the surveillance systems.

The interface will accommodate users with different levels of experience in creating complex and valid queries. The process will be natural and follow the same patterns that one uses to express a mathematical equation. At the same time, it will give the more experienced users, who are familiar with the data elements, the freedom to define complex queries by sidestepping the guiding tools. The advanced users will have the ability to type in their queries and the tool will validate them and provide feedback on possible syntax errors.

The interface will allow users to save and share queries with other public health professionals, even in different jurisdictions. After defining a complex query the user has the ability to store the query for future investigations. One should be able to execute the stored query repeatedly in the future, include it as a segment of a bigger query, or customize and execute it. These saved queries can then be shared as part of collaborative efforts among users in different departments and jurisdictions. AQT will provide an interface for disease surveillance systems to store, retrieve, and share queries. These capabilities are especially valuable for users employing a case definition for following a disease outbreak. A case definition is a set of symptoms, signs, etc., by which public health professionals define those patients considered to be directly a part of the disease outbreak.

Finally, the tool should be self-contained and generic. This allows most web-based disease surveillance systems to incorporate the AQT into their systems.

The entire functionality of the tool is placed within a single web page ( Figure 2 ).

The screen in Figure 2 is divided into 5 major sections. Starting at the top, the user can filter the data by picking the data source from a dropdown list, start and end date. The surveillance system should supply this list of data sources to the AQT. The next area below is the Message Area where the GUI communicates with the user. Any information, warnings, or error messages are displayed in this section. The next area, the query section, contains the query expression. The users can either directly type the query expression or use the tool to generate the query expression and paste it in this area. Alternatively, they can use a combination of the two methods by typing part of the expression and pasting the rest using the query builder. The query section is followed by the query builder section where the tool provides list boxes, buttons, etc., to direct the user through the process of generating the query expression. The bottom section is where an action on the query is performed. Users can validate the expression's syntax, save the query for their own future use, save it to be shared with others in the user community, clear the query expression and start over, or simply execute the query and get the results.

As mentioned earlier, the capability to generate queries on data from a variety of sources is one of the objectives of the AQT. Each data source has its own distinctive set of data elements. The interface has to provide a list of data elements pertaining to the chosen data source. For example, the data might represent different geographic regions from one data source to the other. That is, one source might have data identified by zip codes while another source uses some other type of defined region such as hospitals, pharmacies, and schools. Another area where data sources can be different is in medical groupings. For example, office visits often use ICD-9 codes [7] , while emergency departments use patient chief complaints. The interface is designed to distinguish valid data elements for each data source and populate the data element list box accordingly.

After selecting a data source the tool populates a list box with a set of associated data elements for the data source. The list box is divided into three major areas:

• The geography system • The medical grouping system • Others such as age, sex, saved and shared queries. Figure 3 shows how the medical grouping systems differ for Emergency Room (right) and over the counter (left) data sources. As mentioned earlier, a main objective of the AQT is to provide an interface that caters to both novice and experienced users. The experienced users simply type the query, while beginners and those who are more comfortable with a guided interface can use list boxes and buttons to generate the queries. In fact, one can type part of the query and use the tool to generate the rest of the query (Figure 4) . When a user types a query directly, it is assumed that the user knows the syntax and valid data elements pertaining to the data source, though the tool does check the syntax and provide feedback. Because we want the users to define and build their query as if they were writing a logical expression for a mathematical computation, the syntax is simple and close to the ""where"" clause of a Structure Query Language (SQL) statement. However, one does not need to know SQL to write the expressions. A query consists of one or more simple expressions joined by ""AND"" and/or ""OR,"" negated by ""NOT,"" and grouped by parentheses. A simple expression is enclosed within square brackets ([]) and defined by a variable, a logical operator, and a value. For example, if an investigator is searching for reported fever cases within a specified zip code, the query then consists of two simple expressions; one which searches for the specified zip code and the other which checks the fever syndrome. The final query may look like the expression below:

If the investigators want to narrow the search into a certain age group they can type or use the tool to add AND [AGE = ""0-4""] to the above expression. Hence, the users can add more conditions without worrying about the underlying data model.

The most complex part of the syntax occurs when searching for values that contain, start with, or end with a set of characters ( Figure 5 ). In this case, the syntax uses ""*"" as the wildcard character. For example, a user would type [chief-complaints = ""*head*""] in the query box if he/she is looking for all the records of chief-complaints that include the word ""head."" Similarly, if a user types [chief-complaints = ""head*""] or generates it using the tool (selects the Starts With from the operator list box and types head in the text field), the resulting query would search for all the records where the chief-complaints field begins with the word ""head."" The procedure for generating expressions follows the same pattern a person would use to create a logical expression. The interface will provide a natural flow to help the users to create an expression as if they are typing it. They may start with selecting a data element or variable such as 'SEX', then a logical operator like '=', and finally a value like 'MALE' or 'FEMALE'. The user can add 'AND' or 'OR' and create the next expression using this same process.

The user can interject expressions in the middle of a query, remove parts of the query, or undo the last change made to the query. As changes are being made, the tool validates the entire query in the background and provides instant feedback. This method of constructing queries is more intuitive to the users than that of creating all the individual expressions first and then joining them together.

Once the data source is selected, a list of core data elements is provided in a list box. From the list box the user can select a data element. Based on the type of the data element, a list of valid logical operators for that data element is placed in another list box. Figure 6 shows the list of valid operators for text fields.

In cases such as zip code and syndrome, '=' and '<>' operators are also valid. For age the operators '>', '<', '<=', and '>=' are added to the list. Once the user selects a data element, a list of valid values pertaining to the data element is listed in yet another list box. The user can select one or more of these values, and if more than one value is selected the user can choose to group these values using 'AND' or 'OR'. Note that the AQT generates the expression in a left to right progression in the same manner as one typing the expression (Figure 7 ).

The next step is to add this expression to the query. By clicking on the ""Add Expression"" button, the expression is pasted at the cursor location in the query area. One can add more expressions to this query by clicking AND or OR buttons and following the same process ( Figure 8 ).

The AQT helps users quickly identify limits for variables with large sets of values. Because data elements such as zip codes and ICD-9 codes have a lot of values for dropdown lists, finding a particular value in these list boxes is very cumbersome. The tool provides an intermediate step for filtering these options into a more manageable list (Figure 9 ). For example, if the investigators are interested in data from certain zip codes in a state, they can reduce the options by typing the first two digits of the zip code and thereby filtering the list. The tool will generate valid expressions and provide a mechanism to check the query expressions when a user types parts or all of them. Every time an expression is generated by the tool and the Add Expression button is clicked, the tool examines the entire query expression, checking it against the syntax rules. Before saving or executing the expression the AQT automatically checks the syntax and if it detects any syntax errors it will provide meaningful error messages in the message area ( Figure 10) . Additionally, at any point the user can click on the validate button and check the syntax. Frequently, investigators want to execute a query over time, run the same query with different values, or use the query inside more complex queries. Similarly as all the other data elements (zip code, syndrome, region, etc.), the permanent storage and retrieval of queries (File system, database, or any other mechanism) are the responsibility of the disease surveillance system. The AQT is merely an interface to assist the investigators with their research by hiding the complexity and inner workings of the underlying data model.

Once the users define the desired query they can click on [Save Public Expression] or [Save Private Expression] buttons. If the query is valid, the screen provides an area to enter a unique name for the query (Figure 11 ).

If the query is successfully validated the AQT passes the name and query expression to the surveillance system. It is the surveillance system's responsibility to confirm that the query's name is unique and provide feedback to the AQT the success or failure of the save operation. Based on the feedback received the AQT provides an appropriate message in the message area.

In a collaborative environment users would like to share their findings and queries with others. Providing the capability to save and share the queries for collaborative use enables others in the user community to run these queries as they are or to make the modifications necessary to help with their own investigations. The AQT facilitates saving public queries by providing an interface similar to saving private queries ( Figure 11 ). The surveillance system should implement the inner workings of the permanent storage and retrieval of public queries. The next step is retrieving these saved queries. There are two options in the data element list box in the query builder section of the AQT: one option is for retrieving the private saved queries, and the other option is for retrieving public saved queries ( Figure 12 ). Upon selection of either one, a list of corresponding queries will be presented to the users. This list includes the text of the query and the unique name given to that query. By clicking on the query name the saved query will be added to the expression in the Query area.

At this point users can add more conditions to the same query, such as specifying a zip code, changing the value for age, etc.

The final objective of this project is for the AQT to have the capability to be used with most web-based surveillance systems. One can think of the AQT as a widget, or an add-on with some defined interfaces. The back end can be implemented in a variety of popular technologies such as .NET, Java Servlet, or any other server technology as long as it can communicate via an http protocol. The surveillance system has to provide the interfaces that supply values for the different parts of the screen, and the functionality to parse the final query text and run it against the underlying database.

Making the tool adaptable to many web-based systems requires the AQT to contain all the processing dynamically, including validating the query syntax and changing the contents of the list boxes. In a web-based environment, this means using browser components such as HTML, Cascading Style Sheets (CSS) [13] , JavaScript, and the Document Object Model (DOM) [14] to implement application logic. In developing AQT, we utilized HTML, JavaScript, and AJAX (Asynchronous JavaScript and XML) and placed all the processing on the local machine to avoid any server dependency.

We used JavaScript to apply validation, data handling, and screen processing on the browser side, and AJAX for communicating with server applications. AJAX is used for creating interactive web applications and is a cross-platform technique usable on many different operating systems, computer architectures, and web browsers, because it is based on open standards such as JavaScript and XML. The intent of this technique is to make web pages more responsive by exchanging small amounts of data with the server behind the scenes, so that the entire web page does not have to be reloaded each time the user requests a change. This feature increases the web page's interactivity, speed, functionality, and usability. AJAX is asynchronous in that loading does not interfere with normal page loading.

The AQT uses AJAX calls to obtain required data for populating the different list boxes on the screen. For example, when the user selects a data source the tool calls the surveillance system, passes the selected data source, gets a list of data elements from the server (the surveillance system), and then populates the data element list box. The communication to the server is done by an AJAX call, and the JavaScript processes the returned data and populates the list.

ESSENCE has been one of the early adaptors of AQT. Although the capability to create efficient custom queries for emergency room chief complaints data existed prior to the AQT, the query building process was cumbersome and not user-friendly. It was easy to make syntax errors while typing a query, and there was no mechanism to validate the logic of the query statement. Furthermore, while ""AND"" and ""OR"" and ""ANDNOT"" expressions were possible, there was no method to construct complex Boolean operations with parentheses to clarify the order of operations. The previous capability allowed the user to base the custom query on Data Source, Geography System, or Medical Grouping System, however, since the selections were not part of the query statement they could not be modified without returning to the pre-selection screens and re-starting the query process. Additionally, the original capability did not allow for querying of data beyond the fundamental chief complaints-level. The following screen shot shows the query options that were available with the original feature. A sample chief complaints query designed to capture Influenza-Like-Illness is shown in Figure13.

The AQT not only contains several capabilities that were not previously available, but also provides an intuitive user-friendly interface that allows the user to build simple or highly ^cough^,and,^fever, or,^sorethroat^,and, fever^,andNot, ^Asthma^ complex queries more easily. Two new features in the AQT are parentheses, which allow the user to clarify the order of operations, and the ability to select variables such as Region, Zipcode, Hospital, Syndrome, Sub-syndrome, Chief Complaint, Age, and Sex, as part of the query statement. This allows for easy query modifications. Additionally, the AQT lets the user query data beyond the fundamental chief complaints level into a more sensitive Sub-syndrome or Syndrome level. It also allows users to develop queries that contain combinations of chief complaints, syndromes, and sub-syndromes into one query. The query can also contain combinations of different geographies such as zipcodes and regions. This capability is not available without AQT.

During the query building process the AQT automatically validates the logic of query expression as it is created, and the user has the option to conduct a final validation prior to executing the query. This feature allows the user to quickly identify syntax errors and correct them before adding further complexity or executing the query. The following screen shot ( Figure  14) shows the query options available within the AQT feature. A sample chief complaints query designed to capture Influenza-Like-Illness in Region_A is shown. 

",0.7901909392466197
Smart home technology for telemedicine and emergency management,"With the ageing population, mobility is an important issue and it deters the elderlies to visit health clinics on a regular basis. Individuals with disabilities also face the same obstacles for their out-of-home medical visits. In addition, people living in remote areas often do not get the needed health care attention unless they are willing to spend the time, effort and cost to travel. Advances in information and telecommunication technologies have made telemedicine possible. Using the latest sensor technologies, a person's vital data can be collected in a smart home environment. The bio-information can then be transferred wirelessly or via the Internet to medical databases and the healthcare professionals. Using the appropriate sensing apparatus at a smart home setting, patients, elderlies and people with disabilities can have their health signals and information examined on a realtime and archival basis. Recovery process can be charted on a regular basis. Remote emergency alerts can be intercepted and responded quickly. Health deterioration can be monitored closely enabling corrective actions. Medical practitioners can therefore provide the necessary healthrelated services to more people. This paper surveys and compiles the state-of-the-art smart home technologies and telemedicine systems.","In the past decade, one of the fastest growing multidisciplinary research areas is telemedicine. Many definitions of this term exist depending on the specific context. Other terms that often used interchangeably, in conjunction with, or under the umbrella of telemedicine, include telehealthcare, telemonitoring, e-health, e-care, ambient assisted living, smart homecare etc. All these terminologies, though, have the same objectives of delivering remote healthcare to an individual's home and enabling continuous health monitoring and emergency management.

The increasing attention given to telemedicine is due to many factors. With unaccommodating transportation, and crowed hospitals and clinics in most cities, it is difficult for the elderlies and physically challenged to venture out of their homes to meet with health professionals for the needed treatment or for their routine clinical check-ups. Similarly, healthcare delivery in remote rural areas is a demanding endeavour. Having the capability of delivering healthcare services at the patients' home not only saves a great deal in monetary and human resources, but it also reduces response time in emergency situations. This is especially beneficial in the case where a large number of patients can be dealt with via communication technologies quickly from a centralized location without sending medical personnel to their individual homes.

Telemedicine has drawn worldwide attention in the 2000s as modern technologies have made remote healthcare delivery a reality. Within the European Community, there were the HHH (home or hospital in heart failure) trials of a low-cost, self-managed home monitoring system for patients with chronic heart failures (Pinna et al. 2003) . It was found that monthly home telemonitoring of cardiorespiratory signals being sent to a centralized interactive voice responding system is feasible and patients' compliance is high. The SM4All (Smart hoMes for ALL) project initiated by the European commission aims to provide a middleware platform for pervasive embedded service in a smart environment (Baldoni 2009 ). The objectives of SM4All are to provide dependability, scalability, privacy and security to users with disabilities.

The Brazilian family health program was initiated for preventive medicine delivered to the lower income population living in remote regions (Correia et al. 2008) . The associated Borboleta system enables healthcare professionals to use PDAs (personal digital assistants) and mobile communication technologies for providing on-site home healthcare and improving the quality of public health services.

In the USA, healthcare is a big financial burden for government, employers, and citizens. White et al. (2006) discussed major challenges to improve healthcare quality and concluded that a distributed diagnosis and home healthcare paradigm is the best approach for affordability and quality.

Germany has an initiative to provide encrypted health data for patients during emergency using electronic health card (Dunnebeil et al. 2011) , though there is resistance by some medical professionals mainly due to privacy issues.

Telemedicine is a multidisciplinary research and application area using advanced technologies in information processing, telecommunication, bio-sensing, and artificial intelligence. Specifically, smart environment and technologies play important roles in making home telemedicine feasible. One can view sensors being the foundation and communication networks as the pillars of a building, supporting various telemedicine applications under the roof to facilitate and provide a smart home environment to individuals, as shown in Fig. 1 .

This work presents a survey of the latest advances in smart environment and home telemedicine. It is an extensively updated version of an earlier work on home telemedicine review (Li and Li 2011) . In addition, the current focus is on smart environment and emergency management. There exist numerous works in the literature and therefore only representative, technologically innovative, and interesting systems and approaches are presented. To properly introduce smart environment in telemedicine and emergency management, the fundamental technologies, potential applications, their evaluations, and future directions are presented in a sequential coherent perspective. Section 2 introduces smart environment enabling technologies including communication, sensor and sensor network. Applications of smart telemedicine technologies for specific diseases, emergency management, and the elderly and physically challenged, are presented in Sect. 3. Target user groups' concerns and healthcare professionals' evaluation of telemedicine and smart homecare systems are discussed in Sect. 4. In the concluding Sect. 5, challenges, concerns and issues related to smart homecare are considered, and future trends of smart homecare environment are identified.

In order to provide telemedicine effectively, various techniques and methodologies from different areas have to be integrated seamlessly into a smart system. This section examines the primary state-of-the-art enabling technologies for smart environment within a homecare setting for telemedicine: communication and associated telehealth standards, sensors, and sensor networks.

Many telemedicine systems leverage the latest mobile and wireless communication technologies as well as the widely available internet infrastructure to deliver quality services to home patients (Castro et al. 2001; Bonacina and Masseroli 2006; Li et al. 2008) . A home patient monitoring system proposed by Figueredo and Dias (2004) uses a simple RS232 serial interface to connect a mobile phone to patient monitoring devices, and transmits vital signs over the internet to the hospital. vNurse is a system developed by Rehunathan et al. (2011) that uses the smart phone platform to provide secure and remote patient monitoring. Bio-data in vNurse are collected from a wireless body network and transmitted using mobile networks with IP (internet protocol) connection. Multi-agent systems are commonly found in smart telehealth applications. One of the earliest telemedicine projects, the independent lifestyle assistant, incorporates techniques in sensing model, distress situation detection, network response planning and machine learning (Haigh et al. 2002) . The Assistant is implemented as a multi-agent oriented assistive living architecture. Liao et al. (2009) presented a telemonitoring architecture based on mobile agents and they also addressed the issues of reliability, security, and manageability of this platform. Bramanti et al. (2010) employed GIS (geographical information system) technology to identify the optimal locations of clinics that could provide neurological telemedicine services to patients. A remote-client, servicecenter architecture was proposed that uses 3G/CDMA network and the internet to transfer vital sign information and medical analysis reports (Zhang et al. 2010) .

The web is a common and well-developed medium for communicating monitored and emergency data. Raad and Yang (2009) used the web for reporting real-time status and actions of a home bound elderly. Voice over IP (VoIP) is another family of technologies and communication protocols that can be used effectively in smart environment and telehealthcare. Menkens and Kurschl (2010) investigated and evaluated VoIP enabled communication platforms and proposed a system for regular monitoring as well as providing responsive actions in emergency situations.

Citing decision and notification delays in typical telemedicine homecare settings, as a result of the large volume of information occupying the limited communication resources and consuming valuable computation resources on the remote server, Chen (2010) team designed a smart gateway bridging a home-based wireless senor network and a public communication network. Implemented as a standalone embedded system, the gateway is also capable of tracking a patient's health state and providing fast response in emergencies.

Communication is the foundation of providing telehealthcare in the smart home environment. There are many existing and emerging standards such as the IHE (integrating the healthcare enterprise) recommendations for information exchange (IHE 2012) , and the very popular Zigbee (Zigbee Alliance 2012) that have been used in many smart environment projects (Lin and Sheng 2008; Fraile et al. 2010) .

de Toledo et al. (2006a, b) argued that in order for e-health to progress, the use of standards in providing plugand-play devices and interoperable modules is necessary. ISO/IEC 29341 universal plug and play (UPnP) device architecture (ISO 2012a) is a standard for plug-and-play. Lin and Sheng (2008) carried out a study of using UPnP network protocols to facilitate services in a residential network. Park et al. (2011) examined the ISO/IEEE 11073 point-of-care medical device communication (ISO 2012b) and its compatibility to legacy devices.

One of the important works related to standards is Anagnostaki et al. (2002) development of a codification scheme for vital signs in health information interchange. They also discussed the practical aspect of integrating the proposed scheme into medical devices. Jang et al. (2007) provided a noteworthy comparison of the key features, including power, complexity, number of nodes, latency, range, expandability, data rate, security, and audio connection, among the short-range communication protocols: Wi-Fi IEEE 802.11b standard (IEEE 2012), ZigBee (ZigBee Alliance 2012), and Bluetooth (Bluetooth 2012). Ultra-wideband radio has also been proven to be effective in a smart home environment (Bonato 2010).

Many research projects have drafted an exhaustive list of sensors to be used in their prototype work such as the TAFETA (technology assisted friendly environment for the third age) group's implementation (Arcelus et al. 2007 ). TAFETA has shown how various sensors can be used effectively for different purposes in a smart home: magnetic switches on doors to monitor entry and exit, thermistor to track temperature, accelerometers on chairs and flooring to measure impact which is indicative of a falling condition, RFID (radio-frequency identification) to assist people with Alzheimer or dementia, infrared motion sensor to detect mobility and presence, microphone array to detect abnormal noises and cries for help, smart grab bars with pressure sensors embedded to measure strength and balance trend, and pressure sensitive mats under bed to monitor deviated entry and exit patterns.

In a typical smart home telemedicine system, there are two major types of data that need to be collected. The occupant's health state and the environmental information are important parameters to monitor and track. Different types of sensors can be used to detect vital signs such as ECG (electrocardiogram) for heart rate, EMG (electromyogram) for muscle activities, and blood pressure monitor for hypertension, while accelerometer, microphone, light sensor, air pressure recorder, and humidity indicator can be used to detect motion and location (Fraile et al. 2010 ). In addition to signals obtained from sensors that are attached to the body or physical measurement devices in the home, there are other media that could be monitored such as video and sound. Though, sound is the preferred medium due to privacy concerns. Smart home technology 537 Istrate et al. (2006) team developed a sound analysis system to detect and classify alarming sounds in a noisy environment. Using wavelet technology to process sound signals, their system achieves good performance comparable to other systems using different monitoring technologies. Laydrus et al. (2007) investigated the use of frequency domain features in an automated sound analysis system and was able to achieve over 90 % accuracy in classifying 19 real-world sounds. Vacher et al. (2009) presented a sound classification and speech recognition system with microphones placed in the home to detect short sentences or words that may indicate a distress situation. Though, Fleury et al. (2008) found that screams and distress sentences are difficult to differentiate due to distress sentences are similar to short words. Hollosi et al.

(2010) devised an acoustic event detection and classification system for ambient assistive living environment and interpreted coughing as an indicator of upcoming illness.

Pressure sensors have been used extensively in various parts of a smart home. Arcelus et al. (2009) installed pressure sensors under the bed and on the floor next to the bed. Their collected data include sitting pressure, standing pressure, and the time from sit to stand, which provide valuable information on a patient's mobility and other health-related data. Leusmann et al. (2011) used arrays of Piezo sensors to implement a sensor floor to track the whereabouts of the home occupants. Moreover, data collected can be used to analyze impact patterns in case of fall or other emergency situations. Similarly, Miao et al. (2009) proposed a fall detection approach based on head tracking using two video cameras. They constructed three-dimensional head position models with unimportant objects and background filtered out.

Location tracking is an important aspect of providing a smart home environment. Global positioning system (GPS) has limitation operating indoor. Various techniques have been used for indoor position location including the use of cascade correlation network and neural network modeling (Chen et al. 2011 ). Helal et al. (2003 at the University of Florida has been working on the house of Matilda project for the past several years. Matilda is an in-laboratory mock up house to experiment various innovative ideas for smart home. One of their focuses is location tracking of the elderly by the use of ultrasonic sensor technology.

Wearable devices have been research extensively in the past decade. With the advances in device miniaturization and communication techniques, wearable devices can serve many different roles in telehealthcare, for instance, as a standalone monitoring device or as a node in a sensor network (Hung et al. 2004; Axisa et al. 2005; Kang et al. 2006) . Chaczko et al. (2008) developed a small wearable device that incorporates data storage, display screen, speaker, microphone, GPS, RFID and accelerometers. The ability to monitor and locate makes this an ideal device to be installed in various places in the home and on the patient. Wang et al. (2007) presented a body piconet based on smart sensor nodes for ECG. Their work uses Bluetooth technology for in-home data transfer and the internet for network data transfer to a hospital. Nag and Sharma (2006) presented their work on a wireless smart clothing system that is capable of acquiring ECG, pulse oximetry, body motion/tilt and skin temperature data. Bonato (2010) gave an extensive survey and review of the major approaches to implement wearable systems. Enabling technologies discussed in that work include miniaturization, lower-power radio, and dedicated operating system, which make body sensor network a reality.

Instead of focusing on specific or a few signals, multimodal systems are gaining popularity. Medjahed et al. (2009) presented a fuzzy logic system utilising a wide range of sensors to recognize activities in a home environment. These sensors include physiological types (cardiac frequency, activity, agitation, posture and fall detection), microphones, infrared, water debit and statechange in appliances.

Many innovative and non-intrusive approaches have been developed in the past several years. Ichihashi and Sankai (2007) developed a small 'smart telecom' unit that integrates sensing circuit, digital signal processor, and wireless communication into a device of size 25 by 37 mm. Such small devices can monitor one's vital signs without being invasive. Kim et al. (2007) developed an integrated home health monitor using a chair as a non-invasive platform. This all-in-one monitor can record ECG, estimate blood pressure, measure blood glucose level and ear temperature. These sampled signals are then transmitted to a home server using Zigbee technology, which in turn are transmitted to a healthcare center via the internet.

Ho et al. (2009) presented a very interesting initiative in an attempt to harvest body temperature and vibrations in the house to power implantable glucose sensors within a wireless home network. Ping et al. (2009) home healthcare research project places heavy emphasis on recognizing a patient's emotional state in addition to physical state. Facial, vocal, eye-movement, and physiological signals are tracked and analyzed to provide the psychological aspects of a patient's health.

Hussain and colleagues (2009) explored the integration of RFID and wireless sensor network in a smart home environment. Their idea is to use the wireless sensor network for tracking the movement and location of individuals while the identification of a person can be accomplished by the wearing of RFID tags. The separation of these events allows for quick decision-making and responses to various situations.

This section presents the major application areas that smart telemedicine plays a dominant role.

Unlike face-to-face medical consultation, useful information must be derived from the raw data in telemedicine. It is almost impossible for humans to examine this large volume of data manually and to detect any changes. Therefore, techniques have to be developed in order to automate the diagnostics and abnormality detection process. This section introduces smart telehealth systems developed for specific illnesses.

Back in 2000, Finkelstein and Friedman (2000) presented a home asthma telemonitoring system that has the capability of assisting asthma patients in self-care plans as well as alerting healthcare personnel when distress situation is detected. Lau et al. (2002) implemented a multimedia system that employs bio-sensor data, messages and video transmission to allow physicians and patients with shoulder replacement surgery to communicate and monitor the progress. Chun et al. (2003) developed a home telemedicine system for cardiac and respiratory disease, diabetes, and hypertension. Using artificial intelligence techniques, this system is capable of providing automated diagnostic and consultation. Performance evaluation has shown that the implemented system is a viable alternative to personal monitoring and consultation. Rudi and Celler (2006) presented an expert telemedicine system that is capable of storing various data and measurement related to diabetes management. In addition to present to medical professionals records associated with individual patients, the system is also capable of recommending insulin dose adjustment to patients. Jiang and Xie (2009) proposed a telehealth monitoring system that uses data mining techniques to deal with the large volume of biological data. They also utilized association rules to recommend actions to be taken for hypertension. Zhang et al. (2009) developed a chronic disease management system using a real-time knowledge base and the case based reasoning approach, together with a web patient monitoring system, to improve the diagnostic of diabetes and hypertension.

Fergus et al. (2009) developed a body area sensor network intended for physiotherapist's use. This system collects and stores motion data of the home patient, and provides some quantitative assessments of the patient's progress. Gobbi et al. (2009) designed a system that addresses issues related to real-time data processing, network architecture, and web-based data management and services. As an automated home monitoring system for lung obstruction, that system was shown to be reliable and efficient. Silva et al. (2010) developed an internet-based system for home monitoring of patients with respiratory muscle disorders. They concluded that their system would be a useful tool for the evaluation of inspiratory muscle in telemedicine services, thus reducing the costs of handling these patients.

A smart home with technologies to enhance the quality of life of quadriplegia patient was proposed by Uzunay and Bicakci (2007) . They placed heavy emphasis on the security aspect of their voice activated smart home facilities. To monitor blood flow velocity in a wireless home environment, Wu et al. (2011) presented a self-monitoring device to measure blood flow velocity that is integrated in a LabVIEW (LabView 2012) mobile environment. Smart home telemedicine technology is ideal for patient selfmonitoring and quality of life improvement, which are the subjects in the next section.

The segments of our population that need quality healthcare the most are the elderly and physically impaired. Home telemedicine and telehealthcare systems are the solution to deliver low-cost yet quality services for these individuals.

In 2006, the United Nation estimated that at least 650 million people have disabilities worldwide (United Nation 2006) . There are various types and degrees of disabilities. According to the Massachusetts Department of Elementary and Secondary Education, the impairments can be classified into communication, emotional, health, intellectual, neurological, physical, and sensory (MDESE 2000) . Andrich et al. (2006) presented their smart home project with case studies involving people with disabilities in different categories: hemiplegia, paraplegia, quadriplegia, motor impairment, and cognitive impairment. Disabled person with the specific type of impairment would require specialised care and tailored smart home technologies.

Many telehealth and assistive systems have been proposed and are in place for the elderlies. Meng et al. (2000) has been working on teleoperated mobile robots via the internet to provide quality healthcare to the elderly. They have also designed robotic pets that are capable of physiological data collection and transmission, as well as simple healthcare tasks (Ling et al. 2005) . Briere and colleagues (2009) presented a teleoperated mobile robot with videoconferencing capability for in-home telerehabilitation services. Vergados et al. (2008 Vergados et al. ( , 2010 discussed their INHOME project which goal is to provide intelligent ambient assistive living services to elderly people at home for independent living. Lim et al. (2010) presented a home healthcare set-top box specially designed for the elderlies. Their work focused on standard compatibility and Smart home technology 539 adherence to the ISO/IEEE 11073 personal health data standard. Vinjumur et al. (2010) implemented a web-based medicine management system that uses RFID tags to monitor medication intake for the elderly at home. With multimodal signal sampling, it is imperative to incorporate data fusion techniques in telehealth monitoring and decision-making. Virone et al. (2003) investigated data fusion of collected video and sound signals in the monitoring of elderly patients at home. Medjahed's group focused on data fusion of multi-senor inputs to provide medical recommendation using a rule based approach (Medjahed et al. 2011) .

Artificial intelligence techniques play a major role in smart home systems. Papamatthaiakis and colleagues (2010) presented an indoor activity recognition system for the elderlies. Using association rules and Allen's temporal relations, they claimed a 100 % recognition rate in identifying everyday activity such as bathing, preparing meals, doing laundry, etc. The MavHome project aims to provide a smart and adaptive environment for inhabitants (Jakkula et al. 2007) . In order to meet the goals of comfort and efficiency in MavHome, health trend monitoring and prediction are made using support vector machine and forecasting tools based on time series data collected in a sensor network. Huo et al. (2009) presented a healthcare environment for the elderlies using a home sensor network and a body sensor network. The interesting aspects of this work lie in the functionalities provided, including outdoor monitoring, and emergency decision and alarms. Various communication techniques, such as automated telephone call, SMS and email, are integrated into the system, transmitting emergency signals to caregivers and family members. Smart decision-making using hidden Markov model is proposed to speed up the decision process, increase the accuracy of event detection, minimise measurement errors, and correct transmission errors. Franco et al. (2010) proposed the use of passive infrared sensors placed in the living quarters of an elderly person to detect abnormal changes in behavior. This system would allow early admission of dependent care for those who show a shift in daily routine, for example, individuals with Alzheimer disease. The monitoring of behavioural changes is an area of interest in telehealth since the onset of certain alarming situations could be detected and healthcare professionals could then be notified. The next section focuses on using smart healthcare system to handle emergency.

Emergency planning and management with respect to inhome healthcare has been a vigorously research topic. Rosen et al. (2002) proposed the Cybercare system to handle national scale disasters. Taking a military approach to map strategy, operations and technology to the healthcare realm of policy, functionality and network infrastructure technologies, they argued the necessity of a nation-wide support information infrastructure to cope with disaster, and of incorporating surge-capacity into a national disaster response system.

Citing the fact that the patients who receive critical emergency services often are not the actual victims of the disaster or outbreak such as SARS (severe acute respiratory syndrome), Joseph et al. (2005) developed a home healthcare disaster planning, recovery, and management system to facilitate treatment of the actual victims. By integrating in-home intelligent devices to provide timely measured information to public offices such as Red Cross, local police etc., they reasoned that the medical personnel can provide homecare remotely and deal with a mass-scale epidemic and natural disasters appropriately.

Smart environment not only can be used in a home environment but can also be used outdoor. Smalls et al. (2009) devised a health monitoring system for use in mass causality emergency situations. By placing a health monitoring device on the body of a victim and integrating this node to a wireless ad-hoc network, vital signs of the victim can be transmitted to regional and national emergency response institutions. This is particular useful in cases where infrastructures are non-operational due to catastrophic disaster or when communication channels are overwhelmed by emergency requests. Advances in wireless technology have driven down the cost of ad-hoc networks, and with fewer attending field personnel, Smalls et al.'s approach may prove to be cost effective in the very near future. Wouhaybi et al. (2010) also proposed a system with similar objectives but they focused on the reduction of false alarm using a rule-based decision system. A disaster management framework proposed by Bessis et al. (2010) integrates various technologies including web services, grid and cloud computing, ad-hoc mobile networks, and wearable and body sensors. Their aim is to provide coherent and collective resource utilization and decisionmaking for multiple parties in emergency situations. Jang et al. (2007) proposed a system using multiple sensor technologies including bio-sensors to monitor an elderly's daily physiological status. Many services are provided to the patient including early warning due to a change of health status, advice for health improvement, and appointment with specific medical practitioners. In addition to sensors, an active robot that provides services to human is part of a smart assistive environment proposed by Lin et al. (2009) . Using multimodal observations to recognize voice and other events, their project shows that health predictions, evaluations, and decisions become more reasonable in the evaluation of emergency level and the assistance required in critical events.

In one of the early studies, de Lunsignan et al. (2000) examined the effectiveness of a home cardiopulmonary monitoring system. This system collects various vital measures via sensors attached to the body and transmits the data wirelessly to a nearby unit at the patient's home. The data are then transferred to a centralised monitoring station. They found the system is acceptable to patients, functionally satisfactory in the home environment, and very reliable in the collection of objective data. Capomolla et al. (2004) monitored patients with CHF (chronic heart failure) either under usual care or telemonitoring care. Their study showed that home telemonitoring is more effective than usual care in reducing healthcare requirements and can improve outcome in CHF patients. Raju et al. (2004) presented their study on the cost effectiveness of mobile telemedicine applications, and the quality of care and medical error reduction. They reported on their extensive literature search and felt that the findings are inconclusive and well-designed protocols are necessary to conduct further large-scale investigations. Zhang and Bai (2005) used a queueing model to evaluate the performance of a home ECG and blood pressure monitoring system based on trial data over a 4-month period. Their objective was to study the traffic load, response time, and scalability of the system. The results are more than satisfactory and show great potential of the examined monitoring system. de Toledo et al. (2006a) reported their experience in using a telemedicine homecare system for patients with chronic obstructive pulmonary disease. Their results suggest that home telemedicine services provide good support to the healthcare professionals, improve patients' condition, and incur low costs.

Jasemian (2008) argued that a successful telemedicine application depends on the patient's compliance, trust and comfort in such home-based systems. An experiment was carried out in 2008 with the patients using ECG monitoring device with a real-time wireless system continuously for a long period of time. Over half of the participants found the system user friendly, reliable, and usable with acceptable performance. Though they found the ECG device heavy and not user-friendly.

Some have argued that the evaluation of home telehealth information systems should not be assessed simply on grounds of technical innovation but should use a holistic interpretive approach. In 2008, Kaldoudi and colleagues (2008) proposed a framework for evaluating home telehealth intervention, together with its application for peritoneal dialysis at home. Ojasalo et al. (2010) conducted a study on smart home technologies and services for intellectually disabled. They found that safety technologies are very well received, and the balance of safety and privacy can be addressed by the appropriate technologies. A 2011 study conducted by Ziefle et al. (2011) on the acceptance of video monitoring in smart home environment for elderly and disabled reveals that acceptance and users' needs and wants are the main issues in a successful deployment of such home medical monitoring technology. The study also shows that there are serious concerns that data may be altered, illegally accessed, and deleted intentionally or due to system failure or virus attack.

Beer and Takayama (2011) performed a study on elderlies who used a mobile remote presence (MRP) system. This study reveals many interesting issues. Benefits of MRP identified by the elderlies include being able to see and be seen with a MRP system, reduction in travel costs and hassles, and a decrease in isolation in a social context. However, concerns raised include MRP usage etiquette, personal privacy, possible overuse, and reliance on the system. Though the elderlies in the study are not technologically inclined, they prefer to operate the MRP system by themselves.

Smart home healthcare and telemedicine systems are here to stay, but there are still many challenges and issues to be resolved. Innovation is important to improve the efficiency and effectiveness, and hence acceptance, of these systems.

Security is major concern in the acceptance of homecare with modern technologies. Many research groups have focused on one of the most important issue in telemedicine, that of the integrity and security of transmitted data. Simpson et al. (2006) posed several challenging issues regarding continuous home monitoring: who should receive information as the patient, family members, caregivers and medical professionals all have interest; what information should each person receive without violating any data privacy and security concerns; how should information be presented since each party may prefer different mode of communication such as telephone, computer, etc. Proposed solutions to the security issue include cryptography (Mantas et al. 2009 ) and context-aware access control rules (Guennoun and El-Khatib 2009).

Due to the sensitive nature of health information, privacy is an important issue in data transmission and storage in telehealth systems. Without proper and established protocols and regulations in place, telemedicine would not be used widely as many people are concerned with the Smart home technology 541 privacy of their health information. Kotz and colleagues (2009) compared existing privacy frameworks, identified a set of desirable privacy properties in mobile healthcare systems, and used their findings to propose a privacy framework. They also raised several privacy issues and questions that need to be addressed by technical people, governmental agencies and regulatory organisations. Reliability is another critical factor in the implementation of smart homecare. Wireless technologies make telehealth care possible, however, such networks that operate in hostile outdoor environment have many reliability issues. Using rural China as a case study, Fong and Pecht (2010) identified and reviewed the factors that can impact on the reliability of telehealth care networks: physical obstacles, atmospheric absorption, inadequate fade margin and system failure. Gaddam et al. (2008) studied extensively the various issues related to the implementation of sensor networks for home healthcare and emergency. They have identified potential challenges including interoperability and interference, real-time data acquisition and processing, reliability and robustness, energy conservation, data management, data privacy and security, and comfort and unobtrusive operation. In addition, design issues for wireless networks considered are deployment of sensor nodes, mobility of wireless sensor, cost and size of the wireless node, infrastructure of the network, network coverage, network size, power management, life-time of the sensor networks, and quality of service requirements in such networks.

Since the second-half of the 2000s, there have been numerous reported work and innovation in smart healthcare systems. Taylor and Dajani (2008) gave a sound argument that the future home healthcare systems should take into consideration a well-balanced implementation infrastructure based on the web, mobile, and broadband technologies. Moritz et al. (2009) emphasized the importance of various fundamental issues in using interoperable devices in home healthcare. These issues include energy consumption, power supply, memory, computing power, and bandwidth.

Koufi and colleagues (2010) implemented a grid portal that provides services to people who need medical advice at their homes. The portal is an integrated system of wireless personal digital assistants (PDAs) and grid technology, with heavy design emphasis placed on security access and storage of data.

For telehealth, the monitoring of an individual creates a continuous stream of data that must be stored either locally at home or at a centralised database. Lasierra et al. (2010 Lasierra et al. ( , 2011 used an ontology approach to arrange patient data and records in a formal structure so as to support health information interoperability.

Invasive devices are being recognised as one of the prime reasons at-home patients are hesitant to use telehealth care monitoring systems. Motoi et al. (2010) demonstrated the possibility of using non-invasive technologies for home healthcare. Devices are installed on toilets, bathtubs, and beds to measure various vital signals. They deployed this prototyping system for subjects with cardiovascular disease or sleep disorder successfully.

Modifying and conditioning patient's behaviour is another innovative trend that is gaining attention these days. Evers and Krose (2010) developed a monitoring system tracking the physical and emotional condition via a sensor network. Patients are provided with feedback to promote activities and behaviour with positive health impact. Also, the integration of wearable technology and robots is a promising avenue to facilitate therapeutic intervention for chronic conditions, which potentially can reap benefits from these combined technologies (Bonato 2010).

Point-of-care testing is another developing trend in enhancing home healthcare. Many innovative devices are being investigated that potentially shorten the time between testing and diagnostic, and are especially useful in rural settings. Beyette et al. (2011) provided a comprehensive survey and a special volume on these innovative handheld and home-based devices. These devices include sensors for multispectral imaging, tomography, cardiac, EEG, Chlamydia trachomatis, bladder pressure, respiratory impairment, as well as for detection and analysis of macula swelling, pathogen detection, metabolic syndrome prediction, energy metabolism, Chagas disease, vascular tree identification, Parkinson's disease, etc.

Back in 2005, Stankovic et al. (2005) at the University of Virginia identified three critical development issues in wireless sensor networks for in-home healthcare. The first issue is the enabling technologies for future medical devices: interoperability, real-time data acquisition and analysis, reliability and robustness, and new node architectures. Embedded, real-time, networked system infrastructure is the other critical development area which includes patient and object tracking, communication, interference, multimodal collaboration, energy conservation, and multi-tiered data management. The third issue concerns medical practice-driven models and requirements including role-base access control and delegation in realtime, unobtrusive operation, and records and data privacy and security.

The critical development areas of concern, as suggested by Stankovic's team, have not yet been fully addressed with today's technology. There are ample opportunities for research and development in improving telemedicine systems as they are still at an infancy stage. Some of the challenges and issues in building a smart home environment are shown in Fig. 2 . Without a doubt, smart home telemedicine systems will be deployed in an increasingly rapid pace in the years to come.

Anagnostaki A et al (2002) 

",0.7843679437754867
Evaluating Hospital-Based Surveillance for Outbreak Detection in Bangladesh: Analysis of Healthcare Utilization Data,The International Health Regulations outline core requirements to ensure the detection of public health threats of international concern. Assessing the capacity of surveillance systems to detect these threats is crucial for evaluating a country's ability to meet these requirements.,"We propose a framework to evaluate the sensitivity and representativeness of hospital-based surveillance and apply it to severe neurological infectious diseases and fatal respiratory infectious diseases in Bangladesh. We identified cases in selected communities within surveillance hospital catchment areas using key informant and house-to-house surveys and ascertained where cases had sought care. We estimated the probability of surveillance detecting different sized outbreaks by distance from the surveillance hospital and compared characteristics of cases identified in the community and cases attending surveillance hospitals.

We estimated that surveillance detected 26% (95% CI 18%-33%) of severe neurological disease cases and 18% (95% CI 16%-21%) of fatal respiratory disease cases residing at 10 km distance from a surveillance hospital. Detection probabilities decreased markedly with a1111111111 a1111111111 a1111111111 a1111111111 a1111111111

distance. The probability of detecting small outbreaks (three cases) dropped below 50% at distances greater than 26 km for severe neurological disease and at distances greater than 7 km for fatal respiratory disease. Characteristics of cases attending surveillance hospitals were largely representative of all cases; however, neurological disease cases aged <5 y or from the lowest socioeconomic group and fatal respiratory disease cases aged !60 y were underrepresented.

Our estimates of outbreak detection rely on suspected cases that attend a surveillance hospital receiving laboratory confirmation of disease and being reported to the surveillance system. The extent to which this occurs will depend on disease characteristics (e.g., severity and symptom specificity) and surveillance resources.

We present a new approach to evaluating the sensitivity and representativeness of hospitalbased surveillance, making it possible to predict its ability to detect emerging threats.

• Many countries rely on hospital-based surveillance for the detection of infectious diseases of national and global public health relevance.

• It is often difficult to access suitable external reference data to assess the capacity of a surveillance system to detect cases and outbreaks or to characterize cases.

• We demonstrate a novel approach using healthcare utilization data to evaluate the sensitivity and representativeness of severe infectious disease surveillance in Bangladesh.

• The capacity to detect cases and outbreaks decreased with distance from surveillance hospitals.

• Cases captured by surveillance differed from cases in communities by age and socioeconomic status.

• Geographic coverage of surveillance could be improved by including other hospitals in the surveillance system.

• The presented approach is applicable for a wide range of infectious diseases in different settings, taking some practical considerations into account.

• Hospital-based surveillance may have low sensitivity in rural areas at greater distances from surveillance hospitals, suggesting a risk of unrecognized transmission of emerging infectious diseases.

A well-functioning disease surveillance system is crucial for the identification and control of outbreaks, and hence the prevention of national and global health emergencies [1] . The World Health Organization (WHO) highlighted the value of national surveillance systems in the International Health Regulations (2005) , an agreement among all member states to develop and maintain sufficient capacity for the detection, reporting, and control of public health threats of international concern [2] . Infectious disease surveillance should enable (i) the timely detection of outbreaks, (ii) the quantification of health problems, (iii) the identification of subpopulations at risk, and (iv) the assessment of temporal trends including the impact of control strategies [3, 4] . National surveillance systems typically collect data from patients seeking care at sentinel hospitals or other healthcare facilities and can provide useful information for public health purposes. However, hospital-based surveillance generally underestimates disease burden since only a proportion of cases visit a hospital for care [5] . Low case detection may also undermine the value of hospital-based surveillance for outbreak detection. Moreover, if patients captured by the surveillance system are not representative of all cases in the community, surveillance statistics could lead to erroneous interpretations of disease patterns and misallocation of prevention resources. In particular, sex, socioeconomic status, or distance can affect healthcare seeking at hospitals, especially where access to care is limited [6] [7] [8] [9] . Surveillance evaluation guidelines, such as those established by the US Centers of Disease Control and Prevention, list sensitivity and representativeness among the attributes that a public health surveillance system should possess and that require assessment [10, 11] . In order to follow these guidelines, we need external reference data that are often unavailable in resource-poor settings [12] .

Here, we present a new approach to evaluating the capacity of a surveillance system to detect and characterize disease cases, with emphasis on outbreaks of emerging infections that often occur as small case clusters in remote areas. We apply our methodology to assess hospital-based surveillance of severe neurological infectious disease and fatal respiratory infectious disease in Bangladesh.

The field teams obtained written informed consent from participants or their guardians (if <18 y of age) during community surveys. Healthcare utilization survey protocols were reviewed and approved by the Ethical Review Committee of the International Centre for Diarrhoeal Disease Research, Bangladesh.

how epidemiological studies can be used to identify cases with severe symptoms in communities and capture their personal and healthcare utilization characteristics (data collection stage) (Fig 1) . In addition to detailing how we collected the data in this study, we provide information about how the approach could be varied in other settings. We subsequently demonstrate how such data can be used to evaluate the sensitivity and representativeness of surveillance systems (evaluation stage). We then apply our approach to the detection of severe neurological infectious diseases and fatal respiratory infectious diseases in Bangladesh as a case study.

Selecting study locations. The first step was to randomly select communities at differing distances from the surveillance hospitals. We specified catchment areas of selected hospitals based on hospital records and subsequently randomly selected small administrative units from which all communities were surveyed. Selection of communities could also be done through census data or using detailed population maps of the area.

Identifying people with diseases in the selected community. Study teams visited the selected communities and identified cases that had had the disease of interest. The retrospective identification of severe disease cases in the community was based on syndromic criteria, used as a proxy for clinical case definitions that would be applied in healthcare facilities. The In the Bangladesh example, the catchment areas of surveillance hospitals were first defined based on hospital records (e.g., areas where >50% or >75% of cases reside) [13, 14] . Subsequently, small administrative units were chosen at random from within the catchment area, and all communities in the selected areas were surveyed. Cases in the community were identified based on lists of deaths in addition to community networking strategies (rural settings) or house-to-house surveys (urban settings). Information on symptoms (to establish case definitions), healthcare seeking behavior, and characteristics of cases was collected. In other settings, the exact survey procedures may vary according to the context. identification of such cases in the community is often the most problematic step, and the optimum strategy will depend on the local context, the severity of the disease, and the specificity of disease symptoms.

Collecting information on healthcare seeking and personal case characteristics. To estimate case detection probabilities, identify biases in case statistics, and characterize the healthcare utilization behavior in the population, we needed information about the healthcare seeking and personal characteristics of cases. In particular, we needed to identify whether the cases attended a surveillance hospital. Such information was obtained during household visits of identified cases. To understand the impact of distance from the hospital, we approximated the locations of households by the central positions of the small administrative units. Alternatively, household locations could be recorded precisely using GPS devices.

Quantifying the probability of detecting a case. We estimated the case detection probability as the proportion of cases who reportedly attended a surveillance hospital among all cases identified in the community. We further assessed how this probability changed with distance from the surveillance hospital.

Quantifying the probability of detecting outbreaks. We subsequently used the estimated case detection probabilities to quantify the capacity of the surveillance system to identify disease outbreaks. We estimated outbreak detection probabilities for varying outbreak sizes and for outbreaks occurring at different distances from surveillance hospitals.

Assessing the representativeness of detected cases. We evaluated the representativeness of detected cases by estimating the difference between case statistics (proportions of specific case characteristics) based on all cases in the community and based on identified cases who attended the surveillance hospital. The investigated characteristics included sex, age, and socioeconomic status.

Assessing alternative surveillance strategies. To investigate how sensitivity and representativeness of the surveillance system could be improved by integrating other healthcare providers, we applied the evaluation procedures as described above to other healthcare provider types.

We demonstrate the application of the proposed evaluation strategy by using it to assess the capacity of hospital-based surveillance for severe infectious diseases in Bangladesh, which is based on tertiary care hospitals located throughout the country. We used data from two surveys carried out in catchment areas of some of these hospitals that investigated the healthcare utilization behavior of individuals with severe neurological infectious disease or fatal respiratory infectious disease (Fig 2A) [14, 15] . These disease types are of great public health relevance in Bangladesh (e.g., Japanese encephalitis and influenza) but also represent symptoms typical of other emerging infectious diseases (e.g., Nipah and severe acute respiratory syndrome). A first survey collected data between 10 June 2008 and 30 March 2009 about cases with symptoms of severe neurological infection that occurred within the previous 12 mo in 60 small administrative units (mean population size of 28,000 people) in the catchment areas of three surveillance hospitals [14] . A second survey collected data between 3 April 2012 and 22 February 2013 about acute respiratory infection (ARI)-related deaths that occurred within the previous 24 mo in 22 administrative units in the catchment areas of 11 surveillance hospitals [15] . We considered ARI-related deaths as a proxy for respiratory disease of sufficient severity to require medical attention. The surveillance hospital in Dhaka City was excluded from the original studies because of the difficulty of defining the catchment area (a step necessary for the original study purpose), as people nationwide seek medical care in Dhaka. The surveys followed procedures as previously described and summarized below [13, 14] . Characteristics of the study population are described in Fig. A in S1 Text. [16] . Sixty-eight percent of the population in Bangladesh lives >30 km from a surveillance hospital (including the Dhaka surveillance hospital), a distance at which case and outbreak detection probabilities are low. (C) Probability of surveillance case detection by distance. The observed probability was calculated as a moving average over a 25 km distance window. Case detection probabilities were estimated using logbinomial regression models including distance as an explanatory variable. Evaluating Hospital-Based Surveillance

The catchment areas of selected hospitals were first specified based on hospital records (S1 Text). Small administrative units (mean population of 28,000 people) were subsequently selected randomly within the catchment areas, and all communities in the selected areas surveyed. The identification of cases in selected communities was based on social structures, i.e., cases were identified by visiting public meeting points, such as mosques, markets, or tea-stalls, where health problems in the community are often publicly discussed. Cases were subsequently confirmed by household visits. In urban areas, house-to-house surveys were conducted to compensate for less pronounced community structures. Additional fatal respiratory infectious disease cases were identified through lists of deaths provided by administrative officers. For both disease types, the identification of cases was based on syndromic criteria. We defined severe neurological infectious disease as fever with altered mental status for >6 h or with unconsciousness for !1 h, or fever with altered mental status, unconsciousness, or a new onset seizure that resulted in death. Fatal respiratory infectious disease (ARI-related death) was defined as having any two of the following symptoms in the 30 d prior to death: sudden onset of fever, cough, breathing difficulty, feeding difficulty, or runny nose. Deaths in children aged <5 y were also classified as ARI-related deaths if there was a sudden onset of breathing difficulty in the 30 d prior to death.

During surveys, information was collected on healthcare utilization behavior and personal characteristics of identified severe neurological and fatal respiratory disease cases. Cases or their household members were asked whether the case visited the surveillance hospital or any other healthcare provider, including other nonlocal hospitals, during his/her illness. Further, information on sex, age, socioeconomic status, and geographic location of households of cases was collected.

We defined ""community cases"" as all severe neurological or fatal respiratory disease cases identified during community surveys (whether they attended a surveillance hospital or not) and ""surveillance cases"" as the subset of community cases who reportedly attended a surveillance hospital. For each case identified in community surveys, we identified whether they attended their nearest surveillance hospital. We then estimated the distance to that surveillance hospital as the distance between the residence administrative unit centroid and that specific surveillance hospital using QGIS [17] . Age was categorized as <5, 5-14, 15-59, and !60 y. A socioeconomic status index was generated by principal component analysis based on household assets (electricity, working television, bicycle, motorcycle, sewing machine, mobile phone) and categorized into tertiles (lowest, middle, and highest) [18] . In sensitivity analyses, we explored the use of continuous age and socioeconomic status classified into quintiles (S1 Text). Socioeconomic status was missing for 45 of 1,633 fatal respiratory disease cases, who were excluded from the analysis where this information was required. Three fatal respiratory disease cases were excluded from all analyses due to missing healthcare seeking information.

We estimated the disease-specific case detection probability as the proportion of cases who reportedly sought care at a surveillance hospital among all cases identified during community surveys (number of surveillance cases/number of community cases) and computed 95% confidence intervals (95% CIs) based on the Clopper-Pearson exact method [19] . We quantified case detection probabilities by distance from a surveillance hospital using log-binomial regression analysis separately for severe neurological and fatal respiratory disease cases. We further investigated more complex functional forms of distance in log-binomial regression models. We fitted models with polynomial terms up to the fifth degree and models with basic splines with knots at various positions (between 20 and 50 km distance). Model fit was compared based on the Akaike information criterion (AIC), and the models with lowest AIC were selected. The fit of selected models was compared to the observed proportion of cases who attended surveillance hospitals at different distances (moving average over a distance window of 25 km). We estimated the proportion of the population living >30 km and >50 km from a surveillance hospital using gridded population density estimates of 100 × 100 m resolution [16] .

To quantify the capacity of the surveillance system to detect outbreaks of varying sizes, we calculated the probability that at least one case was detected:

Pr outbreak1 is the outbreak detection probability based on a one-case threshold, Pr is the case detection probability, and s is the outbreak size. This calculation assumes that the probability of detecting a sentinel case is independent of other cases. We used distance-specific case detection probabilities estimated by log-binomial regression and obtained confidence intervals of outbreak detection probabilities based on the 95% CI limits of case detection probabilities. We further estimated the size of the smallest outbreak that would be detected with !90% probability by distance from the surveillance hospital. For emerging infectious diseases of global health importance, such as Nipah, severe acute respiratory syndrome, or avian influenza, a single detected case may be considered an outbreak. For other disease systems (e.g., endemic diseases or diseases for which differential diagnosis is difficult), an outbreak may be declared only after more than a single case is detected over a specified period of time and within specified geographic boundaries [20] . We can extend the framework to estimate the probability of identifying an outbreak with different outbreak thresholds applied, and we provide examples for outbreaks defined as detection of at least two cases or at least five cases. We calculated the probability of detecting at least two cases (Pr outbreak2 ) as one minus the probability of detecting no cases (Pr 0 ) and exactly one case (Pr 1 ):

Likewise, we estimated the probability of detecting at least five cases (Pr outbreak5 ) as one minus the probability of detecting no cases (Pr 0 ) and exactly one (Pr 1 ), two (Pr 2 ), three (Pr 3 ), and four cases (Pr 4 ):

We investigated the representativeness of surveillance cases (sex, age, and socioeconomic group) by comparing the proportion of cases with a specific characteristic (and exact binomial confidence intervals) among community cases to the proportion of cases with that characteristic among surveillance cases. We quantified the absolute difference in proportions (proportion of cases with characteristic among surveillance cases minus proportion among community cases) with 95% CIs and p-values using bootstrapping (2,000 bootstrap iterations) [21] .

Based on the collected healthcare utilization data, we evaluated how the sensitivity and representativeness of a surveillance system may be improved by integrating other healthcare providers. We classified healthcare providers as (i) surveillance hospitals, (ii) other hospitals (government and private clinics), (iii) qualified private practitioners, and (iv) the informal sector (unqualified practitioners such as traditional healers, village doctors, homeopaths, and pharmacies). We estimated the proportion of cases attending each healthcare provider class, with exact binomial confidence intervals, and estimated outbreak detection probabilities based on proportions attending the surveillance hospital plus (i) other hospitals, (ii) qualified private practitioners, or (iii) informal healthcare providers. Furthermore, we compared the proportion of cases with each characteristic (sex, age, and socioeconomic group) among community cases to the proportion among those attending each healthcare provider class and quantified absolute differences in proportions with 95% CIs and p-values using bootstrapping (2,000 bootstrap iterations).

All statistical analyses and graphics were implemented in the R computing environment; maps were created using QGIS software [17, 22] .

The studied communities were located within 95 km (severe neurological infectious disease) and 62 km (fatal respiratory infectious disease) of a surveillance hospital. In these communities, 76 of 426 severe neurological disease cases (18%, 95% CI 14%-22%) and 234 of 1,630 fatal respiratory disease cases (14%, 95% CI 13%-16%) attended a surveillance hospital. Adjusting for distance, the case detection probability was nearly twice as high among severe neurological disease cases than among fatal respiratory disease cases (risk ratio 1.8, 95% CI 1.4-2.3; p < 0.001). At 10 km distance, an estimated 26% (95% CI 18%-33%) of severe neurological disease cases and 18% (95% CI 16%-21%) of fatal respiratory disease cases were detected by the hospital-based surveillance. The detection probability decreased with distance from the surveillance hospital, and the decline was faster for fatal respiratory disease than for severe neurological disease. A 10 km distance increase resulted in a 12% (95% CI 4%-19%; p = 0.003) relative reduction in case detection probability for severe neurological disease but a 36% (95% CI 29%-43%; p < 0.001) relative reduction for fatal respiratory disease (Fig 2C) . Including more complex functional forms of distance in the log-binomial regression models did not improve model fit based on AIC (Table A and Figs . B and C in S1 Text).

The probability of detecting an outbreak of exactly three cases (if a single detected case was considered an outbreak) dropped below 50% at distances greater than 26 km for severe neurological disease and at distances greater than 7 km for fatal respiratory disease (Fig 3A) . Fig 3B  and 3C show the minimum number of cases required for surveillance to detect outbreaks with a probability of !90% if different outbreak thresholds are applied. For outbreaks defined as detection of at least one case, we found that an outbreak of fatal respiratory disease required 12 cases (95% CI 11-13) to be detected with 90% probability at 10 km from a surveillance hospital, but 30 cases (95% CI 24-39) to be detected at 30 km. In contrast, the impact of distance on the outbreak size requirement was much more limited for severe neurological disease: eight cases (95% CI 6-12) at 10 km and 11 cases (95% CI 9-14) at 30 km. For outbreaks defined as detection of at least two cases, 14 severe neurological disease cases (95% CI 11-20) and 20 fatal respiratory disease cases (95% CI 18-23) would be necessary for an outbreak to be detected at 10 km distance, and 19 severe neurological disease cases (95% CI 15-24) and 51 fatal respiratory disease cases (95% CI 41-66) at 30 km. The necessary outbreak sizes increased further when a five-case threshold was applied, so that 28 severe neurological disease cases (95% CI [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] and 39 fatal respiratory disease cases (95% CI 35-44) would need to occur for an outbreak to be detected at 10 km distance, and 36 (95% CI 30-46) and 97 (95% CI 79-128), respectively, cases at 30 km.

Surveillance hospital attendance among community cases varied by case characteristics, leading sometimes to biased disease statistics among surveillance cases (Table B in S1 Text). For severe neurological disease, individuals aged <5 y represented 48% of community cases but only 29% of surveillance cases (p < 0.001). Additionally, the proportion of cases in the lowest socioeconomic group was lower among surveillance cases than among community cases (43% versus 57%; p = 0.012), while the proportion of individuals aged 15-59 y was higher (43% versus 29%; p = 0.005) (Fig 4A) . For fatal respiratory disease, the proportion of individuals aged !60 y (47% versus 62%; p < 0.001) was lower among surveillance cases than among community cases, while the proportion of individuals aged <5 y (24% versus 18%; p = 0.020), individuals aged 15-59 y (27% versus 18%; p < 0.001), and cases in the highest socioeconomic group (43% versus 37%; p = 0.022) was higher (Fig 4B) . We observed a slight difference in the proportion of females for fatal respiratory disease (34% among surveillance cases versus 38% among community cases; p = 0.108), but not for severe neurological disease (39% versus 40%; p = 0.861). Results were consistent in sensitivity analyses with age as a continuous variable and socioeconomic status classified into quintiles (Figs. D and E in S1 Text).

A substantial proportion of cases (severe neurological disease 42% [95% CI 38%-47%]; fatal respiratory disease 26% [95% CI 24%-28%]) visited multiple healthcare providers during their illness. Forty-eight percent (95% CI 44%-53%) of severe neurological disease cases and 31% (95% CI 29%-34%) of fatal respiratory disease cases attended any hospital, including surveillance hospitals (Fig 5) . Including other hospitals that were attended by cases in the surveillance system could have increased the overall case detection probability by 31% (absolute increase) for severe neurological disease cases and 17% for fatal respiratory disease cases. The capacity to detect outbreaks would have increased, so that outbreaks containing four severe neurological or eight fatal respiratory disease cases would have been detected with !90% probability for any distance in the range 0-40 km from the original surveillance hospital, compared to 13 and 47 cases, respectively, with the current system ( Fig. F in S1 Text) . However, since individuals who attended any hospital had similar characteristics in terms of sex, age, and socioeconomic status as those attending surveillance hospitals (Fig. G in S1 Text), this expansion would not have increased disease detection in key groups such as the lowest socioeconomic group. Only with the informal sector incorporated in the surveillance system would cases in such groups be detected.

We described an analytic approach for evaluating the sensitivity and representativeness of hospital-based surveillance systems and applied it to surveillance for severe neurological diseases and fatal respiratory infectious diseases in Bangladesh. We quantified the proportion of cases detected and the probability that the surveillance system would detect different sized outbreaks by distance from the surveillance hospital. Finally, we characterized biases in surveillance statistics and identified potential improvements to the surveillance platform.

We estimated that approximately one-quarter of severe neurological disease cases and onefifth of fatal respiratory disease cases occurring 10 km from a surveillance hospital would be detected with current surveillance. The proportion of cases attending a surveillance hospital declined significantly with increasing distance between individuals' residence and the surveillance hospital, substantially faster for fatal respiratory disease than for severe neurological disease. These low detection probabilities mean that hospital-based surveillance in Bangladesh (like in most other resource-poor countries presumably) would likely miss a high proportion of single-case public health events. Of greater relevance is that surveillance system capacity to detect outbreaks and detection probabilities increased substantially with the number of cases. The required number of cases to detect outbreaks with high probability varied with disease type and distance from the surveillance hospital. It could be as low as about ten cases if the outbreak occurred <10 km from the surveillance hospital but increased quickly with distance for Evaluating Hospital-Based Surveillance fatal respiratory disease. For outbreaks defined as a single detected case, we found that more than half of outbreaks with ten cases of fatal respiratory disease would be missed if the outbreak occurred >32 km from the hospital. Such detailed quantification of outbreak detection probability is essential to ascertain the likelihood that an emerging threat can be detected early enough to be contained [23] .

In some circumstances, authorities may have to wait until more than a single case is detected to recognize that an outbreak is occurring. In particular, difficult differential diagnoses and lack of appropriate diagnostic tests mean that only when a number of cases are detected from the same area and over a short time frame will an outbreak be identified and further investigations conducted. In addition, where a low background level of transmission is expected (such as with endemic diseases), public health authorities may wait until a particular threshold is exceeded before declaring an outbreak. In both of these scenarios, where multiple cases need to be detected by the hospital before an outbreak is recognized, the optimal number of detected cases and their spatial and temporal separation will depend on the disease system. We can incorporate this information into our flexible framework and provide examples where we calculate the size an outbreak needs to be for scenarios where at least two or five cases need to be detected (Fig 3B and 3C ). In particular, this demonstrates that if an outbreak is identified only once five cases are detected at the surveillance hospital, the size of the outbreak would have to be substantially larger (e.g., nearly 100 total cases of a fatal respiratory disease at 30 km from a surveillance hospital) for there to be a 90% chance of an outbreak being identified. This highlights the possibility that, by the time an outbreak reaches sufficient size to be detected by the system, outbreak control measures may be much less effective at controlling spread. Thresholds for case counts that trigger an outbreak response should be crafted taking this possibility into account.

Low detection probabilities for outbreaks that occur far from surveillance hospitals are an important concern because pathogens with high case fatality such as Japanese encephalitis and Nipah virus are nearly exclusively found in rural communities in Bangladesh [24, 25] , and Evaluating Hospital-Based Surveillance these communities are usually located far from surveillance hospitals. Rural environments are also considered to be at highest risk for the emergence of novel pathogens [26, 27] . Population distribution maps suggest that 68% of the population in Bangladesh live in communities >30 km from a surveillance hospital (representing 108 million individuals) and 40% live >50 km from a surveillance hospital (representing 63 million people) (Fig 2B) . Strengthening healthcare-based surveillance in these areas should be a priority, and cost-effective approaches to achieving surveillance targets need to be identified. There is increasing recognition of the value of novel data sources to improve the sensitivity of infectious disease surveillance, some of which can provide crucial information in remote areas [20] . Novel approaches include surveillance for media reports of disease clusters, as used for various infectious diseases in Bangladesh [12, 28] , and training of local drug sellers to recognize and report disease symptoms, as rolled out nationally to enhance tuberculosis surveillance in Ghana [29] . Other surveillance data streams, such as monitoring over-the-counter medication sales, telephone triage, and webbased queries, have been successfully integrated in surveillance systems in resource-rich settings [30] .

We found that cases attending surveillance hospitals were not necessarily representative of all cases in the community. In particular, the youngest severe neurological disease cases and the oldest fatal respiratory disease cases were less likely to attend surveillance hospitals, and attendance was also lower among cases in the lowest socioeconomic group. Similar variation in hospital attendance has been reported in other resource-poor settings [6, 8, 9] , indicating that hospital-based surveillance in these countries may have comparable limitations. Disease statistics obtained through hospital-based surveillance have to be interpreted in the light of detected biases, and correction factors may need to be applied. For example, underestimating severe neurological disease among young children may mislead any future Japanese encephalitis vaccination strategy [31, 32] . Differential surveillance hospital attendance may also influence the capacity to detect emerging infections, such as the avian influenza A (H7N9) virus that emerged in 2013 in China with observed cases mainly among elderly men [33] .

Overall, access to appropriate care was poor-over 30% of community cases with severe disease or who died in our study never saw a qualified provider. Such low access is a common problem in low-income settings and means that a large proportion of the population, and particularly subgroups that are potentially at highest need, do not receive the required medical attention [6] [7] [8] [9] . For example, difficulties accessing qualified healthcare providers for elderly people, who are often at greatest risk of respiratory disease, can have severe consequences for the outcome of disease. Previous studies have demonstrated that accessibility to healthcare is a significant predictor of morbidity and mortality among elderly individuals with respiratory disease [34] .

The study showed that healthcare utilization behavior varied by disease type, which may be due to different characteristics of cases such as their age, socioeconomic status, and geographic location (Fig. A in S1 Text) . The majority of fatal respiratory disease cases were !60 y old and may have faced limitations in mobility; moreover, rapid progression of disease to death may have prevented cases in this age group from seeking appropriate care. Cases and their family members in general may have also perceived neurological symptoms as more severe, resulting in higher motivation to attend a qualified healthcare provider [7] .

We evaluated potential improvements of surveillance by analyzing healthcare seeking behavior among cases identified in communities. While the majority of individuals did seek care, much of this was in the informal sector, which cannot easily be incorporated into surveillance activity. Nevertheless, including other hospitals attended by cases in the surveillance system (the exact location and number of these hospitals was unfortunately not identified during surveys) would double case detection probabilities and allow detection of medium-sized outbreaks (<10 cases) in a wider geographic area. However, in the case of Bangladesh, such extension is likely to be prohibitively expensive. Mapping other hospitals in Bangladesh that may serve as surveillance sites would allow testing of various surveillance scenarios to identify the optimal location of surveillance sites while keeping the same total number or to quantify the number of sites needed to achieve a target surveillance coverage [35] . Many emerging infectious diseases originate as spillover infections of zoonotic diseases into the human population [36] . Therefore, mapping the occurrence of relevant zoonotic diseases (e.g., avian influenza) and combining such maps with the estimated outbreak detection probabilities would allow highlighting of surveillance gaps for particular types of emerging infectious diseases.

The capacity of surveillance systems to detect outbreaks will depend not only on the probability that a case attends a surveillance hospital, but also on whether the case undergoes confirmatory laboratory testing and is subsequently reported through the surveillance system by the hospital. Here we assumed a ""best-case scenario"" with a fully functional surveillance system at the hospital level, where each case who attends the surveillance hospital is ultimately recognized and confirmed as a case. Case detection at the hospital may however be incomplete, since case definitions at hospitals may differ from syndromic definitions, a surveillance sampling frame may be applied, or resources and trained personnel for the diagnosis and reporting of cases may be limited [37] . The calculation of case and outbreak detection probabilities may be adjusted for misdiagnosis and underreporting at hospitals if such information is available. We further assumed complete detection of cases in communities during surveys. Although a few cases may have been missed, this assumption is justified as we investigated severe disease conditions that are easily remembered by family and community members. Moreover, survey procedures combining interviews of key informants and house-to-house visits were specifically designed to capture near-complete case information. Further, any missed cases are unlikely to impact our estimates, as such impacts would occur only if there was differential healthcare seeking between those detected and those missed. We investigated spatial differences in hospital attendance based on the straight-line distance of communities from the surveillance hospital. If available, other distance measures such as travel distance or travel time may provide a more accurate indicator of distance from the surveillance hospital. In some cases, these measures may strongly vary with the season, and it would be interesting to explore how that may impact the probability of detecting an outbreak. We assumed that cases did not visit other surveillance hospitals than the catchment hospital. Given the poor road infrastructure in the country, it would be very unusual to travel to a tertiary care hospital that was not the closest one. It is possible that some individuals traveled to Dhaka; however, these are likely to be wealthier individuals who would visit small private healthcare facilities that are not part of the surveillance network. The surveillance hospital in Dhaka was not included in our study. This is unlikely to have biased our assessment of the performance of the surveillance system outside the capital. Indeed, this would introduce a bias only under the unlikely scenario that many cases in our study who did not attend the nearest surveillance hospital (and were therefore not captured there) instead attended the surveillance hospital in Dhaka (and were captured there). Surveillance system performance in the capital city may however differ from elsewhere, and a comprehensive assessment of the national surveillance system would therefore have to include Dhaka. Moreover, hospital-based surveillance is only one surveillance type in Bangladesh, and other data sources need to be considered to assess the country's overall capacity to detect public health events.

The described methodology is applicable to assessing surveillance for other severe diseases in resource-poor settings, keeping in mind practical constraints. Conducting community surveys may be labor intensive, time consuming, and expensive depending on the setting and may be particularly challenging in densely populated areas such as Dhaka. Nonetheless, such surveys are valuable tools for obtaining external reference data and simultaneously assess heterogeneities in healthcare access. The effectiveness of community networking may depend on the social structures in the study area; where social links are weaker (e.g., in urban areas), house-to-house surveys, even though more labor intensive, may be more suitable for the identification of cases in the community. The proposed strategy is valid for diseases of sufficient severity to require medical attention and to be remembered by cases and family members. The approach is syndromic (i.e., disease types are classified based on a set of symptoms), and the classification specificity may vary by disease.

In conclusion, this study allowed us to quantify the sensitivity and representativeness of hospital-based surveillance and to identify weaknesses, particularly in detecting small-to medium-sized outbreaks in remote areas. These findings highlight difficulties that low-middle-income countries may have in meeting International Health Regulations requirements, despite considerable investment in hospital-based surveillance platforms. 

",0.7725496565463779
Integrated Sensor Systems and Data Fusion for Homeland Protection,"As stated by John Naisbitt in his bestseller ""Megatrends"" [1] , published in 1982, about the new trends and directions transforming our lives: ""We are drowning in information but starved for knowledge. This level of information is clearly impossible to be handled by present means. Uncontrolled and unorganized information is no longer a resource in an information society, instead it becomes the enemy."" This successful sentence can be taken as a statement of the problem of information fusion: how can knowledge, awareness and decision making capability be achieved starting from the available information?","and data links. After conceiving also algorithms to track targets on the basis of the angle and identification measurements provided by ESM on a moving platform, data fusion of active and passive tracks was provided [14] [15] [16] [17] .

Nowadays concepts have been developed and spread to be applied to very complex systems with the aim to achieve the highest level of intelligence as possible and hopefully to support decision. Data fusion is aimed to enhance situation awareness and decision making through the combination of information/data obtained by networks of homogeneous and/or heterogeneous sensors. A sensor network presents advantages over a single sensor under different points of views, as it supplies both redundant and complementary information. Redundant information is exploited to make the system robust to the failure in order that a malfunction of an entity of the system means only a degradation of the performances, rather than the complete failure of the system, since information about the same environment can be obtained from different sources. More robustness can be achieved also with respect to interferences, both intentional and unintentional, due to frequency and spatial diversity of the sensors. Complementary information build up a more complete picture of the observed system; for example sensors are dislocated over large regions providing diverse viewing angles of observed phenomenon and different technologies can be employed in the same application to provide improved system performance.

A large number of different applications, algorithms and architectures have been developed exploiting these advantages. Several examples can be found in robotics, military applications, Homeland Protection and management of large and complex critical infrastructures. Although the specific nature of each problem is different, the final goal, from the point of view of the sensed information, is always the same: using all the available data to better understand the investigated phenomena. The aim of this chapter is to give an overview of the several approaches that can be followed to design and analyze systems for Homeland Protection. Different fusion architectures can be drawn on the basis of the employed algorithms; according to this approach, three general categories can be identified in the literature [18, 19] : centralized, hierarchical, and decentralized/netcentric.

The traditional architecture is centralized: in this framework several sensing devices are connected to a central component, the fusion node. For example, in the case of a sensor network employed for the surveillance of an area, usually the information traffic goes from the sensor nodes to a single sink node called information fusion center. According to the information received from the sensors, the fusion center monitors the area where the sensors are deployed and decides the actions to take. Conceptually, the algorithms employed in this case are relatively simple and the resource allocation is straightforward because the central component has an overall view of the whole system. This kind of architecture presents several drawbacks: high computational load, the possibility of catastrophic failure when the fusion node goes down and the lack of flexibility to changes of the system and sensor entities. Therefore this approach is still valid if the number of sensors, whose information is fused, independently of the width of the area to be monitored, is limited and also the relationship and interconnections among sensors are limited too.

In hierarchical architectures, there are several fusion nodes, where intermediate fusion processes are performed, and an ending central fusion node. The principle of a hierarchy is to reduce the communications and computational loads of centralized systems by distributing data fusion tasks among a hierarchy of sensor entities. However in a hierarchy there is still a central component acting as a fusion center. Entities constituting local fusion center, locally process information and send it to the central fusion node. This approach is commonly used in robotics and surveillance applications. Although this architecture reduces the computational and communication loads, there are still some drawbacks

The diagram of Figure 22 .2 provides a decomposition of the Homeland Protection domain: the two main sub-domains are Homeland Defense (HD) and Homeland Security (HS) [21] .

HD includes the typical duties and support systems of military joint forces and single armed forces. Usually HD systems are strictly military, are employed by military personnel only, satisfy specific technical requirements, operational needs and environmental scenarios, and in most cases are designed to face only military threats. The new trend aims to employ military surveillance systems in combined military and civil operations, especially to face terrorism [22] . The military domain has also been swept in recent years by the NCO paradigm; NCO predicates a tighter coupling among forces, especially in the cognitive domain, to achieve synchronization, agility and decision superiority and it is a strong driver in the transformation from a platform-centric force to a network-centric force [20] .

HS is a very broad and complex domain that requires coordinated action among national and local governments, private sector and concerned citizens across a country; it covers issues such as crisis management, border control, critical infrastructure protection and transportation security [23, 24] . Crisis management is the ability of identifying and assessing a crisis, planning a response, and acting to resolve the crisis situation. Border control aims to build a smart protection belt all around a country to counter terrorism and illegal activities; yet it is not resolutive due to the difficulty of controlling the country boundaries along their full and variegated extension, the non necessarily physical nature of attacks in the current information age, and the threats which often arise internally to the country itself. HS includes also land security that is particularly critical because of its complexity and strategic importance; the security of critical assets, such as electric power plants, communication infrastructures, strategic areas and railway networks, must be ensured continuously in space and time [25] [26] [27] . The most recent terrorist attacks have shown the vulnerability of national critical infrastructures [28] and have made the world aware of the possibility of large-scale terrorist offensive actions against civil society: the September 11th, 2001 attack on the World Trade center in New York City is the most dramatic example of this new terrorism. The main emphasis has been put on the terrorist threat, but what emerges is the fragility and vulnerability of modern society to both deliberate threats and natural disasters. The HP domain includes also the protection from deliberate attacks against the commercial activities of a Country led also out of the national territory, comprehensive also of the territorial waters and Exclusive Economic Zone (EEZ). Seaborne piracy against transport vessels remains a significant issue (with estimated worldwide losses of US$13-16 billion per year), particularly in the waters between the Red Sea and Indian Ocean, off the Somali coast, and also in the Strait of Malacca and Singapore, which are navigated by over 50,000 commercial ships a year [29, 30] .

The globalization, the pervasiveness of information technologies and the transformation of the industrial sector and civil society have created new vulnerabilities in the system as a whole, but all this has happened without a corresponding effort to increase its robustness and security. As an example, single infrastructure networks have grown over the years independently, creating autonomous ""vertical"" systems with limited points of contact; around year 2000, as a consequence of the change of trend in the socio-techno scenario, the infrastructures have begun to share services and thus to create interconnected and interdependent systems. Nowadays infrastructures are interconnected and mutually dependent in a complex way: a phenomenon that affects one infrastructure can have a direct or indirect impact on other infrastructures, spreading on a wide geographical area and affecting several sectors of the citizen life. This is schematically represented in Figure 22 .4 [31, 32] .

Beside the physical protection of territory, citizens, critical assets and activities, the security of information and computer systems is one the greatest challenges for a Country. Information and communication technologies have enhanced the efficiency and the comfort of the civil society on one hand, but added complexity and vulnerability on the other hand. The cyber security consists in ensuring the protection of information and property from hackers, corruption, or natural disaster, maintaining however the information and property accessible and productive to its intended users. This problem is pervasive in nearly all the systems supporting a nation: financial, energy, healthcare and transportation. The new trend toward the mobile communications is revealing a new cyber vulnerability, for instance the sheer mass of mobile endpoints gives more protection to hackers leading a cyber attack starting from a mobile. Therefore, the mobile infrastructure is becoming a critical infrastructure as well [33] .

Nowadays the challenge is to understand this new scenario and to address the use of new and efficient algorithms for the information fusion in the domain of large integrated systems [34] . To integrate such heterogeneous information the necessity emerges to develop new algorithms of data fusion and information fusion to achieve an operational picture. In such scenario, where the attack can be lead with unconventional manners, information of heterogeneous sources, despite appearing uncorrelated, Interdependencies between present infrastructures. (From [31] , reprinted with permission.)

can be related and hence exploited by its fusion. Therefore particular attention is due to the information sources; Section 2.22.4 is devoted to this aspect of the problem, giving an overview of the sensors and the systems that traditionally provide information.

Before addressing in more detail the topic of data fusion applied to the domain of Homeland Protection, it is useful to briefly review the evolution of data fusion and, more recently, the definition of the new paradigms and the introduction to high-level data fusion and information fusion.

A definition of data fusion is provided in [35] : ""Data fusion is a process that combines data and knowledge from different sources with the aim of maximizing the useful information content, for improved reliability or discriminant capability, while minimizing the quantity of data ultimately retained."" Another definition is provided by the Joint Directors of Laboratories (JDL) Data Fusion Subpanel (DFS) which, in its latest revision of its data fusion model, Steinberg and Bowman [36] settle with the following short definition: ""Data fusion is the process of combining data or information to estimate or predict entity states."" Due to its generality, the definition of JDL encompasses the previous one. One aspect of the data fusion process, which is not included in the first definition and is implicit in the second, is process refinement, i.e., the improving of data fusion process and data acquisition. Many authors, recognize process refinement and data fusion to be so closely coupled that process refinement should be considered to be a part of the data fusion process. This is not a new technique in itself, rather a framework for incorporating reasoning and learning with perceived information into systems, utilizing both traditional and new areas of research. These areas include decision theory, management of uncertainty, digital signal processing, and computer science. The data fusion process comprises techniques for data reduction, data association, resource management, and fusion of uncertain, incomplete, and contradictory information.

In 1986, an effort to standardize the terminology related to data fusion began and the JDL data fusion working group was established. The result of that effort was the conception of a process model for data fusion and a data fusion lexicon. The so-called JDL fusion model [37] is a functional model, developed to overcome potential confusion in the community and to improve communications among military researchers and system developers. The model provides a common frame of reference for fusion discussions and to facilitate understanding and recognizing the problems where data fusion is applicable. The first issue of the model, dated 1988, provided four fusion levels:

• level 1: Object refinement, • level 2: Situation refinement, • level 3: Threat refinement, • level 4: Process refinement.

In 1998 Steinberg et al. [38] revised and expanded the JDL model to broaden the functional model and related taxonomy beyond the original military focus. They introduced a level 0 to the model for estimation and prediction of signal/object observable states on the basis of pixel/signal-level data association and characterization. They also suggested renaming and re-interpretation of level 2 and level 3 to focus on understanding the external world beyond military situation and threat focus. Figure 22 .5 reports a block diagram representing this functional model. Although originally developed for military applications, the model is generally applicable. Furthermore, the model does not assume its functions to be automated, they could equally well be maintained by human labor. Hence, the model is both general and flexible. The revised JDL model levels specify logical separations in the data fusion process and divide information into different levels of abstraction depending on the kind of information they produce, where the lower levels yield more specific, and the higher more general, information. The model is divided into the following five levels [18] :

• Level 0-sub-object assessment: the pre-detection activities such as pixel or signal processing, spatial or temporal registration is present. Level 0 deals with the estimation and prediction of signal/object observable states on the basis of pixel/signal level data association and characterization. • Level 1-object assessment: is concerned with estimation and prediction of target locations, behavior or identity. In this level, which is sometimes referred to as multi-sensor data fusion or multisensor integration, data is combined to assign dynamic features (e.g., velocity) as well as static (e.g., identity) to objects, hence adding semantic labels to data. This level includes techniques for data association and management of objects (including creation and deletion of hypothesized objects, and state updates of the same). Level 1 addresses the following functions: data alignment, data/object correlation, object positional/kinematic/attribute estimation, object identity estimation. • Level 2-situation assessment: investigates the relations among entities such as force structure and communication roles. This level involves aggregation of level 1 entities into high-level, more abstract entities, and relations between entities. An entity in this level might be a pattern of connected objects of level 1 entities. Input data are assessed with respect to the environment, relationship among level 1 entities, and entity patterns in space and time. Level 2 addresses the following functions: object aggregation, contextual interpretation/fusion, event/activity aggregation, multi-perspective assessment. • Level 3-impact assessment: outlines sets of possible courses of action and the effect on the current situation. The impact assessment, which is sometimes called significance estimation or threat refinement, estimates and predicts the combined effects of system control plans and the entities of level 2 (possibly including estimated or predicted plans of other environment agents) on system objectives. Level 3 addresses the following functions: estimate/aggregate force capabilities, predict enemy intent, identify threat opportunities, estimate implications, multi perspective assessment. • Level 4-process refinement: is an element of Resource Management used to close the loop by re-tasking resources to support the objectives of the mission. Process refinement evaluates the performance of the data fusion process during its operation and encompasses everything that refines it, e.g., acquisition of more relevant data, selection of more suitable fusion algorithms, optimization of resource usage with respect to, for instance, electrical power consumption. Process refinement is sometimes called process adaption to emphasize that it is dynamic and should be able to evolve with respect both its internal properties and the surrounding environment. The function of this level is in some literature handled by a so called meta-manager or meta-controller. It is also rewarding to compare level 4 fusion to the concept of covert attention in biological vision which involves, e.g., sifting through an abundance of visual information and selecting properties to extract. Level 4 addresses the following functions: evaluation (real-time control/long term improvement), fusion control, source requirements, mission management.

The 1998 revised JDL fusion model recognized the original Process Refinement level 4 function as a Resource Management function. In 2002, a level 5 was added [39, 40] , named User Refinement, into the JDL model to support a user's trust, workload, attention, and situation awareness. Mainly the level 5 was added to distinguish between machine-process refinement and user refinement of either human control action or the user's cognitive model. In many cases the data fusion process is focused on the machine point of view, however a full advantage can be taken by considering also the human factor, not only as a qualified expert to refine the fusion process, but also as a costumer for whom the fusion system is designed. Figure 22 .6, taken from [40] , shows the JDL fusion model including also the level 5.

Later in [41] also a level 6, Mission Management, was added; this level tackles the adaptive determination of spatial-temporal control of assets (e.g., airspace operations) and route planning and goal determination to support team decision making and actions (e.g., theater operations) over social, economic, and political constraints. Figure 22 .7 shows a multi-sensor data fusion architecture with a representation of the levels involved into each process of data fusion. Level 0 and level 1 concern the combination of data from different Data fusion architecture. sensors, level 2 and level 3 are often referred to as information fusion. Under the proposed partitioning scheme, the same entity can simultaneously be the subject of level 0, 1, 2, and 3 fusion processes. Entity features can be estimated from one or more entity signal observations (e.g., pixel intensities, emitter pulse streams) via a level 0 data preparation/association/estimation process. The identity, location, track and activity state of an entity (whether it be a man, a vehicle, or a military formation) can be estimated on the basis of attributes inferred from one or more observations; i.e., via a level 1 data preparation/association/estimation process. The same entity's compositional or relational state (e.g., its role within a larger structure and its relations with other elements of that structure) can be inferred via level 2 processes. Thus, a single entity-anything with internal structure, whether man, machine, or mechanized infantry brigade-can be treated either as an individual, subject to level 1 observation and state estimation-or as a ""situation,"" subject to compositional analysis via level 2 entity/entity association and aggregate state estimation. The impact of a signal, entity, or situation on the user goal or mission can then be predicted based upon an association of these to alternative courses of action for each entity via a level 3 process.

There are also other fusion models developed on the basis of different perspectives, including a purely computational and a human information processing. In the following an overview of different models [42] .

The DIKW (Data Information Knowledge and Wisdom) [43] hierarchy organizes data, information, knowledge, and wisdom in layers with an increasing level of abstraction and addition of knowledge, starting from the bottommost data layer. The hierarchy can be considered alike the JDL data fusion model because both start from raw transactional data to yield knowledge at an increasing level of abstraction.

The JDL model and many other computational models do not simulate the complex human cognitive process that leads to ""become aware,"" because they do not model the fusion process from a human perspective. In 1988, Endsley defined the situation awareness as ""the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their state in the near future"" [44] . In [45, 46] he identified three levels of situation awareness, namely perception, comprehension, and projection, parallel to the corresponding levels in the JDL model. Therefore the levels in the JDL model can be considered as processes producing results to help a human operator became aware of the situation. In [47] in addition to this three different aspects identified by Endsley, the model included also ""intention"" (i.e., the understanding of own options and courses of action relative to own goals) and ""metacognition"" (i.e., accounting for how reliable own situation awareness is likely to be). These levels summarize the fact that situation awareness requires the understanding of information, events, and the impact of own actions on own goals and objectives. This process involves several capabilities as learning, detection of anomalies, prediction of future behaviors, managing uncertainty, and analysis of heterogeneous sources.

The OODA (Observe-Orient-Decide-Act) loop, developed by Boyd in 1987 [48] , is one of the first C4I (Command, Control, Communications, Computers, and Intelligence) architectures and it represents the classic decision-support mechanism in military information operations. Because decision-support systems for situational awareness are tightly coupled with fusion systems, the OODA loop has also been used for sensor fusion [49] . Observations in OODA refer to scanning the environment and gathering information from it; orientation is the use of the information to form a mental image of the circumstances; decision is considering options and selecting a subsequent course of action; and action refers to carrying out the conceived decision. Bedworth and O'Brien [50] report a comparison of the OODA loop to the levels of the JDL model.

The human information processing can be modeled by the Rasmussen model [51, 52] . It is composed of three layers, namely skill-based, rule-based, and knowledge-based processing. The input of the process is a perception (e.g., the detection of a target by a sensor) and the output is an action. An example of result at the first level may be represented by the automatic identification of a tank by processing of row sensors data; at the next level an enemy unit composition can be indentified on the basis of its number and relative locations. Knowledge-based behavior represents the most complex cognitive processing used to handle novel, complex, situations where no routine or rule is available to manage situations. An example of this type of processing may be the interpretation of unusual behavior, and the consequent generation of a course of actions based on enemy unit size and behavior.

The Generic Error modeling System (GEMS) [53] is an extension of Rasmussen's approach, which describes the competencies needed by workers to perform their roles in complex systems. GEMS describes three major categories of errors: skill-based slips and lapses, rule-based mistakes, and knowledge-based mistakes. Table 22 .1, from [42] , shows a correspondence, and not a comparison, among levels and layers of various models presented before. This table is intended as a guide to identify the components of a data fusion architecture, where the separation between the columns is not so sharp. Notice that the JDL model does not explicitly model into a level the action consequent to the threat assessment. The action Rule/knowledgebased processing n/a n/a level, with the sense of a reaction is only in part included in the process refinement level 4, for this reason the column ""action"" has been inserted in the table, to allow a more clear correspondence with the other models that explicitly account for the reaction. The JDL model is the one that allows the most global view of the data fusion process from an operative perspective: there is not any correspondence of the other models with JDL level 4.

This section gives a broad and very general description of the basic categories of intelligence that are the source of data/information employed to perform the fusion process. The USAF (United States Air Force) in 1998 first and the ODNA (Office of Directors of National Intelligence) later in 2008 described in their studies that there are six basic intelligence categories [54, 55] : In addition, there is also Scientific and Technical (S&T) Intelligence resulting from the analysis of foreign scientific and technical information. In the following is an overview of the categories.

SIGINT is achieved by the interception/detection of electromagnetic (em) emissions. SIGINT includes Electronic Intelligence (ELINT) and Communications Intelligence (COMINT). The former derives from the processing and analysis of em radiation emitted from emitters, in most of cases radars, not employed for communications, other than nuclear detonations or radioactive sources. An emitter may be related closely to a specific threat. The information that can be achieved by a typical ESM (Electronic Support Measures) device consists of an estimate of the emitter category, location, with a certain accuracy, and various electronic attributes, such as frequency and pulse duration. This information can be employed in a high-level fusion process. COMINT derives from the processing and analysis of intercepted communications from emitters. The communications may be encrypted and they may be of several forms such as voice, e-mail, fax and the like.

IMINT is obtained by sensors working in several bandwidths which are able to produce a view of the scenario or of the specific target: electro/optical sensors, infrared, radar (e.g., Synthetic Aperture Radar (SAR) and Inverse SAR (ISAR), and Moving Target Indicator (MTI)), laser, laser radar (LADAR), and multi-spectral sensors. Each sensor has a unique capability. Some work in all weather conditions, some may work also in night conditions, and some produce high-quality images with detectable signatures.

MASINT is obtained by the collection and the analysis of several and heterogeneous sensors and instruments usually working in different regions or domains of the em spectrum, such as infrared or magnetic fields. MASINT includes Radar Intelligence (RADINT), Nuclear Intelligence (NUCINT), Laser Intelligence (LASINT), and Chemical and Biological Intelligence (CBINT). RADINT, for example, is a specialized form of ELINT, which categorizes and locates as active or passive collection of energy reflected from a target.

HUMINT is the collection of information derived by the human contact. Information of interest might include target name, size, location, time, movement, and intent. HUMINT typically includes structured text (e.g., tables, lists), annotated imagery, and free text (e.g., sentences, paragraphs). HUMINT provides comprehension of adversary actions, capability and capacity, plans and intentions, decisions, research goals and strategies.

OSINT is publicly available information appearing either in print or in electronic form including radio, television, newspapers, journals, the Internet, commercial databases, videos, graphics, and drawings. OSINT can be considered as a complement to the other intelligence categories and can be used to fill gaps and improve accuracy and confidence in classified information. A special mentioning is for the Internet, that, with its blogs, e-mails, videos, messages and mobile systems, favors an ever greater interaction between users. Moreover notice that there is a little overall planning in the development of the World Wide Web, but rather a myriad of initiatives by individuals of small groups. Government have always tried to use telephone tapping, surveillance, files, i.e., intelligence. Now this is possible on a different scale given the technical possibilities offered by satellites, mobile, phones, credit cards management systems, information storage, etc. From the topological point of view, Internet is a scale-free complex network with a power-law of the distribution of the nodes [56] ; this technical remark should be considered in the data exploitation analysis.

GEOINT is the analysis and the visual representation of the activities on the earth related to the security achieved by the sensors (radar, optical, IR, multispectral) deployed in the space. The information related to GEOINT is obtained through an integration of imagery, imagery intelligence, and geospatial information.

Stand-alone sensors usually provide a fragmentary view of a complex situation of interest. A significant enhancement of performance can therefore be accomplished by a combination of networked sensors in the close vicinity to the region of interest. Using efficient methods of centralized or decentralized multiple sensor fusion, the quality of the produced situation picture can significantly be improved. In practice, improvements with respect to the following aspects are of interest:

• production of accurate and continuous tracks (e.g., objects, persons, single vehicles, group objects), • system reaction rates (e.g., track extraction, detection of target maneuvers, track monitoring), • sustainment of reconnaissance capabilities in case of either system or network failures (e.g., graceful degradation), • system robustness against jamming and deception, • compensation of degradation effects (e.g., sensor misalignment, limited sensor resolution), • robustness against sub-optimal real-time realizations of sensor data fusion algorithms, • processing of eventually delayed sensor data (e.g., out-of sequence measurements).

In the following, several sections tackle different aspects related to homogeneous sensor networks.

Sensor fusion networks can be categorized according to the type of sensor configuration. Durrant-Whyte distinguishes three types of sensor configuration as schematized in Figure 22 .8 [57, 58] . Sensors configuration (from [57] , reprinted with permission).

Competitive sensor data fusion: Sensors are configured competitive if each sensor delivers independent measurements of the same property. Sensor data represent the same attribute, and the fusion is to reduce uncertainty and resolve conflicts. Competitive sensor configuration is also called a redundant configuration. Sensors S1 and S2 in Figure 22 .8 represent a competitive configuration, where both sensors redundantly observe the same property of an object in the environment space. Complementary sensor data fusion: A sensor configuration is called complementary if the sensors do not directly depend on each other, but can be combined to give a more complete image of the phenomenon under observation. Fusion of the sensor data provides an overall and complete model. Examples for a complementary configuration is the employment of multiple cameras each observing disjoint parts of a room, or using multiple spectrum signatures to identify a land cover type, or using different waveform to identify an aircraft type. Sensor S2 and S3 in Figure 22 .8 represent a complementary configuration, since each sensor observes a different part of the environment space.

In both competitive and complementary sensor configurations, there is an improvement of the accuracy of the target characteristics estimation consequent to the data fusion. In their seminal work H. Cramer and C.R. Rao found how to compute the best theoretical accuracy that can be achieved by an estimator. The lower bound of accuracy, i.e., the mean square error of any unbiased estimator, is given by the inverse of the so-called Fisher Information Matrix (FIM). The computation of the CRLB (Cramer-Rao Lower Bound) applies to problems involving the maximum likelihood estimation of unknown constant parameters from noisy measurements [59] . The best achievable improvement of target location and track accuracy can be quantified by the reduction of the CRLB consequent to the track fusion. In [60] this computation is reported in case of fusion of data from two sensors with an ideal unitary detection probability. In [61, 62] the same computation has been proposed in case of detection probability less than one and false alarm probability higher than zero. Cooperative sensor data fusion: A cooperative sensor network uses the information provided by two independent sensors to derive information that would not be available from the single sensors. An example for a cooperative sensor configuration is stereoscopic vision: by combining two-dimensional images from two cameras at slightly different viewpoints a three-dimensional image of the observed scene is derived. Cooperative sensor fusion is the most difficult to design, because the resulting data are sensitive to inaccuracies in all individual participating sensors. Thus, in contrast to competitive fusion, cooperative sensor fusion generally decreases accuracy and reliability. Sensor S4 and S5 in Figure 22 .8 represent a cooperative configuration. Both sensors observe the same object, but the measurements are used to form an emerging view on object C that could not have been derived from the measurements of S4 or S5 alone.

These three categories of sensor configuration are not mutually exclusive. Many applications implement aspects of more than one of the three types. An example for such a hybrid architecture is the application of multiple cameras that monitor a given area. In regions covered by two or more cameras the sensor configuration can be competitive or cooperative. For regions observed by only one camera the sensor configuration is complementary.

Sensor networks have countless applications, for example, we mention the sensor networks used in computer science and telecommunications, in biology, where they can be used to monitor the behavior of animal species such as birds or fishes, and in habitat monitoring, where they can be used to provide real-time rainfall and water level information used to evaluate the possibility of flooding. In the field of Homeland Protection one of the main task to be assigned to a sensor network is the surveillance with its most general significance. Automatic surveillance is a process of monitoring the behavior of selected objects (targets and/or anomalies) inside a specific area by means of sensors. A target generally consists of an object (e.g., a tank close to a land border or a rubber approaching to the coast) whose presence and characteristics can be detected and estimated by the sensor; an anomaly consists in a non usual behavior (e.g., a jeep moving off-road, the increasing of the radioactivity level within an area) that can be revealed by the sensor. Sensors typically provide the following functions:

• detection of a targets or anomalies inside the surveillance area, • estimation of target position or the anomaly localization and extension, • monitoring of the target kinematic (tracking) or of the anomaly behaviors, • classification and/or recognition of the targets.

To perform the previous functions, the sensors can be organized on the bases of several approaches. The classical approach to surveillance of wide areas is based on the use of a single or few sensors with long range capabilities. The signal received by the single sensor is processed by means of suitable digital signal processing subsystems. In this case the sensors are costly, with adequate computation and communication capabilities. Sensors are normally located in properly selected sites, to mitigate terrain masking problems; nevertheless, they provide different performance depending on the location of target inside the surveillance area. Typical sensors are radars (ground-based, air-borne, ship-borne or space-based), infrared or TV cameras, seismic, acoustical, radioactive sensors. Usually in this kind of networks, as represented in Figure 22 .9, the information traffic goes from the sensor nodes to a single Block-diagram for optimal system resource management in a sensor network. sink node called information fusion center that performs the target localization and tracking. According to the information received from the sensors the fusion center monitors the area where the sensors are deployed and decides, on the basis of the state estimates and their accuracy (e.g., a covariance matrix for a Kalman filter or a particle cloud for a particle filter) the actions to take.

In [63] an example of high-performance radar netted for Homeland Security application with a centralized data fusion process is described. The same classical approach is presented in [64] where this kind of sensor network is employed for natural resource management and bird air strike hazard (BASH) applications.

However if an intruder reaches and neutralizes the fusion center, the communication between the network nodes are interrupted and the whole network is exposed to the risk of becoming useless as a network even if the individual sensors may still be all working.

Nowadays, a novel approach to the automatic surveillance has been adopted; it is based on the use of many sensors with short range capabilities, low costs, and limited computation and communication capabilities. In case of a huge number of sensors, the use of information fusion centers is unpractical and their functioning is based on the information exchange between ""near-by"" sensors. The sensors can be distributed in fixed positions of the territory, but they could also be deployed adaptively to the change of the scenario. There are several approaches: they can be randomly distributed inside the surveillance area and if the number of sensors is high, the performance of the surveillance system can be considered independent of the location of the targets; then the signal received by each sensor is processed using the computational capabilities of a sub-portion of the sensor system and employed to re-organize dynamically the network. Sensors may be agile in a variety of ways, e.g., the ability to reposition, point an antenna, choose sensing mode, or waveform. Notice that the number of potential tasking of the network grows exponentially with the number of sensors. The goal of sensor management in a large network is to choose actions for individual sensors dynamically so as to maximize overall network utility. This process is called Collaborative Signal and Information Processing (CSIP) [65] . One of the central issues for CSIP to address is energy-constrained dynamic sensor collaboration: how to dynamically determine who should sense, what needs to be sensed, and who the information must be passed onto. This kind of processing system allows a limitation in the consumption of power. Applying a surveillance strategy which accounts for the target tracking accuracy and the sensor random location, only a limited number of sensors are awake and follow/anticipate the target movement; thus, the network self-organizes to detect and track the target, allowing an efficient performance from the energetic point of view with limited sensor prime power and with a reduced number of sensors working in the whole network. For example in [66] , instead of requesting data from all the sensors, the fusion center iteratively selects sensors for the target localization: first a small number of anchor sensors send their data to the fusion center to obtain a coarse location estimate, then, at each step a few non-anchor sensors are activated to send their data to the fusion center to refine the location estimate iteratively. Moreover the possibility to actively probe certain nodes allows to disambiguate multiple interpretations of an event.

In [67] the techniques of information-driven dynamic sensor collaboration is introduced. In this case an information utility measurement is defined as the statistical entropy and it is exploited to evaluate the benefits in employing part of the network that consequently is re-organized. Other cost/utility functions can be employed as criteria to dynamically re-organize the sensor network as described in [68, 69] .

Several analytical efforts have been done to evaluate the performance of such networks in terms of tracking accuracy. As usual the CRLB has been taken as reference of the best achievable accuracy; in particular a new concept of conditional PCRLB (Posterior Cramer Rao Lower Bound) is proposed and derived in [70] . This quantity is dependent on the actual observation data up to the current time, and is implicitly dependent on the underlying system state. Therefore, it is adaptive to the particular realization of the underlying system state and provides a more accurate and effective online indication of the estimation performance than the unconditional PCRLB. In [71, 72] the PCRLB is proposed as a criterion to dynamically select a subset of sensors over time within the network to optimize the tracking performance in terms of mean square error. In [73] the same criterion is proposed as a framework for the systematic management of multiple sensors in presence of clutter.

Self-organization can be defined as the spontaneous set-up of a globally coherent pattern out of local interactions among initially independent components. Sensors are randomly spread out over a two dimensional surveillance area. In a self-organized system, its elements affect only close elements; distant parts of the system are basically unaffected. The control is distributed, i.e., all the elements contribute to the fulfillment of the task. The system is relatively insensitive to perturbations or errors, and have a strong capacity to restore itself. Initially independent components form a coherent whole able to efficiently fulfill a particular function [74] . Flocks of birds, shoals of fish, swarms of bees are examples of self-organizing systems; they move together in an elegantly synchronized manner without a leader which coordinates them and decides their movement. It has been shown that flocks of birds selforganize into V-formations when they need to travel long distances to save energy, by taking advantage of the upwash generated by the neighboring birds. Cattivelli and Sayed [75] propose a model for the upwash generated by a flying bird, and shows that a flock of birds is able to self-organize into a Vformation as if every bird processes spatial and network information by means of an adaptive diffusive process. This result has interesting implications. First, a simple diffusion algorithm is able to account for self-organization of birds. Second, according to the model, that birds can self-organize on the basis of the upwash generated by the other birds. Third, some information is necessarily shared among birds to reach the optimal flight formation. The paper also proposes a modification to the algorithm that allows birds to organize, starting from a V-formation, into a U-formation, leading to an equalization effect, where every bird in the flock observes approximately the same upwash. The same algorithm based on birds flight is extended in [76] to the problem of distributed detection, where a set of sensors/nodes is required to decide between two hypotheses on the basis of the collected measurements. Each node makes individual real-time decisions and communicates only with its immediate neighbors, in order that any fusion center is not necessary. The proposed distributed detection algorithms are based on diffusion strategies described in [77] [78] [79] and their performance is evaluated by means of classical probabilities of detection and false alarms.

These diffusion detection schemes are attractive in the context of wireless and sensor networks thanks to their intrinsic adaptability, scalability, improved robustness to node and link failure as compared to centralized schemes, and their potential to save energy and communication resources.

Several studies have shown how a simple self-synchronization mechanism, borrowed from biological systems, can form the basic tool for achieving globally optimal distribution decisions in a wireless sensor network with no need for a fusion center. Self-synchronization is a phenomenon first observed between pendulum clocks (hooked to the same wooden beam) by Christian Huygens in 1658. Since then, self-synchronization has been observed in a myriad of natural phenomena, from flashing fireflies in South East Asia to singing crickets, from cardiac peacemaker or neuron cells to menstrual cycles of women living in strict contact with each other [80] . The goal of these studies is to find a strategy of interaction among the sensors/nodes that could allow them to reach globally optimal decisions in terms of a ""consensus"" value in a totally decentralized manner. Distributed consensus algorithms are indeed techniques largely studied in distributed computing [81, 82] . The approaches suggested in [83, 84] give a form of consensus achieved through self-synchronization that may result critical in wide-area networks, where propagation delays might induce an ambiguity problem. This problem is overcome in [85] [86] [87] where also a model of the network and of the sensors is proposed. Each of the N nodes composing the network is equipped with four basic components: (1) a transducer that senses the physical parameter of interest y i (e.g., temperature, concentration of contaminants, radiation, etc.); (2) a local processing unit that provides a function g i (y i ) of the measurements; (3) a dynamical system, initialized with the local measurements, whose state x i (t) evolves as a function of its own measurement g i (y i ) and of the state of nearby sensors; (4) a radio interface that makes possible the interactions among the sensors. The criterion to reach a consensus value is the asymptotical convergence toward a common value of all the derivatives of the state, for any set of initial conditions and for any set of bounded. This condition makes the convergence to the final consensus independent of the network graph topology. However the topology has an impact on several aspects: the overall energy necessary to achieve the consensus and the convergence time. In general there exists a trade-off between the local power transmitted by a each sensor and the converge time depending on the algebraic connectivity of the network graph, as shown in [88] . In the practical applications these aspects cannot be neglected; for instance, the design of a network should account for the precision to achieve, and the time to get the consensus value at the given precision, versus such constraints as the energy limitations of the sensors. A global overview of the problem is given in [89] .

Moving from the functional model to a working implementation in a real environment involves a number of design considerations: including what information sources to use and what fusion architecture to employ, communication protocols, etc.

Admittedly, the fusion of data is decoupled from the actual number of information sources and, hence, does not require necessarily multiple sensors: the fusion, in fact, may be performed also on a temporal sequence of data that was generated by a single information source (e.g., a fusion algorithm may be applied to a sequence of images produced by a single camera sensor). However, employing a number of sensors provides many advantages as well explained in the previous Sections. Unsurprisingly, there are also difficulties associated with the use of multiple sensors.

A missed sensor registration may cause a failure in the correct association between signals or features of different measurements. This problem and the similar data association problem are very important and apply also to single sensor data processing. To perform data registration, the relative locations of the sensors, the relationship between their coordinate systems, and any timing errors need to be known, or estimated, and accounted for otherwise a mismatch between the compiled picture and the truth may result. An overstated confidence in the accuracy of the fused output, and inconsistencies between track databases, such as multiple tracks that correspond to a single target may appear. A missed registration can result from location and orientation errors of the sensor relative to the supporting platform, or of the platform relative to the Earth, such as a bearing measurement with an incorrect North alignment. Errors may be present in data time stamping, and numerical errors may occur in transforming data from one coordinate system to another. Automatic sensor registration can correct for these problems by estimating the bias in the measurements along with the kinematics of the target. However, the errors in sensor registration need to be known and accounted for [90] . In [91] a maximum likelihood (EML) algorithm for registration is presented using a recursive two-step optimization that involves a modified Gauss-Newton procedure to ensure fast convergence. In [92] a novel joint sensor association, registration, and fusion is performed exploiting the expectation-maximization algorithm incorporated with the linear Kalman filter (KF) to give simultaneous state and parameter estimates. The same approach can be followed also with non linear filtering techniques as the Extended KF (EKF) and the Unscented KF (UKF) as proposed in [93] , where also the performance is evaluated by means of the PCRLB.

Next to the spatial sensor registration also the temporal alignment cannot be neglected. For instance, a critical aspect of a sensor network is its vulnerability to temporary node sleeping, due to duty-cycling for battery recharge, permanent failures, or even intentional attacks.

Other realistic problems, such as conflicting information and noise model assumptions, may enable the use of some fusion techniques. Noisy input data sometimes yield conflicting observations, a problem that has to be addressed and which does not arise in single sensor data processing. The administration of multiple sensors have to be coordinated and information must be shared between them.

Most of the optimization algorithms have been developed in a centralized framework, i.e., they have been conceived to perform centralized data fusion process. In the last years the trend is to employ network centric approaches, and the mathematical optimization algorithms must be able to support this approach. In the following an example of the adaptation of a ""centralized-conceived"" algorithm to the new trend is presented.

Consider the following minimization problem to solve:

where α, β, γ , δ, ε, η are real positive values and the function f x, y, z = w ≥ 0 represents an ellipsoid function, whose axes do not coincide with the reference frame axes if δ = 0, ε = 0, η = 0. The problem of Eq. (22.1) can be solved by the steepest descent method in a centralized fusion process frame, hence it will be named ""centralized steepest descent."" The centralized steepest descent method when used to solve minimization problems is an iterative procedure that, beginning from an initial guess, updates at every iteration the current approximation of the solution of the function to minimize with a step in the direction of the gradient of the own function. In a network centric approach it may Data Fusion centralnode be solved by the application of the Jacobi method 2 usually employed for the iterative solution of linear system equation.

Consider three agents (namely agent 1, 2, and 3) controlling the three variables x, y, and z. In the centralized data fusion process, represented in Figure 22 .10a, the communication between the three agents is completely performed at the same instant of time; in the network centric case this does not happen. Consider the model of Figure 22 .10b with the following communication scheme:

agent 1 communicates to agent 2, agent 2 communicates agent 3, agent 3 communicates to agent 1; moreover the communications among agents is not instantaneous, but they succeeds in time.

The method of the centralized steepest descent applied to the function f (x, y, z), given a starting point (x 0 , y 0 , z 0 ), is based on the following iterations:

where k = 0, 1, . . ., and h ≥ 0 represents the step employed in the steepest descent method. 2 The Jacobi method is an algorithm for determining the solutions of a system of linear equations with largest absolute values in each row and column dominated by the diagonal element. Each diagonal element is solved for, and an approximate value plugged in. The process is then iterated until it converges. This algorithm is a stripped-down version of the Jacobi transformation method of matrix diagonalization. The method is named after German mathematician Carl Gustav Jakob Jacobi [96] .

A network centric steepest descent method can be derived by the communication scheme represented in Figure 22 .10b and described below. Given the starting point (x 0 , y 0 , z 0 ), the following iterations can be done:

where k = 0, 3, 6, . . . and h ≥ 0 represents the step employed in the steepest descent method. Figure 22 .11 shows the comparison of the two methods for the previous model. Note that the three agents in the net-centric approach are those looking at the function to be minimized along the x, y, and z axes respectively. The black square and the red diamond in the curves represent respectively the starting point of the iteration and the final position. The black solid line shows the trajectory described by the variables (x, y, z) obtained by the application of the centralized steepest descent method; the red solid line shows the behavior of the variables obtained by the net-centric steepest descent method. Note that the red line approaches the minimum by moving along the x, y, and z axes separately. The ellipsoids of Figure 22 .11 represent the iso-level surfaces of the objective function. Notice that the telecommunication network modeled for the net-centric steepest descent determines the usual Jacobi iteration employed for the solution of linear systems associated to minimization problems [94] [95] [96] . In the following Section 2.22.6.1 this approach is applied to reach the optimal deployment of a sensor network.

This section proposes several study cases of sensor networks employing novel approaches. Section 2.22.6.1 proposes an optimization method, projected in the network centric frame, to obtain the optimal deployment of a cooperative sensor network; Section 2.22.6.2 describes how to employ the so-called bio-inspired models of dynamic sensor collaboration in a chemical sensor network to detect a chemical pollutant; finally Section 2.22.6.3 gives a description of the typical problem of detection of radioactive sources.

This section presents a mathematical model for the deployment of a sensor network, for the creation of consensus values from the noisy data measured and a statistical methodology to detect local anomalies Starting point Convergence point Centralized steepest descent Net-centric steepest descent

Comparison between the trajectories computed by the centralized and the network centric steepest descent.

in these data. A local anomaly in the data is associated to the presence of an intruder. The model of sensor network presented here is characterized by the absence of a fusion center. In other words the deployment, the construction of the consensus values, and the detection of local anomalies in the data are the result of local interactions between sensors. Nevertheless the local interactions will lead to global solution of the considered problem. This is an example of model of a network centric sensor network. The sensors are assumed to be identical and they measure a quantity pertinent to the properties of the area to survey able to reveal the presence of an intruder. In the proposed study case the sensors are able to measure the temperature of the territory in the position or in the ""area"" where they are located; in absence of anomalies there is a uniform temperature on the territory where the sensors are deployed. The sensor measures are noisy and can be considered synchronous. This measurement process is repeated periodically in time with a given frequency. From these measures a ""consensus"" temperature is deduced, pertinently to the territory where the sensors are deployed and an estimate of the magnitude of the noise contained in the data. Finally using these consensus values as reference values local anomalies are Territory of the city of Urbino (Italy) selected for the study case.

detected by the individual sensors. In the following we give some analytical details of the consensus method [97] . Let be a bounded connected polygonal domain in two dimensional real Euclidean space R 2 . The domain represents the territory where the sensor network must be deployed; in our case the downtown part of the Italian city of Urbino, shown in Figure 22 .12. Let · denote the Euclidean norm of in R 2 . Consider N sensors s 1 , s 2 , . . . , s N , located respectively, in the points ξ 1 , ξ 2 , . . . , ξ N ∈ , assumed to be distinct. To the sensor network deployed in the points ξ 1 , ξ 2 , . . . , ξ N corresponds a graph whose nodes are the sensors location and whose edges join the sensors able to communicate between themselves. This graph is assumed to be connected and can be imagined as laid on the territory. The assumption that the graph is connected is equivalent to assuming that the sensors constitute a network. For i = 1, 2, . . . , N , a polygonal region i ⊂ is associated to each sensor s i ; this region is defined by the condition that the points belonging to i are closest to the sensor s i , that is they are closest to ξ i , than to any other of the remaining sensors s j located in ξ j , j = i, j = 1, 2, . . . , N . It follows:

When for a given x ∈ the minimizer of the function f ( j) = x − ξ j , j = 1, 2, . . . , N is not unique we attribute x to i , where i is the smallest index between the indices that are minimizers of the function f.

The collection of subsets { 1 , 2 , . . . , N } defined in Eq. (22.4) and further specified by the condition above is a partition of and it is a Voronoi partition of associated to the Voronoi centers ξ 1 , ξ 2 , . . . , ξ N , as represented in Figure 22 .13 [98] , where the sets 1 , 2 , . . . , N are the Voronoi cells. The sensor s i is located in ξ i , with ξ i ∈ i , i = 1, 2, . . . , N , and monitors the sub-region i of . Note that there is a Voronoi partition of associated to each choice of the Voronoi centers After the definition of a Voronoi partition of , we want to determine the optimal one with respect to a pre-specified criterion, that in this study case is the fact that the Voronoi centers ξ 1 , ξ 2 , . . . , ξ N should coincide (as much as possible) with the centers of mass of the corresponding Voronoi cells 1 , 2 , . . . , N . This property translates in mathematical terms the request that the sensors are well distributed on the territory. That is what is called optimal Voronoi partition, i.e., the Voronoi partition associated to the Voronoi centers whose coordinates ξ * 1 , ξ * 2 , . . . , ξ * N are the solution of the following problem: (22.5) subject to the constraints:

where B j is the center of mass of the Voronoi cell j , j = 1, 2, . . . , N . Moreover we require:

That is the Voronoi centers and the centers of mass of the Voronoi cells coincide. Note that in general B j depends on ξ 1 , ξ 2 , . . . , ξ N and that the function .7) is not unique and it can be solved by the application of the steepest descent concept, revised in a network centric frame as shown conceptually in Section 2.22.5.7 [94] . This method can be used to solve the problem of Eq. (22.5) with an iterative procedure, that beginning from an initial guess, updates at every iteration the current approximation of the solution with a step in the direction of the gradient of the .7) and to the requirement that its implementation must lead to a network centric solution of the deployment problem. For sake of brevity, how to impose Eq. (22.6) will not be discussed here, however a treatment of constraints in the continuous analog of the steepest descent algorithm can be found in [99] . Note also that the solutions of Eqs. (22.5) and (22.6) that are of interest are usually interior points of the constraints (6) . That is the constraint issue usually is not relevant in the solution of Eqs. (22.5) and (22.6) . Similarly we will not pay attention to condition of Eq. (22.7). In fact with respect to Eq. (22.7), we will simply verify if the solution of the optimization problem determined by the steepest descent method satisfies Eq. (22.7). Let us concentrate our attention on the issue of building a network centric implementation of the continuous analog of the steepest descent method to solve Eq. (22.5) . Assume that the sensor s i knows only the position of its neighbor sensors, that is of the sensors that belong to a disk with center ξ i and radius r > 0, i = 1, 2, . . . , N . Later we will show how to choose r. The solution of the optimization problem of Eq. (22.5) is found approximating the solution of the system of differential equations:

where λ 1 denotes a real parameter, with the solution of the ""network centric"" system differential equations:ξ

where

andB i, j being the center of mass of the Voronoi cell˜ i, j obtained computing the Voronoi partition of associated to the Voronoi centers ξ j , j ∈ L i , i = 1, 2, . . . , N . Assume that r > 0 is large enough to guarantee that ξ j is neighbor of ξ i when the distance between j and i is zero, i, j = 1, 2, . . . , N .

Note that with this assumption we haveB i,i = B i , i = 1, 2, . . . , N . In Eqs. (22.8) and (22.9 ) the dot denotes the differentiation with respect to λ 1 . We observe that Eq. Remind that we have assumed that the graph G associated to the optimal deployment is connected (see Figures 22.14 and 22.13b) . Moreover we remind that, since there is not a fusion center, each node of the graph G does not know the positions of all the remaining nodes of the graph, in fact it knows only the positions of its neighbor nodes. Let L be the Laplacian matrix associated to G [100] . The matrix L is a symmetric positive semi-definite N × N matrix. Let x(λ 2 ) = (x 1 (λ 2 ), x 2 (λ 2 ), . . . , x N (λ 2 )) T , λ 2 > 0, be a real N dimensional vector depending on the real parameter λ 2 . The superscript (·) T means transposed. We consider the system of ordinary differential equations:

x(λ 2 ) = −Lx(λ 2 ), λ 2 > 0 (22.12) equipped with the initial conditions:

where Lx denotes the usual matrix vector multiplication, α = (α 1 , α 2 , . . . , α N ) T is a known initial condition and the dot denotes differentiation with respect to λ 2 . Since G is connected we have:

where x(λ 2 ), λ 2 > 0, is the solution of Eqs. (22.12) and (22.13) . This result follows easily from the spectral properties of L [100] . Note also that the right hand side of Eq. (22.14) is the ""average"" of the initial condition α. Note that Eq. (22.12) can be interpreted as the ""heat equation"" on the graph G, that the problem of Eqs. (22.12) and (22.13) can be seen as an initial value problem for the heat equation on G and that Eq. (22.14) can be understood as the approach to an asymptotic equilibrium ""temperature"" in an ""heat transfer"" problem. We assume that during the monitoring phase the sensor measures a physical quantity, such as, for example, the temperature, of the region i where it is located. The sensors are identical, the measures made by the sensors are synchronous, repeated periodically in time and of course they are noisy. Moreover they are assumed to be independent. A first set of measures is taken by the sensors at time t = t 0 and is collected in the vector β 0 = (β 0,1 , β 0,2 , . . . , β 0,N ,) T , where β 0,i is the measure done by the sensor s i . The set of measure β 0 will be used to obtain the ""consensus"" value β 0 of the quantity monitored in at time t = t 0 . We choose:

Remind that the sensor s i located in ξ i knows β 0,i and communicates with the sensors s j located in ξ j , j ∈ L i , i = 1, 2, . . . , N . In order to provide to the sensor s i , the consensus value β 0 in a network centric manner we proceed as follow: we choose α = β 0 in Eq. (22.13) and we integrate numerically the initial value problem of Eqs. (22.12) and (22.13) using the explicit Euler method to obtain a numerical approximation of lim λ 2 →+∞ x(λ 2 ). Note that the ith differential equation of Eq. (22.12) is integrated in the location ξ i , and that using the explicit Euler method this can be done using only information available in the location ξ i . Note that the analytic solution of Eqs. (22.12) and (22.13) is not ""network centric"" but its approximation with the explicit Euler method is ""network centric."" In the former case to achieve the solution each node should know the whole graph, i.e., all the nodes. The ith node is not able to achieve the solution exploiting only the information in its posses: in this sense the solution is not ""network centric."" Otherwise, exploiting the Euler approximation of the exponential of a matrix, the whole knowledge of the graph is not necessary: in this sense a ""network centric"" solution is achieved.

Once obtained β 0 we consider the following vector:

Then we choose α = γ 0 in Eq. (22.13) and we integrate Eqs. (22.12) and (22.13) with the explicit Euler method as done above. In this way we obtain asymptotically a numerical approximation of γ 0 where:

This approximation of γ 0 is provided to each sensor in a network centric manner. Note that γ 0 is an estimate of the magnitude of the noise contained in the data; in fact γ 0 is the ""sample"" variance of the measures β 0,i made by the sensor at time t = t 0 . The approximation of β 0 and γ 0 obtained integrating numerically Eqs. (22.12) and (22.13) are the consensus values. These values are ""global"" values (that is they depend on all the measures made by the sensor network at time t = t 0 ) and have been provided to each sensor in a network centric manner (that is using only ""local"" interactions between sensors).

The sensor s i repeats periodically in time the measure of the quantity of interest and after a given time interval has as its disposal a set of measures that can be compared with the consensus values β 0 and γ 0 to detect (local) anomalies. Let us assume that the set of measures made by the sensor s i is a sample taken from a set of independent identically distributed Gaussian random variables of mean μ i and variance σ 2 i . In these hypotheses the Student t-test and the Chi-square test [101] are the elementary statistical tools that must be used to compare μ i and σ 2 i (that are unknown) to β 0 and γ 0 . The result of this comparison is the detection of local anomalies. A (statistical) significance is associated to the detected anomalies. The statistical tests used are based on the assumption that the measures come from a set of independent identically distributed Gaussian random variables. Note that the estimators β 0 and γ 0 can be used in more general circumstances.

Typically the challenge in the deployment of an operational wireless sensor network (WSN) resides in establishing the balance between its operational requirements (e.g., minimal detection threshold, the size of surveillance region, detection time, the rate of false negatives, etc.) and the available resources (e.g., energy supply, number of sensors, communication range, fixed detection threshold of individual sensors, limited budget for the cost of hardware, maintenance, etc.) [102] . The issue of resource constraints is particularly important for a network of chemical sensors, because modern chemical sensors are equipped with air-sampling units (fans), which turn on when the sensor is active. Operating a fan requires a significant amount of energy as well as a frequent replacement of some consumable items (i.e., cartridges, filters). This leads to the critical requirement in the design of a WSN to reduce the active (air-sampling) time of its individual sensors.

One attractive way to achieve the described balance between the requirements and the constraints of WSN is to exploit the idea of dynamic sensor collaboration (DSC) [103, 104] . The DSC implies that a sensor in the network should be invoked (or activated) only when the network will gain information by its activation [104] . For each individual sensor this information gain can be evaluated against other performance criteria of the sensor system, such as the detection delay or the detection threshold, to find an optimal solution in given circumstances. However, the DSC-based algorithms involve continuous estimation of the state of each sensor in the network and usually require extensive computer simulations [103, 104] . These simulations may become unpractical as the number of sensors in the network increases. Furthermore, the simulations can provide the numerical values for optimal network parameters only for a specific scenario.

This motivates the development of another simple and analytic approach to the problem of network analysis and design. The main idea is to phenomenologically employ the so-called bio-inspired (epidemiology, population dynamics) or physics inspired (percolation and graph theory) models of DSC in the sensor network in order to describe the dynamics of collaboration as a single entity [105] [106] [107] [108] [109] [110] . From a formal point of view, the equations of bio inspired models of DSC are the ones of the ""meanfield"" theory, meaning that instead of working with dynamic equations for each individual sensor we use only a small number of equations for the ""averaged"" sensor state (i.e., passive, active, faulty, etc.), regardless of the actual number of sensors in the system.

The analytic approach can lead to the valuable insights into the performance of the proposed sensor network system by providing simple analytical expressions to calculate the vital network parameters, such as the detection threshold, robustness, responsiveness and stability and their functional relationships.

The fluctuations in concentration C of the pollutant are modeled by the probability density function (pdf) with the mean C 0 as a parameter [111] :

Here the value γ = 26/3 can be chosen to make it compliant with the theory of tracer dispersion in Kolmogorov turbulence [111] , but it may vary with meteorological conditions. The parameter ω, which models the tracer intermittency in the turbulent flow, can be in the range We adopt a binary model of a chemical sensor, with reading V specified as:

where C * is the threshold (an internal characteristic of the sensor). It can be shown [112] that the probability of detection of an individual sensor embedded in the environmental model described by Eq. (22.18) is given by:

where

is the cumulative distribution function corresponding to pdf of Eq. (22.18), see [113] .

Examples of WSN network operating in the tracer filed with different correlation structure [117] . (reprinted with permission.)

Suppose that N chemical sensors are uniformly distributed over the surveillance domain of area S and adopt the following network protocol for dynamic collaboration. Each sensor can be only in one of the two states: active and passive. The sensor can be activated only by a message it receives from another sensor. Once activated, the sensor remains in the active state during an interval of time τ * ; then it ""dies out"" (becomes passive). While being in the active state, the sensor senses the environment and if the chemical tracer is detected, it broadcasts a (single) message. The broadcast capability of the sensor is characterized by its communication range r * . This network with the described dynamic collaboration can be modeled using the epidemic SIS model (susceptible-infected-susceptible) [114] : (22.22) where N + , N − denote the number of active and passive sensors, respectively. The nonlinear terms on the right hand side of Eq. (22.22) are responsible for the interaction between the sensors; parameter α is a measure of this interaction. The number of sensor is assumed constant, hence we have an additional equation: N + + N − = N . Since the parameter alpha describes the intensity of social interaction in a community [114] we can propose that:

where m is the number of contacts made by the activated (""infected"") sensor during its infectious period τ * (i.e., the number of sensors that received the wake-up message from an alerting sensor). In our case m = π · r 2 * · N /S. Then we have:

where G is a calibration constant. In order to simplify notation we will further assume that G is absorbed in the definition of r * . Equation (22.22) combined with N + + N − = N can be reduced to one equation for y = N + :

where b = α N − 1/τ * . By simple change of variables z = α y/b, this equation can be reduced to the standard logistic equation [115, 116] :

The solution of the logistic equation is well-known:

where z 0 = z(0). Observe that the WSN will be able to detect the presence of a pollutant only if b > 0, because then z → 1 as t → ∞ independent of z 0 . In this case, after a certain transition interval, the WSN will reach a new steady state with:

From (22.27) and using the expression for b stated above, the activation time (transition interval) is given by:

From Eq. (22.29) it follows that the key requirement for the network to be operational b > 0 is that ατ * N > 1, that is:

where R 0 is a well-known parameter in epidemiology, referred to as the basic reproductive number [114] . Observe that R 0 is independent of τ * ; however, according to Eq. (22.29) the response time of the WSN is strongly dependent on τ * . It remains to specify q, the number of sensors that should initially be active for the described WSN with dynamic collaboration to be effective. The initial condition is simply q · p > 1, that is on average q > 1/ p. Eqs. (22.28)-(22.30) are important analytic results. For a given level of mean pollutant concentration C 0 and meteorological conditions (γ, ω), these expressions provide a simple yet rigorous way to estimate how a change in network and sensor parameters (i.e., N , C * , τ * ) will affect the network performance (i. e., N + , τ ) .

The examples of agent-based simulation of ""information epidemic"" in WSN, which satisfies the threshold condition of Eq. (22.30) is presented in Figure 22 .16. We can observe that by change of the configuration parameters of WSN we can vary the activation time and the saturation limit of the detection system. Further development of the theoretical framework presented in this section can be found in [117] [118] [119] [120] . 

Recently there has been an increased interest in detection and localization of radioactive material [121] [122] [123] [124] [125] . Radioactive waste material is relatively easy to obtain with numerous accidents involving its loss or theft reported. The danger is that a terrorist group may acquire some radiological material and use it to build a dirty-bomb. The dirty bomb would consist of waste by products from nuclear reactors wrapped in conventional explosives, which upon detonation would expel deadly radioactive particles into the environment. The ability to rapidly detect and localize radioactive sources is important in order to disable and isolate the potential threat in emergency situations. This section is concerned with radiological materials that emit gamma rays. The probability that a gamma radiation detector registers z ∈ N counts (N being the set of natural numbers including zero) in τ seconds, from a source that emits on average μ counts per second is [126] :

where λ = μτ is the mean and variance of the Poisson distribution. The measurements of radiation field are assumed to be made using a network of low-cost Geiger-Müller (GM) counters as sensors.

In general, the problem of detection and localization of point sources or radioactive sources can be solved using either controllable or uncontrollable sensors. Controllable sensors can move and vary the radiation exposure time [127, 128] . In this Section we will focus on uncontrollable sensors, placed at known locations with constant and known exposure times.

Assume that r ≥ 0 sources (r is unknown) are present in the area of interest. Furthermore, the assumption is that the area is flat without obstacles (""open field""). Each source i = 1, 2, . . . , r is parameterized by its 2D location (x i , y i ) and its equivalent strength α i (a single parameter which takes into account the activity of the source, the value of gamma energy per integration and scaling factors involved, see [129] ). Thus the parameter vector of source i is ϑ i = x i y i α i T , while the total parameter vector is a stacked vector: ϑ = ϑ T 1 · · · ϑ T r T . Suppose a network of GM counters is deployed in the field of interest. Let GM counter j = 1, . . . , m, located at ξ j ζ j , reports its count z j every τ seconds. Assuming that each GM counter has a uniform directional response and that attenuation of gamma radiation due to air can be neglected, the joint density of the measurement vector z = z 1 · · · z m T , conditional on the parameter vector ϑ and the knowledge that r sources are present, can be modeled as [129] :

Here λ j (ϑ) is the mean radiation count at sensor j:

being the distance between the source i and sensor j, and λ b the average count due to the background radiation (assumed known). The problem for the network of GM counters is to estimate the number of sources r and the parameter vector for each source ϑ i , i = 1, . . . , r . In this section we will present the experimental results obtained using real data and a Bayesian estimation algorithm combined with the minimum description length (MDL) for source number estimation. A radiological field trial was conducted on a large, flat, and open area without any obstacles at the Puckapunyal airfield site in Victoria, Australia. The measurements were collected using the DSTOs 3 Low Cost Advanced Airborne Radiological Survey (LCAARS) survey system which consists of an AN/PDR-77 radiation survey meter equipped with an RS232 interface module, a gamma probe and software written in Visual Basic running on a laptop computer. The gamma probe contains two GM tubes to cover both low and high ranges of dose rates. It was capable of measuring gamma radiation dose rates from background to 9.99 Sv/h 4 without saturating [130] with a fairly flat response [131] . Three radiation sources were used in the field trial: source 1 was a cesium sources ( 137 Cs) with ϑ 1 = 11 m 10 m 9105 µSv/h T , source 2 was also a cesium source with ϑ 2 = 3 m 50 m 1868 µSv/h T , and source 3 was a cobalt source ( 60 Co) with ϑ 3 = 41m 5m 467 µSv/h T . The aerial image of the experimental site with the location of sources and the local Cartesian coordinate system is shown in Figure 22 .17. Four data sets were collected during the field trails in the presence of r sources, with respectively r = 0, 1, 2, 3 [132] . Data sets with r > 0 sources contains 50 count measurements in each measurement point. Estimation of parameter vector ϑ, under the assumption that r is known, was carried out using the Bayesian importance sampling technique known as the progressive correction [125, 133] . This technique assumes that prior distribution of ϑ, denoted p 0 (ϑ), is available. The information contained in the measurement vector z is combined with the prior to give the posterior pdf: p(ϑ|z) ∝ l(z|ϑ) · p 0 (ϑ). The minimum mean squared error estimate of ϑ is then the posterior expectation:

The problem is that the posterior pdf and hence the posterior expectation of Eq. (22.35) cannot be found analytically for the described problem. Instead, an approximation of Eq. (22.35) is computed via the importance sampling: it involves drawing N p samples of the parameter vector from an importance density and approximating the integral by a weighted sum of the samples. This is carried out in a few stages, each stage drawing samples from a ""target distribution"" which is gradually approaching the true posterior. The ""target distribution"" at stage s = 1, . . . , S is constructed as: p s (ϑ|z) ∝ l(z|ϑ) G s · p 0 (ϑ), (22.36) where G s = s l=1 γ l with γ ∈ [0, 1) and G S = S l=1 γ l = 1. An adaptive scheme for the computation of S and factors γ 1 , γ 2 , . . . , γ S is given in [125, 133] . Assume that a random sample ϑ n s−1 N p n=1 from p s−1 (ϑ|z) is available and one wants to generate the samples or particles from p s (ϑ|z). The progressive correction algorithm steps are then as follows [125] :

compute not-normalized weight of each sample as: w n s = l(z|ϑ) γ s , for n = 1, . . . , N p ; 3. normalize weights; 4. perform re-sampling of particles [134] ; 5. carry out Markov chain Monte Carlo (MCMC) move step for each particle [134] .

The procedure is repeated for every stage s < S until G s < 1. The initial set of particles is drawn from the prior density p 0 (ϑ). The final estimate in Eq. (22.35) is approximated aŝ

The number of sources was estimated using the MDL algorithm [59] , which will choose r ∈ {1, 2, . . . , r max } that will maximize the following quantity: β r = log l(z|θ(r )) − 1 2 log J(θ(r )) , (22.38) whereθ(r ) is the estimate obtained under the assumption that r sources are present and

is the Fisher Information Matrix. It can be shown that The inverse of the FIM gives us the CRLB, which represents the theoretical lower bound for estimation error covariance [135] . Figure 22 .18 shows the output of the progressive correction algorithm for data set 3 (with three sources present) after (a) s = 2 and (b) s = 11 stages of processing. The red stars indicate the locations of three sources. The green line shows the initial polygon A for the location of sources. The prior density for sampling the initial set of particles for source i = 1, . . . , r is: (22.42) where U A (x i , y i ) stands for uniform distribution over the polygon A and κ.ν (α i ) is the gamma distribution with parameters κ = 1.5 and ν = 8000. From Figure 22 .18 we observe how the progressive correction algorithm localiszs the three sources fairly accurately. As we mentioned earlier, 50 count measurements have been collected by each sensor. This allows us to find the root mean square (rms) estimation error using each snapshot of measurement data from all sensors. Table 22 .2 shows the resulting rms errors versus the theoretical CRLB.

The theoretical CRLB was computed using the idealized measurement model as stated by Eqs. (22.32)- (22.34) . Considering that this measurement model was very crude with a number of factors neglected (e.g., uniform directional response, neglected air attenuation, perfect knowledge of sensor locations, known and constant average background radiation, etc.), the agreement between the theoretical bound and the RMS estimation errors in Table 22 .2 is remarkable. The experimental results in this table effectively verify the measurement model as well as the estimation algorithm. Results for estimation of r are shown in Table 22 .3. The table lists the number of runs (out of 50) that resulted in r ∈ {0, 1, 2, 3}. It can be observed that the number of sources is estimated correctly in the majority of cases.

More results of experimental data processing can be found in [131, 132] . In a recent study [136] it was found that by using all 50 snapshots of measurement data for estimation by progressive correction, results in a posterior pdf which is very narrow but does not include the true source positions. This indicates that the measurement model is not perfect, which is not surprising considering that it is based on many approximations. In situations where the measurement likelihood is not exact, it is necessary to introduce a degree of caution to make the estimation more robust. In the framework of progressive correction this can be achieved by G S = S l=1 γ l < 1. In this way the measurement likelihood is effectively approximated by a fuzzy membership function which has a theoretical justification in random set theory [137, Chapter 7] .

If one wants to relax the assumption that radioactive sources are point sources, the problem becomes the one of radiation field estimation. This is an inverse problem, difficult to solve in general. By modeling the radiation field by a Gaussian mixture, however, the problem becomes tractable and some recent results are reported in [138] .

Multi-sensor management concerns with the control of environment perception activities by managing or coordinating the usage of multiple heterogeneous sensor resources. Multi-sensor systems are becoming increasingly important in a variety of military and civilian applications. Since a single sensor generally can only perceive limited partial information about the environment, multiple similar and/or dissimilar sensors are required to provide sufficient local pictures with different focus and from different viewpoints in an integrated manner. As viewed, information from heterogeneous sensors can be combined using data fusion algorithms to obtain synergistic observation effects. Thus the benefit of multi-sensors system are to broaden perception and enhance awareness of the state of the world compared to what could be acquired by a single sensor system. The increased sophistication of sensor assets along with the large amounts of data to be processed has pushed the information acquisition problem far beyond what can be handled by human operator. This motivates the emerging interest in research into automatic and semi-automatic management of sensor resources for improving overall perception performance beyond basic fusion of data.

Multi-sensor management is formally described as a system or process that seeks to manage or coordinate the usage of a suite of sensors or measurement devices in a dynamic, uncertain environment, to improve the performance of data fusion and ultimately that of perception.

The basic objective of sensor management is to select the right sensors to do the right service on the right object at the right time. Sensor management, aiming at improving data fusion performance by controlling sensor behavior, plays the role of level 4 functions in JDL model presented in Section 2.22.3. Mainly the same considerations made for homogeneous sensor networks are still valid: the criteria followed to manage the network remains the same, however there is an increasing of complexity due to the diversity of the sensors. In the following Sections the problems related to multi-sensor management are divided into three main categories i.e., sensor deployment, sensor behavior assignment, and sensor coordination.

Sensor deployment is a critical issue for intelligence collection in an uncertain dynamic environment. It concerns with making decisions about when, where, and how many sensing resources need to be deployed in reaction to the state of the environment and its changes.

Sensor placement needs special attention in sensor deployment. It consists of positioning multiple sensors simultaneously in optimal or near optimal locations to support surveillance tasks when necessary. Typically it is desired to locate sensors within a particular region determined by tactical situations to optimize a certain criterion usually expressed in terms of global detection probability, quality of tracks, etc. This problem can be formulated as one of constrained optimization of a set of parameters. It is subject to constraints due to the following factors:

• sensors are usually restricted to specified regions due to tactical considerations; • critical restrictions may be imposed on relative positions of adjacent sensors to enable their mutual communication when sensors are arranged as distributed assets in a decentralized network (e.g., net-centric approach);

• the amount of sensing resources that can be positioned in a given period is limited due to logistical restrictions.

In simple cases, decisions on sensor placement are to be made with respect to a well-prescribed and stationary environment. An example of a stationary problem is the placing of radars to minimize the terrain screening effect in detection of an aircraft approaching a fixed site. Another example is the arrangement of a network of intelligence gathering assets in a specified region to target another well-defined area. In the above scenarios, mathematical or physical models such as terrain models, propagation models, etc. are commonly available and they are used as the basis for evaluation of sensor placement decisions. Paper [139] presents a study for finding a solution to the placement of territorial resources for multi-purpose telecommunication services considering also the restrictions imposed by the orography of the territory itself. To solve this problem genetic algorithms 5 are used to identify sites to place the resources for the optimal coverage of a given area. The used algorithm has demonstrated to be able to find optimal solutions in a variety of considered situations.

More challenging are those situations in which the environment is dynamic and sensors must repeatedly be repositioned to be able to refine and update the state estimation of moving targets in real time. Typical situations where reactive sensor placement is required are, for instance, submarine tracking by means of passive sonobuoys in an anti-submarine warfare scenario; locating moving transmitters using ESM (Electronic Support Measures) receivers; tracking of tanks on land by dropping passive acoustic sensors.

The basic purpose of sensor management is to adapt sensor behavior to dynamic environments. By sensor behavior assignment is meant efficient determination and planning of sensor functions and usage according to changing situation awareness or mission requirements. Two crucial points are involved. Firstly the decisions about the set of observation tasks (referred to as system-level tasks) that the sensor system is supposed to accomplish currently or in the near future, on the basis of the current/predicted situation as well as the given mission goal. Secondly the planning and scheduling of actions of the deployed sensors to best accomplish the proposed observation tasks and their objectives.

Owing to limited sensing resources, it is prevalent in real applications that available sensors are not able to serve all desired tasks and achieve all their associated objectives simultaneously. Therefore a reasonable compromise between conflicting demands is sought. Intuitively, more urgent or important tasks should be given higher priority in their competition for resources. Thus a scheme is required to prioritize observation tasks. Information about task priority can be very useful in scheduling of sensor actions and for negotiation between sensors in a decentralized paradigm.

To focus on this class of problems, let us consider a scenario including a number of targets as well as multiple sensors, which are capable of focusing on different objects with different modes for target tracking and/or classification. The first step for the sensor management system should be to utilize evidences gathered to decide objects of interest and to prioritize which objects to look at in the time 5 Genetic algorithm (GA) is a search heuristic that mimics the process of natural evolution. This heuristic is routinely used to generate useful solutions to optimization and search problems. GA belongs to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection, and crossover [140] .

following. Subsequently, in the second step, different sensors together with their modes are allocated across the interesting objects to achieve best situation awareness. In fact, owing to the constraints on sensors and computational resources, it is in general not possible to measure all targets of interest with all sensors in a single time interval. Also, improvement of the accuracy on one object may lead to degradation of performance on another object. What is required is a suitable compromise among different targets.

As stated in the previous Sections, there are two general ways to integrate a set of sensors into a sensor network. One is the centra1ized paradigm, where all actions of all sensors are decided by a central mechanism. The other alternative is to treat sensors in the network as distributed intelligent agents with some degree of autonomy. In such a decentralized architecture, bi-directional communication between sensors is enabled, so that communication bottlenecks possibly existing in a centralized network can be avoided. A major research objective of decentralized sensor management is to establish cooperative behavior between sensors with no or little external supervision. In a decentralized sensor network scenario a local view perceived from a sensor can be shared by some members of the sensor community. Intuitively, a local picture from one sensor can be used to direct the attention of other sensors or transfer tasks such as target tracking from one sensor to another. An interesting question is how participating sensors can autonomously coordinate their movements and sensing actions, on grounds of shared information, to develop an optimal global awareness of the environment with parsimonious consumption of time and resources.

As for homogeneous sensor network, the CSIP approach can be exploited [141, 142] : the network consists of different kinds of sensors, randomly distributed inside the surveillance area and if the number of sensors is high, the performance of the surveillance system can be considered independent of the location of the targets. Each sensor has a different functioning level. A first level sensor, with small sensing and communication capabilities may provide only detection information; a second level sensor may provide detection and localization information, with medium sensing and communication capabilities. Finally a third level sensor may provide tracking information and may be able to perform target recognition and classification. Usually the number of low level sensors exceeds the number of higher level sensors and only close sensors exchange data.

In [143] the network consists of two types of sensors: simple and complex as represented in Figure 22 .19a. The simple ones have only the capability of sensing their coverage area with a reduced computation capabilities and they transmit data to complex sensors. The information they provide may be encoded, for example, by a ""1"" if sensor detects something crossing its coverage area and by a ""0"" otherwise. Complex sensors, instead, have computation capabilities; they are able to locate the target by applying sophisticated algorithms (e.g., in [143] the maximum likelihood estimation algorithm is applied). The topology simulated in [143] , constituted by 80 simple sensors and 20 complex sensors, is represented in Figure 22 .19b: the sensors are indicated by circles; the complex sensors are connected by the solid lines, simple and complex sensor by dashed lines. Figure 22 .20 shows the number of active sensors during the target tracking: the theoretical value and the simulated value are compared. It is evident that in a self-organizing configuration the number of active sensors is optimized with the consequent advantage of saving of power. An adaptive self-configuring system consists of a collection of independent randomly located sensors that, carrying ahead local interactions, estimate the position of the target without a centralized control unit that coordinates their communication. It is fault tolerant and adapts to changing conditions. Furthermore, it is able to self-configuring, i.e., there is not an external entity that configures the network. Finally, the task is performed efficiently, i.e., it guarantees both a reasonably long network life and good target tracking performances. From local interactions, sensors form an efficient system that follows the target, i.e., local communication leads to a self-organizing network that exploits the features of the theories of random graphs and of self-organizing systems. The most natural way to approach random network topology is by means of the theory of random graphs [144, 145] . The theory of random graphs allows, for instance, to compute an upper bound to the estimated number of active sensors at each time step.

When the fusion of heterogeneous signals is performed, there is a formal problem to solve. The signal received by the different sensors may be statistically dependent because of the complex intermodal interactions; usually the statistical dependence is either ignored or not adequately considered. Usually the multiple hypotheses testing theory is based on the statistical independence of the received signals, in our case this condition is not maintained, therefore techniques as the ""copula probability theory"" may be useful.

In probability theory and statistics, a copula can be used to describe the dependence between random variables [146] . The cumulative distribution function of a random vector can be written in terms of marginal distribution functions and a copula. The marginal distribution functions describe the marginal distribution of each component of the random vector and the copula describes the dependence structure between the components. Copulas are popular in statistical applications as they allow one to easily model and estimate the distribution of random vectors by estimating marginal distributions and copula separately. The Sklar's theorem ensures that the joint cumulative distribution function (cdf) F Z (z 1 , z 2 , . . . , z N ) of random variables Z 1 , Z 2 , . . . , Z N are joined by a copula function C(·) to the respective marginal distributions F Z 1 (z 1 ), F Z 2 (z 2 ), . . . , F Z N (z N ) as [147] : F Z N (z N ) ). (22.43) Further, if the marginals are continuous, C(·) is unique. By the differentiation of the joint cdf, the joint pdf is obtained:

The copula density c(·), function of the N marginals from the N sensors, represents a correction term of the independent product of densities of Eq. (22.44) .

Processing heterogeneous data set is not straightforward as they may not be commensurate. In addition, the signals may also exhibit statistical dependence due to overlapping fields of view. In [148] the authors propose a copula-based solution to incorporate statistical dependence between disparate sources of information. The important problem of identifying the best copula for binary classification problems is also addressed and a copula based test-statistic, able to decouples marginals and dependency information, is developed.

This section tackles the problem of the surveillance of the borders of a nation. The region of interest, in general, may be very wide consisting even of thousands of kilometers of coastline and land border line, and millions of square kilometers. Such a system must face threats such as drug trafficking, intrusions (man, vehicles and airplanes), illegal immigration, smuggling, human trafficking, arms smuggling, unauthorized deforestation, terrorist activities over the military defense of the borders in order to ensure the territorial defense and the national sovereignty in the areas close to the border line. In the following Sections an overview of the range of possibilities and solutions in the design of the surveillance asset and data fusion process of such systems devoted to border control is given.

The size of the region, the nature of the border and the complexity of the scenario require the provision of different pictures of the region with different field of view at different resolution and time scales, suggesting a multi-sensor/multi-scale approach integrated in a hierarchical architecture of the whole system. Typically a global field of view of the whole region is necessary at the higher Command and Control (C2) level to capture the overall situation. A higher level of resolution and refresh rate is necessary at the lower and local level to analyze and control in depth each single zone of a region. Therefore the surveillance segment may be structured according to a multilayer architecture where layers realize different trade-offs in terms of field of view and granularity and refresh time. The surveillance segment comprises several types of sensors, each one characterized by different achievable resolution, field of view, and revisiting time. A pictorial sketch of the surveillance architecture is depicted in Figure 22 .21 for a notional country: sensors on board of satellites are expected to provide a global coverage of the monitored area at medium resolution with a low refresh rate, typically in the order of several hours or days; a higher resolution data and a higher refresh rate, in the order of seconds or tens of seconds, is provided by ground sensors on limited areas; airborne sensors (e.g., Unmanned Air Vehicle, UAV) will provide data on remote areas with good resolution data and short deployment time.

All data collected by the sensors are exploited by the fusion engine, highlighted in the figure. It is responsible to track and classify relevant entities present in the scenario and to provide a high quality representation of the situation. Also the data fusion process supports this multi-scale approach performing a distributed and network-centric processing at the various levels of the architecture, in accordance with available communication bandwidth and latency.

Pictorial of the surveillance architecture.

The surveillance of critical perimeters is one of the most important issues in Homeland defense and Homeland Protection systems. The ground surveillance needs are relevant to border protection applications, but include also local area protection, such as critical infrastructure, military/civilian posts.

During the last 10 years special attention has been focused on the realization of so-called ""electronic fence"" for perimeter/border control and several developments have been carried out to demonstrate the efficiency of such systems. However several problems occurred when the electronic fences became operational, showing lacks in the practical use by the operators (i.e., high number of false alarms, loss of/slow communication links) together with the problem of the high funding required for the whole system. One example is described in [149] , that requires now a total different approach for the surveillance of a wide national border (>500 km).

In the following an overview of the problems and solutions related to the implementation of an electronic fence is presented. The major components are:

• Sensors: they may be either active or passive, radar networks or heterogeneous sensor networks, (e.g., passive IR-infrared, seismic, acoustic, electro-optic-E/O, etc.). • Communication network: necessary to data exchange, may be subdivided into sub-networks if necessary. • Fusion engines: they perform data collection, data fusion and classification; this capability can be spread across the layers that compose the electronic fence (i.e., in the master stations, but also in the C2 centers).

Depending on the geographical deployment of the protection system, the data are then exchanged with C2 centers, both at local level and wide area (i.e., national) level. In Figure 22 .22 an example of an electronic fence architecture is depicted. In this case a wide area to be controlled, such as a border of a nation, has been considered; the subnets are geographically distributed along the boundaries. The architecture has the advantage to be modular and scalable and it can be organized with different level C2 centers (local, regional, national), depending also on the size of the considered boundaries. Each subnet is able to ensure the data exchange among the sensors. An overview of the sensors that can be employed in an electronic fence is presented.

Example of electronic fence architecture.

Microwave (X, Ku, Ka band) ground based radars are widely used to perform the monitoring of open wide areas. The monitoring of walking people and vehicles for ground applications, and of small sized boat for sea and river applications are relevant. The detection ranges varies from 2 km to 10 km for people, and from 5 km to 20 km for vehicles. Aerial targets (e.g., helicopters, low level aircraft) are also detected. Depending on the technology used these radars can be subdivided into the following two categories:

• Incoherent: they are low cost devices, FMCW (Frequency Modulated Continuous Wave) or pulsed (often a magnetron is used as most of the navigational radars), where the detection of the moving targets is based on inter-clutter visibility. Resolutions are typically of few meters or tenths of meters both in range and cross-range. • Coherent: they are solid state transmitter based, FMCW or pulse compressed, where the detection of the moving targets is based on sub-clutter visibility. The MTD (Moving Target Detection) filtering, even if the radar is working at X-band, requires low scan rates (in the order of 1-3 RPM-Round Per Minute) to allow high Doppler frequency resolution (0.2-0.5 m/s) to resolve slow moving target also in presence of strong clutter [150] .

The attention is for sensors able to operate in critical environments and many studies have been performed, in this direction, mainly using aerial platforms equipped with SAR. The aircraft equipped with sensors are used for wide areas where ground based sensors are not suitable or cannot be installed, such as in forest or jungle. However the use of airborne platforms to perform surveillance, are limited to missions ""on spot"" because it is not practical or cost/effective for continuous surveillance. The radar sensor can be mounted on manned or unmanned aircraft, usually equipped with electro-optic devices, and they can be used to monitor areas of several tenths of kilometer length. Other solutions take into account the installation of the radar either on a tethered aerostat or on a hovering helicopter. GMTI (Ground Moving Target Indication) from a stationary platform has been demonstrated.

Fixed radars for border control are usually in X and Ku band, but, because of the attenuation they suffer from foliage, they cannot be used for FOPEN applications. The ability of traditional microwave radars in operating in an environment with dense foliage is severely limited by foliage backscatter and attenuation of microwave frequencies through foliage [151] . As attenuation falls with increasing wavelength, lower frequencies such as those in the VHF and UHF bands (30-1000 MHz) may be suitable for FOPEN radar applications [152] [153] [154] [155] . FOPEN SAR (Synthetic Aperture Radar) systems started to be used in the early 1990s. They are usually mounted on manned or unmanned aircraft and mainly address illegal activity control and search-and-rescue operations. The focus is now for ground based systems and/or sensors with capabilities to detect walking personnel and moving vehicles [156] . Logistic constraints drive the technology to very low power devices, that are able to operate for several months or years, without maintenance. Another important issue is, together with a good probability of detection, the low false alarm probability, that is requested to be lowered up to 1 false alarm per day, or lower, even in presence of specific weather conditions (rain, wind) and/or local seasonal fauna. A special attention is due to the effect of environment. In dense foliage environments the main clutter effects are the backscatter and the attenuation. Backscatter: The fixed clutter returns can have a zero Doppler component raising up to 60-70 dB above the noise level with spectra amplitude and shape without large variations with frequency, but depending mainly on the wind strength [150] . Considering the measurements reported in [150] of the backscatter Doppler spectra, in order to perform efficient clutter rejection, two values of thresholds can be used: i.e., 1 m/s in case of light air, 2 m/s in case of windy/gale. Attenuation: The attenuation depends mainly on the frequency used and the radar beam grazing angle, even if small variations are reported with different polarizations [153] . Many studies have been carried out for SAR application and several studies report data for attenuation measured directly at ground level [151, 153, 154, 157] . The total attenuation, taking in account the major effects of the environment for a ground radar, can be summarized as follows: (22.45) where: 4 is the attenuation due to the ground reflection at the heights of the antenna (h r ) and the target (h t ), for the wavelength λ, • L f is the attenuation due to the foliage: it depends on the distance, the polarization and the forest type. It depends also on the distribution of the trees and the diameter of the masts, that can limit the line of sight, together with the height and density of lower canopy level.

The main requirements/constraints addressed are the range of the detections, which is reduced by the attenuation due to foliage and the low antenna height, that is usually limited to 1-2 m for logistic purposes. Also the power consumption must be kept at minimum level, also considering that photovoltaic cells are not suitable for installation on the ground in the forest. As a consequence the emitted power must be kept at a level of several mW. Camouflage and anti-tamper are often required. Very low cost is a mandatory requirement. Low Probability of Intercept (LPI) capabilities are necessary. Walking personnel and moving vehicles should be detected.

Even if the FOPEN radars are referred to the forest environment, the sensor described above is suitable to operate also in different installations, considering, for example, riverside or sea harbor protection applications. In these cases the different environmental conditions allow to achieve better radar performances. In addition, several other constraints (for example the management of transmitted power) can be mitigated by the use of photovoltaic cells and/or different antenna installations.

In Figures 22.23 and 22 .24 some outputs of the target detected by the UHF radar are shown. The information are displayed on range-Doppler maps, that are suitable to be read by a trained operator, giving information on the radial speed and, with a medium-high resolution in range, it helps the operator in the targets discrimination and alarm recognition. 

In this section we consider the Unattended Ground Passive Sensors (UGPS) and Electro-Optic (EO) to detect moving people or vehicles.

UGPS. They are used in case of small areas or critical infrastructure perimeter surveillance. They give alarms in presence of target in the operational range and, in some cases, can give a pre-classification of the target detected. The range of each sensor is usually limited to 10 m, but the latest technologies promise to reach detection ranges up to 50 m. They have very small dimension (less than 1 l volume) and low weight (less than 1 kg); they can be rapidly installed on rough ground or roads. Figure 22 .25 gives an example of positioning of UGPS in an operative field. They are of following basic types:

• seismic: to detect seismic movement produced by vehicles wheels or people walking, • acoustic: to detect vehicle engine noise, • infrared: to detect differences in thermal data from the environment due to the infrared signature of people and vehicles, • magnetic: to detect magnetic filed variation produced by vehicles.

Electro-Optic: They are widely used for surveillance, and many signal processing techniques assist the operator for target detection alerts.

They can be fixed or rotating covering up to 360 • in azimuth. For the night vision infrared EO are used, either passive or active, and they can reach a visibility of several kilometers in range. The EO are normally used stand alone or connected with radar sensor to help the operator for classification and identification of the detected targets. For example, with active infrared the operator can read (up to 2 km far from the camera) the license plate of a vehicle previously detected and tracked by the radar.

The sensors operate in cluster, and they are connected via a low power RF link, operating at UHF or L/S bands. The data of the unmanned radars can be combined with the data of other UGPS sensors C2 and operator console

Unattended ground radar network.

(infrared, acoustic, seismic), or connected to an existing network, to perform a more reliable detection system.

In Figure 22 .26 an example of sensor network is reported. As shown, adjacent sensor nodes are connected together and the information are sent, to the master station, via the short range radio link; the master station performs data fusion and medium range connection with the other master stations, or the C2 center. In case of long range connection the master stations are connected via radio link repeaters or satellite connections.

Special care must be taken to avoid interactions among the sensors, where two or more sensors share the same visibility area. Mutual interferences can be avoided using different frequencies and/or different timing for the transmitted waveform and also orthogonally coded waveforms.

The data transfer among the nodes is performed using the radio link between adjacent nodes. In case of linear geometric distribution the data grow up linearly with the number of nodes in the subnet; as a consequence the number of nodes in the subnet is limited by the maximum data rate of the single connection link.

The linear electronic fence can be composed of two or more parallel sections to allow redundancy in case of failure or loss of visibility of one or more sensors.

An example of electronic fence is shown in Figure 22 .27. In this case different environment conditions have been considered (riverside, forest, manmade buildings, and obstacles) and a network of FOPEN unattended ground radar sensors is used.

The fusion engine allows to fuse heterogeneous sensor data at multiple levels to perform tracking and classification of relevant entities present in the scenario and to provide a high quality representation of the situation together with cartographic layers and sensed images of the terrain. Figure 22 .28 provides an example of architecture for the fusion engine.

Border surveillance: a notional case.

Fusion engine architecture.

The tracking function processes the raw data provided by sensors and generates a set of tracks, representative of the real entities present in the scenario. A track typically carries the following information: a timestamp, position coordinates, velocity components, uncertainty on the kinematic components as expressed through the covariance matrix and additional attributes such as class/type and identity. In consideration of the potentially huge geographic extension of the system and of the importance to optimize the deployment of sensors as well as communication and processing power resources, a distributed tracking architecture is necessary. At the first level of the tracking architecture each sensor produces its own ""local"" tracks, in order to make available to the fusion engine a filtered information. Then a second level tracking combines local tracks originating from different sources into system tracks. This solution distributes the computational load on the peripheral nodes and reduces considerably the communication traffic which must be transmitted from the local level to the higher echelons; this is extremely important in consideration of the reduced bandwidth generally available between the peripheral elements and the center of the system.

In this step of the process, information of different nature can be fused producing a unique high quality information. Radar tracks can be fused with multiple images acquired by SAR and optical sensors, even if acquired at different resolutions, to achieve an improved representation of the scene with respect to the one achievable by processing data sets separately, in particular in terms of detection and false alarm probabilities when dealing with small targets (i.e., targets that occupy only few pixels of the image) [158] [159] [160] [161] [162] . The cartographic layers, superimposed with SAR or optical images, allow to put into context all the available information and support the fusion process (e.g., target tracking for ground vehicles especially during maneuvers).

Another output of the fusion engine is the classification of the tracked targets and entities of the scenario, i.e., the attribution of a class to the track under examination, hence supporting the capability to achieve a situation awareness.

From an operational point of view, the fusion engine can be considered as the responsible of producing a multi-resolution and multi-layer COP (Common Operating Picture), whose definition, as provided by [163] , is the following: ""A single identical display of relevant information shared by more than one command that facilitates collaborative planning and assists all echelons to achieve situational awareness."" The COP therefore provides to the operators at the different levels the capability to view each time a well-suited map, both in terms of proper scale (with respect to the scale of the observed situation) as well as in terms of number and type of information, according to the situation under analysis. This characteristic allows the system to properly support the operator without overloading him with unimportant information and keep him focused on events and information that might be related with his goal in terms of spatial, temporal, and logic correlation.

In the following the main constituents of the fusion engine are described.

The local tracking function processes the measurements provided by the sensor and produces a local track for each of the observed targets present in the surveillance region. The task of the tracking function at the local level is therefore of using the measurements made available by the sensor to estimate the number of targets and their kinematic components [164] [165] [166] . Local tracks provide position and velocity estimates at a given time, together with an indication of track quality; the track may also include other attributes relative to track classification, derived directly from radar measurements, from other sensors (EO/IR, UGPS, UAV) or assigned by a human operator.

In the scenario of a generic land border may be necessary to form low altitude tracks, surface tracks and ground tracks. Tracking of ground targets is especially critical due to the characteristics of the ground environment and of ground targets. The main criticality may be the masking effect due to terrain orography and vegetation. Another interesting feature of the ground environment is the presence of areas, mainly roads, where the probability of finding targets is higher, and areas such as off-road where the presence of targets is less probable. Distinguishing features of ground targets are high maneuverability and move-stop-move dynamics.

Even a well trained operator would be unable to select the correct hypothesis when a ground target is maneuvering since available information is insufficient. In these situations the best strategy is to defer the final decision until more data is available. To take into account these difficulties, the tracking function must be designed so as to handle several concurrent hypotheses and to make final decisions with a deferred logic [167] [168] [169] , i.e., when more data is available which allows to make a final decision with sufficient confidence. The choice of hypotheses is also dependent on the environment and on the target type. The management of multiple hypotheses is then the capability of the function to consider at each time instant a set of hypotheses, such as:

• the target is proceeding regularly/is maneuvering on road; • the target is moving/maneuvering off-road;

• the target has stopped, etc.

The tracking function assigns a score to each hypothesis and identifies the most probable; the function keeps alive for some time not only the most likely hypothesis but also a set of alternative hypotheses which represent different kinematic evolutions of the target. Figure 22 .29 shows an example of the set of hypotheses generated by the function: each hypothesis is relative to a path in the tree from time t0 to time t3 and the single branches may be relative to the choice of a specific dynamic model and/or a specific correlation hypothesis with a measurement in the set. For example in the path highlighted in red it is assumed that the target trajectory in the interval t0-t3 is described by the dynamic model m1; the other branches are relative to alternative hypotheses where it is assumed for example that the target has maneuvered (m2) or stopped (m3), etc. As new information is acquired, the probability of each hypothesis is updated according to new information; hypothesis which initially have a low score may gain credibility and vice versa. This characteristic, i.e., defer the decision until the available information is considered sufficient, allows to resolve most critical situations.

To take into account terrain and geographic information, the tracking solution leverages also context information provided by the GIS (Geographic Information System) in accordance with logics of terrain and road aided tracking. Digital Terrain Elevation Data (DTED) are also used to perform accurate projections of the tracks on the terrain and to identify zones where the target trajectory will be masked by obstacles and thus improve track continuity and the estimate of track kinematic parameters (e.g., maximum target velocity given the terrain type). The following Figure 22 .30 shows, for instance, how environmental knowledge can be exploited to improve the tracking function [170, 171] . Figure 22 .30a shows a landscape covered by forests and crossed by a network of paths; due to the nature of the environment, targets especially if motorized, will preferentially move along the track, avoiding off-road areas more difficult to traverse. The blue line represents the trajectory of a track which moves along a winding path in the forest. Figure 22 .30b on the other side shows how information relative to roads and viability in general can be exploited to improve the tracking performance. When the track approaches a bifurcation or a crossing, different hypotheses are generated to take into account possible target trajectories, such as on-road, off-road and also move-stop motion. More specifically the adoption of techniques such as road aided tracking is specifically important since it allows to improve the accuracy in the estimation of target kinematic parameters and therefore to make longer term projections. Finally weather information is exploited to further improve the tracking processing by feeding in information about areas where target detection is less probable (e.g., flooded areas) and expected target velocity is low given the past days weather conditions (e.g., heavy rain is expected to result in limited target velocity).

The classification function allows attributing a class to the track under examination, i.e., to determine its belonging to a class of targets. Target classification is extremely important since it helps to determine target identity and its threat level. Part of the classification process is the non-cooperative target recognition (NCTR), in order to avoid fratricide and to allow proper allocation of defensive means against the threat. In a coastal scenario NCTR capabilities are needed against ships, potentially involved in terrorism, illegal immigration or contraband operations, in order to assess and prioritize threats and to provide the appropriate response. Sensors such as radar, EO/IR, may provide useful information for classification. In the radar case, the NCTR technology facilitates the identification of non-co-operative targets by transmitting wide band signals and by processing the radar echoes in a suitable multidimensional domain; e.g., time-frequency and range-angle. In the former case the target is discriminated on the basis of the jet engine or the helicopter rotor modulations of the echo [172] [173] [174] [175] ; in the latter case the target is discriminated on the basis of the measured two-dimensional radar image obtained by ISAR techniques [176] [177] [178] (Figure  22 .31 shows a snapshot of the radar image of a ship).

The automatic classification, that the radar is capable of providing by means of these processing techniques, is used directly within the tracking function, to support the plot-track correlation process and to attribute a class to the track. The classification process allows therefore determining the class to which the track belongs (such as pedestrians, vehicles, convoys, helicopters, and small low altitude aircrafts) and performing cueing to other sensors (e.g., EO/IR sensors, high resolution radars) or demanding a patrolling mission (e.g., a mission with UAV).

While data provided by sensors are needed to perform the classification processing, once the target has been assigned to a class, this information can be exploited at sensor level to achieve better accuracy in the performed processing (e.g., target classification can be used to refine kinematic target parameters used in the tracking processing).

The range-Doppler information can be furthermore employed to produce a confusion matrix useful for target classification. The confusion matrix expresses the a posteriori probability that a target has been classified correctly among a finite number of classes that have been a priori established. References [21, 179] give an example of the use of confusion matrix in the classification issue.

Epidemics can impose serious challenges on societies in modern times. The poor health of general population due to a disease causes hardship and pain but also negative trends in the economy through absenteeism from work, missed business opportunities, etc. The ongoing epidemics of AIDS (Acquired Immune Deficiency Syndrome), tuberculosis and the recent outbreaks of SARS (Severe Acute Respiratory Syndrome) and H1N1 (swine flu) provide some revealing examples.

In the absence of an effective cure against an infectious disease, the best approach to mitigate its malicious or natural epidemic outbreak resides in the development of a capability for its early detection and prediction of its further development [180] . This enables typical countermeasures, such as the quarantine, vaccination, medical treatment, to be much more effective and less costly [181, 182] . Therefore this issue can be approached as a surveillance problem in the context of Homeland Protection.

Syndromic surveillance is referred to as a systematic collection, analysis, and interpretation of public health data for the purpose of early detection of an epidemic outbreak and the mobilization of a rapid response [180, 182] . The key idea is to detect an epidemic outbreak using early symptoms, well before the clinical or laboratory data result in a definite diagnosis. The rationale is that a spread of an infectious disease is usually associated with the measurable changes in the social behavior, which can be measured by non-medical means. Recent studies [183] [184] [185] have demonstrated that these non-medical sources of syndromic data streams, such as the absenteeism from work/school, the pharmaceutical sales, internet queries, twitter messages, and alike, can enable one to draw important conclusions regarding the epidemic state in the community. The ""Google Flu"" project [186] (flu-related searches in Google) is a well publicized example of this approach.

The algorithms for syndromic surveillance and have recently attracted significant attention by scientists and practitioners; there is a vast amount of literature devoted to this topic (for more comprehensive review see [180, 182] and references therein). In general, all algorithms applied in this area can be divided into two main groups, the data mining methods and the information fusion (also known as data assimilation) methods. Data mining is primarily concerned with the extraction of patterns from massive amounts of raw data without using dynamic models of the underlying process (i.e., epidemic spread) [183, 185] . Information fusion algorithms, on the contrary, strongly rely of mathematical models: in this case, the dynamic model of an epidemic outbreak and the measurement model of a particular syndromic data stream [187, 188] . Naturally, the accuracy of information fusion algorithms is significantly determined by the fidelity of the underlying models.

This section presents a study of a recursive information fusion algorithm for syndromic surveillance, formulated in the Bayesian context of stochastic nonlinear filtering and solved using a particle filter [134] . While a similar work has been considered earlier, see [189] [190] [191] [192] , this section introduces two novelties. First, in order to overcome the limitations of the standard ""compartment"" model of epidemic spread (the ""well-mixed"" approximation) we employ a more flexible alternative, see [193, 194] . The adopted epidemic model has the explicit parameter of ""mixing efficiency"" (or level of social interaction) and is therefore more appropriate to represent a variety of social interactions in a small community (e.g., self-isolation and panic). An advantage of the adopted epidemiological model is also that it enables to estimate the scaling law of the noise level with respect to the population size of a community. Second, a more flexible model of syndromic measurements, validated with data sets available in the literature [183, 186] , is adopted in the section. This measurement model is robust in the sense that some of its parameters are specified imprecisely, as interval values. The optimal sequential estimator (filter) and predictor are then formulated in the Bayesian framework and solved using a particle filter.

To describe the dynamics of an epidemic outbreak we employ the generalized SIR (Susceptible, Infectious and Recovered) epidemic model with stochastic fluctuations [195] [196] [197] . According to this model, the population of a community can be divided into three interacting groups: susceptible, infectious and recovered. Let the number of susceptible, infectious and recovered be denoted by S, I, and R, respectively, so that S + I + R = P, where P is the total population size. The dynamic model of epidemic progression in time can be then expressed by two stochastic differential equations subject to the ""conservation"" law for the population:

where s = S/P, i = I /P, r = R/P, and ξ, ζ are two uncorrelated white Gaussian noise processes, both with zero mean and unit variance. The terms σ q ξ and σ β ζ are introduced into Eq. (22.47) to capture the demographic noise (random variations in the contact rate α and in the recovery time β) [197, 198] . Parameter ν in Eq. (22.47) is the population mixing parameter, which for a homogeneous population equals 1. In the presence of an epidemic, however, ν may vary as people change their daily habits to reduce the risk of infection (e.g., panic, self-isolation). In general, model parameters α, β, ν can be assumed to be partially known as interval values. In order to insure P {s, i, r ∈ [0, 1]} ≈ 1, standard deviations σ q , σ β need to satisfy [199] :

Assuming that non-medical syndromic data are available for estimation and forecasting of the epidemic, we adopt a measurement model verified by [185, 186] , where a power law relationship holds for the odds-ratio between the observable syndrome z j and the (normalized) number of infected people i:

The power law exponent ς j in Eq. (22.48) is in general syndrome specific. Since at the initial stages of an epidemic (which is of main interest for early detection and forecasting) we have: i 1 and z j 1, Eq. (22.48) can be reduced to a simple power-law model:

where b j is a constant and τ j is introduced to model the random nature of measurement noise. It is assumed that τ j is uncorrelated to other syndromes and dynamic noises ξ,ζ . Since z j ≥ 0 (e.g., number of Google searches), the noise term τ j associated with syndrome j should be modeled by a random variable that provides strictly non-negative realizations. For this purpose we adopt the lognormal distribution, that is τ j = σ j η j , with η j ∼ ln N (0, 1) and N (0, 1) being the standard Gaussian distribution.

Parameters b j , σ j , ς j typically are not known, but with a representative data set of observations the model of Eq. (22.49) can be easily calibrated (see for example the results of the linear regression fits in [186] ). The data fit reported in [183] suggests that ς j may be close to unity, although it is difficult to precisely specify its value because of significant scattering of data points). To cater for this uncertainty, we assume that ς j can take any value in an interval, ς ∈ ς 1 , ς 2 around ς = 1. Unfortunately [185, 186] do not report any specific values of fitting parameters, so we use in this study some heuristic values for b j , σ j in our simulations.

The problem now is to estimate the (normalized) number of infected i, and susceptible s at time t, using syndromic observations z j of Eq. (22.49) , collected up to time t. Let x denote the state vector to be estimated; it includes i and s, but also the imprecisely known epidemic model parameters α, β and ν. The formal Bayesian solution is given in the form of the posterior pdf p(x t |z 1:t ), where x t is the state vector at time t and z 1:t denotes all observations up to time t. Using the posterior p(x t |z 1:t ), one can predict the progress of the epidemic using the dynamic model of Eq. (22.47).

For the purpose of computer implementation, first we need a discrete-time approximation of dynamic model of Eq. (22.47) . The state vector is adopted as: x = i s α β ν T , where T is the matrix transpose.

Using Euler's method with small integration interval δ, the nonlinear differential equations in Eq. (22.47) can be approximated as The optimal Bayes filter is typically presented in two steps, prediction and update. Suppose the posterior pdf at time t k is given by p(x k Z | 1:k ). Then the prediction step computes the pdf predicted to time t m = t k + δ as [194] :

p(x m |z 1:k ) = π(x m |x k ) p(x k |z 1:k )dx k , (22.52) where π(x m |x k ) is the transitional density. According to Eq. (22.50), we can write π(x m |x k ) = N ( f k (x k ), Q). The prediction step is carried out many times with tiny sampling intervals δ until observation z j,k+1 about syndrome j becomes available at t k+1 . The predicted pdf at t k+1 is denoted p(x k+1 |z 1:k ).

In the standard Bayesian estimation framework, the predicted pdf is updated using measurement z j,k+1 by multiplication with the measurement likelihood function [200] . According to Eq. (22.49) , the likelihood function in this case is g(z j,k+1 |x k ) = ln N (h(x k+1 ; ς j ), σ 2 j ), where h(x; ς) = b j · x 1 ς .

The standard Bayesian approach, however, cannot be applied because h(x; ς) defined in this way is not a function: ς is effectively an infinite set (an interval) and therefore h(x; ς) is one-to-many mapping. An elegant solution to the imprecise measurement transformation is available in the framework of random set theory [137] . In this approach h(x; ς) + τ is modeled by a random set x and the likelihood function represents the probability:g(z|x) = Pr {z ∈ x }, and is referred to as the generalized likelihood. More details and a theoretical justification of this approach can be found in [201] . The Bayes update using syndromic measurement z j,k+1 is now defined as [137] :

p(x k+1 |z 1:k+1 ) =g z|x k+1 · p x k+1 |z 1:k g z|x k+1 · p x k+1 |z 1:k dx k+1 . (22.53) For the measurement model Eq. (22.49) with additive Gaussian noise, the generalized likelihood has an analytic expression [201] :g (z j |x) = ϕ z; x , σ 2 j − ϕ z; x , σ 2 j , (22.54) where x = min{h(x; ς ), h(x; ς )}, x = max{h(x; ς ), h(x; ς )} define the limits of the set and ϕ(u; μ, P) = u −∞ ln N (y; μ, P)dy is the cumulative log-normal distribution. The recursions of the Bayes filter start with an initial pdf (at time t k = 0), denoted p(x 0 ), which is assumed known. The proposed Bayesian estimator cannot be solved in the closed form. Instead we developed an approximate solution based on the particle filter (PF) [134, 202] . The PF approximates the posterior pdf p(x k |z 1:k ) by a weighted random samples; details can be found in [134, 202] . The only difference here is that importance weight computation is based on the generalized likelihood function.

Epidemic forecasting will be demonstrated using an experimental data set obtained using a large-scale agent based simulation model [203, 204] of a virtual town of P = 5000 inhabitants, created in accordance with the Australian Census Bureau data. The agent based model is rather complex (takes a long time to run) and incorporates a typical age/gender breakdown, family-household-workplace habits, including the realistic day-to-day people contacts for a disease spread. The blue line in Figure 22 .32 shows the number of people of this town infected by a fictitious disease, reported once per day during a period of 154 days (only first 120 days shown). The dashed red line represents the adopted SIR model fit, using the entire batch of 154 data points and integration interval δ = 0.0052 days, with no process noise, i.e., w k = 0 in Eq. (22.50 ). The estimated model parameters are:α = 0.2399,β = 0.1066,ν = 1.2042. These estimates were obtained using the importance sampling technique of progressive correction [202] . Figure 22 .32 serves to verify that the adopted non-homogeneous mixing SIR model, although very simple and fast to run, is remarkably accurate in explaining the data obtained from a very complex simulation system.

The true number of infected people in forecasting simulations is chosen to be the output of the agent based population model, shown by the solid blue line in Figure 22 .32. The measurements are generated synthetically in accordance with Eq. (22.49) and discussions above, using the following parameters: ς = 1.05, b j = 0.25, σ j = 0.01, for all j = 1, 2, 3, 4 monitored syndromes. Independent measurements concerning all Nz = 4 syndromes are assumed available on a daily basis during the first 25 days. The problem is to perform the estimation sequentially as the measurements become available until the day number 25, and at that point of time to forecast the number of infected people as a function of time.

The initial pdf for the state vector was chosen as p( , and uniform distribution, respectively. The imprecise measurement parameter is adopted as ς ∈ [1.03, 1.07], while its true value The number of particles is set to 10,000. Figure 22 .33 shows the histograms of particle filter estimated values of α, β, and ν, after processing 25 days of syndromic data (i.e., in total 100 measurements). The histograms in this figure reveals that the uncertainty in parameters α and β has been substantially reduced after processing the data (compared with the initial p(α 0 ) and p(β 0 )). The uncertainty in ν, on the other hand, has not been reduced, indicating that this parameter cannot be estimated from syndromic data. While this is unfortunate, it does not appear to be a serious problem in forecasting the epidemic mainly because the prior on ν in practice is fairly tight ( ν ≈ 1). This is confirmed in Figure 22 .34 which shows a sample of 100 overlaid predicted epidemic curves (gray lines) based on the estimate of i, s, α, β, ν obtained after 25 days. Figure 22 .34 indicates that the forecast of the peak of the epidemic is fairly accurate, while the forecast of the size of the peak is more uncertain. Most importantly, however, the true epidemic curve (solid red line) appears to be always enveloped by the prediction curves. More experimental results can be found in [199] .

Integrated sensor systems and data fusion have been the main focus of this chapter. The discussed matter has been subdivided in nine sections which have covered a long trip starting from the description of the Homeland Protection problem, to the illustration of a wide spectrum of information sources (sensors and the like), to the netting of such sensors (both homogeneous and heterogeneous), with a broad range of practical applications: cooperative sensing to defend a urban territory, network of cooperative chemical sensors, detection and localization of radioactive point sources, use of so-called electronic fence to protect long borderlines of a territory, up to the estimation and forecasting of an epidemic. This work, an unofficial collaboration between experts from industry, research centers and academia, has brought together a wide spectrum of competences scientific, technical/technological/systemic and on the field. 

",0.7693372826688204
Leveraging the Laboratory Response Network Model for the Global Health Security Agenda,"Promoting global health security as an international priority is a challenge; the US Centers for Disease Control and Prevention (CDC) in its Global Health Security Agenda has articulated the importance of accelerating progress toward a world safe and secure from infectious disease threats. The goals are to (1) prevent and reduce the likelihood of outbreaks-natural, accidental, or intentional; (2) detect threats early to save lives; and (3) respond rapidly and effectively using multisectoral, international coordination and communication. Foundational to this agenda is the World Health Organization (WHO) Revised International Health Regulations (IHR) of 2005, which provide the legal framework for countries to strengthen their health systems in order to be able to respond to any public health emergency of international concern. This article proposes leveraging the distributed structure of the US-managed Laboratory Response Network for Biological Threats Preparedness (LRN-B) to develop the core capacity of laboratory testing and to fulfill the laboratorystrengthening component of the Global Health Security Agenda. The LRN model offers an effective mechanism to detect and respond to public health emergencies of international concern.","I n recent years, considerable resources have been invested in improving laboratory systems in resourcelimited settings. Noticeable improvements in infrastructure and laboratory diagnostics can be observed in some but certainly not all of these settings. More coordination is needed to implement national laboratory plans, improve the quality of laboratory services, and expand surveillance for emerging infectious disease threats. In February 2014, the Obama Administration launched the Global Health Security Agenda (GHSA) with the aim of moving toward a world safe and secure from infectious disease threats. 1 Foundational to this agenda is the World Health Organization (WHO) Revised International Health Regulations (IHR) of 2005, which provide the legal framework for countries to strengthen their health systems in order to be able to respond to any public health emergency of international concern. The IHR goals are ''to prevent, protect against, control and provide a public health response to the international spread of disease in ways that are commensurate with and restricted to public health risks, and which avoid unnecessary interference with international traffic and trade.'' 2 The regulations call on countries to meet 8 core public health capacities, one of which is laboratory testing.

High-quality laboratory testing is an essential component in initiating a public health response to terrorism, natural disasters, and emerging threats. It allows for effective decision making to close buildings or entire geographic areas, to treat exposed people, and to deploy therapeutics. In a recent editorial coinciding with the launch of the GHSA, Secretary of State John Kerry, then-Secretary of Health and Human Services Kathleen Sebelius, and Assistant to the President for Homeland Security Lisa Monaco provided the sobering update that ''80% of the world's nations still are not prepared to deal with new pandemics.'' 3 The GHSA offers a path forward to support countries in achieving the core capacities of the IHR. This article proposes leveraging the distributed structure of the US-managed Laboratory Response Network for Biological Threats Preparedness (LRN-B) to develop the core capacity of laboratory testing and to fulfill the laboratory-strengthening component of the GHSA. The LRN model offers an effective mechanism to detect and respond to public health emergencies of international concern.

The Laboratory Response Network

The creation of the LRN stemmed from Presidential Decision Directive 39, which outlined antiterrorism policies and missions for federal departments and agencies. 4 One such mission was the ability of the US Department of Health and Human Services (HHS) to identify threat agents, conduct epidemiologic investigations, and provide public health, medical, and pharmaceutical support. To that end, in 1999, the Centers for Disease Control and Prevention (CDC), in partnership with the Federal Bureau of Investigation (FBI) and the Association of Public Health Laboratories (APHL), launched the LRN to strengthen the nation's ability to rapidly detect biological and chemical agents that could be used by violent, nonstate actors. 5 The LRN-B began with only 17 laboratories and has since expanded to approximately 160 member facilities, which include both domestic and international laboratories, and thousands of sentinel clinical laboratories, 6 which form the foundation of the system. The LRN-B is diverse and encompasses public health, military, veterinary, environmental, and food laboratories that provide reference testing for a wide array of sample types including environmental samples and clinical specimens. The network has many strategic partners, which allows for coordination among federal, state, and local public health agencies, clinical laboratories, first responders, and law enforcement. 7 The foundation of the LRN is a unified operational plan and standardization of laboratory testing, so that a test result generated from one LRN member laboratory is the same as a result generated from another network laboratory, thus providing for rapid, high-confidence results to inform public health decisions.

At the beginning stages of the LRN, the primary focus was to prepare for and respond to potential bioterrorism events. In fact, the preparation efforts of the network enabled the US to have a rapid and extensive response to the 2001 anthrax attacks. Lessons learned from this response were used by APHL and CDC to strengthen outreach to clinical laboratories and first responders and to develop tools to assist laboratories in planning for surge capacity. Over the years, the LRN mission has expanded to include response to chemical threats 8 and other public health emergencies, such as severe acute respiratory syndrome (SARS), monkeypox, influenza A virus subtype H5N1 (avian influenza), influenza A virus subtype H1N1 (2009 pandemic influenza), and, in 2014, the Middle East respiratory syndrome coronavirus (MERS-CoV), and Ebola virus. Today's vision for the LRN-B is a laboratory system for rapid, high-confidence results to inform critical public health decisions about biological threats.

The LRN-B is organized as a 3-tiered pyramid (see Figure 1 ). At the base are thousands of sentinel clinical laboratories, which perform initial screening of potential biological threat agents. When sentinel clinical laboratories cannot rule out the presence of a threat agent, they refer specimens and isolates to an LRN reference laboratory.

The LRN is a national security asset that, with its partners, will develop, maintain, and strengthen an integrated domestic and international network of laboratories to respond quickly to biological, chemical, and radiological threats and other high-priority public health emergency needs through training, rapid testing, timely notification, and secure messaging of laboratory results. More than 160 state, local, and federal facilities provide reference testing, producing high-confidence test results that are the basis for threat analysis and intervention by both public health and law enforcement authorities. State and local public health laboratories comprise approximately 70% of the 164 LRN-B member laboratories. At the apex are national laboratories, such as those at the CDC and the Department of Defense (DoD). National laboratories primarily provide specimen characterizations that pose challenges beyond the capabilities of reference laboratories, and they provide support for other LRN members during a serious outbreak or terrorist event. The most dangerous or perplexing pathogens are handled only at BSL-4 laboratories at CDC and the US Army Medical Research Institute of Infectious Diseases (USAMRIID).

The LRN-B is an excellent model of an interconnected yet distributed system that has proven its value by detecting several threats. However, perhaps the most successful aspect of this network is its role in strengthening the US public health laboratory system. With the creation of this network in 1999, US public health laboratories received federal funding to help build laboratory capability and capacity to prepare for and respond to acts of terrorism, natural disasters, and emerging threats. These primary characteristics of the LRN-B model can be used to develop the laboratory framework for the GHSA:

Strengthens existing public health and defense laboratory systems; public health laboratories use CDCprovided funds to ensure they have safe and secure facilities, procure instrumentation and associated maintenance agreements; military laboratories use the DoD resources to support their LRN activities Invests in public health workforce; federal funds support personnel at the state and local levels Provides training to ensure a highly skilled workforce Provides technical assistance such as troubleshooting scientific assays and equipment Provides standardized protocols and tests Uses molecular diagnostics Ensures coverage for human, animal, food, and environmental specimens Provides standardized electronic data messaging with laboratory results that contribute to sound public health decisions Uses a specific system for communications and provides help desk support to member laboratories Uses a restricted access website to house materials that can be accessed by member laboratories Invests in continuous quality improvements, such as conducting multicenter evaluations of assays, seeking feedback on operations from laboratories, providing scientific and policy guidance to laboratories, and collecting and sharing model practices Maintains qualified membership by using performance measures that include ongoing proficiency tests and designated timeframes for results and communications Promotes partnerships by convening member laboratories via routine conference calls and national meetings, and by engaging with subject matter experts across the federal agencies to ensure scientific integrity

In addition to detecting biological threats and emerging infectious diseases, there are 2 other core areas to support the detection of chemical and radiological threats. CDC has initiated plans to develop the LRN for Radiological Threats Preparedness (LRN-R). They are currently working on newer and more rapid methods to analyze clinical specimens for radioactive materials, a vital component of the response to an event involving human radiation exposure. Once funded, the LRN-R will be structured similarly to the LRN-C, with some laboratories having full capability to analyze radiological specimens; others will have more limited capability, and still others will have the capacity for packaging and shipping the specimens to a laboratory with higher functionality.

The LRN for Chemical Threats Preparedness (LRN-C) was established in 1999 and was composed of CDC and 4 public health laboratories, with a fifth public health laboratory joining the network in 2000. It quickly became apparent that 5 public health laboratories would not be sufficient to respond to a large-scale event. Today, there are 55 LRN-C members (CDC and 54 public health laboratories). All laboratories are qualified to package and ship clinical specimens (Level 3); 47 laboratories have the capability to test for exposure to toxic chemical threat agents (Level 2); and 11 laboratories (10 state public health laboratories and CDC) have expanded capability to test for exposure to additional threat agents, such as mustard agents, nerve agents, and other industrial chemicals (Level 1). They also have expanded capacity to provide 24/7 analytical analyses in a large-scale event.

The methods that are developed are all based on mass spectrometry and are quantitative, detecting the actual chemical agent, or more commonly, a metabolite of the agent, in urine or blood. In an overt incident, the agent will most likely be known, based on symptoms and injuries. For those with obvious symptoms or injuries, detecting exposure or the extent of exposure to the agent is probably not necessary. These methods take on importance in identifying those individuals who have been exposed but do not display symptoms or injuries immediately following the incident, and in alleviating the concerns of the majority of individuals who have not been affected but are concerned.

The initial focus of the LRN-C was to develop analysis methods for detecting exposure to chemical threat agents. Today, the methods have expanded to include a variety of different chemicals that may or may not be used overtly but pose a public health risk, including a variety of toxins and poisons ( Figure 2 ).

Both the LRN-B and LRN-C have extensive performance measures to ensure that member laboratories are meeting the requirements and demonstrating scientific integrity in their work. Further, the partnerships aspect of the LRN ensures that new technologies are evaluated in a collaborative manner and that there is a focus on continuous quality improvements. The LRN is constantly seeking feedback from its membership and uses this information to make quality improvements to all operations. In essence, the LRN brings a standardized framework for preparedness and response to public health threats that can be used to support the GHSA.

Promoting global health security as an international priority is a challenge. In its GHSA, CDC has articulated the importance of accelerating progress toward a world safe and secure from infectious disease threats. The goals are to (1) prevent and reduce the likelihood of outbreaks-natural, accidental, or intentional; (2) detect threats early to save lives; and (3) respond rapidly and effectively using multisectoral, international coordination and communication. Encompassed in these goals are 9 specific objectives to ensure a coordinated and targeted approach with partner countries.

The LRN provides a model system that can be leveraged to fulfill the detection goals of the GHSA. Following are some specific examples of how the LRN model can be used.

One of the objectives is to promote biosafety and biosecurity systems. Prior to the development of the LRN, state-operated laboratory facilities were aging, underfunded, and often lacked the BSL-3 capacity to safely handle agents that could be used as biological weapons. The public health infrastructure had deteriorated over the years due in large part to a lack of resources, the migration of laboratory workers away from laboratories and into higher-paying positions, and a decrease in the number of students pursuing careers in microbiology. In the face of these challenges, the LRN employed the existing public health infrastructure as the backbone of its network and rebuilt it through funds awarded to states by HHS's CDC Cooperative Agreement on Public Health Preparedness and Response for Bioterrorism (now called the Public Health Emergency Preparedness [PHEP] Cooperative Agreement). 9 Since the inception of the PHEP Cooperative Agreement, LRN-B public health laboratories have used the funding to build, expand, and maintain BSL-3 suites. 10 A BSL-3 suite is a contained area that must meet stringent biosafety requirements, including biosafety cabinets, controlled double-door access, and engineering controls, such as negative air pressure relative to surrounding rooms and microfiltration of air. BSL-3 agricultural facilities have additional safeguards, including decontamination of liquids prior to release into the sewage system, separate HEPA filtration of exhausted air, and pressure-impervious walls, floors, and ceilings designed to protect workers and the environment from accidental exposure to certain regulated agents. 11 Finally, BSL-3 enhanced facilities have even more safety features.

All US state public health laboratories currently have at least 1 BSL-3 laboratory with BSL-3 suites in the facility. These laboratories also comply with stringent US government regulations, such as those that govern the possession and transfer of select agents and toxins. Given their extensive experiences, LRN-B public health laboratories can serve as ''twinning'' centers to assist international laboratories with strengthening their biosafety and biosecurity systems to protect the laboratorians and the public.

Laboratory twinning initiatives are an effective and efficient way of capacity building, training, mentorship, and skill transfer. The Laboratory Twinning Initiative was launched by the WHO Lyon office, with the support of an independent steering committee, as a mechanism to support national efforts to meet IHR competencies. 12 The twinning agreements were designed to support public health laboratory services by establishing long-term mentoring partnerships between national and state public health laboratories and linking these sites. The requirements of the twinning agreements were: (1) approval of the national government as a prerequisite for a twinning project; these partnerships need to fit in with government priorities and national plans for building a functional public health laboratory system; (2) acknowledgment by the national government that the resource-limited laboratory is its national public health laboratory or a component of its public health laboratory structure; (3) the work of the laboratories participating in twinning projects must be relevant to national and regional plans for the organization Figure 2 . The LRN Structure for Responding to Chemical Threats. A designation level 1, 2, or 3 identifies laboratory capabilities and defines member network participation.

of an epidemic alert and response network; (4) the objectives and expected outcomes, evaluation process, conditions of collaboration, and roles and responsibilities of each partner must be clearly defined; and (5) where possible and appropriate, the development of commonly defined research projects is encouraged, as these often provide good opportunities to strengthen capabilities and attract donors' support.

APHL has supported 4 WHO twinning agreements. The project scopes of work varied from country to country and were carefully crafted based on the laboratory needs and interests of the participating organizations. Examples of the projects supported include: In 2009 APHL established a WHO twinning partnership specific to quality, biosafety, and biosecurity systems between the New Mexico Department of Health, Scientific Laboratory Division, and the Paraguay National Laboratory that could be replicated in other countries to support the GHSA (Figure 3 ). Despite a limited program budget, technical assistance and training exchange visits coupled with distance mentorship provided a solid mechanism for knowledge transfer and capacity building. This partnership yielded the following for the Paraguay laboratory: Development of quality assurance and biosafety manuals; Institution of a pipette calibration program; Biosafety training sessions delivered; BSL-3 facility planning and design completed; and Establishment of a quality committee at the laboratory.

In April 2014, through the support of CDC, APHL launched a twinning partnership between the New Mexico Department of Health, Scientific Laboratory Division, and a GHSA pilot country, Uganda (Uganda National Health Laboratory). This twinning agreement will center on the 6 pillars of the IHR laboratory core competencies outlined by the WHO-Afro office: Coordination of laboratory services; Laboratory capacity for the high-priority diseases; Laboratory quality management systems; Specimen collection and transportation; Biosafety and biosecurity; and Laboratory-based surveillance and public health actions. 13 The twinning model has proven a sustainable and costeffective method for technical assistance and knowledge transfer, in part because of the efficiency gained by linking entire institutions. Technical assistance visits, training, and informational tours are able to be conducted across laboratory disciplines and departments, allowing for results that can affect the accurate identification and control of infectious diseases. Leveraging the LRN model and instituting twinning agreements to support the model could significantly accelerate progress in efforts to strengthen laboratory systems. Laboratory twinning could also be considered and implemented for ''South-to-South'' collaborations, allowing for countries with similar laboratory systems to support each other and establish regional laboratory networks.

There are 4 detection objectives that are intrinsically core LRN principles.

1. Launch, strengthen, and link global networks for real-time biosurveillance. The LRN is a critical system for ongoing surveillance of threat agents. Member laboratories have detected a number of threats ranging from Bacillus Strengthening the existing LRN to be the mechanism that CDC uses to deploy assays for infectious threats such as MERS-CoV, dengue virus, and chikungunya virus will ensure an extensive standard laboratory system for real-time surveillance, detection, and response. Further, expanding the concept of the existing LRN framework and engaging other partner countries in strategic planning discussions to share lessons learned and exchange model practices could help in the development of laboratory networks across the world. Similar to the APHL and CDC partnership, in which APHL assists with the daily operations of the LRN and provides recommendations on strategic issues, 16 CDC can engage with other organizations such as the African Society for Laboratory Medicine (ASLM) to support a standard laboratory response network concept. This approach would eliminate unnecessary costs of establishing a new framework for laboratory response, use existing tools and resources, and thus increase efficiency.

Launched in Addis Ababa, Ethiopia, in March 2011, ASLM was established to advocate for laboratory medicine, laboratory professionals, and laboratory networks throughout Africa. 17 This important organization, and specifically its intent to support laboratory networks, provides a critical platform for the GHSA. The President's Emergency Plan for AIDS Relief (PEPFAR) blueprint outlines the US government's strategy for ongoing efforts to defeat the HIV/AIDS epidemic and specifically calls for groups to ''support the African Society for Laboratory Medicine, and assess whether other country or regional health systems strengthening initiatives are needed to strengthen technical capacity.'' 18 The laboratory networks supported by ASLM could provide the communication system, governance, and management support necessary to respond to a public health emergency, following the LRN model.

Strengthen the global norm of rapid, transparent reporting and sample sharing. One of the hallmarks of the LRN is rapid data exchange. LRN member laboratories must have a secure mechanism to share laboratory data on threat agents with the CDC. Currently, most laboratories use an application called LRN Results Messenger (LRN RM), which allows LRN member laboratories to instantly manage and share standard LRN-related laboratory data. LRN RM represents the first iteration in an incremental approach to providing full standards-based electronic data exchange for this vital laboratory network. However, the nature of the application requires laboratorians to enter their data into LRN RM and their Laboratory Information Management System (LIMS), a time-consuming double entry process that could take up critical time in an emergency. Over time, increased capabilities have allowed for improved methods, and the LIMS Integration (LIMSi) project is the LRN's next step in electronic data exchange. The LIMSi project addresses the delay by aiding laboratories in connecting their existing LIMS to CDC, enabling LRN member laboratories to fulfill electronic data exchange needs for the LRN using their own systems and eliminating the need for LRN RM and double data entry. To implement LIMSi, CDC is collaborating with APHL to provide public health laboratories with necessary resources. There is still much to be done with LIMSi, and thus LRN RM remains a vital platform for many laboratories to rapidly exchange data with CDC.

The area of rapid and transparent reporting will require significant coordination with domestic and international partners and perhaps will be the most challenging aspect of the GHSA. Reviewing US public health laboratories and the LRN approach to data exchange could help to shape the data exchange model of this initiative.

Additional challenges may arise with sample sharing across international borders. However, the WHO IHR (2005) recognized this issue and included language to facilitate sample processing and disposal: ''States Parties shall, subject to national law and taking into account relevant international guidelines, facilitate the transport, entry, exit, processing and disposal of biological substances and diagnostic specimens, reagents and other diagnostic materials for verification and public health response purposes under these Regulations.'' 13 3. Develop and deploy novel diagnostics and strengthen laboratory systems. The LRN has a long and rich history of collaborating with federal agencies to develop new assays; partnering with state and local public health laboratories to evaluate new assays; and working with APHL to deploy these diagnostics to member laboratories. As noted earlier, the LRN is often credited with strengthening the US public health laboratory system.

The LRN-B is a unique asset in responding to all-hazard threats, providing immediate and sustained laboratory testing and communication and performing threat agent surveillance. LRN member laboratories have an impressive MANGAL AND MARYOGO-ROBINSON track record of responding to real-time threats in a timely and efficient manner and building partnerships across all levels of government and with the private sector. During the 2009 H1N1 pandemic, the public health emergency infrastructure built as part of the LRN was key to the response. ''Dual-use'' public health emergency infrastructure built as part of the LRN was integral to the H1N1 response by ensuring the following:

Availability and use of real-time PCR capacity using ABI 7500 Fast in 43 state and 11 county public health LRN reference-level laboratories. Much of this capacity was in place due to LRN approval for purchase of real-time PCR instrumentation using CDC/PHEP Cooperative Agreement funds, followed by ABI 7500 Fast instrument upgrades supported through the CDC Influenza Division. Availability and use of automated extraction systems with many of these in place due to LRN approval for purchase with PHEP funds for LRN capacity. Availability and use of molecular testing staff in public health laboratories that were funded and trained by both the CDC/LRN and the CDC Influenza Division. Availability of appropriate biosafety facilities in public health laboratories. Use of existing CDC LRN PHEP-funded systems enabled rapid and effective communication between clinical laboratories and public health laboratories.

4. Train and deploy an effective biosurveillance workforce. APHL has an extensive history of training laboratorians, 19 including the LRN workforce, and hosting fellowship programs. An example of a successful training initiative is the Emerging Infectious Diseases (EID) Laboratory Fellowship Program, sponsored by APHL and CDC, which trains and prepares scientists for careers in public health laboratories and supports public health initiatives related to infectious disease research. The EID Advanced Laboratory Training Fellowship is a 1-year program designed for bachelor's or master's level scientists, with emphasis on the practical application of technologies, methodologies, and practices related to emerging infectious diseases. The EID Laboratory Research Fellowship is a 2-year program designed for doctoral level (PhD, MD, or DVM) scientists to conduct highpriority research in infectious diseases. The process is competitive both for fellows and host laboratories.

Human resource development is critical to strengthening laboratory systems. Currently, there is a ''lack of sufficient numbers of well-trained laboratory scientists in public health service and inadequate laboratory management and leadership skills.'' 20 Nurturing laboratory leadership and management has to be supported for the effective oversight and administration of public health laboratory networks. APHL instituted the APHL-George Washington University (GWU) Institute for Public Health Leadership and Management in 2007 to provide senior-level public health professionals with essential tools for assessing national laboratory systems and strategies for strengthening the capacities and capabilities of high-quality public health laboratories. Initially taught at the GWU campus, and more recently transferred to the University of Dar-es-Salaam, Tanzania, and the Polytechnic of Namibia, this seminar has convened leaders from all over the world for 2 weeks of intense training on such topics as organizing an effective laboratory network; program evaluation, planning, and resource allocation; performing laboratory-based disease surveillance and response; developing and managing a central laboratory database; promoting professional development and a sustainable workforce; integrating laboratory information management systems and electronic medical records; electronic reporting of laboratory test results; and strategic planning and policy development. This type of training allows for leaders to gain the tools necessary and return to their home countries to implement effective laboratory networks.

Promoting a training culture similar to the LRN and leveraging existing APHL domestic and global partnerships for laboratory leadership and management will greatly enhance the capabilities of the biosurveillance workforce.

One of the objectives here is to develop an interconnected global network of emergency operations centers and a multisectoral response to biological incidents. APHL has provided guidance to US public health laboratories to assist in the development and exercise of continuity of operations plans. 21 Encompassed in these model continuity of operations plans guidelines is information on establishing an incident command system and ensuring linkages with various local, state, and national emergency operations centers. APHL's domestic expertise in supporting and testing continuity of operations plans can translate into the global arena with the goal of ensuring timely notification and sharing of laboratory results with appropriate agencies and partners.

In the anthrax attacks of 2001 (Figure 4) , 22 people were infected, 5 people lost their lives, and the cleanup cost was more than $1 billion. US public health laboratories tested thousands of samples to rule out or confirm the presence of any possible threat agent as well to restore public confidence that postal and other facilities were not contaminated with anthrax spores. CDC estimated that 284,000 people died worldwide during the 2009 H1N1 influenza pandemic. 3 Interconnected global systems are needed to ensure the rapid detection and containment of emerging threats as well as to reassure the public that it is safe to resume their daily living activities. Thus, strengthening the capacities LEVERAGING THE LRN MODEL FOR GHSA Figure 4 . Laboratories in Action: Preparing, Detecting, and Responding to Public Health Threats and capabilities of laboratories must be a priority as the GHSA moves forward. Leveraging the highly successful LRN model allows for internal coordination with CDC to better use subject matter expertise, training models, emergency operations center procedures, data exchange, and other technical resources; continued and expanded partnerships with DoD, the Department of State, and ministries of health; and use of the vast member network including state and local public health laboratories as potential twinning and training centers. Engaging APHL and other partners in strategic discussions to determine how best to meet the overarching goals of ''prevent, detect, and respond'' will ensure that all existing resources are brought to bear in this global effort to strengthen infectious disease detection systems.

",0.7691351076051554
JAMIA Focus on Media-based Biosurveillance Model Formulation Ⅲ HealthMap: Global Infectious Disease Monitoring through Automated Classification and Visualization of Internet Media Reports,"A b s t r a c t Objective: Unstructured electronic information sources, such as news reports, are proving to be valuable inputs for public health surveillance. However, staying abreast of current disease outbreaks requires scouring a continually growing number of disparate news sources and alert services, resulting in information overload. Our objective is to address this challenge through the HealthMap.org Web application, an automated system for querying, filtering, integrating and visualizing unstructured reports on disease outbreaks.","Internet-based resources, such as online newspapers, blogs, and discussion forums, have increased in number, volume, and coverage, and show potential as useful data sources for disease surveillance and early outbreak detection-currently, nearly all major outbreaks investigated by the World Health Organization are first identified through these informal online sources. 1, 2 However, electronic sources of infectious disease news are not well organized or integrated. Reading and assimilating a broad range and large number of reports as they appear on a daily basis has already become increasingly burdensome. 3, 4 The HealthMap project has begun to address this challenge through automated querying, filtering, integration, and visualization of Web-based reports on infectious disease outbreaks, to facilitate knowledge management and early detection. 5, 8 A freely available Web site operating since September 2006, HealthMap.org integrates data from a variety of electronic sources, including news through the Google News aggregator, expert-curated accounts such as ProMED Mail, and validated official alerts such as World Health Organization announcements. Through the use of automated text processing algorithms, the system classifies alerts by location and disease and then overlays them on an interactive geographic map. It currently processes an average of 30 disease alerts per day; with the default 30-day time window, the system typically displays approximately 1,000 alerts at any particular time. The filtering and visualization features of HealthMap thus serve to bring structure to an otherwise overwhelming amount of information, enabling the user to quickly and easily see those elements pertinent to her area of interest.

HealthMap is part of a new generation of health surveillance systems that help supplement existing public health systems by focusing on event-based monitoring of infectious diseases by leveraging Internet news and other electronic media. One of the earliest systems to harness some of these resources is the Global Public Health Intelligence Network (GPHIN). 9 ,10 GPHIN has shown that extensive monitoring and analysis of news media around the world can effectively aid in early detection of emerging disease threats. Most notably, GPHIN was able to identify the 2002-2003 outbreak of Severe Acute Respiratory Syndrome (SARS) well in advance of official reporting. 10, 11 On an ongoing basis, GPHIN also provides a large fraction of initial outbreak reports directly to the WHO for investigation. 1, 2 Another successful online disease alerting service is the ProMED Mail email announcement list, with 38,000 subscribers and a panel of expert moderators. [12] [13] [14] Other systems include MedISys, 15 Argus, 16 and EpiSPIDER, 17 all of which also leverage informal electronic datasets for disease outbreak information.

While projects such as GPHIN and ProMED serve public health authorities, infectious disease Web sites that serve the general public are also gaining in popularity and helping to increase awareness of public health issues, especially for international travelers. One such site, FluWikie.com, which reports on avian influenza and other topics relating to pandemic influenza, is heavily trafficked and was cited along with similar sites by the CDC as ""critical to CDC's ability to prepare for and respond to an influenza pandemic."" 18 In addition to existing online public health resources, recent years have seen the rise of ""Web 2.0"" technologies 19 including the proliferation of Really Simple Syndication (RSS) 20 and Asynchronous JavaScript and XML (AJAX). 21, 22 These tools create new opportunities for interactive software such as HealthMap. On the backend, RSS is a first step towards the goal of a ""semantic Web,"" 23 allowing for greater possibilities in extracting structure algorithmically from a variety of disparate data sources. On the frontend, the Google Maps public API allows the Web developer to create mapping applications using a powerful and well-known user interface. Finally, rich JavaScript and asynchronous HTTP requests, the AJAX building blocks, enable us to create responsive, highly customizable Web user interfaces that begin to approach the desktop software experience.

The power of HealthMap as a disease surveillance tool lies in its potential to bring together automated processing of a broad range of Internet data sources and rich, accessible visualization tools for lay and public health users alike. In this report, we describe the software architecture and implementation, as well as challenges and future plans.

The principal objective of HealthMap is to provide access to the greatest amount of potentially useful health information across the widest range of geography and pathogens, without overwhelming the user with excess information or obscuring important and urgent elements. To accomplish this goal, the system must be able to correctly classify reports, provide flexible and useful visualization output, and be responsive under heavy usage load.

The system is only useful to the extent that it can correctly identify the primary locations, diseases and other outbreakrelated factors of a large percentage of alerts, based on words, phrases and other available contextual information for each report.

In addition to the ""correctness"" of classification, the system must also take end-user objectives into account. For example, if a single alert contains references to fifty different places, the strictly correct classification would generate markers in all fifty locations. However, this alert, likely a summary of known ongoing activity, would then overload the map view with less important information and provide little benefit to the user. Another condition where optimum classification is difficult is in the case of multiple country involvement in a single outbreak. For instance, Switzerland may send disease specialists to help combat a dengue fever outbreak in Paraguay. In this case the primary locations of the alert are Switzerland and Paraguay, but if the system presents alert classifications in such a way as to imply that an outbreak of dengue fever is occurring in Switzerland, the user will be justifiably confused. The classifier must thus be designed to integrate its output with the user display.

With respect to visualization, a key objective of the system is to maximize flexibility in two key areas: in the user interface and in the collection of the underlying data. Specifically, HealthMap is designed to organize data across different dimensions (such as date, location and disease) and allow users to customize the view according to the geographic location, disease, and type of outbreak. However, the system must balance flexibility with simplicity; in certain cases, it should impose assumptions in organizing the data, so as not to overwhelm the user with customization controls. In general, the visualization interface should be intuitive and easy to use for the novice user-who may be a novice with respect to both software interfaces and infectious disease epidemiology-as well as allow the advanced user sophisticated and flexible customization of the display.

Behind the user interface, as the system collects reports, the goal is to allow the underlying data to shape the view as much as possible. Avian influenza, for example, is currently a topic of significant public health concern and extensive media coverage. However, the system should not place a priori emphasis on any given disease; instead it should adapt its mode of display to infectious disease threats as they emerge. The next global threat may come from an unexpected source, or the focus of public health and media attention may shift.

Accordingly, while HealthMap focuses primarily on human disease surveillance, one of our design objectives is comprehensive coverage of disease activity, encompassing animal and plant diseases, as well as some insect pests and other invasive species. This disease coverage is important as many infectious diseases of public health concern are zoonotic, naturally circulating among wildlife reservoir hosts before emerging in the human population. 24 -26 Along the same lines, the system should, where possible, avoid biases towards specific geographic areas. The next Journal of the American Medical Informatics Association Volume 15 Number 2 Mar / Apr 2008 noteworthy outbreak may as easily come from a major urban center in North America as a rural village in Africa.

As the system scales to include more sources and more dimensions of classification, it must be capable of rapidly processing a large number of reports. And as the user interface is enhanced to provide more sophisticated data visualization and customization, it must be able to accommodate a large number of simultaneous users and still be responsive. This scalability is critical, as the Web site could receive a burst of traffic in the event of a broadly publicized disease outbreak.

The HealthMap system consists of five modules: the Data Acquisition Engine, Classification Engine, Database, Web Backend, and Web Frontend. As illustrated in Figure 1 , the system gathers alerts, classifies them by location and disease, stores them in a database, and then displays them to the user.

As the system loads raw data from the Web, it converts each disease outbreak report into a standard ""alert"" format, containing four fields: headline, date, description, and info text. The headline is the alert headline, date is the date of issue of the alert, and the description is a brief summary of the alert, generally the first few sentences of the article. The info text is the text that will be fed into the parsing engine for the initial classification pass. In general, this initial text consists of the alert headline, stripped of elements that may trigger a false positive. For example, with Google News, the system removes the name of the originating publication from the headline.

The standardization of the alerts, when not already available from the RSS structure, is accomplished through the use of basic assumptions about the HTML and text formatting of the input for each feed. The drawback to making these assumptions is that the data source may change its format without warning, creating unexpected results in the data acquisition and requiring rapid adaptation of the system, though this has not yet proven to be a problem.

The classification engine determines the primary locations and diseases associated with each alert. It is comprised of two modules: the Preparation Module, which takes the raw input from the source, segments it and prepares it for input to the parser, and the Parser Module, which takes text input and produces disease and location codes as output.

While many alerts contain references to multiple locations or multiple diseases, the aim of the classifier is to identify the primary locations and diseases for each alert. To this end, the input is processed in stages: if the classifier is unable to identify location and disease from the initial input provided by the feed, namely the modified headline, it can request additional text from the feed. For example, in the case of the Google News aggregator, the system examines the headline, then the description, which generally consists of the first one or two sentences of the article, followed by the article's body text, and finally, the name of the online news source. Frequently, a publication originating in one area will refer to events occurring in another area, making the publication name and location an unreliable source for the location of the alert. However, articles that don't refer to a well-known location, such as ""Suburban school closed after flu outbreak,"" generally refer to a location near the publication headquarters. By processing the input in stages, the classifier avoids the incorrect classification of the first case while capturing the true location in the second case.

The extraction component of the preparation module processes the full HTML body of the article itself. Clearly, the article text contains the best indicators as to the locations and diseases of the event in question. However, blindly feeding the full article into the parser, while increasing sensitivity, would also significantly increase the false positive rate, especially due to JavaScript code, CSS and hyperlinks mixed with the body text, any of which may contain text elements that would trigger an incorrect match. The extractor must also contend with the wide variety of HTML formats of different news sources, including potentially malformed HTML code. By means of a collection of regular expressions and cautious assumptions about the input, the system confronts some of these challenges.

The Parser Module uses a word-level N-gram approach to match input against a dictionary of known patterns (an N-gram, as applied in the HealthMap software, is an Nword text extract, generally 1 to 10 words in length). After the initial data acquisition, the parser receives the input text, strips it of non-alphanumeric characters and splits it into word tokens. It then converts all capital letters to lowercase, except for those tokens that are two characters or fewer in length. The parser then compares the input to its dictionary of place and disease patterns, mapping text patterns to the database IDs of all locations and diseases known to the system. As part of the ongoing development of HealthMap, the dictionary is updated daily to improve the accuracy of the system; at the time of this writing it consists of over 2,300 location and 1,100 disease patterns.

Because the dictionary patterns are stored in memory as a tree, where each node is a hash table that maps single tokens to either subnodes or IDs (leaves), the system can look up each input token in constant time (see Figure 2 ). Thus, the classification time is linear on the number of input tokens, i.e., the length of the input. In the case where a word may have multiple spellings, for example the American ""diarrhea"" and the British ""diarrhoea,"" we simply stock the dictionary with multiple patterns. With the addition of patterns to the dictionary, memory consumption increases, but lookup time does not increase substantively.

The disadvantage of this approach is that because the input is hashed, each token must match exactly, making it difficult to accommodate fuzzy matching, wildcards, or regular expression approaches. Further, if we change the input processing, for example, to retain more of the input data, such as capitalization and punctuation, we must update the entire pattern dictionary. A further disadvantage of the dictionary approach is that the system can only identify locations and diseases already known and stocked in the database. Moreover, a key step in enhancing the parser resolution consists of augmenting the database by capturing correct locations and disease names, often involving careful manual data entry. As national borders shift and names of places change, albeit infrequently, the system must be manually updated to reflect new geography. For example, we have already been affected by this issue, as we needed to update the parsing system to reflect the designation of Serbia and Montenegro as separate nations on June 5, 2006.

A key advantage to the pattern dictionary approach is that it is relatively easily translated to other languages: we can simply employ a different dictionary within the existing architecture. A language expert is needed to perform the initial translation, refine the pattern library, help with capitalization and punctuations subtleties, and provide other adaptations, but the basic approach can be re-applied without major changes to the system. Further, the language expert need have only very minimal technical knowledge with respect to natural language syntax or software development to contribute to the dictionary. With the help of collaborators at the Naval Medical Research Center Detachment in Peru, we have already successfully adapted the classification engine to accommodate Spanish-language input, albeit with a smaller pattern dictionary.

A key component of the location classifier is its use of relationships among geographical entities. Our goal is to identify the most specific primary location or locations for a given alert. In many cases, we are presented with input such as ""UK (England),"" or ""Boston, MA."" In these cases, each input contains two distinct patterns that are coded as separate locations in the dictionary. However, Boston is contained by Massachusetts and England is contained by the United Kingdom. In order to correctly process this type of input, after it has identified a list of locations, the classification engine executes a secondary step, eliminating apparently redundant locations based on container relationships. In the given example, the system will initially identify both Boston and Massachusetts as locations for the alert, and then eliminate Massachusetts, as it is considered to be redundant with Boston.

We also apply container relationships to disease matching, as the input can contain analogous cases. For instance, avian influenza is a type of influenza and Norwalk-like viruses cause gastroenteritis-if the system identifies both Norwalklike virus and gastroenteritis in an alert, it thus eliminates gastroenteritis as a redundant, less specific disease category.

One key difference in the case of disease taxonomy is that unlike a location, a disease can ""belong"" to more than one container disease: E. coli is more specific than food poisoning, while norovirus, cholera and E. coli can each cause diarrhea (or gastroenteritis). If no disease category can be identified from the text, we designate the alert as Not Yet Classified. Such alerts may be non-disease-related news items that have slipped through the filter, but they may be important if they indicate initial investigation of an unknown disease or a rare condition that is not yet represented in the HealthMap database.

Once the alerts are classified by location and disease, the system stores them in a MySQL database. The database is designed according to standard relational database normalization principles. The primary tables store alerts, diseases and locations, while linking tables map alerts to their respective categories as identified by the classification engine. This standardized data model allows the HealthMap software flexibility to perform a variety of queries and display different views of the data. While the database is designed primarily to support features of the Web application, the data as they are stored are readily accessible for retrospective epidemiological studies, public health risk mapping and other research applications.

The initial Web page is loaded by the user's browser from a server-side cache which is updated every hour, following the capture and classification of new alert data. If the user adjusts the viewing parameters, he will trigger an AJAX request to the server. The request indicates the current state of the page controls, and from it the server generates a database query. The database then returns the alerts that match these parameters.

From these query results, the system then tallies the number of alerts, diseases, and feeds for each day at each location. To this tally it applies an algorithm, based on an exponentially weighted average, to determine a ""heat"" rating for each location. In order to give particular emphasis to more recent alerts, through qualitative assessments, we have currently set the decay parameter ""alpha"" of the exponential weighting to 0.17. (A greater alpha value means the weighting will decay more rapidly as we progress into the past.) Locations that have a greater number of feeds and diseases associated with them are also given increased weighting. Our qualitative justification for this boost is that if multiple sources have corroborated an outbreak it deserves more emphasis, and if the same source is reporting the same disease, it deserves less emphasis.

After the computation is complete, the system normalizes the heat scores across the set of markers and assigns each marker an integer value from 0 to 10. Because it computes the Heat scores for the currently requested marker set, the user can, for example, choose a particular disease category and quickly see where the hotspots are for that disease, in addition to the default view indicating general levels of outbreak activity. Figure 3 shows the HealthMap main page, featuring a variety of information boxes and user controls. The ""Available feeds"" box ( Figure 3a ) allows the user to select which sources to display on the map by means of the checkboxes along the left-hand side. Below the feeds menu, the ""Diseases, last 30 days"" box serves both to display the currently active diseases as well as to allow the user to select which diseases to display (Figure 3b) . The ""i"" button brings up a menu with links to further information about the particular disease from the Wikipedia, WHO, CDC, PubMed, and Google Trends Web sites. In the next section, the ""Alerts by country"" box indicates the number of alerts active in each country for the currently selected parameters (Figure 3c ). Clicking on a country name zooms the map view to that country for easy viewing of alerts in that location. The ""Latest alerts"" box displays the most recent alerts in reverse chronological order (Figure 3d ). An icon next to each headline indicates the alert source.

Moving across to the map display window, the date slider at the bottom allows the user to control the date range of displayed alerts (Figure 3e ). The end date is fixed as the current date, but the user can set the start date to any point in the previous thirty days. ""Full Screen"" mode expands the map to cover the full browser window, allowing for richer display and navigation (Figure 3f ). It also allows for ""situation room"" use, allowing the user to display the map on a non-interactive screen and monitor ongoing alert activity. On the map itself, the color of a marker indicates the Heat Index value for the location, with the deeper red color indicating more intense recent activity as contrasted with the paler yellow color.

To illustrate the functioning of the system, we examine a sample report and how it is processed by the HealthMap classification engine. A local newspaper report concerning an outbreak of shigellosis at a school in Wisconsin enters the system via the Google News aggregator. The system begins by examining the article headline: 

As there are no known patterns found for either location or disease, the classifier then progresses to the article ""description,"" an extract provided by Google News: While there is an indication of the location provided in this extract, ""Fond du Lac"" is currently not included in the dictionary, and therefore not recognized. Still lacking both location and disease information, the classifier examines the article body text, as prepared by the parsing engine from the original HTML: "" We want to get the information out to parents: Here it is and here are steps you can take,"" Marian Sheridan, the Fond du Lac school health and safety coordinator said. The concern is that this infection is fast-spreading. Although the Wisconsin health department says 300 to 400 cases are reported each year, the uncomfortable abdominal cramps, fever, and diarrhea are symptoms no one wants running rampant through schools. ""I think we re getting the message out early enough, and I think that s one of the benefits of working with school districts staff to get the word out so we can contain it before it s widespread,"" Joyce Mann of the Department of Health and Family Services said. ""Parents are used to the school sending them health notices, and it s never to alarm but it s rather to inform,"" Sheridan said. ""Normally what we do is go in with a ten-percent bleach solution and everything gets wiped down-telephones, door knobs, desk chairs, desktops, the bathrooms are thoroughly gone through,"" building and As indicated in bold, the classifier now matches three different patterns in the text. The first identifies the disease category as Shigellosis; the second places the report in Wisconsin. The third match corresponds to the Diarrhea disease category, but based on the container relationships described above, the system correctly identifies Diarrhea as redundant with Shigellosis, and eliminates the former. At this point, the classifier has completed its work, and proceeds to the next report. Had it not identified both disease and location from the body text, it would have further examined the name of the publication as provided by Google News:

Upon processing of this text, it would also have identified the location based on the abbreviation WI, which is listed in the dictionary as a synonym of Wisconsin. However, in this particular case, the publication information is ignored as the classifier has already achieved matches using other components of the report.

Because the classification engine places alerts into many hundreds of different location and disease categories (currently over 700 total), as well as combinations of multiple categories of each type, it is not possible to apply traditional binary classification metrics such as precision and recall to measure its performance. However, because we curate all reports on a daily basis to correct misclassifications, we can examine various aspects of performance based on the changes performed.

At the most basic level, the accuracy of the classifier can be measured by the percentage of reports entering the system that need not have their disease or location classifications corrected in any way. At a more detailed level, we can examine the number of alerts requiring a correction of disease classification as compared with the number requiring a location correction. Table 1 provides a full breakdown of the classifier performance both by source and by disease and location. As shown, the overall accuracy of the system is 84%, thus correctly classifying 655 out of 778 reports over the one-month period from October 10 to November 9. As one might expect, performance on ProMED alerts, at 91%, is substantially better than on Google News reports (81%), as ProMED messages represent data curated specifically for disease outbreak reporting and follow a more regular structure.

There are, however, important limitations to this performance analysis. In particular, in some cases, the correction of the classification serves merely to shift between related categories, such as reclassifying Gastroenteritis as Norovirus, or UK as England. In other cases, the correction is more drastic, such as correcting Influenza to Equine Influenza, or Washington, DC to Washington State. Clearly the change is more significant in the latter cases, but we don't capture this distinction in the current analysis. As it is difficult to capture rigorously, for the moment we take the most conservative view in computing accuracy. As part of our ongoing research, we are developing more fine-grained metrics.

As HealthMap is still in the early stages of development, a number of important enhancements are either currently under development or in the planning stage. The primary design goal of HealthMap is to provide broad coverage of ongoing outbreaks without overwhelming the user. In the pursuit of improved coverage, we are exploring the use of other sources, including additional news aggregators-such as Yahoo news, Factiva, and LexisNexis-blogs, and veterinary news sources such as the World Organization for Animal Health (OIE). In pursuit of improved filtering, we are developing natural language processing techniques for additional automated data categorization, such as clustering similar reports, identifying specific outbreak pertinence, distinguishing discrete outbreaks from endemic activity, and identifying reports indicating the absence of disease or the end of a previously identified outbreak. As part of our own evaluation, as mentioned in the Formulation Process, an important goal of the system is to cover as broad a range of geography and disease as possible, without bias toward particular regions or pathogens. While the internal architecture of the system itself largely meets these goals (particularly as we add more geographical subdivisions around the world), the alert data we process and display leaves much to be improved. Because we currently rely heavily on the US edition of Google News for reports, the system is biased toward the US and Canada as well as other English-speaking countries around the world, as shown in Figure 4 . To address this problem, we have developed a Spanish-language version of the system and are currently expanding to other languages and data sources as resources permit. However, given the uneven distribution of media and reporting resources around the world, we will continue to face this issue for the foreseeable future.

In addition to adding new capability, we are also working on improving the accuracy of the existing classifier, both by expanding the pattern dictionary and by improving the preparation module. We will add more locations and diseases, including administrative divisions for countries such as Indonesia, Brazil, Sudan and Mexico as well as major cities worldwide. For diseases, we will be adding more disease categories and refining our disease taxonomy, as well as tagging diseases with category metadata to allow for improved searching. We will also explore more advanced techniques such as fuzzy matching and Bayesian machine learning for improving the resolution and accuracy of our automated classification algorithms, as well as categorizing alerts by relevancy, clustering similar alerts, and extracting other useful attributes. [27] [28] [29] On the human side, taking inspiration from the highly successful Wikipedia model, 30 we plan to work with networks of experts to evaluate community collaboration as a mechanism for alert acquisition and classification.

As we expand functionality, performance will naturally become an increasing concern. We have a few optimizations in progress, such as moving to memory-based caching, more intelligent, ""lazy"" loading of the pattern dictionary, and better optimized database queries. We are also exploring ways to better employ client-side caching without overloading the browser.

On the frontend, we have plans to improve the user experience with added features and improved customization. Examples include keyword searching, RSS output, saved preferences, endemic background disease rates, notification messaging via email, and temporal visualization. (Notably, the EpiSPIDER system has already taken steps in this area, incorporating a timeline view of ProMED reports. 17 ) We also plan to conduct a usability observation study, to gather feedback from our target demographic on priority features as well as how best to improve the HealthMap user interface.

Along with user-level evaluation, we are also working to develop more rigorous evaluation metrics for the integrated system, including its ability to cover a broad range of geography and pathogens, limit noise, detect outbreaks early, and accurately characterize alerts in each dimension of classification.

HealthMap is part of a new generation of disease surveillance systems that process unstructured and unclassified data sources. Comprehensive evaluation of these types of systems and data sources is also an important area and part of our ongoing and future research and collaboration with other disease tracking systems such as GPHIN, EpiSPIDER, MedISys, and Argus, would enable an in-depth comparison. With that said, there are a few broad comparisons we can draw between systems. One key area is accessibility: HealthMap is freely available to the public, whereas some systems are currently closed systems, requiring either paid subscription or approved access. Another key area is in the use of automation. While we certainly perform manual curation in maintaining HealthMap, our goal is to maximize automation in order to leverage the human contribution. The value of a full-time staff of language and domain experts to read and analyze reports around the clock should also be addressed as part of a broader research initiative. 6,7

The promise of HealthMap lies in its ability to extract useful, customizable messaging and views from a mass of unstructured data. While the site has already generated significant interest as a publicly available surveillance tool, many improvements remain to be made for it to be a truly useful resource for both public health professionals and the general public. In particular, adding more languages and expanding our usage of general data sources such as newspapers and blogs will increase coverage and further demonstrate the value of the visualization and filtering features. Moreover, only as time progresses, as more people use the system, and further significant outbreaks unfold in the global disease ecosystem, will we know the true potential of the software, and how best to improve it.

",0.7669271855664646
Application of next generation sequencing technology on contamination monitoring in microbiology laboratory,"The surveillance and prevention of pathogenic microbiological contamination are the most important tasks of biosafety management in the lab. There is an urgent need to establish an effective and unbiased method to evaluate and monitor such contamination. This study aims to investigate the utility of next generation sequencing (NGS) method to detect possible contamination in the microbiology laboratory. Environmental samples were taken at multiple sites at the lab including the inner site of centrifuge rotor, the bench used for molecular biological tests, the benches of biosafety cabinets used for viral culture, clinical sample pre-treatment and nucleic acids extraction, by scrubbing the sites using sterile flocked swabs. The extracted total nucleic acids were used to construct the libraries for deep sequencing according to the protocol of Ion Torrent platform. At least 1G raw data was obtained for each sample. The reads of viruses and bacteria accounted for 0.01 ± 0.02%, and 77.76 ± 12.53% of total reads respectively. The viral sequences were likely to be derived from gene amplification products, the nucleic acids contaminated in fetal bovine serum. Reads from environmental microorganisms were also identified. Our results suggested that NGS method was capable of monitoring the nucleic acids contaminations from different sources in the lab, demonstrating its promising utility in monitoring and assessing the risk of potential laboratory contamination. The risk of contamination from reagents, remnant DNA and environment should be considered in data analysis and results interpretation.","The prevention and control of contamination caused by cultured viruses, bacteria and other infectious materials are the most important part in biosafety management in the laboratory [1] [2] [3] . Microbiological contaminations in laboratories have been frequently reported in instruments [4] , rooms [5, 6] , and the operators [7, 8] . These contaminations are mainly from the performance of experiments, for example, bacteria/virus isolation and culture, gene amplification, and clinical samples preparation. As such, it is important to establish an effective and unbiased method to evaluate and monitor the risk of lab contaminations.

The potential contamination in the environment of a microbiology lab is very complicated, as usually the source and background of contaminates is unknown. However, it is hard to conduct effective contamination surveillance in a common laboratory due to the methodology limitation of contaminant detections. The isolation of microbial species is difficult because of its tedious work and poor sensitivity. Molecular tests such as PCR is sensitive, however it can only detect the gene of known pathogens. It is desired to develop a comprehensive technology which can detect the potential contaminates of unknown resources, characterized by culture-independency and high sensitivity.

The development of next-generation sequencing (NGS) technology provided a culture-independent method to obtain microorganisms genome information with high sensitivity [9, 10] , and has been widely used to detect the pathogenic microorganisms [11] [12] [13] . In this study, we evaluated the performance of NGS as a promising tool on the surveillance and risk assessment of potential lab contaminations. Frequently used microbiology laboratories which included biosafety laboratory level I (BSL-I) and II (BSL-II) were investigated in our study. Various sites were inspected in different rooms. We aimed to establish an unbiased and effective evaluating technique involving NGS to monitor the laboratory contamination.

Four rooms were investigated in the study. Room 1 is a BSL-II used for viral isolation and culture. Room 2 is a BSL-II, used for pre-treatment of respiratory samples from patients with respiratory tract infections and also used for nucleic acid extraction. Room 3 is a BSL-I, where a negative-pressure PCR hood used for nucleic acid template adding was placed. Room 4 is a BSL-I, used for molecular tests and immunological experiments. The sampling sites included the biosafety cabinet, the bench, and the inner sites of centrifuge rotor (Table 1) .

For each site, five different locations were selected, sterile flocked swabs (Copan Diagnostics, Inc., Murrieta, CA) soaked in sterile saline were used to collect samples by scrubbing repeatedly on each sampling point for 10 times. All sampling swabs from one site were rinsed into 1 ml sterile saline in one tube. The swabs were discarded after repeat pressing. Swab rinsed into 1 ml saline without sampling was used as negative control, with code number of H0. A positive control was included, which was sputum collected from a pneumonia patient infected with human coronavirus (HCoV) 229E determined previously by RT-PCR method [14] . All procedures were performed according to the biosafety regulations and requirements.

Total 400 μl of each sample was used for nucleic acids extraction by using QIAamp® viral RNA mini Kit (Qiagen, Hilden, Germany) and the elution volume was 80 μl. Since both the DNA and RNA molecules are of interest to us, no DNase was used in our protocol. All nucleic acids were used for library construction. The OD260/280 value of total nucleic acids was measured by NanoDrop Spectrophotometer (ND-1000, Thermo Fisher, Wilmington, DE, USA). The RNA was specifically quantified by Qubit® 2.0 Fluorometer (Thermo Fisher, Wilmington, DE, USA). The libraries were constructed by using Ion Total RNA-Seq Kit v2 (Thermo Fisher, Wilmington, DE, USA). Total 30-50 ng of RNA was fragmented by RNase III (Thermo Fisher, Wilmington, DE, USA), then hybridized and ligated with the Ion Adaptor Mix v2 at 65°C for 10 min, 30°C for 30 min. The reverse transcribed single-stranded cDNA was amplified with Ion 5′ and 3′ PCR Primer v2. The sequencing libraries were qualified and quantified by using Agilent 2100 Bioanalyzer (Agilent Technologies, Santa Clara, CA, USA) and Qubit® 2.0 Fluorometer.

Each sequencing library was diluted to 15 pM. Templated Ion Sphere Particles were prepared and enriched by using Ion OneTouch 200 Template Kit. Deep sequencing was performed by using Ion Personal Genome Machine™ system with Ion PGM 200 Sequencing Kit in Ion 318 Chips (Thermo Fisher, Wilmington, DE, USA). The sequencing output from each sample was no less than 1 G. The raw data reads were filtered by a minimum length of 36 bp, and the remaining reads were aligned to the NCBI nt database (Feb 2016 version) by MegaBLAST [15] (-evalue 1e-10 -max_target_seqs 10 -max_hsps 1 -qcov_hsp_perc 60). Taxonomy was assigned by MEGAN (-ms 100 -sup 1 -me 0.01 -top 10) [16] . The community composition was analyzed at both the genus and species levels. Only reads that could be mapped to specific species with high confidence were considered for genome mapping. Alpha diversity (observed genera/species and Shannon index) calculation and principal coordinate analysis (PCoA) (based on abundance weighted Jaccard distance) were performed with Qiime [17] (v1.9.1). Mann-Whitney test was performed with GraphPad prism 7.

The quality of nucleic acids measured by OD 260/280 was 1.8-2.2 for all samples. Similar quantity of RNA was yielded from H1-H9 samples (88.8-127.2 ng, median of 108.80 ng) ( Figure 1A ), which indicated similar biomass in all sampled locations. As expected, the amount of nucleic acids from H0 was ultralow (below the detection limit) and all nucleic acids were used to construct the sequencing library. After sequencing and data filtering, a range from 3,046,082 to 6,000,548 reads from H1 to H8 was used for taxonomy classification and more than 66.69% of 

This study investigated the performance of next generation sequencing (NGS) technology in monitoring laboratory microbiological contamination.

Evidence before this study Contamination is a big issue in microbiology laboratories, while it is hard to conduct effective contamination surveillance in a common laboratory due to the lack of proper methodology. The isolation of microbial species is difficult which needs tedious lab work and has low sensitivity. Although molecular tests such as PCR are sensitive, only limited number of known microbes could be detected simultaneously.

A holistic assessment of the biological background in the environment of a microbiology laboratory was conducted through NGS technology in this study. In general, the richness of viral and bacterial sequences is positively associated with the frequency of laboratory activities in the room and on the bench. The data indicated that viral reads were mainly from experimental activities, materials and environment. All types of bacteria seem to be derived from the external environment. The gene fragments from exogenous microorganisms have a great impact on the deep sequencing results, especially for the clinical samples with low microbial biomass. Application of the NGS technique on biosafety monitoring would enable us to disentangle the background noise presenting in the sequencing data, which in turn help to distinguish true contamination caused by microorganisms from false positives.

The study demonstrated that NGS could be a promising approach in monitoring the contamination from different sources in the lab, suggesting its promising usage in monitoring and assessing the risk of potential laboratory contamination.

reads from these samples could be classified. The samples from H0 (480,074 reads) and H9 (1,354,249 reads) had dramatically low amount of reads, and only 15.76% and 23.58% of reads were successfully aligned and classified to a certain taxonomic rank ( Figure 1B) . The data obtained from the positive control was consistent with the real-time PCR results. The reads of known pathogen, HCoV 229E accounted to 72.4% of all microbial reads, thus verified the reliability of NGS procedure.

As the major activities in the laboratory involved respiratory viruses, we first analyzed the viral reads in the data. The results demonstrated that viral sequences were in a relatively low abundance (0.01 ± 0.02%), with the viral reads ranged from 64 to 733 (median of 148). The number of viral species ranged from 8 to 34 (median of 12) in different samples including H0. The distribution of virus species in each sampling site was shown in Figure 2 . The samples collected from Room 4, which was used for molecular biological and immunological experiments, had the highest number of viral species (15 species). The three most abundant viruses were cercopithecine herpesvirus 5 (CeHV-5) (23% reads, observed at 7 sites), influenza A viruses (IFVA) (16%, 8 sites) and lactococcus phage 936 sensu lato (8%, 2 sites).

Due to the high sensitivity of the NGS method, viral reads that derived from routine laboratory work were identified, such as IFVA, human enterovirus 71 viruses, HCoV HKUI and NL63 (seen in Figure  S1 ). These reads were mainly from Room 2 and Room 4. Among these, reads of IFVA accounted for the highest proportion. They were detected in 6 sites except the biosafety cabinet. However, the obtained gene sequences were all close to the primer positions for viral genes amplification (Figure 3) , as PCR tests were performed on the viral positive clinical samples within a month before sampling.

The sequences of bovine diarrhea virus were only found on the biosafety cabinet in Room 1, which was used for viral isolation and culture experiments ( Figure 2 ). The existence of bovine diarrhea viral nucleic acids in fetal bovine serum (FBS) was further confirmed by PCR methods (data not shown) [18] , which may represent contamination of bovine serum used for cell culture as reported also in previous studies [18] [19] [20] [21] .

The reads of CeHV-5 were identified at all sampling sites except H2, H3 and H9. This virus was previously isolated from the tissue of rhesus macaque [22] , and had been detected in the cell line obtained from the kidney of rhesus macaque [23] . There are two rhesus macaque kidney cell lines, LLC-MK2 and Vero cells, used in the lab for virus isolation during the study period. However, we did not find CeHV-5 nucleic acids in these cultured cells by PCR. There were also viral sequences aligned to human mastadenovirus, polyomavirus, hepatitis virus, human metapneumovirus and rotavirus, but their read numbers and covered lengths were very limited, thus were likely to be false positives (Figures 2 and S1 ).

The alpha diversity of genera measured by observed genera and Shannon index did not differ significantly among H1-H9 (Figures 4A, 2B). However, when going down to the species level, H2 and H8 showed much higher diversity than other samples except for the negative control ( Figures 4C, 2D) . Besides H2 and H8, another sample collected from the operating bench, H5, also had considerable higher alpha diversity, indicating a generally high diversity of biological substances on the bench. It is noteworthy that H0 had high evenness of biological substances background, as shown by the high Shannon index at both the genus and species levels ( Figure 4B, D) , which might reflect that the contamination from reagents and/or environment is more severe for low biomass samples.

A tight clustering of H1, H2 and H3, which were all taken from the viral culture room (Room 1), was highlighted on PCoA plot that based on weighted Jaccard distance at the genus level ( Figure 4E ). This suggests a considerable impact of the physical distance/boundary on the spread of biological substances.

Of all the biomass sequences classified to the domain level, Bacteria is the most abundant in all samples (77.76 ± 12.53%), followed by fungi and all other Eukaryota, accounting for 5.89 ± 4.61% and 16.32 ± 12.21% of the classified reads, respectively. The amount of Archaea (0.01 ± 0.01%) and Viruses (0.01 ± 0.02%) is relatively low.

Zooming into the species level, the top 20 abundant ones include four species belonging to the phylum Actinobacteria of Bacteria, twelve species belonging to the phylum Proteobacteria, two species belonging to the phylum Chordata of Eukaryota, and two species belonging to the phylum Streptophyta of Eukaryota. For the Bacteria domain, Micrococcus luteus (M. luteus) is outstandingly abundant, accounting for 58.88 ± 31.72% of the reads classified to the species level. This result is reasonable because M. luteus distributes widely in soil, water, dust and air, and it is part of the normal flora of human skin, mouth, mucosae, oropharynx and upper respiratory tract [24] . The rest of most abundant bacteria includes both gram-positive (Actinobacteria) and gramnegative (Proteobacteria) bacteria, which originate from a diverse range of environments: Escherichia coli, Sphingomonas, Pseudomonas putida and Variovorax paradoxus are frequently observed in soil and water; Cutibacterium acnes is a human skin-associated organism; Moraxella catarrhalis and Acinetobacter baumannii are common opportunistic pathogens. Ralstonia had been identified as a common contaminant of DNA extraction kits or PCR reagents [25] .

For the Eukaryota domain, the abundances of human-originated reads in H1 (10.26%) and H6 (7.93%) were higher than that in other samples (below 2.15%). They were possibly derived from personnel entered the rooms for equipment maintenance at that time. Additionally, H6 had a dramatically high percentage of mouse-originated reads (12.9%) compared to that in other samples (below 0.01%). This could be associated with the isolation of mouse lymphocytes at that location. Regarding plant-originated biological substances, Gossypium hirsutum, known as upland cotton and often present in clothes, had a much higher percentage in H8 (4.89%) than in other samples (below 0.05%, Figure 4F ), suggesting a greater impact of human activity on the operating bench in the BSL-1 room. The relative abundance of Populus trichocarpa, namely California poplar, was generally higher in the BSL-1 rooms with ordinary pressure system comparing with that in the BSL-2 rooms with negative pressure system (1.01 ± 1.32% vs 0.12 ± 0.27%, p = 0.17, Mann-Whitney test, Figure 4F ). This indicates that the environmental contamination could be affected by the season, considering that the samples were taken in April, when poplar blossomed in Beijing.

NGS technology is an effective approach for laboratory management and microbe monitoring [26] . In this study, we carried out a holistic assessment of the biological background in the environment of a microbiology laboratory. In general, the diversity of viral and bacterial sequences is positively associated with the frequency of laboratory activities in the room and on the bench.

We are more concerned about the distribution of viruses in this microbiology laboratory, where respiratory viruses-related experiments were performed regularly. Firstly, all the viral reads obtained from the environment samples were compared to the positive control. It showed the HCoV-229E reads dominant in the positive control was only detected in H1 and H4 samples with less than 10 reads. The two sampling sites were biosafety cabinet bench in Room 1 and 2, used for virus culture and clinical samples treating, indicating an occasional contamination from samples or nucleic acids. No other samples showed positive on HCoV 229E, which helped to ensure us the quality control of NGS procedure in this study. There are viral reads in the negative control (H0) mapping to CeHV-5, IFVA and flock house virus, which similar to the distribution in H7, the PCR hood used for adding nucleic acids templates, indicating a residual nucleic acids contamination. Viral reads related to the experiments performed in recent one month were also identified in these environment samples. In a summary, we considered these viral sequences were mainly from three parts. (1) Experimental activities. The obtained short viral reads are mainly mapped to the terminal gene region or the PCR primers targeting regions (see Figure 3 ). No complete viral genome was identified even in samples collected from viral isolation and culture sites. Moreover, the reads of human mastadenovirus, polyomavirus, enterovirus, and human betaherpesvirus 5, were found only in samples collected from bench, centrifuge rotor (H8, H9) in BSL-I and PCR hood (H7), correlated to the experiments performed in these sites. Of note, since RNA were preferentially amplified in our protocol, DNA contamination (e.g., PCR products) level should be underestimated, thus we are presenting a lower boundary of DNA contamination. The results indicated these viral reads were mainly from the gene amplification products or residual degraded nucleic acids. As the viral reads related to the viruses used in recent one month could be detected, the residual DNA fragments could help to trace experimental activities within a month. (2) Experimental materials. For example, the fetal bovine serum was confirmed to be contaminated by the bovine viral diarrhea virus [18] [19] [20] [21] . Besides that, the reverse transcriptase which derived from murine leukemia virus [27] [28] [29] , and the recombinant protein expression system which involves the use of viral vectors are potential sources of contamination [30] [31] [32] . (3) Environment. Reads of the tobacco mosaic virus and cucumber mosaic virus were found in most of the samples except negative control and the samples collected from biosafety cabinets in BSL-II (H1 and H4). As reported, the two kinds of viruses are widely distributed in water and soil [33, 34] . The hepatitis C virus and rotavirus, not used in the lab, were found in samples from the centrifuge rotor (H6) and the bench in BSL-I with less than ten reads. Whether these viral reads were from environment or not need to be investigated further.

The largest part of biomass in our data is contributed by bacteria, of which M. luteus is the most prevalent. All types of bacteria seem to be derived from the external environment [24] . Room 4 has the most abundant microbial species, as frequent molecular biological experiments and immunology related experiments occurred in this room. A large number of microbial sequences in all centrifuge rotors were found, which emphasized the necessity of routine clean and disinfection for this equipment. The gene fragments from exogenous microorganisms have a great impact on the deep sequencing results, especially for the clinical samples with low microbial biomass, such as cerebrospinal fluid, blood, and bronchoalveolar lavage fluid [25, [35] [36] [37] [38] . Application of the NGS technique on biosafety monitoring would enable us to disentangle the background noise presenting in the sequencing data, which in turn help to distinguish true microorganisms from false positives. Meanwhile, laboratory contamination should be suspected when obtaining the complete genome sequence of cultured viruses, or abnormally enrichment of specific microbes.

NGS method was capable of monitoring the nucleic acids contaminations from different sources in the lab, demonstrating its promising utility in monitoring and assessing the risk of potential laboratory contamination. The risk of contamination from reagents, remnant DNA and environment should be considered in data analysis and results interpretation.

Supplementary data to this article can be found online at https://doi. org/10.1016/j.bsheal.2019.02.003.

",0.7614842887942681
BMC Public Health Distributed data processing for public health surveillance,"Background: Many systems for routine public health surveillance rely on centralized collection of potentially identifiable, individual, identifiable personal health information (PHI) records. Although individual, identifiable patient records are essential for conditions for which there is mandated reporting, such as tuberculosis or sexually transmitted diseases, they are not routinely required for effective syndromic surveillance. Public concern about the routine collection of large quantities of PHI to support non-traditional public health functions may make alternative surveillance methods that do not rely on centralized identifiable PHI databases increasingly desirable.","Timely identification and subsequent reaction to a public health emergency requires routine collection of appropriate and accurate data about the occurrence and location of cases of illness. There is substantial interest in using rou-tinely collected electronic health records to support both the detection of unusual clusters of public health events and the response to public health threats detected by other means. Such data are also useful to reduce an initial alert level, if it is clear that no unusual illness clusters exist in a community. Ideally, such systems operate automatically and include sensitive and specific statistical surveillance software and alerting systems. These are often referred to as syndromic surveillance systems [1, 2] , because they typically rely on the non-specific signs and symptoms that may provide the earliest evidence of a serious public health threat, such as anthrax or SARS.

Many syndromic surveillance systems gather potentially identifiable, individual patient-level encounter records. These records are typically collected without name or address, but they do contain enough identifiers to allow re-identification in some circumstances. The potential for re-identification is greatest when records are collected from ambulatory settings or health systems that supply a unique identifier that allows the very useful identification of repeated visits over time. The risk of disclosing sensitive information that can be linked to the individual also increases when the health care facility provides more than occasional care.

In the United States, the Health Insurance Portability and Accountability Act [3] (HIPAA) specifically exempts transfer, use and retention of identifiable electronic personal health information (PHI) to support public health activities. This exemption also applies to syndromic surveillance activities, although HIPAA was developed before large volumes of such data concerning individuals who are not suspected of having a reportable condition were being used for public health purposes in the absence of any known public health emergency. Despite the exemption, data providers may be unwilling to offer identifiable data for surveillance purposes in the face of increasing awareness of the potential costs of inadvertent disclosure or inappropriate use of PHI. Additionally, their patients may object to their providing it. These concerns are common to many developed countries and under these circumstances, designs that minimise the risk of inadvertent disclosure may be needed in order to gain the cooperation of data custodians, for surveillance systems to be feasible. The focus of this paper is on one such design, in which initial data aggregation is performed to decrease the risk of any PHI being inadvertently disclosed, before the aggregate data is centralised for subsequent statistical analysis. Although the system we describe is currently operating in the United States and many of the implementation details are specific to that context, some of the conceptual issues we describe and some of the lessons we have learned may be directly relevant to public health practice in other countries.

While it is possible to centrally collate and process deidentified records, there is a potential problem with statistical inference if multiple records from the same individual are not distinguished. This problem arises because many statistical analysis techniques applicable to surveillance, such as Generalised Linear Mixed Models [4] (GLMM), depend on the assumption that observations are statistically independent. Inference based on this assumption using ambulatory care encounter data will likely be biased if the model cannot distinguish observations from multiple encounters during a single course of illness from a single individual patient. Although the extent of this bias has not been quantified, the problem is clearly illustrated by real data. In more than half of the individuals with multiple lower respiratory syndrome encounters over a four year period from one large ambulatory care practice, a second encounter with the same syndrome was noted less than 21 days after the first encounter [1] . Our approach to this problem of statistical independence is to aggregate multiple encounters from a single individual into ""episodes"" of illness, and is described in more detail below. Reliably automating this aggregation requires that every patient's records be uniquely identifiable.

To support the National Demonstration Bioterrorism Surveillance Program (NDP), we developed a system in which no PHI leaves the immediate control of the data provider, and only aggregate data is transferred to the datacenter [2, 5] . Each data provider performs initial aggregation of the PHI within their own existing, secured data processing environment, producing data that is aggregated beyond the point where any individual patient is identifiable. Since data processing is distributed to the site of data collection rather than being performed at one central location, we describe this as a distributed processing surveillance system. Although this particular aspect of our work has briefly been mentioned in previous publications [1, 2, [4] [5] [6] , we present it in greater detail here, because we believe that it represents a potentially valuable alternative surveillance system design option that deserves more explanation and wider debate than it has received to date.

The basic principle of distributed processing is simple. Rather than collecting all needed identifiable, individual PHI records centrally for statistical processing, all PHI is pre-processed remotely, and remains secured, under the direct control of the data provider. Only aggregate data are transferred to the central datacenter for additional statistical processing, signal detection, display and distribution. At an appropriate level of aggregation, the risk of inadvertent PHI disclosure becomes very small, and may prove acceptable to data custodians and to individual patients. Although this risk is never completely absent, it is certainly decreased in aggregate data, making this approach far more acceptable to data providers in our experience, than the more traditional approach of centralized collection of directly identifiable PHI.

Before describing our distributed system, we briefly review the more familiar model of centralized aggregation and processing of PHI for surveillance. In the more traditional type of system, individual patient records, often containing potentially identifiable information, such as date of birth and exact or approximate home address, are transferred, usually in electronic form, preferably through some secured method, to a central secured repository, where statistical tools can be used to develop and refine surveillance procedures. One of the main benefits of this data-processing model is that the software and statistical methods can be changed relatively easily to accommodate changes in requirements, because they only need to be changed at the one central location where analysis is taking place. As long as appropriate details have been captured for each individual encounter of interest, the raw data can be re-coded or manipulated in different ways. Only one suite of analysis code is needed, and because it is maintained at a single, central location, costs for upgrading and maintenance are small. Inadvertent disclosure of PHI is always a potential risk with centralized systems. Even where minimally identifiable data are stored in each record, the probability of being able to unambiguously identify an individual increases as multiple, potentially linkable records for that individual accrue over time.

Rather than gathering identifiable PHI information into a central repository for analysis, a distributed system moves some of the initial data processing, such as counting aggregated episodes of care (see below), to the site where the data is being collected. This aggregation minimizes the number of individuals who have access to PHI and diminishes the risk of inadvertent PHI disclosure from the sur-Distributed processing model and data flow Figure 1 Distributed processing model and data flow.

veillance system, while still allowing effective use of the information of interest. The focus of this report is on the model used to collect surveillance data while providing maximum protection for PHI, so the statistical methods we use in the NDP, which have been described elsewhere [4] are not discussed further here.

Data flows for the NDP are illustrated in Figure 1 . Data pre-processing, detection of repeated visits by the same patient for the same syndrome, and data aggregation is performed using a custom software package, written, maintained, and distributed by the NDP datacenter. Data providers maintain complete control of the security of their own PHI and also maintain control over the operation of the data processing software, which runs on one of their secured workstations. Since the pre-processing takes place within a secured environment under the control of the data provider, there is no need for the individual patient identifiers to be divulged to the datacenter. In the case of the NDP [5] , the only data that is centrally collated consists of counts of the number of new episodes of specific syndromes over a defined time period (currently set at each 24 hour period ending at midnight), by geographic area (currently, 5-digit zip code area). More detailed definitions of ""syndromes"" and ""new episodes"" are provided below. Table 1 illustrates the data transferred from each data provider each day to the datacenter for statistical processing, reporting and alerting. Note that the although this data does not contain any obvious identifiers such as date of birth or gender, there is always a risk that a specific individual might be identifiable using additional data, and that this risk is greatest in zip codes with very small populations.

All source code required to build the data processing software is provided to the data provider at installation and whenever the software is updated, so that the local information services staff can check that there are no ""backdoors"" or other ways the distributed software could compromise the security of their systems. All information transferred to the datacenter is stored in text files (in XML format) and can be readily accessed by local staff to ensure that no PHI is being transmitted.

Participating data providers have near real-time ICD9 codes for every encounter, usually assigned by clinicians at the time of the encounter. Since much acute infectious disease manifests as broad suites of nonspecific symptoms, we monitor 13 syndromes -Respiratory, Lower gastro-intestinal (GI), Upper GI, Neurological, Botulism-like, Fever, Hemorrhagic, Skin lesions, Lymphatic, Rash, Shock-death, Influenza-like illness and SARS-like illness. All syndromes except Influenza-like illness and SARS-like illness were defined by a working group led by CDC and Department of Defense [7] . Individual ICD9 codes are used to aggregate encounters into one of these 13 syndromes. The definitions (ICD9 code lists) of 11 of these syndromes are available [7] . The definitions comprising the other two syndromes were developed in consultation with both CDC and the Massachusetts Department of Public Health.

Our surveillance algorithms [4] require statistically independent observations and are based on new episodes of syndromes. Our goal was to distinguish health care encounters that were related to ongoing care for any given episode of acute illness from the initial encounter that indicated the start of a new episode of a syndrome of interest. The derivation of the specific method for identifying first encounters for an episode of illness has been described in more detail elsewhere [1] . We define a new episode to begin at the first encounter after at least a 42day encounter-free interval for that specific patient and that specific syndrome. If there has been any encounter for that specific syndrome for the same individual patient within the previous 42 days, the current encounter is regarded as part of the usual ongoing care for the original encounter that signalled the start of an episode of illness of that syndrome. The start of a new episode for a different syndrome can occur during ongoing encounters for any given specific syndrome -ongoing encounters during an episode are counted as new episodes only if they are outside (i.e. at least 42 days since the last encounter)of an existing episode of the matching syndrome. As will be described later, all ongoing encounters within any syndrome are recorded, and are visible through reports under the control of the data provider, but they do not contribute to the counts that are sent to the datacentre for analysis. All of this processing requires consistent and unique patient identifiers for all encounters. We use the local patient master index record number for this purpose in the software that we provide, but these identifiers are not required once the processing is complete, and they remain under the complete control of the providers.

The distributed software requires the data providers to extract information about encounters of interest (daily, in our case) and convert it into the uniform format used by our distributed software. This kind of uniform representation is required for any multi-source surveillance system and is not peculiar to the distributed model we have adopted. In practice, we found that data providers could easily produce text files containing data as comma separated values in the format which we specified, and which the distributed software has been written to process. However, this requires dedicated programming effort that was supported with resources from the NDP grant.

Our project receives support from the CDC, so we are required to comply with relevant CDC standards.

Although the data being transferred to the datacenter is arguably not identifiable PHI because of the high level of aggregation, we use the Public Health Information Network Messaging System [8] (PHINMS), a freely available, secure, data transfer software suite developed by the CDC, to transfer aggregate data. A PHINMS server operates at the datacenter and each data provider operates a PHINMS client, using a security certificate supplied by the datacenter for encryption and authentication. PHINMS allows fully automated operation at both the datacenter and at each data provider. PHINMS communicates over an encrypted channel and usually requires no special modification to the data provider firewall, since it is only ever initiated by an outgoing request (the data provider always initiates the transfer of new data) and uses the same firewall port and protocol (SSL on port 443) as commercially encrypted services such as internet banking. PHINMS is reasonably robust to temporary connectivity problems, as it will try to resend all messages in the queue until they are delivered. Data transmission is one of the least problematic aspects of maintaining this system. We provide automatic installation software and it runs more or less instantaneously and transparently, without intervention in our experience. No training is needed as the process is fully automated.

All data is transferred to the datacenter in the form of eXtensible Markup Language (XML) since this is a flexible machine-readable representation and is easy to integrate with PHINMS. We used the Python [9] language for the development of the distributed software package. This choice was partially motivated by the fact that Python is an open-source language and thus freely distributable, partly by our very positive experience with Python as a general purpose application development language, and partially because in our experience, Python can be installed, and applications reliably run without any change to source code, on all common operating systems (including Linux, Unix, Macintosh and Windows), making it easy for the datacenter to provide support for systems other than Windows PC's. It is also a language with extensive support for standards such as XML, and securely encrypted internet connections. In addition, our existing web infrastructure has been built with the open-source Zope [10] web application framework, which is written mostly in Python.

A major design goal for our distributed software was that it should offer potentially useful functions for the data provider. This was motivated by our desire to encourage data providers to look at their own data in different ways that might not only help them manage the data more efficiently, but might also help them to more easily identify errors. In our experience, the task of maintaining a system like the one we have developed is far more attractive and interesting to the staff responsible at each participating institution if they gain some tangible, useful and immediate benefits. In addition, easy access to data flowing through our software is useful for ensuring transparency and to facilitate security auditing by each data provider.

The distributed software optionally creates reports that show one line of detailed information about each of the patient encounters that was counted for the aggregate data for each day's processing. These reports are termed ""line lists"" and were designed to support detailed reporting of encounter level data, so that a data provider can quickly make this information available in response to a public health need. Two versions are available, one with and one without the most specific identifying details, such as patient name and address. These standard line lists are used most often to support requests by public health agencies for additional information about the individual cases that contribute to clusters identified in the aggregate data. These lists are never transmitted to the datacenter but may be used to support public health officials investigating a potential event.

When unexpectedly high counts of particular syndromes are detected in geographically defined areas, the datacenter automatically generates electronic alerts, which are automatically routed to appropriate public health authorities. For example, in Massachusetts, electronic messages are automatically sent to the Massachusetts Alert Network within minutes of detection, where they are automatically and immediately forwarded to the appropriate Public Health personnel for follow up. Available alert delivery methods in the Massachusetts system range from email through to an automated telephone text-to-speech delivery system. Responders can configure the alert delivery method for each type of alert they have subscribed to. This alerting system is independent of our distributed system, but in practice, the ready availability of reports in electronic format containing both fully and partially identifiable clinical data for all cases comprising any particular period or syndrome makes the task of the clinical responder much simpler whenever a query is received from a public health official. Electronic reports, containing clinical information and optionally, full identifiers for all encounters can be generated as required, at the provider's site, from where they can immediately be made available to public health agencies. In the NDP's current operational mode (see Figure 1 ), a public health official calls a designated clinical responder to obtain this information. Table 2 ). The ""narrow"" version, which contains fewer identifiers, provides each patient's five-year age group instead of date of birth and does not include the physician ID or medical record number (Table 3) . At the provider's discretion, the clinical responder can provide the ""narrow"" list corresponding to the cases of interest to the public health department. If on this basis public health officials decide that further investigation is warranted, they can call the clinical provider and request a review of medical records, identifying the cases of interest by date and an index number (unique within date) in the narrow line list. The clinician finds the medical record number by looking up the date and index number in the wide line list and then accesses the record itself through the usual HMO-specific means. Resources to support clinical responders were provided through our NDP grant to participating data providers.

It would be straightforward to send detailed lists of encounters that are part of clusters directly to the relevant health department whenever the datacenter detects an event and sends an automated alert to a health department. We have not implemented this feature because all the participating health plans prefer to have an on-site clinical responder participate in the initial case evaluation with the public health agency. It would also be simple to allow designated public health personnel to initiate requests for specific line lists, even when no alert has occurred. Public health officials may, on occasion, wish to inspect the line lists to search for specific diagnoses that do not occur frequently enough to trigger an alert for their syndrome, but may be meaningful in the context of information that arises from other sources.

Although not currently implemented in the NDP, it would be feasible to allow a remote user to perform adhoc queries on the encounter data maintained by the health plan. Examples of these queries include focused assessment of disease conditions affecting subsets of the population or specific diagnoses. This type of direct query capability is currently used at some of the same participating health plans to support the CDC's Vaccine Safety Datalink project [11], a surveillance system that supports post-marketing surveillance of vaccine safety [12] .

This distributed data model supports active surveillance and alerting of public health agencies in five states with 7 participating data providers. The system has proven to be workable, and it supports the syndromic surveillance needs of the participating health departments. There are fixed costs such as programming to produce the standard input files, installation and training, associated with adding each new data provider, so we have focussed our efforts on large group practices providing ambulatory care with substantial daily volumes of encounters, completely paperless electronic medical record systems, and substantial technical resources, since these enable us to capture large volumes of transactions with each installation. Relatively large numbers of encounters are needed to ensure that estimates from statistical modelling are robust. Applying a distributed architecture to surveillance from multiple smaller practices may enable appropriately large numbers of encounters to be gathered, but may prove infeasible because of costs and lack of appropriate internal technical support and because of heterogeneity in the way ICD9 codes are recorded and assigned by each data provider. Once the programming for standard input files is completed, installation and training take approximately one day total, usually spread out over the first two weeks. Nearly all problems are related to providers getting the standard file format contents exactly right, and to transferring these to the

The distributed architecture currently in use by the NDP allows clinical facilities to provide the aggregated information needed to support rapid and efficient syndromic surveillance, while maintaining control over the identifiable PHI and clinical data that supports this surveillance.

The system provides support for the clinical providers to respond quickly to public health requests for detailed information when this is needed. In our experience, such requests involve only a tiny fraction of the data that would be transferred in a centralized surveillance model, providing adequate support for public health with minimal risk of inadvertent disclosure of identifiable PHI.

We believe this design, in which patients' clinical data remains with their own provider under most circumstances, while public health needs are still effectively met, conforms to the public's expectations, and so will be easier to justify if these surveillance systems come under public scrutiny. Many of the details of our approach are specific to the United States context, but the general principle of using distributed processing to minimise the risk of inadvertent PHI disclosure is of potential utility in other developed countries, although the specifics of our implementation may be less useful.

The benefit of decreased risk of inadvertent PHI disclosure from our approach entails three principal disadvantages compared with routine, centralized collection of identifiable data. First, a clinical responder with access to the locally stored PHI data must be available to provide case level information when a cluster is detected. It would be technically straightforward to provide detailed information for relevant cases automatically when signals are detected. We deliberately did not implement this feature in the current system, since the participating health plans expressed a strong preference for direct involvement in this process.

The second disadvantage is the need to pre-specify the syndromes, age groups, and other data aggregation parameters in advance, since changing these requires the distribution of a new release of the aggregation software.

In practice, we have addressed this by means of configura-Distributed software screen, showing results (synthetic data) after daily processing of encounter records Figure 2 Distributed software screen, showing results (synthetic data) after daily processing of encounter records.

tion data for syndrome categories read from a text file as the application loads, so the application code itself does not need alteration. This limitation could be largely overcome by creating a remote query capability to support ad hoc queries on identifiable data that remains in the control of the provider.

The third disadvantage is the technical challenge of maintaining distributed software that must reliably process data that the programmers are not permitted to examine. While the software can be exhaustively tested on synthesized data, we have occasionally encountered subtle problems arising from previously unnoticed errors in the input data. Our experience suggests that when writing this kind of distributed application, extensive effort must be devoted to detecting and clearly reporting errors in the input data before any processing takes place.

An archive of Python source code for the distributed software will be made available by the corresponding author upon request. Unfortunately no resources are available to provide technical or other support outside the NDP.

In summary, we have implemented a near real-time syndromic surveillance system that includes automated detection and reporting to public health agencies of clusters of illness that meet pre-specified criteria for unusualness [5] . This system uses a distributed architecture that allows the participating health care provider to maintain full control over potentially identifiable PHI and health encounter data. The distributed software loads simple text files that can be created from the data stored in virtually any proprietary EMR system. It sends summary data suitable for signal detection algorithms via a freely available messaging system, to a datacenter that can manipulate the aggregated information and combine it with data from other providers serving the same geographic region, and which automatically generates and sends alerts when unusual clusters of syndromes are identified. The distributed software also facilitates efficient access to fully identified patient information when needed for following up a potential event.

",0.7611366816313694
Investigating public health emergency response information system initiatives in China,"Infectious diseases pose a great danger to public health internationally. The outbreak of SARS has exposed China's fragile public health system and its limited ability to detect and respond to emergencies in a timely and effective manner. In order to strengthen its capability of responding to future public health emergencies, China is developing a public health emergency response information system (PHERIS) to facilitate disease surveillance, detection, reporting, and response. The purpose of this study is to investigate the ongoing development of China's PHERIS. This paper analyzes the problems of China's existing public health system and describes the design and functionalities of PHERIS from both technical and managerial aspects.","Recent years have witnessed a number of microbial threats which have greatly jeopardized public health in many areas of the world. One of the most publicized cases is severe acute respiratory syndrome (SARS) whose outbreak and rapid spread in more than 25 countries in early 2003, claimed numerous lives and caused tremendous economic losses. Other public health threats include the recent anthrax attack and outbreak of avian influenza in the United States. It is predicted that new pathogens, originating either naturally or from bioterrorism, will continue to emerge and cause new public health emergencies [1] . In order to contain the spread of infectious diseases, preparedness planning, a vigilant health system, strengthened domestic, and global disease surveil-lance, and strong interdisciplinary partnerships are needed. Admittedly, information technology which facilitates access to critical information at the opportune moment will play a strategic role in improving the delivery and effectiveness of quality patient care, and helping domestic and international communities to adequately respond to medical emergencies in a timely and efficient manner.

After the first patient at China's Guangdong province was infected by SARS, this strong contagious disease was transmitted to health care workers and family members in Hong Kong, Vietnam, Singapore, and Canada within days, providing a vivid illustration of the rapidity and ease with which infectious disease can spread to pose a global public health threat [1] . Therefore, response to public health emergencies should be regarded as a global issue and a close worldwide co-ordination is needed. Particular attention should be paid to developing countries that have relatively fewer resources to deal with public health emergencies. China was one of the countries that were the most affected by SARS. Among the 8445 cases re-ported to the World Health Organization (WHO), 5327 (63%) were from China [2] . Exploiting many effective control measures, China was able to successfully deter the outbreak in a relatively short time [3] . Nonetheless, the rampancy of SARS in the early stage of the outbreak has resulted in a chaos and exposed the fragility of China's public health information systems. As Qiang Gao, the secretary of the Ministry of Health (MOH), admitted [4] :

In terms of our work, MOH was not well prepared against the sudden public health hazards and its epidemic prevention system was relatively weak. Following the outbreak, the Ministry failed to institute a timely and unified mechanism for collecting, processing and reporting the relevant information nationwide and it did not give out clear cut instructions or effective guidance. . . . Due to a lack of a sound system of information collection, surveillance reporting and contact tracing there have appeared major weaknesses in statistics compilation on the epidemic on the part of the relevant agencies and accurate figures have not been reported to higher authorities in a timely manner.

Realizing the weakness of its public health systems, China is undertaking tremendous efforts to improve its public health system by leveraging the power of information technology. The Chinese central and local governments have appropriated sizable financial resources to build a public health emergency response information system (PHERIS). The purpose of this paper is to investigate the ongoing development of China's PHERIS. The paper reviews the literature regarding emergency response information systems, analyzes the weakness of China's old public health system, and describes the design and functionalities of PHERIS from both technical and managerial aspects.

A large portion of past research and development on emergency response systems has been conducted in the United States mostly because of the past and potential future terrorist attacks [5] [6] [7] [8] . In order to identify the relevant literature, we searched research databases including PubMed, IEEE Explore, and ACM Digital Library. The search was performed, using different combination of keywords, such as ''public health'', ''emergency system'', ''emergency response'', ''emergency response systems'', ''surveillance'', and ''bioterrorism''. We also collected information from the official websites of organizations, including the Centers of Disease Control and Prevention (CDC), the Agency for Healthcare Research and Quality (AHRQ), and the Health and Human Services (HHS), which have been sponsoring emergency response information systems initiatives. In this section we will review some major IT development and research initiatives that have been completed or are currently undergoing in the US. It should be noted that this review does not represent a comprehensive coverage of the entire body of research and practice relating to public health emergency information systems, as we only selected those that we believed are most relevant to the topic of this article.

Motivated by a series of naturally occurring and manmade disasters, federal and state governments, as well as private companies, have implemented numerous initiatives to create co-ordinated systems for surveillance, detection, and response. Given the wide recognition that the application of information technology (IT) provides previously unfathomed opportunities to improve public health practice [9] [10] [11] , many of these initiatives have sought the exploitation of IT to resolve public health threats. According to the Agency for Healthcare Research and Quality (AHRQ), there exist 217 information technologies and decision support systems of potential use in the event of a bioterrorist attack or other public health emergencies involving naturally occurring epidemics [12] . One of the most important applications of IT is public health surveillance which encompasses the ongoing systematic collection, analysis, and interpretation of health-related data for use in planning, implementing, and evaluating public health practice [13, 14] . Some example surveillance systems include a national retail data monitor which was designed to collect and analyze sales of over-the-counter health products to detect outbreak of disease [15] , a clinical data warehouse which was used for hospital infection control [16] , a data mining surveillance system which can provide sophisticated capability to control hospital infection and public health [17] , and a real-time outbreak and disaster surveillance system which was utilized for early detection of disease outbreaks [18] .

In order to integrate isolated information systems which support communications for public health labs, the clinical community, and state and local health departments and to capitalize on the potential for a cross-fertilization of data exchange, some national level projects have been implemented in the US for early detection of public health threats. The Public Health Information Network (PHIN) is a framework created by the Centers of Disease Control and Prevention (CDC). Based on defined data and vocabulary standards and strong collaborative relationships, the PHIN will enable consistent exchange of response, health, and disease tracking data between public health partners. The PHIN is composed of five key components: detection and monitoring, data analysis, knowledge management, alerting, and response. Other federal initiatives include: (1) the National Electronic Disease Surveillance System (NEDSS) which is an initiative that promotes the use of data and information system standards to advance the development of efficient, integrated, and interoperable surveillance systems at federal, state and local levels [19] , (2) the BioNet which is a national network of public health laboratories and federal food regulatory agencies helping the CDC to rapidly detect and determine possible links between disease agents during terrorist attacks [20] , and (3) the National Health Information Infrastructure (NHII) which is an initiative set forth to improve the effectiveness, efficiency and overall quality of health and health care in the US, a comprehensive knowledge-based network of interoperable systems, and the set of technologies, standards, applications, systems, values, and laws that support all facets of individual health, health care, and public health [21] . The commonalties of these national level initiatives are the focus on data integration and standardization which intends to facilitate the information sharing among various healthcare partners and enables early detection of public health threats.

Prior research has addressed various issues related to the design of public health emergency response information systems. An integrated disease surveillance framework has been proposed for rapidly detecting, tracking, and managing public health threats [22] , which contains six functions: data collection, detection, alert and early warning, resource planning, response assessment and evaluation, and investigation and modeling. Another public health emergency response system was proposed as a decentralized model which gives states responsibility for collecting and disseminating all necessary information and co-ordinating surveillance and response [23] . According to this model, systems in different states must be compatible on the basis of common data standards so that the federal government can oversee a national network, and each state should cope with emergencies at a micro-level. In addition, some researchers contended that state and local health departments should build dual-or multiple-use public health information infrastructure [24] , suggesting that the public health information system should be able to respond and detect not only bioterrorism, but also other infectious disease outbreaks, chemical spills or leakages, food and water contamination scenarios, and animal disease outbreaks.

Based on experiences of the ''Emergency Management Information System and Reference Index'' (EMISARI) at the Office of Emergency Preparedness (OEP), Turoff [25] asserts that ''an emergency system must be viewed as a structured group communication system where the protocols and communication structures are provided, but there is little content about a particular crisis situation except as an integrated electronic library of external data and information sources'' (p. 30). As a consequence, the design of emergency information systems should focus on the group communication process by accommodating how humans collect, contribute, and utilize data in a time-urgent manner [25] . Given that Turoff's remark is about generic emergency response information system, it is applicable to public health emergency response. It suggests that public health emergency response information systems can also be viewed as structured group communication systems. Turoff et al. [26] have articulated five criteria for the design of such a group communication system: metaphors, roles, notifications, context visibility, and hypertext. Metaphors, which pertain to the interface design, are the mental models of the system that help users to create a cognitive map between task objectives and interface elements. Roles reflect various parties' responsibilities in an event, and this concept should be built into the design of the group communication system by offering specific privileges and tools to different users. Notifications are relevant alerts to a user of change in status, data, or other information. The group communication system should raise appropriate notifications when events or actions of other users take place to cause the change. Context visibility refers to the idea that the components of the meaningful data objects are presented in a context that relates to the understandings of the user, that is, when the user activates a certain object the system should be able to know the user's intention and provide a list of relevant action choices. Hypertext refers to ''the possibility of multiple two way linkages with semantic meanings that allowed a person to utilize any item in the content of the application as a set of menu alternatives to move to another content or functionalities in the interface'' [26] . The criteria recommended by Turoff et al. assume the existence of communications networks and computing technologies and focuses on the software and interface design features.

The previous studies have provided insights into public health emergency systems. However, they are almost solely based on experiences in the U.S. Few papers examine the initiatives in public health emergency systems in developing countries. Given the important role China plays in the global public health arena, an investigation of China's public health emergency information systems will significantly contribute to the existing knowledge of public health emergency response.

Given the complexity of China' PHERIS and the lack of previous research on this topic, a case research method was employed by this study. Interviews with two professors at Health Science Center of Beijing University, three healthcare informatics experts, and six healthcare IT practitioners were conducted to collect data of the development of China's PHERIS. Additional documents were reviewed to retrieve pertinent information. These documents include government IT policies and regulations, meeting minutes, reports, development plans, media reports, and news. The credibility of the governmental documents is guaranteed, since they were either obtained from government officials or downloaded from official government websites. The quality of media reports and news is likely to be questionable. Nevertheless, this is not considered a problem, since the triangulation method was employed in this study. Triangulation is a widely accepted case research method which requires collecting data from multiple information sources and posits that multi-source data will corroborate each other and increase validity [27] . Using the triangulation method, we only included in this article the information that appeared consistently in multiple sources and discarded the conflicting information. In addition, the manuscript of this paper was reviewed by a leading health informatics researcher in China who has worked with China's central government on major public health information technology initiatives, including the PHERIS project. His review has largely ensured the validity of this paper by verifying the information gathered from interviews and non-governmental sources, such as media reports and news. The rest of this paper is mainly based on the analysis of the data we collected from interviews and documents.

China's public health information system (PHIS) has a hierarchical structure consisting of four layers: county layer, city layer, province layer, and cen-tral government layer. The primary objectives of the system are disease surveillance and monitoring. Major public health data collected by the system include epidemic report, disease surveillance, causes of death statistics, food hygiene, environment hygiene, school hygiene, women and children hygiene, profession hygiene, and public health resources allocation. The outbreak and spread of SARS have uncovered five deficiencies inherent in the PHIS, which are described as follows.

The disease reporting and surveillance have not been carried out in a timely manner. Due to the hierarchical structure of China's PHIS, disease surveillance data are reported by following a bottom-up path and the reporting is performed on a 10-day or monthly basis. Despite the establishment of a virtual private network in 2000 for electronically transmitting disease surveillance data among the four layers of the PHIS, communication between the PHIS and the sites where source public health data are collected is carried out through snail mail or telephone. In addition, surveillance reports have to be approved by relevant officials at each layer before they can move up to the next layer. As a result, the reporting cycle time is prolonged and local officials' interference could affect the accuracy of the reports. During the SARS event, it took 8-9 days on average to report a SARS case from the patient's location to the MOH, and 3-4 days to report a diagnosed SARS case from hospitals to the MOH. The delayed reporting has largely contributed to the rampancy of SARS in China.

The coverage of the PHIS is relatively small. The network of the PHIS comprises a variety of centers of disease control and prevention (CDC) which are at the county level and levels above. Although, these organizations are able to shape a national net on which public health information can be exchanged in real time, the mesh size of this net is too big to catch timely disease information. Health care organizations at the grassroots level are usually the sites where public health emergency occurs, however, these organizations are not part of the PHIS network.

The information flow in care providing organizations is inefficient. Should a public health emergency occurs, it is critical to gather accurate information regarding exploitable medical resources, such as hospital beds available, transportation capability, physicians available, care giving capability, medi-cal devices and equipments, medications, and preventive equipments, so that the emergency situation can be managed to the maximum. Due to the inefficient inter-organizational information sharing in China's hospitals as well as the inefficient communication between hospitals and disease control organizations, the admission and transfer of SARS patients during the SARS event was chaotic.

The public health regulatory system is insufficient. In China, only a few regions have established regulations to mandate and monitor the development and use of public health information systems. There are no unified regulations at the national level to enforce the exploitation of IS in disease surveillance and public health.

There is no standardized national platform for public health information exchange. China did recognize the importance of standard building, but little progress has been made on the development of health data standards, coding schemes, and communication protocols. Although, information systems have been widely used in the health care providers, disease control and prevention organizations, and health management authorities, the implementation initiatives are largely piecemeal, and lack a unified framework. As a consequence, health information is confined inside boundaries of these entities and effective health information sharing could not be easily realized due to the nonexistence of a standardized platform.

In order to be able to better manage future public health emergencies, China's central government has called for the establishment of a public health emergency information system (PHERIS) within 3 years. The PHERIS development project encompasses three phases. The first phase, which ended at the end of 2003, mainly aimed to meet the requirements of dealing with SARS and to build the foundation for the future development of public health information systems. During this phase, CDCs and health care organizations at the county level or above and some health care organizations at the village level have deployed SARS reporting systems and established SARS clinical examination and alert systems. The SARS reporting system is a secure Web-based application which allows users to report SARS cases electronically to a central database. In addition, the disease surveillance system and the command center were designed in this phase and preparations were made to roll out these systems. The second phase is planned to finish at the end of 2004. One of the major objectives in this phase is to build a national computer network infrastructure to support public health data exchange. A five-level computer network will be constructed to connect health administrative agencies, CDCs, and health care organizations at the national, province, city, county, and village levels. Another major objective of the second phase is to complete the development of the disease surveillance system so that the online reporting system not only covers SARS but all types of public health emergencies. Moreover, the second phase also intends to develop some national public health databases which can be used as knowledge bases for healthcare providers when they encounter public health emergencies, and to start and partially complete the development of the command center. The third phase is planned to complete at the end of 2005. The objective of this phase is to complete all the tasks of the development of PHERIS and realize information sharing and co-ordination among relevant organizations so that the public health emergency can be handled efficiently and effectively.

According to the MOH, public health emergencies refer to events that occur unexpectedly and can cause or potentially cause mass destruction to the public health. The Public Health Emergency Response Regulations defined four categories of such events: (1) serious epidemics, (2) mass diseases with unknown causes, (3) large scope food or profession poisoning, and (4) other events that can severely affect public health, for example, leaking of infectious bacteria from laboratories. The public health alert system in China has three levels: yellow alert, orange alert, and red alert. Fig. 1 depicts the structure of this alert system. Two levels of public health emergency response plans are triggered by emergency alerts: the provincial and the national. If the public health emergency is confined within a certain region of a province, the provincial plan will be appropriate. If the public health emergency is diffused across provinces and endangers the national public health, the national plan will be needed. Specifically, in the event of a yellow alert, the provincial public health emergency response will become ready; in the event of an orange alert, a provincial public health emergency response will be started or the national emergency response will be ready; in the event of a red alert, the national emergency response plan will be activated. The national public health emergency response plan signifies the highest level of public health threat, and China responds to this type of crisis by gathering relevant departments in the State Council and military to form a national emergency command center. One of the leaders of the State Council, that is, the premier or a vice premier, will be appointed as the commander to direct actions resolving the emergency. The command center co-ordinates and allocates national resources, makes major deci-sions, determines outward statements in press conferences, and possibly resorts to police and military forces. On the other hand, the provincial public health emergency plan is determined by the provincial government. A command center with the provincial government leader as the commander will be organized to direct emergency response activities. The difference between the national plan and the provincial plan is mainly the scope of the response activities. In addition, it should be noted that when a public health emergency response plan becomes ready, it does not necessarily have to be activated. In this scenario, the major task would be to continue carrying out disease surveillance and epidemiological investigation to examine how the emergency develops. Related stakeholders are informed to make preparations and some resources might be allocated. But if the emergency shows no sign of escalation or is controlled before it escalates, the alert would be cancelled. The major responsibilities of the MOH are supervising and co-ordinating a wide range of activities relating to the emergency, such as organizing public health surveillance and reporting, applying for approval to start the national public health emergency response plan from the State Council, organizing and co-ordinating professional organizations to conduct field investigations, treating infected patients, tracing, sterilizing, or quarantining people who were in contact with infected patients, developing technological standards, declaring new infectious disease as legal infectious disease according to pertinent laws (only legal infectious diseases can trigger public health emergency alerts), and 

The fundamental technical features of PHERIS are described from two aspects: data and network. Six databases need to be created to support the functioning of PHERIS: (1) geographic and natural environment database, (2) national socioeconomics database, (3) national public health risk factor database, (4) national morbidity and mortality database, (5) national emergency resources database, and (6) natural disaster database. Table 1 shows the content of each database. The data collected for PHERIS are heterogeneous. They are of a wide range of types, such as socioeconomics, humanity, hydrology, meteorology, environment, natural disaster, and health care. They could be numbers, texts, figures or pictures, and they could be real time data or historical data. Data from various sources will be integrated under the Microsoft .NET framework. From the network aspect, a ''five-layer and three-level'' network structure will be developed as the underlying infrastructure for PHERIS. The five layers refer to CDCs, health administrative agencies, and healthcare organizations at the levels of: (1) villages and towns, (2) counties and districts, (3) prefectures and cities, (4) provinces, and (5) nation. They are connected to the national public data network by broadband fiber lines, digital microwave, satellite communications, modem, and public telephone systems, thus forming a public health information virtual environment. The choice of communication method depends on the financial capability of different areas. The three levels refer to the establishment of major public health intranets at the city (or prefecture), province, and national level. A hierarchical star topology results from this network structure, where many villages and counties connecting to one city center, many cities connecting to one provincial center, and many provinces connecting to the national center. Fig. 2 depicts the topology. In addition, the provincial and city intranets provide network services and data services to public health related local organizations and form various community public health information networks.

Conceptually, PHERIS can be viewed as a composition of four major components: (1) a surveillance system, (2) a command center, (3) an action system, and (4) a supporting system. Fig. 3 illustrates relationships among these four components and their functional characteristics. The surveillance system captures public health emergencies and reports to the command center which assesses the emergency, raises alert, and makes decisions to respond to the emergency. The action system obtains orders from the command center and carries out the emergency response. The supporting system serves as a compulsory force to ensure all the activities of public health emergency response are undertaken properly. Lessons learned during and after a public health emergency will be incorporated into the knowledge body of improving PHERIS and reflected in future emergency detection and response.

The objective of the surveillance system is to replace the periodic manual reporting with online reporting to meet the requirements of emergency alert and response. A standard national public health emergency reporting platform with an underlying central database was implemented. Starting from 2004, authorized reporting individuals and organizations can perform online reporting, and health administrative agencies and CDCs will be able to download reports of local epidemic events and public emergencies in real time. In order to facilitate public health surveillance, CDCs at the city and county levels will develop infectious disease databases and public health risk factor databases. Regular updates will be made to these databases on the basis of surveillance reports and epidemiological investigations. Given that many Chinese health care workers lack the ability to use computers, personnel training will be carried out along with the development of the surveillance system. The surveillance system is primarily an innovation of online reporting. The implementation of this innovation radically reengineered China's former public health surveillance process by removing many intermediate stops that the public health emergency data had to go through, thus resulting in a much more efficient process.

The command center integrates public health, disease surveillance, medical care, and health regulation information by using the network platform. It intends to utilize the advanced information technology to provide critical information to a central commander team so that appropriate decisions can be made to deal with public health emergencies. The system will monitor the entire process of the emergency and has such functionalities as data collection, crisis determination, decision support, command, deployment, real time communication, and onsite support. The purpose is to make the most appropriate response to the emergency in the shortest time so that available resources can be effectively and efficiently allocated and exploited. The systems in command center encompass three application platforms and seven subsystems. The three application platforms are information platform, professional service platform, and decision making platform. The seven subsystems include database subsystem, geographic information subsystem, remote monitor subsystem, analysis and prediction subsystem, virtual reality subsystem, decision support subsystem, and search engine subsystem. Since, the command center is the central component of PHERIS, the three platforms and seven subsystems are described in more details in the following two paragraphs.

The information platform consists of the development of computer software and hardware, telecommunications infrastructure, data collection system, and data warehouses. The data collection system utilizes various technologies to capture images, graphs, statistical data, professional data, public health surveillance data, and textual data. Data storage technologies include both centralized and distributed database management models. The professional service platform is the core of the command center. Employing simulation techniques, this platform builds models to analyze the spreading processes of various infectious diseases. This exercise is largely based on the disease model database, the methodology database, and the public health knowledge base. The decision making platform is the presentation layer of the command center. All the activities involved in the public health emergency response, which includes monitoring, analyzing, investigating, predicting, decision-making, executing, and getting feedbacks, are carried out at this layer.

The seven subsystems provide technical support to the three application platforms. As shown in Fig. 4: (1) the database subsystem, the geographic information subsystem, and the remote monitor subsystem supply data to the information platform, (2) the analysis and prediction subsystem and the virtual reality subsystem serve the professional service platform, and (3) the decision support subsystem and the search engine subsystem offer services to the decision making platform. The three platforms and the seven subsystems need to share information or exchange data on a regular basis. They are integrated together to support the public health emergency response and their performances are related with each other. It should be noted that China is interested in leveraging the virtual reality technology to display the spreading process of infectious diseases in three dimensional images, to model possible outcomes of emergency response decisions, and to simulate plans of medical resource allocation in the event of an emergency. The design of the virtual reality subsystem will continuously evolve given that the application of virtual reality in public health largely remains an open research filed.

The action system is an important component of PHERIS. It is related to healthcare provision, emergency rescuing, and CDC institutions. In the event of a public health emergency, the action system will report relevant information to the command center and execute orders from the command center to allocate medical resources, such as hospital beds, medications, medical rescuing equipments, telemedicine services, long distance training, and other healthcare services. Under normal circumstances, this system is utilized to manage public health, provide medical services, facilitate emergency room services, and conduct telemedicine and other health activities. The action system plays different roles in the ''peace time'' and the ''war time.''

The supporting system is primarily a public health regulatory information system. The purpose of this system is to monitor the behavior of health care administrative agencies and healthcare organizations in fulfilling their legal obligation of protecting public health. The supporting system requires the establishment of public health regulatory LANs and central databases at the provincial and national levels. All the public health administrative agencies need to connect to the two LANs through the national public data network. The standardized format of regulatory reports will be developed. In terms of data communication of legal issues, online reporting and digital data collection methods are not in place but will be created.

This article provides an insight into the challenges currently facing China's public health and describes the development of PHERIS to prepare for and respond to public health emergencies. China recognizes that surveillance and detection of disease outbreaks is a critical part of the solution and a dynamic system incorporating command, action, and supporting components is also needed to manage emergencies. The ongoing development of PHERIS will provide a complete package for China to rapidly detect emergencies, effectively share critical information among key stakeholders, and competently manage emergencies. Comparing PHERIS with the five criteria recommended by Turoff et al. [26] , we find that the concepts of roles and notifications have been partly built into the system, while metaphors, context visibility, and hypertext are hardly realized in the system. This is not surprising, since China just started developing PHERIS and the system is still relatively rudimentary. At this stage, China's effort in developing PHERIS can be considered to be successful because of the radical emergency response process change re-sulted from PHERIS and its associated performance improvement in public health surveillance and group co-ordination. China expects PHERIS to be an adaptable system which is able to adjust to the changing environment. Thus, PHERIS will play a significant role in protecting China's public health for years to come. In summary, this article contributes to the IS literature with a description of China's PHERIS development and should be beneficial to both IS researchers and practitioners, especially those who are interested in developing countries' public health emergency response information systems.

",0.7562590354545541
Interrupting COVID-19 transmission by implementing enhanced traffic control bundling: Implications for global prevention and control efforts,"We argue that enhanced Traffic Control Bundling (eTCB) can interrupt the community-hospital-community transmission cycle, thereby limiting COVID-19's impact. Enhanced TCB is an expansion of the traditional TCB that proved highly effective during Taiwan's 2003 SARS outbreak. TCB's success derived from ensuring that Health Care Workers (HCWs) and patients were protected from fomite, contact and droplet transmission within hospitals.","patients exhibiting atypical manifestations or awaiting definitive diagnosis. A second adaptation involves enhancing the checkpoint hand disinfection and gowning up with Personal Protective Equipment deployed in traditional TCB. Under eTCB, checkpoint hand disinfection and donning of face masks are now required of all visitors who seek to enter hospitals.

These enhancements ensure that transmissions by droplets, fomites and contact are disrupted both within hospitals and between hospitals and the broader community. Evidencing eTCB effectiveness is Taiwan's success to date in containing and controlling the communityhospital-community transmission cycle. Copyright ª 2020, Taiwan Society of Microbiology. Published by Elsevier Taiwan LLC. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/bync-nd/4.0/).

Repeated recent outbreaks of novel infectious diseases (e.g. H1N1 pdm09, SARS, MERS, avian influenza H5N1, Ebola) that have failed to develop into globe-spanning pandemics have given the international community a sense of security. Coronavirus disease (COVID-19) is undermining this sense of security because it is proving both highly transmissible (like H1N1) and not too deadly (unlike Ebola). The result is a relatively easily transmitted virus with a case fatality rate currently estimated as well above that of the seasonal influenza. With its high transmissibility and rapid global circulation, COVID-19 is being compared with the deadly Spanish flu outbreak of 1918. 1 On 30 January, 2020, the World Health Organization (WHO) declared COVID-19 a public health emergency of international concern. Since erupting in China, it has claimed thousands of lives there, and has now spread globally, causing infections, death and growing alarm. The WHO's initial focus was on managing droplet and contact transmission, but it has since recognized the role fomites play as well. 2 Fomites are important because while populations generally protect themselves from clearly observable droplet transmissions (coughing, sneezing), being invisible, fomite transmission is regularly overlooked both in hospitals and communities. 3e5 Consequently, during the community saturation phase, fomite transmission may play a larger role than droplet transmission as a mechanism in emergent infectious diseases. 3e6 In this paper we argue that by implementing enhanced Traffic Control Bundling (eTCB), 6 an extension of the TCB deployed by Taiwan during the 2003 SARS outbreak, countries can interrupt the community-hospital-community transmission cycle, thereby limiting COVID-19's impact.

During SARS 2003, Taiwan implemented a nationwide version of TCB that proved effective at minimizing nosocomial infection of healthcare workers (HCWs), patients, and visitors to hospitals. 3, 4 Traffic Control Bundling is a version of multi-modal care that Taiwan has successfully implemented in the past. 7,8 TCB is an integrated infection control strategy that include triage prior to entering hospitals; strict separation among zones of risk, and; strict requirements and protocols for personal protective equipment (PPE) use coupled with checkpoint hand disinfection. 3 TCB protocols include initial ""triage outside of hospitals"" -patients found to be infected with SARS-CoV during triage at outdoor fever screening stations are sent directly through a guarded control route to a designated contamination zone; ""zones of risk"" -clearly distinguished contamination, transition and clean zones. HCWs moving from contamination to clean zones must undertake decontamination and degowning in the transition zone, and hand disinfection at every checkpoint between the zones.

During the Taiwan SARS outbreak, this strategy achieved 100% hand hygiene compliance among HCWs. When coupled with strict PPE use and standard infection control procedures, hospital fomite, contact and droplet transmissions were efficiently controlled. As a result, community SARS infection rates declined as well. Success was evidenced by no new diagnosed cases over two one-week incubation periods following TCB implementation. 3, 4 TCB is particularly effective as regards to the role of fomites. As noted, fomite transmission played a central role in SARS 2003. However, fomites have also proven important during other outbreaks. As with SARS, during MERS 2015 there were numerous examples of HCWs contracting the virus even when fully gowned and often despite having had no direct contact with carriers. 6 For example, during the Korean MERS outbreak, one super-spreader infected 82 people in a medical center. Initial analyses pointed to transmission by the index case via droplets and contact Figure 1 . Proposed mechanism of community-hospitalcommunity chain of amplification in COVID-19. with exposed people in the same care zone. However, later analysis found that those without contact history with the index case were likely infected by fomites on bed curtains and/or public restroom fixtures touched by the index case who had been suffering from walking pneumonia and diarrhea. 9 Similar findings were identified regarding interactions of fomite-related nosocomial and community transmissions during H1N1 2009 and Ebola 2014. 4e6 In the COVID-19 case, after a long incubation period, family clusters related to Huanan market developed (Fig. 1,  arrow 1) . Early in the outbreak, much as with SARS, transmission via droplets and fomites began cycling between the local community and clinics and hospitals via waiting rooms crowded by both ill and healthy people (Fig. 1, arrow 2) . In this case, nosocomial spread began among both HCWs and visitors to the hospitals, who when returning home, reinfected their communities (Fig. 1, arrow 3 ). This process extended the transmission cycle in a positive feedback loop, amplifying the outbreak (Fig. 1, arrows 4 &5 ) and cascading to the point of community saturation (Fig. 1) . 4 In short, following a dynamic similar to earlier coronavirus outbreaks, COVID-19 spread from the Wuhan epicenter to nearby communities and outward within China and internationally. However, COVID-19 has some unique characteristics. These include asymptomatic infection; a hyper-affinity to ACE2 receptors resulting in high transmissibility; false negatives, and; an incubation period of up to 22 days. 10, 11 These characteristics necessitate enhancement to the TCB protocols to ensure effective transmission control. 6 Enhanced TCB (eTCB) includes two enhancements to traditional TCB. First, where TCB has a contamination zone (Isolation ward) and a transition zone, eTCB expands the transition zone to incorporate a quarantine ward as well. The quarantine ward houses patients with atypical manifestations and patients awaiting final diagnosis. These patients are transferred to the quarantine ward directly from outdoor triage and are held there for the full incubation period. 6 (Fig. 2) .

As with traditional TCB, the zones are separated by a transition zone with checkpoints for hand disinfection. HCWs moving from the clean zone to quarantine ward must first dress in PPE. When transitioning from the quarantine to the isolation ward, HCWs must engage in comprehensive checkpoint disinfection, a process repeated as they move from the isolation ward to the clean zone (again, via a transition zone).

Second, to address community to hospital infection threats, checkpoint hand disinfection and face masks are required of all visitors entering the hospital. This is supplemented with heightened environmental cleaning and disinfection.

COVID-19 droplet and fomite transmission has been observed both inside and outside of hospitals. By containing nosocomial transmissions, eTCB contributes to breaking the cycle of community-hospital-community infection. Recently, Taiwan's COVID-19 response was praised in public health circles. 12 Taiwan's success can be at least partially attributed to its success in breaking the communityhospital-community transmission cycle by implementing TCB with the enhancements identified here. We therefore strongly recommend implementing eTCB as an important tool for containing COVID-19. Interrupting COVID-19 transmission

",0.7558085163083448
Perspectives of Australian policy-makers on the potential benefits and risks of technologically enhanced communicable disease surveillance -a modified Delphi survey,"Background: Event-based social media monitoring and pathogen whole genome sequencing (WGS) will enhance communicable disease surveillance research and systems. If linked electronically and scanned systematically, the information provided by these technologies could be mined to uncover new epidemiological patterns and associations much faster than traditional public health approaches. The benefits of earlier outbreak detection are significant, but implementation could be opposed in the absence of a social licence or if ethical and legal concerns are not addressed. Methods: A three-phase mixed-method Delphi survey with Australian policy-makers, health practitioners and lawyers (n = 44) was conducted to explore areas of consensus and disagreement over (1) key policy and practical issues raised by the introduction of novel communicable disease surveillance programmes; and (2) the most significant and likely risks from using social media content and WGS technologies in epidemiological research and outbreak investigations.","Significant outbreaks of infectious disease have impacts that extend beyond morbidity and mortality. Since they are often unanticipated and unpredictable, such outbreaks can cause fear, economic instability and social upheaval [1, 2] . Establishing and maintaining surveillance systems are the foundation of effective outbreak detection and public health response. Moreover, effective routine surveillance is central to the rational allocation of resources and addressing health inequalities, which cannot be tackled unless made visible. Because surveillance relies on the collection, collation and interpretation of large amounts of data, technological innovation has the potential to enhance the efficiency and accuracy of population-based communicable disease research and surveillance systems. For example, the increasing availability of pathogen whole genome sequencing (WGS) and the collection of passive data generated by internet and mobile phone use within a population, will provide new resources for communicable disease research and surveillance activities [3] [4] [5] .

Methods for characterisation of microbes have become increasingly sophisticated over the last 20 years. WGS technologies can provide rapid and accurate information about which microbial species and strain type is causing an outbreak and the timing and direction of transmission. Incorporating this information into surveillance systems will permit more accurate biological risk prediction and faster outbreak identification and tracking [6, 7] , but also reveal information about individuals that many people would consider to be private [8] . Concurrent with these developments in microbiological analysis, there has been a similar rate of technological innovation in data management. Using the tools provided by 'Big Data' , syndromic surveillance systems can track and integrate online data collected for unrelated purposes that potentially reflect disease activity in the community such as social media posts and internet searches [9] [10] [11] [12] [13] . Integrating either or both of these new sources of information into communicable surveillance practices has the potential to greatly enhance current systems. If linked electronically and scanned systematically, pathogen WGS data and user-generated online information could be mined to uncover new epidemiological patterns and associations much faster than traditional public health approaches [14] [15] [16] . Incorporating these new technologies and novel sources of information into established communicable disease surveillance systems should improve our understanding of the rate and direction of disease transmission between individuals and within populations, provide earlier warning and more accurate monitoring of outbreaks, and reduce uncertainty and public fear during their early stages [8, 17] .

The benefits of earlier outbreak detection and response are significant but, when there is no immediate threat, the routine use of a new technology to capture more detailed, specific personal health information could be perceived as intrusive and a threat to privacy, no matter how great the potential benefits for research and practice. When implementing such systems, sovereign states, health authorities and surveillance officers must consider the organisational, political, legislative, personal and ethical implications of surveillance [18] [19] [20] . However, there are also ethical and medico-legal risks in not using available information that could protect local and, potentially, international communities from serious disease outbreaks [21, 22] . Health authorities and researchers in Australia, and elsewhere, are beginning to explore the use of pathogen WGS and event-based social media monitoring, but many jurisdictions still lack policies and structures to support these technologies [5, 23, 24] .

In this paper, we report the results of a modified Delphi survey involving policy-makers and experts with backgrounds relevant to the legal, ethical and epidemiological dimensions of technology-enhanced surveillance systems. This method is particularly useful for technological forecasting and the evaluation of complex problems where (1) the rate of socio-technical change exceeds that which can be managed by technocratic styles of governance; and/or (2) objective data (and models and relations dependent on this data) are insufficient to explain or predict social actions [25] . Participants were drawn from two broad groups, namely (1) an established policy community centred on communicable disease control and (2) an emerging issues network of individuals interested in the social, legal and security dimensions of technological change and innovation.

Policy communities and issues networks are at either end of a continuum characterised by differing levels of internal cohesiveness, state support and access to, and ability to regulate, shared resources [26] . Policy communities are stable, institutionally entrenched groups who share values and broad policy preferences, interact frequently and participate in relatively equal communications to produce lasting policy outcomes; conversely, issue networks include increasingly influential academic, industry and interest groups varying in levels of co-ordination and access to resources, who discuss, critique and generate ideas for policy initiatives in specific policy areas [27, 28] . This study is part of a larger project to develop guidance for policy-makers; its aim is to engage these two networks in a conversation about the social, ethical and legal implications of the use of new technologies (online data monitoring and WGS pathogen fingerprinting) in infectious disease research, control and prevention. In this Delphi survey we sought to (1) identify perceived barriers to the adoption and effective implementation of new technology for communicable disease research and surveillance; and (2) explore areas of consensus or disagreement about potential threats or conflicts of interest between individuals, commercial entities and the broader community associated with it.

A heterogeneous and geographically dispersed group of Australian-based policy-makers and experts in infectious diseases, epidemiology, food safety, health informatics systems, and health and technology law were invited to participate in this Delphi survey. Noting that the boundary between the roles of policy-makers and expert advisors can be nebulous, we defined 'policy-makers' as individuals who participate in policy processes to create, order and maintain rules and structures of action, and 'experts' as individuals with knowledge and experience of the law and/or public health practices, relevant to infectious disease control and prevention, technological innovation, or both [29, 30] . Sampling was purposive, to ensure representation of the relevant disciplines and types of actors. Potential participants were identified through institutional websites and researchers' professional networks.

The rationale underpinning Delphi surveys is that consensus about contentious issues carries more weight than individual opinions [31] . Anonymous data are collected from individuals, collated and then re-presented to the group to elicit further responses [32] . In this study, we analysed data iteratively in parallel with data collection. Rather than force consensus, we employed a modified technique that allowed participants to explain their views. Participants were asked to assess and respond to the levels of consensus/disagreement that emerged from each round to provide greater insight into the potential benefits, harms and risks of using new technologies to enhance communicable disease surveillance. Participants who completed each round were invited to participate in the next, but were free to withdraw at any time. We used an online survey platform (Limesurvey).

In round 1, we asked participants to respond to three hypothetical scenarios (available in Additional file 1) describing the use of a new technology for the purposes of communicable disease surveillance in (1) social media/ online, (2) hospital/workplace, or (3) commercial environments. Responses were analysed qualitatively and coded thematically by two authors (CD and JJ) using framework analysis, a deductive matrix-based qualitative research method for ordering and synthesising textual data, developed by the National Centre for Social Research (United Kingdom). Framework analysis methods are especially well suited to conducting applied and policy-relevant research [33, 34] . During rounds 2 and 3, participants' comments, key arguments and levels of consensus from previous rounds were presented as quotations, bar charts and summaries of qualitative findings, taking care to weigh different opinions and arguments equally. Individual comments were de-identified.

Additional data and comments were collected, using Likert scales and free text responses. On completion of each round, participants' Likert scores were tabulated and free text answers analysed qualitatively, as described. The final stage of analysis during preparation of this report drew on the knowledge and professional experience of the research team, which includes expertise in health social science, bioethics, Australian health law, infectious diseases, microbiological laboratory and data sciences, and health protection. This study was approved by the University of Sydney Human Research Ethics Committee (#2016/819).

Email invitations were sent to 92 potential participants, of whom 44 (47%) from a range of relevant roles and disciplines responded (Additional file 2: Table S1 ). Invitations included an individualised link to the online survey, through which participant consent was obtained. As expected, the panel size gradually decreased as participants withdrew [35] , but the balance between sectors and characteristics of participants remained substantially constant (Additional file 2: Table S1 ).

In round 1, panellists answered a series of structured questions about a scenario describing the use of a new online event-based surveillance system that monitors social media (Twitter, Facebook, etc.) for indicators of emerging public health risks. In the scenario, the new system picks up a post on Facebook from Bob, a 25-year-old high school teacher from Sydney. His post suggests that he may have contracted highly pathogenic avian influenza on a recent holiday to Indonesia. Online discussion of symptoms and work absences also suggests that Bob might have spread the infection to several of his colleagues. Delphi participants proposed a number of responses to this scenario. While the mandate to respond to a potential public health risk was seen to be clear, Delphi panellists disagreed about how to do this and how to contact those most at risk (particularly Bob and his colleagues). After reading each other's comments and suggestions on the merits and risks of different strategies for addressing this acute situation, in rounds 2 and 3 we asked the panel to nominate which approach to contacting Bob they thought was the most appropriate. Figure 1 shows that, while a direct approach (speaking to Bob in person) was strongly favoured, a majority of panel members questioned the acceptability of using a social media platform for this communication, even though Bob had posted information about his health status in a public forum. Others were concerned that an online intervention could create panic by identifying Bob as a public health risk. As one panellist noted: DP #12: ""Where individuals have already disclosed information themselves, then one can make a reasonable assertion that the information is therefore not private. This, however, is not the same as that individual allowing or being willing to have their social media platform used to contact or warn others or to make further enquiries, since all of these actions have the potential to alarm, stigmatise or otherwise negatively affect the social media user.""

When asked to answer either yes or no, 83% of round 2 Delphi panellists agreed that members of the public should expect that their activities on social media could be used for public health surveillance. Using a list drawn from responses to previous rounds, panellists were asked, in round 3, to indicate the conditions under which they believed surveillance of social media is appropriate (Fig. 2) . The key differences (revealed in the comments) were the extent to which participants were concerned about the lack of a clear legal and social mandate and the potential for negative Fig. 1 Panellist's preferences as to how public health authorities should contact someone flagged by an online event-based communicable disease surveillance system public reactions. Many panellists also expressed doubts about the veracity and validity of data passively collected from online sources: DP #37: ""…we don't know how reliable social media actually is in these circumstances. People post things on social media for lots of reasons. It may not be accurate and could be misleading. So use of the data may lead to a wasteful, inappropriate and/or damaging response.""

Taken together, these results indicate that most experts agree that using social media as a source of data for routine population health surveillance does not raise significant ethical concerns. However, there is considerable uncertainty about the legitimacy and acceptability of moving beyond population surveillance to use these online systems as a platform for targeted interventions during a public health response, especially when the individuals involved might be publicly identifiable. The sectoral affiliation of panel participants did not appear to influence responses, except in the case of those with legal backgrounds, who strongly preferred that any of the activities discussed should take place under the auspices of a dedicated legislative framework.

Enhanced communicable disease surveillance in hospital/ workplace environments

The second scenario described the use of a novel, highly discriminatory WGS strain-typing system that can routinely type all methicillin-resistant Staphylococcus aureus isolates from hospital patients. Because WGS technology can provide a pathogen's unique genetic 'fingerprint' , it can potentially identify the timing and direction of individual person-to-person transmission events. In the scenario, the new technology shows that two premature babies who were cared for and died in the same neonatal intensive care unit both had the same rare community-associated methicillin-resistant Staphylococcus aureus strain.

Panellists agreed that the most important measures to be taken in response to this scenario were a thorough cleaning of the neonatal intensive care unit facility and a renewed focus on infection control. There was, however, disagreement as to how WGS and strain-typing technologies should be used to mitigate the risks of further infections (other infants, parents, staff, etc.). Most prominent were varying levels of concern about (1) the lack of consent from the parents to use WGS on the isolates collected from the dead neonates for the purposes of conducting an epidemiological investigation; (2) the confidentiality of information elicited through testing and strain typing isolates taken from other patients, parents and/or staff members; and (3) the impact on a staff member or parent if they were found to be the source of the pathogen.

Nevertheless, panellists saw an overriding need to address the outbreak in the interests of public safety, irrespective of any broader questions about consent, confidentiality and discrimination. As one panellist noted: DP #07 ""…the main issue, is not whether strain typing is done, but how the results are handled. It is in everyone's interest to understand how pathogens are transmitted so that transmission can be prevented in future, but there should never be any blame (or, hopefully guilt) or penalties involved.""

To explore this issue further, panellists were asked, in round 3, to indicate under what conditions they believed it is appropriate to waive consent from individuals (patients, families and/or staff ) and perform a test for an epidemiological investigation aimed at addressing a potential risk to the health of future patients. Figure 3 shows that most panellists held that the protection of the public was of overriding importance and that most of the potential harms could be managed by establishing systems to maintain the confidentiality of those effected. However, differences in the priority afforded by panellists to different conditions of use, such as absence of alternatives, rights of appeal, and likely effectiveness, indicate that the appropriate thresholds for taking such actions remained contentious. Once again, neither the policy network from which participants were recruited, nor their sectorial background appears to have been a major influence on panellists' opinions and perspectives. 

The final scenario described a significant and fatal outbreak of infection due to a rare strain of Listeria (15 cases/5 deaths) in the community, which appears to be linked to the consumption of chicken-wraps produced by a commercial food company. In this scenario, a dispute arises between public health authorities, one of the victim's families and the food company as to the limits of 'commercial-in-confidence' and who controls access to isolates (and any related information) previously submitted voluntarily by food producers to commercial food laboratories for routine fee-for-service safety testing.

Panellists' responses to this scenario focused on the need to establish who was the legal owner of the isolates held by the commercial food laboratory and whether private companies have the right to block further testing (WGS or otherwise) that lies outside the purpose for which samples were submitted. New technologies such as WGS were seen as being valuable to the investigation of food-borne diseases. However, they also increase commercial/reputational risk for companies who routinely test their products, such that: DP #22 ""…mandated access [to commercial samples and test results] may result in changes to testing in the commercial domain (e.g. cessation of some testing) such that isolates are no longer available… The relationships between labs, food businesses and primary industry and public health agencies are important, as are the priorities of each entity.""

In round 2, panel members were split as to whether WGS testing should proceed without the permission of the food company (12 in favour, 10 against, 9 uncertain). In their comments, many panellists were keen to emphasise that public interest should always trump commercial interest, and questioned how a company could have effective property rights over a pathogen contaminating its product. To explore this issue further, panellists were asked, in round 3, to indicate the conditions under which they believed it was appropriate to conduct a test on an isolate collected from a commercial company without prior consent. Figure 4 indicates that significant disagreement remained as to what conditions were required for testing to be appropriate. More detailed analysis of individual responses revealed that these differences were also present within disciplinary groups and across policy networks. Reflecting on their experiences, many, but not all, panellists from the communicable disease policy network wanted to establish a clear procedural separation; they wanted to distinguish the need to be able to perform secondary tests on isolates submitted to commercial food laboratories in order to inform epidemiological investigations (finding out what is going on) from issues of commercial-in-confidence and reputational damage (managing commercial and property issues raised by the public health response), whereas participants from both networks with legal backgrounds wanted matters of ownership and access to information to be resolved before any testing could take place. To provide remedies to this situation (in the longer term) participants suggested that legal clarity on the issue of public health access to commercial data and environmental and food isolates needed to be established.

The application of new technologies to communicable disease surveillance can provide great benefits but also create risks. In round 3, we asked participants to rank the four most significant and four most likely risks for individuals (11 items) and for the broader community (9 items) from lists compiled from previous survey rounds. Final rankings were determined by assigning a score equivalent to reverse rank (e.g. a score of 4 to items rated 1st, 3 for items ranked 2nd, … 1 for items ranked 4th); scores were multiplied by the number of participants who gave each rank and the overall ranking was determined by adding scores for each item. Additional file 3: Tables S2 and S3 respectively show the final rankings the panel gave to risks for individuals and risks for the community.

For individuals, participants ranked the psychological consequences of knowing one's social media posts are being monitored and the risk of breach in data security highest, with these unwanted outcomes being seen as both significant and likely (Additional file 3: Table S2 ). Instances and potential social consequences of breaches in the privacy of individuals from primary or secondary data usages such as loss of privacy or stigma were not seen as being major risks. In their comments, many panellists noted that consent for data linkage and secondary use was implied, even as they shared concerns that an erosion of community trust in health authorities was the most significant risk from the systematic use of these technologies (Additional file 3: Table S3 ). Notably, the veracity of the data produced by enhanced surveillance techniques was once again a key concern among panel members, with the risk to the community of unnecessary fear-mongering and resource misallocation (through individuals changing their behaviour) being ranked as significant and likely.

Participants in this Delphi study, whom we assumed to be representative of policy-makers and other stakeholders involved in decision-making about new technologies for communicable disease research and surveillance, shared a belief in their potential benefits. However, our findings indicate that both groups were uncertain about the public acceptability of routine collection and linkage of such data to enhance surveillance. Despite broad agreement that it could be ethically justified, participants were uneasy about creating a perception of threat to individual rights, which could erode public trust. The panel identified the key challenge to systematic use of new technologies as a need to establish publicly supported guidelines for their operation.

There are good reasons for participants' hesitation about the use of these technologies, without a clear social licence. The socio-political impacts of technological innovation and emerging risk uncertainty can quickly undermine public trust in governments and, by implication, the authority and perceived legitimacy of associated policy communitiesas events surrounding the introduction of mobile telephone masts and genetically modified crops attest [36, 37] . More broadly, political sensitivities are easily heightened as new surveillance systems are introducedconcerns about privacy, consent and other individualistic values tend to dominate public discourse [38] . In practice, however, when compared to other state-led surveillance practices, typically there has been little public opposition to use of personal information by health authorities for the purposes of protecting public health [8, 39] . In recent decades, for the reasons described above, opposition to the use of health specific surveillance data in new ways has been more likely among public health officials than members of the wider public [40] [41] [42] . Participants of this Delphi were strongly in favour of enhancing communicable disease surveillance, with many supporting a need for greater public interest consideration, not more privacy.

Where the panellists failed to agree was around appropriate thresholds for public health action and policy protections to be offered to subjects of enhanced surveillance systems. For example, there was consistent disagreement, across all three survey rounds, about the relative importance of confidentiality and public health risk and prioritisation of reasonable alternative interventions; these differences need attention and resolution. Experience shows that, with current analytical tools, interpretation of passively collected and analysed online or pathogen WGS data raises questions about what counts as significant or compelling evidence and what action, if any, should follow [43] . The absence of a policy framework to provide institutional support for new surveillance systems and guide subsequent public health actions comes at a cost [4] . Surveillance systems have limited effectiveness unless they lead to an organised response [44] . Although impossible to quantify, systematic non-use of data can amplify and compound the harms associated with inaction [22] . For reasons of transparency and public acceptability, if not protection, resolving these issues should be a priority because not collecting and using data of significance to public health can lead to preventable harms [8, 45] .

Against this background, the relevant Australian legislation prioritises the pursuit of public goods, such as effective communicable disease surveillance, over protecting private interests. Neither individuals nor companies have a broad right to privacy under Australian law. Instead, the law in this area balances public interest against protecting individual or commercial privacy interests and includes a range of protections for the public [46] . This extends to cyber law, which operates under a general principle that social media users own the online content they create, but have limited rights to control its secondary use by others (either individuals or organisations). Similarly, it is a long-established principle under Australian common law that there is no property in human biological samples. This means a person does not own biological samples taken from them (tissues or microbial isolates), nor does the laboratory or facility that holds the samples. As long as there is a clear public health (rather than research) purpose for epidemiological investigation, it follows that there are no significant legal barriers preventing laboratories from performing secondary tests on isolates without the permission of the individuals from whom they were collected [47] .

In contrast, it would appear that food or environmental samples (including those collected from livestock) are considered property under the law, such that the Public Health and Food Acts do not clearly establish that further testing of the isolate is legal without strong evidence that the threat to public health is ongoing [48] . Most members of the panel were perplexed by this impediment to secondary testing and argued that public interests should always override any property claims. We note that the case for such a pragmatic approach to issues of consent and ownership for secondary testing of isolates owned by commercial entities is bolstered by epidemiological considerations. Some degree of strain typing must be done routinely if this practice is to be useful for identification of transmission events [5, 16] . Therefore, legal reform may be required because any insistence on a high threshold of public health risk before pathogen WGS can be done would limit the timeliness and, consequently, the effectiveness of this type of communicable disease surveillance. The development of clear guidelines for the secondary testing of patient, food and environmental isolates should be a priority before the use of WGS in epidemiological investigations becomes routine.

Finally, our study reveals a gap between existing policy structures relevant to the use of WGS technologies and social media monitoring for public health purposes, and the individualistic focus of the concerns of most participants. As is the case with many activities undertaken in pursuit of public health, some form of trade-off between public and private interests is necessary. In order to strike the right balance, it is important that the potential benefits and harms of enhanced communicable disease research and surveillance are weighed appropriately. The content of interactions between participants from the different policy networks sampled demonstrated a range of attitudes as to how health authorities should collect, link and use data. The panel concluded that privacy concerns and potential conflicts with commercial interests caused by enhanced surveillance techniques need to be managed appropriately, but neither should substantively limit the pursuit of public interests as important as effective communicable disease prevention and control. However, efficient surveillance across a large population is not achievable unless most people participate. A structured dialogue between interested groups and development of an ethically and legally defensible rationale for the design and operation of surveillance systems [49] , prior to their implementation, will help to reassure the public and other stakeholders and enhance the likely success of these systems.

Delphi survey methods have several well-known limitations, including that the substantive outcomes consist of a set of group intuitions and perspectives, constructed through highly structured social processes of justification among experts [50] . However, as noted in the introduction, the method is particularly useful for the evaluation of complex problems where the rate of social or technological change exceeds the rate of innovation in governance, where evidence that can adequately explain or predict social actions is lacking, or both [25] . The initial response to participant invitations for the current survey was sufficient to generate a lively debate, which was gratifying, given that our invitation was unsolicited. Retention of participants over successive rounds was moderate and the balance between members of different sectors remained constant. Moreover, allowing participants to express their views and comment on each other's interpretation, via open-ended free text questions, over multiple survey rounds increased the reliability of the study and improved the robustness of the results.

This Delphi survey has captured the perspectives of representatives of expert groups concerned with enhancing communicable disease surveillance systems in Australia. Because of differences in cultural norms and the surrounding social, legal and public health systems, a similar group brought together in another jurisdiction may come to different conclusions. Perspectives of the Australian public are currently being sought through a series of citizens' juries, to explore what they believe to be acceptable and legitimate use of Big Data and pathogen WGS for the purposes of communicable disease surveillance [51, 52] .

The results of this Delphi survey suggest that there is broad support for using event-based social media monitoring and pathogen WGS technologies to enhance communicable disease surveillance systems across sectoral groups and relevant policy networks. Panellists agreed there is a need to establish a policy framework to ensure appropriate safeguards are in place to protect privacy and that the public is consulted so that they are not unnecessarily alarmed by, or suspicious of, the introduction of new processes for data collection and analyses. However, the emphasis of reforms should be on enabling effective research and surveillance to be conducted, where common benefits are possible. In this regard, the concerns of Australian policy-makers and experts on this issue are not unique. A recent WHO review points to the need to engage with affected communities to establish the conditions and protections under which it is acceptable for surveillance to take place and develop institutional mechanisms that ensure ethical issues are systematically addressed before data collection, use and dissemination [45] . Given that these technologies are already available and have the potential to enhance the capacity of Australian and other health authorities to investigate and prevent outbreaks of infectious disease, with their attendant social and economic costs, the development of clear ethical and legal guidance is urgently needed. The absence of such policy and procedural protections means that public health authorities are likely to only employ these new technologies sporadically, such that opportunities to protect individuals and the wider population from harm will be missed.

Additional file 1 The three hypothetical scenarios describing the use of a new technology for the purposes of communicable disease surveillance in (1) social media/online, (2) hospital/workplace, or (3) 

",0.7549097550090522
Financing Vaccines for Global Health Security,"Recent outbreaks of infectious pathogens such as Zika, Ebola, and COVID-19 have underscored the need for the dependable availability of vaccines against emerging infectious diseases (EIDs). The cost and risk of R&D programs and uniquely unpredictable demand for EID vaccines have discouraged vaccine developers, and government and nonprofit agencies have been unable to provide timely or sufficient incentives for their development and sustained supply. We analyze the economic returns of a portfolio of EID vaccine assets, and find that under realistic financing assumptions, the expected returns are significantly negative, implying that the private sector is unlikely to address this need without public-sector intervention. We have sized the financing deficit for this portfolio and propose several potential solutions, including price increases, enhanced public-private partnerships, and subscription models through which individuals would pay annual fees to obtain access to a portfolio of vaccines in the event of an outbreak.","In this study, we examine the economic feasibility of developing and supporting a portfolio of vaccines for the world's most threatening emerging infectious diseases (EIDs) as determined by scientific experts, drawing from the list of targets made by the recently launched global initiative, the Coalition for Epidemic Preparedness Innovations (CEPI) (1) (2) (3) . Our portfolio is composed of the 141 preclinical assets identified by Gouglas et al. to be targeting the priority diseases.

The risks of EIDs are inherently dynamic and largely unpredictable. New threats persist, including the recent outbreak of a novel coronavirus COVID-19 emerging from Wuhan, China (4) Government leaders face formidable decisions about the provision of health security measures against outbreaks of these threats. Global actors are seeking to diminish the danger that these pathogens pose to the wellbeing of nations, regions, and the world. Given the range of potential biological threats, their unpredictability, and the limited resources available to address them, policymakers must necessarily prioritize their readiness efforts based on limited knowledge. All too often, they are forced to choose between priorities, and construct so-called limited lists of treatments, using testimony from teams of experts to inform these decisions. As history has shown, however, this approach leaves society vulnerable to unforeseen outbreaks. Therefore, a more rational approach is to develop a broad portfolio of vaccines in a coordinated manner, mitigating the future risk posed by unpredictable outbreaks of these diseases.

Uncontrolled outbreaks of EIDs, defined as infections that have ""recently appeared within a population, or those whose incidence or geographic range is rapidly increasing or threatens to increase in the near future"" (5) , have the potential to devastate populations globally, both in terms of lives lost and economic value destroyed. Notable recent outbreaks of EIDs include the 1998 Nipah outbreak in Malaysia, the 2003 SARS outbreak in China, and the 2014 Ebola outbreak. In addition to the thousands of lives lost, the economic costs of these outbreaks are estimated as $671 million, $40 billion, and $2.2 billion, respectively (5) (6) (7) (8) .

As the world becomes more globalized, urbanized, and exposed to the effects of climate change, the danger of infectious diseases has become an even greater concern (9), as emerging and re-emerging strains become more diverse, and outbreaks become more frequent. While distinct from the emerging infectious diseases, influenza serves as the best example of the destruction that viruses with pandemic potential can inflict on the modern world. As a baseline, avian influenza outbreaks in the U.S. since late 2014 have caused economy-wide losses estimated at $3.3 billion domestically, and have significantly disrupted trade (10) . The 1918 influenza pandemic, however, is estimated to have infected 500 million people and killed 3-5% of the world's population. In 2006, Dr. Larry Brilliant stated that 90% of the epidemiologists in his confidence agreed that there would be a large influenza pandemic within two generations, in which 1 billion people would sicken, 165 million would die, and the global economy would lose $1 to $3 trillion (11) (see Supplementary Materials for further discussion). Controlling EIDs before they have the chance to reach comparable scale represents a significant opportunity to prevent similar loss. few economic incentives for manufacturers to develop preventative vaccines for EIDs, due to the high costs of R&D and the uncertain future demand. Even if protection against these emerging diseases were immediately achievable with existing technology, development costs are significant (12) , as they are for any pharmaceutical development program. Pronker et al. (13) estimate that it costs between $200-900 million for a new vaccine to be created. Failure to gain approval also poses a substantial risk, as successful passage through clinical trials only occurs 6-11% of the time (13, 14) . Regulatory challenges are particularly prominent in EID vaccine development, as viable candidates are rarely available for distribution during outbreaks, making safety and efficacy testing difficult. As a result, vaccine development for EIDs has been reactive and technologically conservative (15) .

In spite of these substantial difficulties-or perhaps because of them-new global initiatives have drawn attention to the need for new approaches to encourage the development of vaccines against EIDs (16, 17) . International collaborations like CEPI have drawn extensive public, private, NGO, and academic attention to the perils of global epidemic unpreparedness (18) .

This crisis-driven expanded interest in vaccines to address epidemic threats is encouraging, but there is still much work to be done. There needs to be a viable, sustainable business model that will align the financial incentives of stakeholders to encourage the necessary investment in vaccine development (19, 20) . While governments and international agencies have striven to create incentives to attract additional private sector investment in vaccine development, these efforts have so far failed in attracting sufficient capital to enhance preparedness against the world's most deadly emerging pathogens (21) .

Several mechanisms have recently been proposed or implemented to create incentives for industry to develop vaccines and other medical countermeasures for EIDs (22) . Beyond the ""push mechanism"" of significant R&D support, these mechanisms provide some measure of a ""pull incentive,"" recognizing that traditional market forces are insufficient to secure global health security aims. These strategies include the direct government acquisition of stockpiles of vaccines, the use of prizes, priority review vouchers, and the establishment of advance market commitments, each of which is described in more detail in Supplemental Materials. However, to date, none of these strategies have been deemed to be effective in addressing the growing threat of EIDs.

Previous research has demonstrated that a novel 'megafund' financing strategy is capable of generating returns that could attract untapped financial resources to fund the development of a portfolio of drug development programs (23, 24) . In this study, we address this possibility by simulating the financial performance of a hypothetical megafund portfolio of 141 preclinical EID vaccine development programs across 9 different EIDs for which there is currently no approved prophylactic vaccine. Under current business conditions, we determine a private sector solution for the comprehensive development of EID vaccines is not yet feasible, and quantify the gap so as to inform current policy discussions regarding the need for public-sector intervention.

We conclude with a discussion of three possible solutions to this challenge: 1) establishing a global acquisition fund for EID vaccines, in which governments around the world collaborate; 2) raising the price of portfolio vaccines by two orders of magnitude; and 20 Mar 2020 © 2020 by Vu, Kaplan, Chaudhuri, Mansoura, Lo All Rights Reserved Page 4 of 19

3) creating a subscription model for vaccines, through which the global at-risk population pays an annual fee to fund the development of and ensure access to a predefined list of vaccines for EIDs.

To create further incentives for investing in this space, we hypothesize the creation of an EID megafund based on the model developed by Fernandez et al. (23) , which uses portfolio theory and securitization to reduce investment risk in these assets. In financial engineering, the practice of securitization requires the creation of a legal entity that issues debt and equity to investors, using the capital raised to acquire a portfolio of underlying assets-in this case, vaccine candidates targeting EIDs. These assets subsequently serve as collateral, and their future cash flows service the debt incurred to acquire them, paying the interest and principal of the issued bonds. Once the debt has been repaid, equity holders receive the residual value. If the portfolio's cash flows are insufficient to meet the obligations to the bondholders, the collateral will be transferred to bondholders through standard bankruptcy proceedings.

Given the characteristically high risk of default of candidates in the early stages of development, and the need for increased financial investment in vaccine research as a whole, securitization in the form of a vaccine megafund offers several key benefits. The securitization of vaccine research enables investors to reduce their risk of financial loss to a scale that is not readily achievable under current financing mechanisms, as they can invest in many vaccine projects at once, thus increasing the likelihood of at least one success. The normalization of returns created by the construction of an asset portfolio permits the issuance of debt, which allows fixed-income investors to gain exposure in a space that is traditionally too risky to represent a compelling opportunity for investment. The ability to issue debt is critical, because bond markets have much greater access to capital than does venture capital or the private and public equity markets. This allows the megafund to raise enough funding to purchase an array of assets and reach its critical threshold of diversification.

One notable benefit of our megafund approach is that it hedges against the societal risk that the world will not have the 'right' vaccine it needs for the next EID outbreak. To date, the U.S. government and CEPI programs have been forced to severely limit their portfolios, due to funding constraints. This approach allows us to assess the opportunity of addressing 9 of the world's most threatening EIDs at once.

While the megafund approach is effective at reducing the development risk of EID vaccines, it should be emphasized that the success of this technique hinges upon securitizing assets that have the potential to be profitable individually if the development effort is successful. This flies in the face of conventional pharma wisdom that vaccines are commercially challenging, not only because of development risk but also because of the unpredictability of outbreaks and constraints on pricing when outbreaks occur. However, to quantify the gap between reality and commercial viability-and in light of global stakeholders' ongoing efforts to raise funding to combat these diseases-we suspended belief in this presumption so as to allow the financial analysis to determine the profitability of the EID portfolio in an unbiased fashion. Based on available pipeline data, an analysis by Gouglas 

To apply this portfolio approach to EID vaccine development, we began by analyzing the hypothetical investment returns of a portfolio of 141 preclinical EID vaccine development programs across 9 different emerging infections for which there is currently no approved prophylactic vaccine. Our analysis relies on several assumptions and parameters, including estimates of the cost of vaccine development, the length of time from preclinical testing to the filing of a new vaccine license application, the probability of success of each project, and pairwise correlations of success among the projects in the portfolio. The target diseases were selected from CEPI's Priority Pathogen list, which was based in part upon the WHO's R&D Blueprint focusing on epidemic prevention (1, 2) . We drew our portfolio assets from CEPI pipeline research for each disease on its priority pathogen list (1, 3) . (See Supplementary Materials for more details).

The model design is less complicated than that of Fernandez et al. (23) . Unlike oncology-a domain with many approved drugs and even more under development-there are currently few EID vaccines available on the market, indicating a paucity of data with which to calibrate our simulations. In setting our simulation parameters, we relied on generic information about the vaccine development process, specific estimates posited by CEPI (1), and qualitative input from scientists with domain-specific expertise.

The present value of out-of-pocket development costs for each of the projects in the portfolio was set to $250 million, based on assumptions made by CEPI about the cost to develop a preclinical asset through phase 2 (1). CEPI further estimates that it will take five years for this development to occur ( Figure 1 ). CEPI proposes that assets at this level of development will justify stockpiling, further development, and conditional usage under emergency conditions, a plan that some experts believe may be feasible (1, 25) .

At $250 million per project, a megafund of 141 projects requires $35.25 billion. To determine the returns generated by such a portfolio, we assumed a 15-year period of exclusivity and a 10% cost of capital to calculate the NPV of future cash flows upon approval in year 5. This value must be weighed against the possibility of total loss if the vaccine project fails. An assessment of the megafund's returns therefore requires estimates of the probabilities of success of each of the 141 vaccine candidate projects as well as the pairwise correlation of success of all possible pairs of assets. The probabilities of success are based on estimates of the compounded probabilities of advancement from preclinical testing to vaccine approval. The probability of development through phase 2 of a vaccine at the start of preclinical testing is 32%, based on the transition probabilities provided by CEPI (1 practical assumptions to project revenue. In this model, we assumed that the prophylactic regimen would consist of a single dose of vaccine. The probability of disease outbreak was estimated based on historical outbreaks per disease, while regimen demand was projected using historical outbreak size, potential for pandemic spread, and an assessment of relative clinical severity. These demand parameters were determined respectively by case estimates from documented outbreaks, referencing the Woolhouse assessment for pandemic potential, and comparing the clinical presentation and prognosis for each disease (26, 27) . A perceived demand multiplier was assigned based on Woolhouse classification and clinical severity on a five-step scale ranging from mild to severe. The average number of cases and the perceived demand multiplier were used to calculate the number of regimens sold in an outbreak year for each disease. This product, the expected chance of outbreak in a given year based on historic outbreak data, and the expected selling price per vaccine regimen were used to subsequently calculate the annual expected revenue for each disease. The price per regimen was estimated based on whether the disease in question typically affected high-, medium-or low-income countries. The expected price per regimen for each income level was informed by CDC, GAVI, and PAHO vaccine pricing data, respectively (28) (29) (30) . Please see Supplementary Materials for additional details. The simulated investment performance of an EID vaccine portfolio as a function of the commercial potential of each individual vaccine project is provided in Table 2 and illustrated in Figure 2 (please see Supplementary Materials for more information on how returns were calculated). The commercialization potential of these vaccines is consistently very poor, orders of magnitude lower than what would be required to make them commercially viable.

The parameter values that are closest to industry averages correspond to the highlighted row in Table 2 , in which the expected annual profits upon FDA approval are $1 million, resulting in an NPV per successful EID vaccine of $7.6 million. For these values, the vaccine portfolio's expected return is 61.1%, with a standard deviation of 4.0%.

For completeness, Table 2 also reports megafund performance statistics for several other sets of parameters. The breakeven point, where the megafund's expected 5-year return is 0%, occurs as the NPV of a successful vaccine reaches $772 million, two orders of magnitude greater than our current estimates using past averages for costs, revenues, probabilities of success and outbreak, and other information. However, for an NPV of $1 billion, the vaccine portfolio becomes marginally profitable, and at $10 billion, it is highly profitable. These results suggest that many of the model parameters would have to change drastically for the portfolio to be profitable. In fact, holding all else equal, simply breaking even would require selling vaccines at approximately 100 times the price assumed in our simulations. Megafunds are, of course, not the only business model through which vaccines can be developed. Traditionally, large pharmaceutical companies have incorporated vaccine programs into broader and highly diversified portfolios of therapeutics across many indications. To explore this possibility, we estimated the impact on risk and reward of incorporating the EID vaccines portfolio into a hypothetical pre-existing and profitable 

. CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint Table 3 contains the estimated expected returns and volatilities of a representative top-10, mid-tier, and small-capitalization pharmaceutical company with and without the base case version of the EID vaccine portfolio. The best-case scenario-in which big pharma adds this portfolio to its existing products-turns an otherwise profitable business into an unprofitable one, losing 8.6% per year on average in shareholder value. The results for mid-and small-cap pharma companies are even worse.

These results are consistent with the biopharma industry's trend towards fewer companies willing to engage in vaccine R&D, underscoring the infeasibility of a private-sector EID vaccine portfolio given current cost and revenue estimates, and the need for some form of public-sector intervention. A sensitivity analysis of these results to perturbations in our model's key parameters is provided in the Supplementary Materials. We find that the EID vaccine megafund remains financially unattractive even under relatively optimistic cost and revenue assumptions, implying the necessity for some form of public-sector intervention. These findings may explain the dearth of EID vaccines developed over the past decade.

One intervention is the use of government-backed guarantees to mitigate the downside risk of the EID portfolio. In a guarantee structure, a government agency promises to absorb the initial losses on the portfolio to a predetermined amount, shielding private-sector investors from substantial negative returns. For example, a guarantee on 50% of the portfolio's principal improves the expected annualized return in the base case scenario from −61.1% to −12.6% (see Table S11 in the Supplementary Materials). While this negative-expectedreturn scenario is still unlikely to attract investors, expected returns can be further increased using mechanisms such as advance market commitments and priority review vouchers. The guarantee structure-in combination with other existing revenue-boosting mechanismshas the potential to transform a financially unattractive portfolio of EID vaccine candidates into one that could realistically attract private-sector capital. Finally, we consider a subscription model under which the largest governments around the world would purchase subscriptions to EID vaccines on behalf of their constituents. To fund the cost of pursuing 141 vaccine targets at $250 million per target (for a total of $35.25 billion), suppose that the governments of the G7 countries agreed to pay a fixed subscription fee per capita over a fixed amortization period to cover this cost. How much would this subscription fee be? For an amortization period of 5 years, and an estimated total G7 population of 770,063,285 (as of 2016, according to the World Bank (31)), and a cost of capital of 10%, the per capita annual payment to cover the total cost of $35.25 billion is $12.08 per person per year. If we extend the amortization period to 10 years, the subscription fee declines to $7.45 per person per year. Table 4 contains the per capital subscription fees as a percentage of the annual per capita healthcare expenditure of each G7 country and as expected, the cost is trivial for all countries, ranging from a high of 0.59% for Italy to a low of 0.15% for the US using a 5-year amortization period.

Of course, this subscription model considers only the development cost of vaccines. Once developed, the production and stockpiling of these vaccines would require further funding, but the subscription model can be applied on an ongoing basis, and at a much lower annual cost. Access to these vaccines by non-G7 countries must also be considered, but such access involves political and ethical issues that are beyond the scope of this economic analysis.

These results suggest that a government-led subscription model is financially feasible and would likely yield significant economic and political benefits to all participating governments. While the usual challenges of broad multi-national cooperation must be overcome, early traction from organizations such as Civica Rx suggests that focused, inclusive collaboration can ensure sustained supplies of life-saving drugs (32). 

Financing global health security against biological threats remains a persistent challenge.

Unfortunately, but not unexpectedly, a weak and uncertain pre-crisis market demand has led to a relative lack of interest in developing vaccines against EIDs. This has left the global community increasingly vulnerable to repeated outbreaks of these viruses. The challenges of EID vaccine development, however, are troubling issues for vaccines more generally. The situation has been described as a crisis, and perhaps rightly so, as there are only four remaining major manufacturers that focus on vaccine development (25) .

Vaccines only sell for a fraction of their economic value, in some cases for only a few dollars. They provide myriad benefits, like enabling would-be patients to live longer, healthier lives (33, 34) , and bearing yet-undervalued gains in productivity and positive externalities to society at large (35) (36) (37) . Although the low price of vaccines is meant to benefit individuals and regions with lower incomes, in the long run, it has had the opposite effect, causing them to be medically underserved due to a lack of vaccine investment. Pharmaceutical companies and investors are directing their resources to projects in which the estimated return on investment is more predictable and lucrative. Vaccine prices are currently set far below the prices of drugs that treat other serious conditions, such as cancer, despite the enormous societal value of vaccines in general, and those to ensure global health security in particular.

The typical expected risk-adjusted net present value (NPV) of a vaccine in our hypothetical portfolio upon regulatory approval is on the order of only $7.6 million. This is two or three orders of magnitude lower than the comparable value of an approved cancer drug, yet the out-of-pocket costs to develop an EID vaccine are not dissimilar.

In addition to pricing, another challenge lies in assessing the future demand for EID vaccines. Due to the inherent unpredictability in the scale and timing of outbreaks, the future demand for a specific EID vaccine is typically unclear. An additional factor is geopolitical. Diseases that are traditionally found in only a few, lower-income countries may not attract as many R&D dollars because generating a return on investment is more difficult in those limited markets (25, 38) . While wealthier governments might issue purchase agreements to assure vaccine sponsors of returns (38), these commitments are more difficult to secure for EIDs in lower-income countries or those undergoing economic hardship. However, an increasing number of stakeholders are realizing the danger of this dynamic for low and high-income countries alike, as under epidemic outbreak conditions, diseases like Zika and Ebola have the potential to spread much further than their traditional locales. The Ebola outbreaks in West Africa in 2014 demonstrate how the absence of vaccine demand prior to an event may result in a tragic loss of life and a regional economic setback. It is a significant concern that years after those outbreaks, the demand for Ebola vaccines remains limited and uncertain, allowing gaps in preparedness to persist (39) (40) (41) .

Unless these market challenges are addressed, the global population will remain vulnerable to substantial human and economic losses when epidemics and pandemics arise.

We believe that this represents a significant missed opportunity. Aside from the nuclear threat and climate change, pandemics represent one of the most significant existential dangers facing humanity today (42) . Nevertheless, investments in preparedness for biological threats remain underfunded, leaving the world vulnerable to catastrophic . CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint Our analysis strongly suggests that reliance solely on private sector investment in EID vaccines is insufficient, given the negative returns achieved by an EID-focused megafund, and the negative impact such a pool of assets would have on an otherwise profitable pharmaceutical company. As a result, if EID vaccine candidates are to be developed, continued private-public cooperation will be imperative, and novel approaches to engage and attract capital will be needed. While bond markets are capable of providing access to substantial amounts of capital to help vaccine development efforts, the resources available to the public sector have great potential as well (43) . In 2015, the U.S. spent $9,990 per person on healthcare (44) . If we assume that there are 300 million Americans, just 1.25% of this amount of spending would yield $37.46 billion dollars, greater than the projected $35.25 billion it would take to fund the entire EID portfolio of vaccines. While achieving such an allocation of funding would hardly be as simple as this calculation suggests, this thought experiment illustrates that encouraging the development of vaccines that protect against EIDs of pandemic potential is well within the means of the global public and private sector stakeholders, if there is public support and political will. In fact, there is evidence to indicate that people expect and would support further protection from these threats (45) .

The U.S. government's MCM program has demonstrated a capability to create incentives for the development of vaccines that would otherwise not be developed, once sufficient market demand is guaranteed ahead of time. This has been true for anthrax and smallpox as well as for various strains of pre-pandemic influenza, for which the government provides market commitments on the order of $100-200 million per year for successful vaccine development programs (46, 47) . While challenges exist (e.g., sustained funding commitments), new initiatives such as CEPI can learn important lessons from these examples (48, 49) .

Perhaps key to the problem of EID vaccine funding is a deficiency in the pricing of the risk of infection by EIDs. Although the prevention of epidemics and pandemics saves countless lives and billions of dollars of economic value, the revenue realized by vaccine manufacturers is only a very small fraction of this value. With this in mind, an examination of a capitated fee structure-a subscription model-applied to vaccine development and acquisition is promising. Under the current model, vaccines are purchased a la carte after outbreaks begin.

However, if stakeholders were to pay in advance to develop and stockpile vaccines, viewing their payment as a form of insurance that would maintain epidemic response capabilities and provide protection from EID outbreaks, much like a society-wide immune system, the amount of capital needed to fund these programs might be easier to raise and keep the price per regimen lower. Vaccine developers under this model would most likely sell subscriptions to governments, building upon existing infrastructure, such as the U.S. government's biodefense and pandemic preparedness programs. To balance the concern that nonsubscribers may require vaccine regimens with the objective of encouraging subscription ahead of outbreaks, a tiered pricing scheme rewarding early adoption could be implemented. A private subscription model should also be explored, however, as it would enable individuals, communities, and corporations to take greater ownership in preparedness. Determining precisely who should pay the insurance premium, and who is willing to pay, is essential to this arrangement.

. CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint Although this model is a departure from the status quo, promising innovation in vaccine financing is becoming more commonplace. The recent World Bank issue of pandemic bonds and swaps for a Pandemic Emergency Financing Facility (PEF) suggests that when structured appropriately, assets geared toward preparedness can be attractive to investors (50) . We believe that our model may shed some light on what will encourage more comprehensive pandemic preparedness by addressing shortcomings in the EID vaccine pipeline.

As demonstrated in our simulations, the investment required to reduce the global risk from EIDs is within reach. Securing these resources, however, will require governments to strengthen their commitments to supporting EID vaccine markets, in order to allow private sector stakeholders and untapped capital to engage with these markets substantively. The recent developments around Sanofi Pasteur's Zika collaboration highlight the risks of a variable commitment to preparedness. Due to changing epidemiology and internal disputes over potential product pricing, BARDA and Sanofi have chosen to halt further development of their Zika asset, leaving society vulnerable to future outbreaks (51) .

As cases like this suggest, government buy-in is integral for long-term pipeline sustainability. Governments can catalyze outside investments through a range of strategies, including guaranteed commitments. Fifteen years of guaranteed revenue via purchase commitments, similar to the U.S. government's purchase of smallpox and anthrax vaccines, would do well to encourage development efforts. For example, an annual purchase commitment of $150 million per successful vaccine candidate would represent an NPV of $1.14 billion, exceeding our modeled breakeven NPV of $772 million. Our results suggest that investment in this space is highly unattractive to the private sector, requiring commitments of the aforementioned magnitude for development viability; as highlighted above, either the price per regimen or the demand from outbreaks would have to increase by orders of magnitude to have the same effect. We encourage readers to engage with these assumption parameters critically using our open source software.

While the main focus of this paper is the challenge of financing EID vaccine development, we realize that there are other concerns that must be considered in parallel before a portfolio of novel EID vaccine regimens is made available to the public. These issues include, but are not limited to, preclinical discovery, regulatory approval strategy, and post-approval procurement and distribution. These are matters of great importance and warrant further investigation.

It is indisputable, however, that better business models for global health security are urgently needed. We expect there may be benefits to extending the scope of the megafund approach beyond the particular EID vaccine assets considered in this study, perhaps to antibiotics or MCMs for intentional biological threats, an additional global health security concern. While this would do little to improve the desirability of EID vaccine candidates as assets, broadening the scope of a fund to address additional threats may create greater financial viability to global health security more broadly.

As past efforts demonstrate, the key to generating interest in developing vaccine assets is to offer sufficient financial incentives for would-be developers, such as direct market commitments or priority review vouchers. Closing the gap between the economic value of

Funding support from the MIT Laboratory for Financial Engineering is gratefully acknowledged, but no direct funding was received for this study and no funding bodies had any role in study design, data collection and analysis, decision to publish, or preparation of this manuscript. The authors were personally salaried by their institutions during the period of writing (though no specific salary was set aside or given for the writing of this manuscript).

J.V. and B.K. report no conflicts.

S.C. is a co-founder and chief technology officer of QLS Advisors, a healthcare analytics and consulting company.

Corporation, a not-for-profit organization working in the public interest as an operator of multiple federally funded research and development centers (FFRDCs). She is focused on the sustainability of the biodefense industrial base and the public-private partnerships that are vital to national and global health security. 

Historically, prizes have often been used as an incentive for technological innovation. For example, the first Kremer Prize of £50,000 was awarded in 1977 for the invention of the ""first substantial flight of a human-powered airplane"" (7) . While some experts believe that this approach might create sufficient incentives for research and development in less commercially attractive diseases (8, 9) , there is substantial difficulty in applying this structure to EID vaccines, as the prize pool would have to be large enough to offset the high development costs. As a result, several experts have proposed market-based approaches instead (10, 11) . Most recently, a prize model has been proposed to incentivize the development of novel antibiotics to address the increasing global problem of antibiotic resistance (12) . Importantly, the price is delinked from the volume of sales (as with U.S. government acquisition programs), a key issue for EIDs, where volumes are often insufficient to drive viable markets (13) .

Another mechanism is the FDA priority review voucher program, currently implemented by the U.S. government. Under this program, first proposed by Ridley et al. (11) , companies developing a therapy for a traditionally ""neglected"" disease can apply for an FDA priority review voucher. Such vouchers can be used by the company for the accelerated review of another, potentially more lucrative asset, or sold to another firm for review of one of their own assets. Extending this program to medical countermeasures has been under consideration for years (1) , and the U.S. 21st Century Cures Act expanded the scope of the program to MCMs for material threats (e.g., smallpox), now including Ebola and Zika (14, 15 ). An analysis by Berman and Radhakrishna suggests that these vouchers have tremendous value, with one selling for as much as $350 million on the open market (16) . However, their value may be waning as more become available, as acquisition prices have decreased over the last few years (15) . Even so, the idea has garnered significant attention, and a European equivalent overseen by the European Medicines Agency (EMA) has been proposed (17) .

While some see priority review vouchers as a step in the right direction, vouchers are not without potential drawbacks. For example, vouchers do little to ensure that subsequent vaccine development will be pursued once the first candidate has been approved (8, 15) . It is also unclear that the resultant vaccines will ultimately reach patients after approval, after the vouchers have been assigned, once market realities are taken into account (8) . They provide one-time revenues to a firm, and do little to ensure sustained manufacturing capability or availability of a vaccine. It should also be noted that the FDA priority review may result in a . CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint rejection, making the value of the voucher to a firm more variable than it first appears (15) .

The final mechanism under consideration is the advance market commitment. This concept is similar to the advance purchasing commitments that the U.S. government can make under Project BioShield for MCMs up to eight years in advance of their licensure (18) . Advance market commitments allow vaccine developers to assess the potential demand for their product if approved, and provide some guarantee of expected compensation for their efforts. Levine et al. (10) describe how such a structure would operate. Essentially, stakeholders from wealthy countries would agree to pay a certain price per dose for a successful vaccine against a target disease, subsidizing the amount that a poorer country would pay, should the development project prove successful. While the risk of scientific failure would still be present, some of the potential demand and revenue would be quantified before the project would be undertaken, serving as encouragement to prospective developers. However, this approach assumes that wealthier entities will still be interested in purchasing vaccines for relatively rare diseases that might not have a direct impact on their constituents (19) unless a significant outbreak emerges.

While the methods described here may help mitigate the shortcomings of vaccine investment, they also suggest that a more sustainable long-term solution lies in aligning the incentives for wealthier stakeholders with the incentives of those people most vulnerable to EIDs. Indeed, this alignment is prudent for the former group, as under outbreak conditions their health security may be at risk, even in places where EIDs are unlikely to emerge (19) , as the recent Zika and Ebola outbreaks illustrate.

Recent work by Fan et al. (20) calculated that the global expected loss due to pandemic influenza would be approximately $570 billion annually. In 2015, the WHO noted the emergence of many novel influenza viruses, resulting in an ""especially volatile"" gene pool, left the consequences to human health ""unpredictable yet potentially ominous"" (21) . World Bank projections give a sense of the cost of inaction: a worldwide influenza epidemic would reduce global wealth by an estimated $3 trillion (22) . Even with diligent containment efforts and antiviral therapy, Colizza et al. suggest that a particularly infectious strain might still infect 30-50% of the global population (23), making prophylactic vaccines essential in mitigating pandemic risk (24) .

We present the details of our simulation analysis of the expected risks and returns of a portfolio of 141 preclinical emerging infectious disease (EID) vaccine candidate projects, as well as the assumptions used to estimate the annual expected revenues from direct sales of vaccines to susceptible populations for the 9 different EIDs addressed in our megafund. In our portfolio, we utilize CEPI's (25) targeted EIDs and pipeline research (26) , which is based upon the World Health Organization's R&D Blueprint for epidemic prevention (27) Table S1 . Portfolio of assets 9 target EIDs and the number of projects for each included in the hypothetical portfolio.

We begin with a discussion of the correlation assumptions underlying the Monte Carlo simulation of our EID megafund portfolio's performance, and provide details regarding our projected development costs and phase-transition assumptions. We then turn to how investment returns are defined, and conclude by describing our projected revenue estimates for EID vaccines.

While there are a number of methods for modeling the outcome of clinical trials with certain scientific elements in common, we numerically estimate the performance of our EID vaccine portfolio by modeling projects as pairwise correlated Bernoulli trials. Our methods are similar to Lo et al. (28) .

Denote by ≡ 1 2 ⋯ ′ a column-vector of random multivariate standard normal variables. Then for any positive-definite matrix Σ, the new vector of random variables Σ 1/2 is multivariate normal with covariance matrix Σ, where Σ 1/2 denotes the Cholesky factorization or matrix square root of Σ. Once the success probability, , for each Bernoulli trial random variable is defined, can be simulated as 0 if 1 if where we define Φ 1 1 and Φ 1 • is the inverse of the standard normal cumulative distribution function.

For our purposes, pairwise correlations are meant to capture commonalities among translational vaccine development programs, so that success or failure in one program has predictive power for the success or failure of another program. In addition to specifying values for each entry in Σ that are based on domain-specific knowledge of the underlying science, we must also ensure that Σ is a valid positive-definite covariance matrix.

. CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint In our simulations, we adopt a three-step process in which all pairwise correlations between projects are first evaluated qualitatively as ""low"" or ""high."" These assessments are then translated into numerical values of 10% for ""low"" and 50% for ""high."" The outline of the dimensions used to assign correlation levels is displayed in Table S2 below. Figure  S1 shows a heat map of these assumed correlations. The third step is to apply the numerical algorithm developed by Qi and Sun (29) to compute the closest positive-definite matrix to the one specified manually. In this case, the manually defined correlation matrix shown in Figure 2 in the main text was already positive-definite, indicating that the Qi and Sun algorithm had no impact.

. CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint 

We use the CEPI estimates of phase-transition probability and development time at each phase in our simulation (shown in Table S3 ), seeking to develop each asset through phase 2 (25) . CEPI assumes that the measures taken by global actors in response to the recent Ebola outbreaks indicate that phase 2 development would justify the stockpiling and conditional use of these candidate vaccines in an actual outbreak setting, one that could support later vaccine approval and distribution (25) . We use CEPI's estimate that the cost to develop each preclinical asset through phase 2 is $250 million.

Our simulation assumes trials with a standard progression from phase to phase. If earlier stages of R&D are included, or if a trial must be repeated, the costs and duration will increase, and the post-approval patent life of the asset will decrease. On the other hand, because we have not modeled the transition from one clinical phase to the next, the realized out-ofpocket cost of a typical project could be less than the assumed $250 million because of the early termination of failed projects. CEPI's assumption of $250 million of out-of-pocket costs falls well within industry estimates of the vaccine development costs through phase 2 with limited manufacturing scale. Though not analyzed here, the inclusion of phase 3 and manufacturing facility maintenance and surge/scale-up can be factored into the model using our open-source software. . CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint 

An investment rate of return, , where an initial investment of yields a single payoff is defined as / 1. If the investment is over a duration 1 year, the return is often annualized to simplify comparisons with other investments of different durations. This geometric compounding assumes that interim gains are reinvested, and hence additional interest is paid on the interest earned. The annualized return, , is defined as 1 .

This definition is relatively straightforward. However, a question arises in the computation of expected returns and standard deviations for multi-year returns, which require annualization: should the moments be computed before or after annualization? In the main text, we annualize realized returns before calculating statistics such as expectation and standard deviation. While there is no clear argument for using one method over the other in all contexts, we have chosen to annualize first to calculate the realized internal rate of return (IRR), and then to compute the expected IRR and standard deviation of IRR, which are the more traditional summary statistics.

It is well recognized that predicting the type, frequency, and scale of any future EID outbreak, epidemic, or pandemic with accuracy is not possible, and therefore certain practical assumptions were necessary to project revenues. In our model, the probability of a given disease having an outbreak in a given year is given by the ratio of the number of historical outbreaks to the number of years since the disease was first reported or since the first notable outbreak. Respective probabilities are listed in Table S4 below. This represents a pragmatic approach, and is not expected to reflect actual future epidemiological patterns. While more sophisticated models are available and have been used to support other pandemic financing programs (30) , this approach is intended to provide a baseline assessment of megafund financing (31) .

. CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint (32, 33, (42) (43) (44) (45) (46) (34) (35) (36) (37) (38) (39) (40) (41) .

The number of vaccine regimens sold in response to an outbreak is a function of both the actual and perceived risk to populations. This is subject to significant uncertainty, due in part to gaps in knowledge at the onset of an outbreak about its transmission patterns and its medical and public health impact (19, (47) (48) (49) . We based our projections of the number of vaccine regimens sold on the average number of infections observed per outbreak, and further modulated by three factors: Woolhouse Potential for Pandemic Spread, Severity of Clinical Symptoms, and Mortality Rate. The number of vaccine regimens sold is given by the average number of cases per outbreak multiplied by Woolhouse weighting and clinical severity rating as described below. This is used as a crude proxy for demand extending beyond those immediately affected, e.g., the so-called 'worried well'.

By the nature of the methodology that CEPI used to establish their priority list of vaccines, all of the viruses addressed in our portfolio are known to be potent, contagious pathogens. However, the transmissibility between humans will vary. Woolhouse et al. (50, 51) categorize EIDs into four levels, described in Table S5 Woolhouse Level Interpretation Level 1

Humans exposed but not infected Level 2

Humans infected Level 3

Human-to-human transmission Level 4 Increased potential for epidemics/persist as endemic infection Table S5 . Levels of transmissibility as categorized by Woolhouse et al. (50, 51) , Risks to those not involved in the initial outbreak, both real and perceived, will also drive the demand for these vaccine regimens. As can be seen by the ongoing discussions between the U.S. government and Sanofi regarding licenses of Zika vaccines, any projections of future demand are theoretical at best (52) . However, it is difficult to discount the effect of public perception on the willingness of people and policymakers to take action against these pathogens, as illustrated by the discovery of the connection between Zika and congenital microcephaly.

Though all the EIDs studied are a significant threat to human health, each disease presents itself with different symptoms and a unique prognosis. These differences in presentation and outcome may affect the way in which the public responds to outbreaks. We rate each disease by clinical presentation and mortality rate as mild, mild-moderate, moderate, moderatesevere or severe in Table S6 , and assign a corresponding multiplier in Table S7 . The multipliers in Table S7 were informed by recent developments about a promising new vaccine candidate for Ebola. According to the most recent reports, Merck has promised to produce 300,000 doses of the vaccine (53), while the 2015 outbreak totaled approximately 30,000 cases (54) . Given the severity of clinical symptoms and high mortality rate associated with Ebola infection, we assign a multiplier of 10 to our ""severe"" category of diseases, and adjust our multiplier accordingly based on clinical severity. While each of these diseases has the potential to cause severe illness, some are asymptomatic in most patients, and thus less likely to elicit high demand for a resulting vaccine. In assigning ratings, we also assumed that the potential for certain sequelae will increase demand for certain vaccines; while the presentation of Zika is generally mild, the possibility of birth defects resulting from infection in pregnant women will likely boost the demand for this vaccine, increasing its relative rating. The price per dose of vaccine was estimated by taking the average of prices listed in the CDC Adult vaccine price list 2016 (55); UNICEF's 2016 product menu for Gavi, the Vaccine Alliance (56) ; and the Pan American Health Organization (PAHO) expanded program of immunization vaccine prices for 2016 (57) . These three averages serve as our vaccine price for high, low, and middle income countries respectively. We then use these to price each vaccine based on the income level of the countries most likely to have an outbreak of a particular disease. (The endemic country/disease/pricing per dose pairings are listed in Table S8 below.) It should be noted that we took a conservative stance on pricing in our model, opting to model the lower income country price for diseases that have historically emerged in nations with differing ability to pay. Our pricing is also likely conservative due to our inclusion of each vaccine on each menu in our mean calculations, including older vaccines that are apt to be produced and sold at lower cost than new vaccines.

The annual expected revenue for each vaccine candidate is then given by the price per dose of vaccine times the expected number of vaccines sold in an outbreak weighted by the probability of an outbreak occurring in a given year.

. CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint 

This simulation parameterizes several assumptions about the cost and duration of vaccine development, the probability of success, and pairwise correlations of success between the projects. These estimates were based on the published literature on vaccine development and qualitative input from scientists with domain-specific expertise.

In this section, we investigate the robustness of our results to the parameterized assumptions of our model. We update the investment return statistics of the EID vaccine portfolio as we vary the development cost and probability of success of each project. The expected return and return standard deviation associated with the perturbed parameters are given in Tables S9 and S10.

In Table S9 , we find that the expected return of the portfolio increases as the cost per project decreases. Similarly, Table S10 reports that the expected return of the portfolio increases as the probability of success of each project increases. However, even under more optimistic assumptions, the expected annualized return of the megafund for the base case remains significantly negative, increasing from -61.1% to only -57.4% when the probability of success is increased by 150%. This sensitivity analysis underscores the robustness of our results, and demonstrates that an EID vaccine portfolio remains economically unviable even under relatively optimistic cost and revenue assumptions.

Finally, Table S11 considers the performance of the EID vaccine portfolio under the scenario where a government agency or philanthropic organization agrees to absorb the initial losses on the portfolio for a predetermined amount, which we specify as 25% and 50% of our simulated megafund's principal. We find that, under the base case scenario, the expected return increases from -61.1% to -23.6% and -12.6%, respectively. While these scenarios remain unprofitable, it demonstrates that if combined with other revenue-boosting mechanisms such as such as advance market commitments and priority review vouchers, the guarantee structure has the potential to transform an unattractive portfolio of EID vaccine . CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . CC-BY 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.

is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.20.20039966 doi: medRxiv preprint

",0.7545859365808821
Social Media and Internet-Based Data in Global Systems for Public Health Surveillance: A Systematic Review,"The exchange of health information on the Internet has been heralded as an opportunity to improve public health surveillance. In a field that has traditionally relied on an established system of mandatory and voluntary reporting of known infectious diseases by doctors and laboratories to governmental agencies, innovations in social media and so-called user-generated information could lead to faster recognition of cases of infectious disease. More direct access to such data could enable surveillance epidemiologists to detect potential public health threats such as rare, new diseases or early-level warnings for epidemics. But how useful are data from social media and the Internet, and what is the potential to enhance surveillance? The challenges of using these emerging surveillance systems for infectious disease epidemiology, including the specific resources needed, technical requirements, and acceptability to public health practitioners and policymakers, have wide-reaching implications for public health surveillance in the 21st century.","to develop event-based surveillance, the existing systems have limitations. Accordingly, there is a need for further development of automated technologies that monitor health-related information on the Internet, especially to handle large amounts of data and to prevent information overload. The dissemination to health authorities of new information about health events is not always efficient and could be improved. No comprehensive evaluations show whether eventbased surveillance systems have been integrated into actual epidemiological work during real-time health events.

The acceptability of data from the Internet and social media as a regular part of public health surveillance programs varies and is related to a circular challenge: the willingness to integrate is rooted in a lack of effectiveness studies, yet such effectiveness can be proved only through a structured evaluation of integrated systems. Issues related to changing technical and social paradigms in both individual perceptions of and interactions with personal health data, as well as social media and other data from the Internet, must be further addressed before such information can be integrated into official surveillance systems.

Keywords: surveillance, health information, Internet, social media. R ecent major health events such as severe acute respiratory syndrome coronavirus (SARS-CoV) in Asia (2002) (2003) , pandemic H1N1/09 influenza virus worldwide (2009), and the large outbreak of Escherichia coli O104:H4 in Germany (2011) have prompted infectious disease scientists at government agencies, university centers, and international health agencies to invest in improving methods for conducting infectious disease surveillance. 1, 2 Opportunities for improvement, however, vary and are based on the distinctive features of existing types of infectious disease surveillance, which have been developed over time to address the various critical components in public health efforts against disease. Standard infectious disease surveillance methodologies have been derived from indicator-based surveillance and event-based surveillance.

Indicator-based surveillance systems are the oldest, most common, and most widely used form of infectious disease surveillance by regional, national, and international public health agencies. These systems are designed to collect and analyze structured data based on established surveillance and monitoring protocols tailored to each disease (ie, used for calculating the incidence, seasonality, and burden of disease), in order to gather relevant information about populations of interest to detect changes in trends or distributions in the population. Data on such indicators are reported by health care providers and diagnostic laboratories, by legal mandate or voluntary agreement, and are collected by surveillance specialists in governmental health agencies. This information then can be verified through communication between the governmental health agencies and the persons collecting the data in health care settings.

Indicator-based surveillance systems often contain reliable statistical methods that have been established to compare the observed number of cases of pathogens with an expected rate. The goal is to find increased numbers or clusters at a specific time, period, and/or location that might indicate a threat. Statistical methods set against thresholds of increased cases or clusters are crucial to finding potential health events. They are based on the relevant attributes of each infectious disease, such as epidemiological parameters like regional incidence, seasonality, and the known burden of disease. Thresholds can also be adjusted using statistical algorithms to vary sensitivity and specificity so that the detection procedure is refined to better suit the needs of the epidemiological situation for a disease or a specific area. This helps epidemiologists by giving them a greater capacity to monitor additional information that might signal threats to public health.

The ability of indicator-based surveillance systems to detect potential threats more quickly is lacking, however. Although generating signals based on statistical thresholds can provide an aggregation that will speed up a threat assessment, the data itself may not be the most recent. First, there is often a time lag between the occurrence of an event and the indicator-based surveillance. That is, data input and retrieval for indicator-based surveillance often rely on specific case definitions and reporting requirements that differ for physicians in hospital and community care and for laboratories, thereby causing delays in reporting to health agencies. Delays also may be caused by time lags between reporting procedures from the reporting bodies and the authorities who receive, store, and process the data, that is, by the structure of notification systems in official public health agencies that often trickle up from the local, state, and federal levels. Second, indicator-based systems are sometimes poorly equipped to detect new or unexpected occurrences of disease, owing to the predefined epidemiological attributes assigned to each infectious disease for which information is collected. This was true during the first cases of SARS-CoV in 2002 and pandemic H1N1/09 influenza in 2009, which at first were not detected because the existing systems could track only the clinical and epidemiological attributes for corona or influenza infections that had already been discovered and defined, but not new strains of viral infections. Incidentally, such shortfalls provided the impetus for the systemic improvement of indicator-based systems. By demonstrating the importance of detecting unknown but similar diseases, it became evident that new data sources and methods for monitoring such data were critical. 3 As a result of the SARS-CoV epidemic, for example, health agencies began to seriously consider ways to monitor symptoms and syndromes (ie, clusters of symptoms for particular diseases) in order to provide appropriate and fast detection with the most efficient use of required human resources.

Similar to indicator-based surveillance, event-based surveillance is based on the organized and rapid capture of information about events that can be a risk to public health. But rather than relying on official reports, this information is obtained directly from witnesses of real-time events or indirectly from reports transmitted through various communication channels (eg, social media or established routine alert systems) and information channels (the news media, public health networks, and nongovernmental organizations) ( Table 1 ). Monitoring that relies on data from these Internet sources can be used to detect threats not specifically found by indicator-based surveillance, since this information relies less on data structured and filtered through the aforementioned preestablished structures for surveillance. Event-based surveillance can identify events faster than indicator-based reporting procedures can, and it can detect events that occur in populations not able to access formal channels for reporting. In addition, event-based surveillance can be used with other established indicator-based methods, thereby enhancing the combined arsenal for combatting critically prevalent pathogens with a high threat potential, such as influenza virus or Escherichia coli. The scientific literature recently referred to this comprehensive framework of combined activities from both indicator-based surveillance and eventbased surveillance systems as ""epidemic intelligence,"" a contemporary understanding of the 1950s term with roots in public health innovation for surveillance systems at the US Centers for Disease Control and Prevention (CDC) and the establishment of the Epidemic Intelligence Service (EIS). [4] [5] [6] [7] [8] Event-based surveillance continues to offer innovation for public health surveillance, for example, by capturing information about events that may not otherwise be detected in the routine collection of data from indicator-based surveillance. Events that may be detected in event-based surveillance include the following:

r Events, such as SARS, that are emerging or rarely occur and thus are not specifically part of the purview of standard indicator-based surveillance.

r Events that occur in real time but have not been detected by indicator-based surveillance, such as those events delayed by the required reporting procedures of notifying the designated health authority.

r Events that occur in populations that do not access health care through formal channels or in which formal, indicator-based systems do not exist, such as events that occur in populations in rural areas or countries with a less established infrastructure for surveillance.

Health information monitored via the Internet and social media is an important part of event-based surveillance and is most often the source on which many existing event-based surveillance systems focus. Existing systems for such event-based monitoring contain useful retrieval features that give epidemiologists and public health scientists involved in surveillance quick access to information compiled from many media and news sources. 9, 10 Other new health information technologies using new data sources from the Internet are important drivers of innovation in global surveillance, speeding up the collection and transmission of information to allow for better emergency preparedness or responses. 11 In research, event-based surveillance using data from the Internet, especially emails and online news sources, has been shown to identify surveillance trends comparable to those found using established indicator-based surveillance methods. [12] [13] [14] In practice, however, such systems have not yet been widely accepted and integrated into the mainstream for use by national and international health authorities.

We reviewed event-based surveillance systems that have actually been used, in order to examine the usefulness of event-based surveillance to existing surveillance efforts and its potential to improve future comprehensive infectious disease surveillance systems.

We conducted a systematic review to identify all currently established event-based surveillance systems used in infectious disease surveillance and to look at the type of data collected, the mode of data acquisition used by the system, and the overall purpose and function of each system. As members of a national scientific institute, our aim was to help health policy decision makers decide whether to incorporate new methods into comprehensive programs of surveillance that already contain established indicator-based surveillance.

The previous work in this area includes a systematic review, by Bravata and colleagues, of 17,510 peer-reviewed articles and 8,088 websites on surveillance systems for the early detection of bioterrorism-related diseases, which evaluated the potential utility of existing surveillance systems for illnesses and syndromes related to bioterrorism only. [15] [16] [17] Another review of peer-reviewed articles by Vrbova and colleagues synthesized surveillance systems for emerging zoonotic diseases with selected criteria used to evaluate those systems. 18 Corley and colleagues helped US federal government agencies compile aspects and attributes associated with operational considerations in the development, testing, and validation of event-based surveillance; and Hartley and colleagues drew up an outline of technical Internet biosurveillance processes. 19, 20 Although this work is important, these reviews do not provide systematically collected details of event-based systems used in practice.

We searched for peer-reviewed articles published in the indexes Pubmed, Scopus, and Scirus between 1990 and 2011 21-23 as well as Englishlanguage studies of infectious disease surveillance (and specifically event-based surveillance) and outbreak detection in human health and medicine. We excluded articles on bioterrorism (for which there is less possibility of pathogen threat), articles on solely technical aspects of system implementation or security (eg, video surveillance), those covering sentinel surveillance systems (ie, those set up randomly, periodically, or in another unsystematic way), any surveillance not based on infectious diseases, and articles without available abstracts. We used extraction criteria to collect comparable data on each system. Appendix 1 provides a detailed overview of the search strategy and methods, and the study's complete protocol also is available. 24 

Our systematic review yielded 13 event-based systems used in practice and for which complete information based on our extraction criteria was available (Tables 2 and 3) .

Event-based surveillance systems can be classified as news aggregators, automatic systems, or moderated systems. 25 News aggregators collect articles from several sources that are commonly filtered by language or country. Although their users have easy access to many sources through a common portal, they must examine each article individually. Automatic systems go beyond this by adding a series of steps for analysis but differ in the levels of analysis performed, in the range of information sources, in language coverage, in the speed of delivering information, and in methods for visualization. In moderated systems, information is processed entirely by human analysts or is first processed automatically and then 11 Automatic USA 2000 a GOARN is a WHO-coordinated network analyzed by people. Moderated systems offer a screening for epidemiological relevance of the data found within the information before it is presented to the user. Although each of the systems that we reviewed has different goals (mostly pertaining to various national, international, and regional audiences), they all foster the communication of health events or threats in the infectious disease community of scientists, physicians, epidemiologists, public health officials, policymakers, and politicians.

The systems overwhelmingly rely on media sources for data input, including local and national newspapers; news broadcasts; websites; news wires; or even short message service (SMS), the text messaging service component of phone, web, or mobile communication systems. 26 Some of the systems already have been incorporated into other larger systems. For example, GOARN links 110 existing networks, and GPHIN collects data already processed with ProMED-mail. 27 Surveillance scientists then review this information to assess its epidemiological significance and to support decision making. But because these data are not structured, epidemiologists must spend more time and energy determining their relevance to a particular situation of interest. Year started The year the system started operating 5

Coordinating organization The unit that operates the system 6

Purpose

The purpose of the system 7

Geographic scope The geographic area covered 8

Language

The number of languages the system covers or gets information from 9

Disease type Type of diseases covered by the system; >3 as ""multiple infectious diseases"" 10 Accessibility

The type of access: freely accessible to the general public vs restricted access 11

Data collection and processing The methods employed to collect the necessary data, and data analysis 12

Dissemination of data The method for data dissemination 13

Users

The organizations or individuals using the event-based system 14 System evaluation The existence of a previous system evaluation 15

Homepage

The web location of the system

We identified three types of coordinating bodies for event-based systems: those based at or in cooperation with universities (Argus, BioCaster, GODSN, HealthMap, and Proteus-BIO), NGOs (GOARN, MedISys, MiTAP, and ProMED-mail) and governmental agencies (EWRS, EpiSPI-DER, GPHIN and InSTEDD).

Each of these systems has a different aim: (1) to improve early detection, (2) to enhance communication or collaboration, and (3) to supplement other existing systems. Ten of the systems are intended to improve early detection: Argus, BioCaster, GOARN, GODSN, GPHIN, HealthMap, InSTEDD, MedISys, MiTAP, and Proteus-BIO. Two of the systems are meant to enhance communication or collaboration (EWRS and ProMED-mail), and one system supplements another (EpiSPIDER for ProMED-mail).

All the systems cover 2 or more countries, but their jurisdictions could be classified as (1) 

All the event-based systems that we reviewed focused on outbreaks of different and multiple infectious diseases, with some systems, such as Argus (130), BioCaster (102), and HealthMap (170), collecting information on more than 100 diseases.

We observed 5 levels of access: (1) freely and publicly available (HealthMap, EpiSpider, GODSN, and Proteus-BIO), (2) available with a free subscription (ProMED-mail, BioCaster, and MiTAP), (3) available with a paid subscription (GPHIN, whose subscribers include governmental organizations, NGOs, and universities), (4) access restricted to certain public health officials (EWRS, Argus, GOARN, and InSTEDD), and (5) mixed-level access (MedISys, offering free but restricted access to the public and outside the European Commission [EC] and full access to officials in the EC). Accessibility varies from system to system, depending on both the scope of the system and the intended audience. While it is important to offer freely accessible information, some sensitive information (eg, personal data or other confidential data) is often filtered in specific ways among public health officials with specific restricted access. GPHIN has restricted access for organizations with an established public health mandate, with access varying according to factors like the organization's size and number of users. InSTEDD is one of the few systems using information to advise organizations like the UN, WHO, and CDC on strategic implementation. Such systems, like EWRS, provide, within a closed network, timely information for preparedness, early warning, and responses.

Each event-based system acquires data differently. Some collect information directly from sources on the Internet (eg, RSS feeds or electronic mailing lists); others collect both from formal members and informal sources; and still others collect from subscribers or members only. Ten systems collect from the Internet (Argus, BioCaster, EpiSPI-DER, GODSN, GPHIN, HealthMap, InSTEDD, MedISys, MiTAP, and Proteus-BIO), and 2 systems collect from both formal members and informal sources (EWRS and GOARN). ProMED-mail is the only system obtaining firsthand information from its subscribers.

Most of the systems we studied function as news aggregators. News aggregators (eg, Google News) use RSS to collect real-time news feeds from thousands of news sources from around the world, and many systems deal with a huge amount of information each day. MediSys, for example, monitors an average of 50,000 news articles per day from about 1,400 news portals in 43 languages. GPHIN processes from 2,000 to 3,000 news items per day, of which about a quarter are irrelevant or duplicates. 26 Many of the event-based systems utilize text-mining technology to extract only relevant data, and most have sophisticated processing systems of filtering and classifying relevant information to reduce the amount of data.

Source data (ie, event-based data retrieved from the Internet) should be reviewed for epidemiological relevance, either by human epidemiologists or automated systems. This is technologically simple but timeconsuming and expensive, with human moderation having a different role in each system. The information provided through ProMED-mail, for example, is validated and confirmed by humans. EWRS utilizes an informatics tool that filters and relays information to users via a webbased system that links contact members of the EWRS network.

Human input, hypothesis generation, and review are important components of systems. InSTEDD and GPHIN incorporate human input and review, allowing users to add comments, tags, and ranks during the data-processing phases and confirmation and feedback during the dissemination phases.

Systems without human moderation often focus on data sources that already have been validated. Many systems contain new data on outbreaks or diseases, but only some are relayed as firsthand, primary information. Other data are reported as secondary sources like newspaper articles. Although this information can be useful to surveillance epidemiologists who monitor data and conduct research on a known infectious disease area, because these events already have been reported, it does not help epidemiologists interested in the early warning and alert potential for unknown or new infectious disease areas. Because MedISys offers no human mediation in collating information sources and articles, all information must be examined in order to learn more about the outbreak or event in question. Accordingly, how the information is presented is less easily adapted for use in daily practice.

Almost all the systems not relying on human moderation are automated with thresholds used to reduce noise and to present only the most relevant data. MediSys uses a scraper software, for example, that automatically generates an RSS feed from webpages and applies a textextraction process, which then enables content analysis using analytical technology. 28 The text-extraction process uses document heuristics, an experience-based technique for computer learning that is applied to the information to enable an intelligent decision about its relevance. The heuristics learn as their output is verified against a set threshold for the epidemiological attributes of health events that have been extracted, thus improving monitoring over time. The system aggregates the extracted events into outbreaks, across multiple documents and sources, before returning the extracted information to the system. Users of the system often prefer a more structured approach, but it may present too much information (in some cases up to 1,000 events per day). The large amount of ""information noise"" also may be a hindrance, since users are then required to sift through it manually. HealthMap alleviates noise by integrating data from a variety of electronic sources that already have been moderated (ProMED-mail, WHO-validated official alerts, and the Eurosurveillance RSS multinational outbreak news site), all of which are fed into a classification engine (ie, a parser), which uses the information to produce disease and location output codes. Once classified, articles are filtered into a category and stored in a database.

Three systems are disseminated on a geographic map: BioCaster, EpiSPI-DER, and HealthMap; and 4 systems are disseminated through a website or news aggregator: MedISys, MiTAP, ProMED-mail, and Proteus-BIO. We found 6 systems that were disseminated through a secured or restricted portal: Argus, EWRS, GOARN, GODSN, GPHIN, and In-STEDD.

Our systematic literature review demonstrates the diverse attributes in current, established, event-based surveillance systems. Our review also articulates the factors that might influence the integration of such surveillance activities into official systems. The usefulness of new information sources via event-based surveillance depends on whether the information can enhance the data collection from existing surveillance methods and also on several factors related to the challenges for all systems' acquisition of data on infectious disease surveillance. 29, 30 For most epidemiologists, the process of gathering data from the Internet is complex, as it includes text mining (searching for health-related content from websites or social media), preparation (extracting and filtering relevant health-related information), and presentation of only the most relevant content (disseminating the information). In general, data are acquired and processed either automatically or by people, often relying on individual technologies for users' interaction with the data to tag (mark or catalog) the information for future use, and to comment on the information (for sharing and collaboration with other scientists), which can also be used to inform machine-learning algorithms (eg, statistical filters for data retrieval, such as Bayesian models). Systems often use automatic programming interfaces (APIs), a type of filter through which the data are passed in order to extract specific information. This is a good way of managing large sources of data from the Internet, which can be cumbersome and contain much content not related to health. These APIs process, extract, augment, and compile the epidemiological attributes in the data (ie, metadata) from multiple sources. For example, healthrelated attributes could include the data source, a relevant health term, the location, and the time of transmission. Natural language-processing systems extract from the feeds such relevant concepts as disease names and references to a geographic location. The information is then often assigned a dissemination format based on the information type (ie, epidemiological attribute) retrieved, through a network (eg, GPHIN), the Internet, email (ProMED-mail), or SMS (EWRS), or it is plotted on a geographic information system (GIS) published on the web (eg, HealthMap). The time needed to get from a potential data source to extracting and presenting epidemiological information that can be used as quickly as possible for preparedness or responses is critical. The results vary widely, depending on the combination of technologies used and whether or not human mediation is involved.

Even though event-based surveillance systems have been much improved, they still have limitations:

Information is not always moderated by professionals or interpreted for relevance before it is disseminated to interested surveillance epidemiologists. Information retrieved from event-based systems can originate from either official sources who can be seen as trusted health specialists or unofficial sources, such as the public, who may or may not be health specialists. Information from unofficial sources is often not prescreened by professionals, so it can cause reliability issues and necessitate moderation. Thus, moderation affects the quality of event-based information, compared with information from indicator-based systems, which almost always is provided by official sources.

There is no standardized system for the frequency of updates, often resulting in too much information. Information from event-based information components that use news aggregators often is incomplete and may not be timely. Data may be obsolete by the time it is picked up by epidemiologists because some information may have been published by news agencies after health organizations knew about an event or problem. In existing event-based surveillance systems, the frequency of updates varies from approximately several to hundreds of notifications per day, depending on the system.

Algorithms and statistical baselines are not well developed. Until now, event-based systems have not applied algorithms and statistical baselines to information before it is presented to users, which is a standard feature of most established indicator-based systems. Event-based systems often receive a high volume of information per day, which can overwhelm epidemiologists at public health agencies who perform surveillance and may be seen as a hindrance, since users are then required to spend time moderating the retrieved information.

New information about health events or probable cases is not always disseminated efficiently. Event-based systems filter and organize information about potential events of interest before it is presented to users. Information indexed by topic or subject enables users to decide whether they need to do more research. Some systems use online watch boards offering lists or tables of information on events; others rely on SMS; and still others provide options for other notification, like the ability to subscribe to an RSS-feed or through other web capabilities, like Twitter.

Some studies have shown that automated methods and technologies like those used in event-based surveillance can rapidly signal the detection of infectious diseases. 31, 34 In addition to speeding up detection by bypassing traditional indicator-based surveillance structures, event-based surveillance can also provide innovation in settings with weak or underdeveloped surveillance systems. In developing countries with a large disease burden, surveillance infrastructures that can use health information in the absence of traditional surveillance institutions can be critical to prevent an outbreak or reduce its impact. 35 Recent work has begun in this area to seek out information on health threats using mobile phone technology, Internet-scanning tools, email distribution lists, or networks that complement the early warning function of routine surveillance systems. [36] [37] [38] [39] [40] Our research showed that the majority of event-based surveillance systems are based in North America and Europe, with fewer local, event-based systems monitoring epidemic threats in Africa, Asia, the South Pacific, and South America. Guidance and training to create such systems on the ground should be considered, as this can lead to a faster assessment of health threats and a more rapid response by local authorities.

Previous evaluations of event-based surveillance systems have been limited, so we have very few examples to draw from. 41, 42 Although explored since the mid-2000s, largely in response to the SARS-CoV epidemic, event-based surveillance has yet to be fully integrated into public health surveillance systems. Evidence showing the added value to traditional infectious disease surveillance methods is sparse. 43 The development of appropriate metrics for monitoring and evaluating the quality of the data in event-based surveillance systems has become a priority but has just begun. Standard guidelines for the evaluation of surveillance systems offer much information about the attributes needed for measuring the appropriateness and effectiveness of specific systems. Most guidelines, however, rely on attribute descriptions taken from traditional or indicator-based surveillance. These have seldom been adapted to address specific concerns about the new information from event-based surveillance systems and may be inadequate.

Those standard operating procedures, tools, and guidance for eventbased surveillance that do exist-as is often the case with indicator-based surveillance as well-are not universally applicable, since different regions, countries, and smaller jurisdictions must adapt the surveillance systems to their particular needs. In 2005, the WHO established international health regulations (IHR) for surveillance activities that offer the WHO's 193 member states a multilateral legal framework for surveillance, notification, and responses to disease outbreaks and other emergencies with potential international public health implications. [44] [45] [46] The new IHR require the WHO and its members to develop real-time event management systems for addressing public health risks and emergencies of international concern along with the usual epidemiological tools.

Regulating the identification of disease outbreaks and other emergencies with potential international public health implications also requires technical advice to develop adequate surveillance activities. Innovative methods for screening information will no doubt become a priority as definitions of event-based surveillance, recommendations for implementing activities, and evaluations of surveillance systems are established and grow. Event-based surveillance utilizing the fast electronic communication and news sources on the Internet have been widely successful and will likely continue to help improve event-based surveillance. 32, 33, 47 The Challenges of Integrating Event-Based Surveillance

Our literature review uncovered no systems that are currently part of national programs for surveillance. Instead, they are used intermittently as complementary sources of information. We also have little information about whether or not these systems have been integrated into actual work during real-time health events. The current literature does indicate that event-based surveillance could improve official surveillance activities, but systematic evaluation within a public health agency is needed before it can be realized. 48 This is a circular dilemma, since the willingness to integrate is rooted in the lack of effectiveness studies, yet such effectiveness can be proved only by the structured evaluation of integrated systems.

The number of factors necessary for integrating such services should not be underestimated. These include time-consuming and costly collaboration with statisticians, Internet and media experts, and computer scientists to work on components of data acquisition, data processing and filtering, personalization of results, and automation for dissemination to epidemiologists. Once developed, these technical services will require staff to train and support scientific users (eg, epidemiologists) in monitoring infectious diseases, since such activities are not yet part of regular training programs for epidemiology or public health.

Another challenge is the creation of a strategy to compare and crossverify indicator-based and event-based data, since they can differ, especially in regard to syndromes and locations, which makes it difficult to make conclusions based on specific epidemiologic attributes. 49 Nonetheless, solutions must be found, perhaps newly elaborated epidemiologic ontologies for text mining and a related process of continuous improvement. Can all this be done, and is it worth it?

The benefits to epidemiologists clearly are the data retrieved for analysis and potential public health warnings and intervention. In particular, the data's value to the early warning and detection of outbreaks needs to be demonstrated by evaluating the content found in social media and other Internet data sources. Primary content (ie, firsthand observations) provided by the users themselves is valuable, as it would likely signal a potential health threat more quickly. Here again, a usability study over time is needed to help show how useful primary content would be. Online media, weblogs, scientific and nonscientific discussion forums, and direct electronic communication could help expand event-based surveillance activities, although they may have unforeseen social aspects affecting both the data and the development of a health threat. Learning of the existence of disease through firsthand observations, for example, besides signaling health events can also influence people's perception of what they are observing. If the perceived risk of an outbreak is increased, more firsthand reporting could overinflate the health event. Studies of human behavior and Internet interaction may also help clarify social and behavioral effects (eg, age, gender, education level, income, and personality traits like extraversion, openness, and emotional stability) on content generated by social media and the Internet.

Health authorities who intend to use content from social media and other Internet data also need to consider protection and privacy, such as legal and ethical implications related to using Internet and social media data for public health surveillance. For example, it remains unclear what data may be freely accessed and used and whether or not privacy laws and related issues will prevent the structured analysis of new data. These issues are relevant to any surveillance tool that processes Internet or social media data, especially at governmental institutions. 50 

Even though the importance of social media and Internet-based data to epidemiological surveillance is clear, health agencies have been reluctant to incorporate these data sources into their systems because many technical issues have not yet been addressed. The technologies used in event-based systems must be adapted to the individual perceptions of and interactions with their own epidemiological data and to social media and other data from the Internet. Future work in this field will have wide-reaching implications for investments in systems for early warnings of and responses to health threats across the globe and for optimal public health surveillance in the 21st century.

relevant. Of 827 articles, 584 remained after eliminating 243 duplicates. We carefully reviewed abstracts of the 584 remaining articles and eliminated others based on the full content of each article. The categories for classification were (1) background (ie, articles not directly describing an event-based surveillance system but rather surveillance systems in general) or (2) system (ie, articles describing at least one event-based surveillance system). Those articles categorized as system were further distinguished between those covering only one-off monitoring activities (ie, one-time collection, analysis, and interpretation of health-related data for a defined period only) vs wider surveillance (ie, continuous monitoring, systematic collection, analysis and interpretation of healthrelated data), and, finally, those covering either indicator-based or eventbased data ( Figure A1 ).

Search Results. The combined search terms retrieved 39,000 articles, and after applying the inclusion and exclusion criteria, including a rigorously defined synthesis and extraction methodology, 123 articles were identified as providing ""Background"" information only; 6 articles were identified as describing ""Monitoring Systems""; 166 articles were identified as describing an ""Indicator-Based System""; and 44 articles were identified as describing an ""Event-Based Surveillance"" system. Of those 44 articles, 18 event-based surveillance systems were identified based on reading only the abstracts. After reviewing the full texts of all 44 articles, 5 of the 18 systems that had been classified as ""Event-Based Surveillance"" did not contain sufficient information for assessment and thus were eliminated. A final result of 32 articles enabled us to provide full descriptions based on our rigorous categorical data extraction criteria, which resulted in full descriptions for 13 event-based surveillance systems used in practice.

",0.7520584104331635
xMAP Technology: Applications in Detection of Pathogens,"xMAP technology is applicable for high-throughput, multiplex and simultaneous detection of different analytes within a single complex sample. xMAP multiplex assays are currently available in various nucleic acid and immunoassay formats, enabling simultaneous detection and typing of pathogenic viruses, bacteria, parasites and fungi and also antigen or antibody interception. As an open architecture platform, the xMAP technology is beneficial to end users and therefore it is used in various pharmaceutical, clinical and research laboratories. The main aim of this review is to summarize the latest findings and applications in the field of pathogen detection using microsphere-based multiplex assays.","High-throughput multiplex detection techniques are designed for the rapid, sensitive and specific testing of large numbers of analytes (nucleic acid assays, immunoassays, enzyme assays, or receptor-ligands) in a single biological sample. These techniques enable analysis of large numbers of samples. On the other hand, there are also classical single reaction detection methods based on determination of nucleic acids such as polymerase chain reaction (PCR) (Dunbar, 2006; Taylor et al., 2001) , quantitative real-time PCR (qPCR) (Wuyts et al., 2015; Iannone et al., 2000) , reverse transcription PCR (RT-PCR) (Weis et al., 1992) and reverse transcription quantitative PCR (RT-qPCR) (Bustin, 2000) , or antibody-based tests like enzyme-linked immunosorbent assays (ELISA) (Engvall and Perlmann, 1971; Vanweeme and Schuurs, 1971 ) represent nowadays the ""gold diagnostic standard"" in many laboratories. Despite the previous implementation of these methods for routine rapid, sensitive, specific and cost-effective molecular diagnostics, their ability to simultaneously detect multiple analytes in a single reaction is limited and this limitation has yet to be overcome. The increasing amount of proteomic, transcriptomic and genomic sequence data from a large number of organisms accessible in public databases represents an exceptional opportunity for the development of new, multiplex detection technologies. The Luminex R xMAP technology (x = analyte, MAP = Multi-Analyte Profiling) that was invented in the late 1990s represents such a platform that can benefit from all the advances in DNA research (Angeloni et al., 2014) . Although PCR allows multiplex amplification of several targets in a single run xMAP as a methodology represents a significant step forward, and was designed with the aim of creating a high-throughput bioassay platform, enabling rapid, cost-effective, and simultaneous analysis of multiple analytes within a single biological sample. As an open architecture platform, the xMAP system holds many benefits for the end user and therefore it is used in pharmaceutical, clinical and research laboratories (Dunbar and Li, 2010) . The main aim of this review is to summarize the state-of-the-art of xMAP technology applications in the detection of viral, bacterial, parasitical and fungal pathogens from different matrices.

The principle of xMAP technology is based on the concept of a liquid (suspension) array. In contrast to the conventional microarray technology where the identity of the analyte is characterized by its position on the glass slide, the xMAP technology uses different sets of microspheres in a liquid suspension as determiners of analyte specificity. Microsphere sets are internally dyed with two spectrally different fluorophores. The spectral signature is unique for each microsphere set and is determined by different concentrations of internal dyes, producing a 100-member array of spectrally distinct microsphere sets (Figure 1 ). Integration of a third internal dye has allowed the expansion of up to 500-member microsphere sets (Dunbar and Li, 2010) . The surface of each microsphere set allows a simple chemical coupling of various reagents specific to a particular bioassay, such as nucleic acid assays, immunoassays, enzyme assays or receptor-ligand assays. A further fluorescent reporter (e.g., Streptavidin-R-phycoerythrin, Alexa 532, Cy3) is coupled to a target molecule, which allows its detection after specific capture on the microsphere surface.

There are different types of commercially available microspheres (Table 1) , and their selection is generally FIGURE 1 | The xMAP Technology based on internally dyed microspheres. Different concentrations of red and infrared fluorophores were used to create 100 distinct microsphere sets. Each set is able to conjugate to a specific target molecule (yellow and orange lines = nucleic acid; green star = fluorescent reporter). determined by the type of instrumentation used for detection and the particular analyte of interest (Dunbar and Li, 2010; Houser, 2012) . Basic microspheres are 5.6 µm polystyrene beads whose surface is covered by approximately 10 8 carboxyl groups (COOH) for covalent coupling of capture reagents (Tang and Stratton, 2006) . Magnetic microspheres (Figure 2 ) differ in size and structure through the addition of a magnetite layer (Dunbar and Li, 2010; Houser, 2012) . Usage of magnetic beads improves washing efficiency as the magnetic separation step enables the elimination of unwanted sample constituents. Moreover, MagPlex-TAG microspheres are covalently pre-coupled with unique 24 base pair-(bp)-long anti-TAG oligonucleotides that serve as an anchor for target sequences containing the complementary TAG sequence. This proprietary TAG system (xTAG technology) is optimized to have minimum cross-reactivity. An assay can be easily designed by adding a complementary TAG sequence into the sequence of the primer or detection probe of interest and hybridization to the anti-TAG sequence on the microsphere surface.

The analysis of beads is in general performed by two lasers. The red classification laser/LED (635 nm) excites the inner fluorescent dyes of the microspheres, thus identifying a specific microsphere set according to its spectral signature. If the analyte of interest is present, the green reporter laser/LED (525-532 nm) recognizes the fluorescent reporter bound to the captured analyte on the microsphere surface.

There are approximately 10 4 microspheres from each set present in a single sample. This number represents the range in xMAP, in which it is possible to perform determination of quantity according to a calibration curve, similarly to qPCR. However, one must bear in mind that inclusion of a PCR amplification step prior to xMAP analysis does not reveal the real number of DNA molecules present in the original sample, but can only be used for the approximate estimation of DNA quantity. Therefore, xMAP can provide only semi-quantitative data.

The simultaneous reading of both spectra is performed in purpose-designed readers ( Table 2) . They differ by their mechanisms of fluorescence capture and by the maximum number of samples that can be analyzed.

The basic detection instrument, which is called MAGPIX, is compatible only with magnetic microspheres (MagPlex and MagPlex-TAG). The principle of microsphere analysis in the MAGPIX instrument is based on their immobilization in the monolayer on the magnetic surface (Figure 3) . Unlike flowbased instruments, the fluorescent imager of the MAGPIX system reads all the microspheres at once, while generating data that is comparable with other methods. Reading a 96-well-plate takes about 60 min. The maximal reading capacity of MAGPIX instruments is limited to 50 bead sets.

Advanced detection instruments -the Luminex 100/200 (Bio-Plex 200) and FlexMAP (Bio-Plex) 3D -are based on flow cytometry principles. The microspheres with bound analyte are focused into a rapidly flowing fluid stream. Each microsphere is then individually detected and digitally processed as the stream passes through the imaging cuvette. Flow cytometrybased platforms are convenient for applications with samples of limited size. The reading of a 96-well-plate is faster than in the MAGPIX system and takes 45 min or less. The capacity of the 3D platform is further increased by the possibility of analyzing 384-well plates.

The microsphere-based technology can be applied in various assay formats, which can be divided, according to the type of analyte, into microsphere-based multiplex nucleic acid assay formats (MBMNA) and microsphere-based multiplex immunoassays (MBMI). In general, xMAP-based assay formats are in comparison to other commonly used methods very open and flexible, ensuring the result data within few hours, while requiring only minimal amounts of sample.

Detection assays based on nucleic acids have a potential for high levels of multiplexing, approaching the levels of sensitivity achieved by target amplification methods like multiplex PCR or TaqMan chemistry assays, while using the same protocols of DNA/RNA extraction. Multiplex oligonucleotide ligation PCR assay format (MOL-PCR) is able to simultaneously perform detection and identification, strain typing, detect antibiotic resistance determination, virulence prediction, etc., thereby surpasses other methods like Multiplex Ligation-dependent Probe Amplification (MLPA) or qPCR. The disadvantage of technology is that it is not capable to perform quantitative analysis like qPCR, because providing only semi-quantitative data.

xMAP immunoassays surpass the common enzyme immunoassays in the ability of multiple simultaneous detection, while requiring smaller amount of sample and lower cost. Moreover, these assay formats produce superior dynamic range and sensitivity.

xMAP technology is applicable in numerous nucleic acid assay formats such as, e.g., gene expression analysis, microRNA analysis, single nucleotide polymorphism (SNP) analysis or specific sequence detection. Basically, nucleic acid assays can be developed by coupling sequence-specific capture oligos to magnetic microspheres or by use of xTAG technology (Angeloni et al., 2014) .

When performing xMAP analysis of nucleic acids it is essential to include PCR amplification to enrich the number of targets in the sample to detectable levels. There are two general strategies for including a PCR step in the detection of pathogens using xMAP technology. The main difference between the two lies in which phase the PCR amplification is applied. In direct DNA hybridization (DDH), allele-specific primer extension (ASPE), single base chain extension (SBCE), and Oligonucleotide ligation assay (OLA) all the target DNA sequences are amplified in multiplex PCR prior to hybridization to microspheres. The disadvantage of these methods is that in assays containing large amounts of targets multiplex PCR leads to amplification bias, which is caused by the different lengths of the amplicons (Nolan et al., 2001) . In contrast, in the multiplex oligonucleotide ligation PCR assay (MOL-PCR) sequence discrimination by detection probes occurs before the amplification step, which can subsequently be run just in singleplex PCR with universal primers.

Direct DNA hybridization is one of the basic approaches used for the selective identification of sequences of interest from heterogeneous mixtures of DNAs (Figure 4) . It is often used, e.g., for identification of species (Defoort et al., 2000; Page and Kurtzman, 2005; Righter et al., 2011; Liu Y. et al., 2012) or genotyping of pathogens (Letant et al., 2007; Zubach et al., 2012) . In DDH, the amplification of target sequences is ensured by specific primer pairs, and one primer from each pair is fluorescently labeled at the 5 end, permitting detection of the amplicon (Christopher-Hennings et al., 2013) . The subsequent incubation of amplicon with microspheres leads to a direct and specific hybridization between matching capture and target sequences. Amplicon sequences should be 100-300 bp in length FIGURE 4 | Direct DNA hybridization (DDH, yellow lines = capture oligonucleotide; orange line = amplified target sequence; green star = fluorescent reporter). Target DNA sequence is amplified, while one of the primers is fluorescently labeled. Amplicons are then specifically hybridized (according to complementarity) to capture oligonucleotides on the microsphere surface.

to minimize steric hindrance during hybridization and the capture sequence on microspheres should be 18-20 bp in size (Dunbar, 2006) . The specificity of the capture sequences and stringency of hybridization conditions allow discrimination up to SNP. If the SNP or mutation discrimination is intended, the presumed mismatch should be located at the center of the capture sequence (Livshits and Mirzabekov, 1996) . This assay format then requires a unique capture sequence coupled to a specific microsphere set to score each SNP allele (Kellar and Iannone, 2002) .

Allele-specific primer extension ( Figure 5 ) is an approach usually used for determination of allelic variants of pathogens (Page and Allele-specific detection probes, differing in one nucleotide on the polymorphic side, hybridize to amplified target sequence. After addition of DNA polymerase and dNTPs (one of which is fluorescently labeled), molecules are extended according to complementarity. Products are captured by anti-TAGs on the specific microsphere set. Kurtzman, 2005; Lin et al., 2008) . The defining characteristic of ASPE is the extension of two allele-specific detection probes, which contain a polymorphic site at the 3 end, defining the particular allele variant. In this arrangement, DNA polymerase can extend detection probes by incorporation of dNTPs (one nucleotide is labeled, e.g., biotin-dCTP), if the allele is present in the sample. Just one probe is extended in the case of a homozygous target; conversely, in heterozygotes both probes are extended. The fluorescence signal is generated by a fluorophore bound to labeled dNTPs, incorporated within the extended probe.

The use and assay format of SBCE is similar to the previously described ASPE. However, there are slight differences, mainly in the design of detection probes. In the case of SBCE (Figure 6) , probe sequences are terminated one base before the polymorphic site Ye et al., 2001) . Due to this design the labeled dideoxyribonucleoside triphosphate (ddNTP) terminators serve as a ""query"" nucleotide and are used for single base probe extension at the same time; this assay requires the setting up of separate reactions for each of the four ddNTPs (ddC, ddG, ddA, and ddT). Moreover, PCR products from the previous step of PCR amplification of the target sequence need to be treated with exonuclease I and shrimp alkaline phosphatase (ExoI/SAP) before use as a template in the SBCE reaction (Ye et al., 2001; Dunbar, 2006) to get rid of unincorporated primers and dNTPs. Although SBCE has been proven to be highly specific and reliable (Chen et al., 1999; Syvanen, 1999) , it is in the process of being replaced by less laborious methods.

Oligonucleotide ligation-based formats include a ligation step of two oligonucleotide detection probes, which occurs in the presence of a target sequence of a specific pathogen. These assays are based on the ability of detection probes to hybridize next to each other on a complementary target DNA sequence (Landegren et al., 1988) . If there are no mismatches near the junction site and there is a phosphate group at the 5 end of a second probe (necessary for phosphodiester bond formation), annealing occurs; DNA ligase then recognizes the nick and forms a covalent bond between adjoining nucleotides while creating a single-stranded DNA molecule . The most crucial step during the multiplexing of different ligation assays is the design of suitable probes with similar melting temperatures of between 51 and 56 • C (Dunbar, 2006) .

In the OLA assay format, the target DNA sequence is PCRamplified prior to the ligation step of the annealed probes (Figure 7) . OLA is suitable for SNP genotyping Taylor et al., 2001; Ye et al., 2001) .

The multiplex oligonucleotide ligation PCR assay represents an improved version of the previous OLA assay format. One advantage is that ligation is carried out prior to the PCRamplification (Figure 8 ) (Nolan and White, 2004) . Unlike in the OLA assay, one of the detection probes consists of a sequence complementary to the target sequence and an extension composed of the TAG sequence and primer binding site. The FIGURE 6 | Principle of Single base chain extension (SBCE) (red dot = dideoxynucleotide; green star = fluorescent reporter; red line = anti-TAG). Specific detection probes are terminated one base before the polymorphic site. Utilization of fluorescently labeled dideoxynucleotides necessitates a separate reaction for each nucleotide in focus (minimally two). Target DNA hybridizes with probes after amplification but only the mix with the proper ddNTP leads ultimately to the synthesis of a labeled product, which is captured by anti-TAG on the microsphere surface.

second probe is the same as the first except for the absence of the TAG sequence. Each probe pair is specific for a particular target sequence, but all pairs share the same primer sequence. Basically, these modular detection probes anneal to a target sequence, ligate into a complex single-stranded DNA molecule and only if this occurs does the molecule become a template for singleplex PCR using a universal pair of primers (one is fluorescently FIGURE 7 | Principle of Oligonucleotide ligation assay (OLA) (green star = fluorescent reporter; Pho = phosphate group; red line = anti-TAG). The target DNA sequence is PCR-amplified prior to the ligation step of the annealed probes. One of the detection probes consists of a sequence complementary to the target sequence (polymorphic site at the 3 end if SNP identification is needed) and also an additional TAG tail sequence. The second detection probe is fully complementary to the target sequence and serves as a reporter due to its fluorescent label at the 3 end. Detection probes bind next to each other, DNA ligase recognizes the nick and makes a bond. The product is captured by anti-TAG on the microsphere surface. labeled). Additionally, all the ligation products are very similar in length (approximately 100 bp -120 bp), so the use of a universal primer pair during PCR makes the simultaneous amplification FIGURE 8 | Principle of Multiplex oligonucleotide ligation PCR assay (MOL-PCR) (orange line = detection probe 1; green line = detection probe 2; blue lines = universal PCR primers; burgundy line = amplified negative strand; green star = fluorescent reporter; Pho = phosphate group; red line = anti-TAG). Specific detection probes bind next to each other to target sequence via complementary parts, while the parts including the TAG sequence and binding sites for PCR primers form tails sticking out into space. DNA ligase recognizes the nick and makes a bond. The complex sequence of ligated probes becomes a template for singleplex PCR with universal primers; one of the primers is fluorescently labeled. Labeled amplicon hybridizes via its TAG sequence to capture anti-TAG on the microsphere. of many short fragments highly feasible. All these facts ensure that MOL-PCR is not susceptible to the amplification bias that is characteristic of multiplex PCR or previously mentioned formats. Only a minimal amount of target/sample is required.

The MOL-PCR upgrade has the potential to have widespread impact on genomic assays, because not only is sequence detection and SNP identification possible, but the detection of indels (insertion/deletion), screening tests for pathogens (virus, bacteria, fungi) from various matrices or determination of antibiotic resistances is also feasible Thierry et al., 2013; Wuyts et al., 2015) . MOL-PCR could replace, e.g., MLPA or qPCR in certain applications in routine diagnostics .

Microsphere-based multiplex immunoassay (MBMIs) are typically biochemical tests that allow the detection or measuring of the concentration of an analyte (protein) in a solution through the use of an antibody or immunoglobulin (Angeloni et al., 2014) . Single-analyte ELISA cannot support simultaneous detection of multiple specific antibody responses within a single serum sample (Bokken et al., 2012) , and has further disadvantages, such as the requirement for a relatively large amount of sample, negligible non-specific binding or increased background. MBMIs represent an alternative for commonly used indirect tests like ELISA. The conversion of an ELISA assay to the MBMI format is uncomplicated, efficient, cost-saving and produces an assay with superior dynamic range and sensitivity (Baker et al., 2012) . MBMIs are often used in the diagnostics of various pathogens including multicellular organisms, such as e.g., parasites, in tests where the current methods are not sensitive enough. The methods of choice are usually Capture Sandwich (CS) and Indirect Serological Assay (ISA) (Figure 9) . However, the problems typical for methods based on serology remain: the need for periodical testing in order to avoid false negative results resulting from a wide and inevitable lag between infection and development of a specific response against a parasite in the form of IgG antibodies (sero-positivity) (Nockler et al., 1995) .

The CS assay utilizes microspheres covalently coupled with a capture antibody (polyclonal antibodies should be purified and mono-specific) that takes up target molecules from the sample. This complex is recognized by a labeled detection antibody (Baker et al., 2012; Angeloni et al., 2014) . The CS format can be used in cases where, for example, confirmation of pathogen identity within the inflammatory focus or altered tissue is needed.

In contrast to CS, in ISA a specific antibody against an antigen coupled with a microsphere is captured. If the binding of serum antibody to antigen occurs, a labeled secondary antibody (antiantibody) then provides the visualization. ISA is typically used for serological screenings (monitoring and prevention purposes) that are carried out on serum samples (van der Wal et al., 2013) .

The xMAP technology is used in many different applications. This chapter describes the use of this technology for multiplex detection of viral, bacterial, parasitical and fungal agents using the microsphere-based multiplex nucleic acid-assay formats (MBMNA) and microsphere-based multiplex immuno-assay formats (MBMI) described above.

Viruses are a very diverse group of infectious agents and are divided into groups according to a number of properties, e.g., type of nucleic acid, the presence of the viral envelope, antigenic structure, mode of transmission, pathogenicity, etc. They can be classified also according to the syndromes which they cause and mode of transmission, e.g., respiratory viruses, viruses causing gastroenteritis, tumors, hepatitis, rashes or neuroviruses.

To date, the majority of applications that enable multiplex viral detection and identification are based on the capture of viral nucleic acid by adoption of various DDH modifications.

Respiratory viruses are causative agents of the most common diseases of the human upper and lower respiratory tract, which are often associated with significant patient morbidity and mortality (Berry et al., 2015) , e.g., H5N1 subtype of highly pathogenic influenza A virus (Neumann et al., 2010) .

The MBMNA method for more effective detection and genotyping of H5N1 viral isolates from clinical samples comprising pharyngeal swabs and tracheal aspirates was developed and its efficiency was compared with RT-PCR and qPCR (Zou et al., 2007) . The results using the MBMNA approach showed that this assay is vulnerable to viral mutations although the primers were designed according to conserved sequences. Therefore, there is a need to monitor viral mutations in order to reduce false-negative results and add new primers and probes to adapt to the mutations, which is a disadvantage of MBMNA.

On the other hand, MBMNA holds a number of advantages compared to RT-qPCR and qPCR, e.g., allele-specific probes with TAG sequences can be recognized by a universal set of primers, thus potentially eliminating the problem with different primer sets (which may be incompatible) used in conventional methods. Moreover, amplification is carried out with a single set of universal primers where only one primer is labeled; therefore, the background is low and no post-PCR cleanup is required.

Another application of the MBMNA method was developed for the identification of human adenoviruses (HAdVs). Conventional serological identification of HAdVs serotypes is a time consuming process. Target-specific extension (TSE), which is a variant of ASPE was suggested to accelerate identification through the use of MBMNA for simultaneous identification of different serotypes; this is not possible using commercially available neutralization tests, antibody studies, or antigen detection by immunofluorescence or conventional PCR (Washington et al., 2010) . Universal primers were used for nonspecific PCR amplification and serotype-specific probes coupled to tags were used for TSE. This MBMNA procedure is methodically simple, the cost is relatively low, and it enables diagnosis of up to five HAdV serotypes in a single reaction.

Besides the in-house assays described above, commercial kits have also been developed for the detection of respiratory viruses by xMAP, e.g., xTAG R Respiratory Viral Panel (xTAG RVP) (Krunic et al., 2011) . xTAG RVP is multiplex nucleic acid test designed for detection of multiple respiratory virus nucleic acids in human nasopharyngeal swabs (Selvaraju and Selvarangan, 2012; Smith et al., 2012) . Qualitative detection of a panel including respiratory syncytial virus (RSV), Influenza A virus (influenza A matrix, H1 subtype, H3 subtype, H5 subtype), Influenza B (Parainfluenza 1,2,3, and 4), Metapneumovirus (hMPV), HAdV, Entero-Rhinovirus, Corona NL63, Corona HKU1, Corona 229E, Corona OC43, and Bocavirus is possible. Bacteriophage MS2 and bacteriophage λ DNA were used as the internal controls. The detection of respiratory virus targets using the xTAG RVP, which detects 20 respiratory viral targets, was compared with individual qPCR nucleic acid amplification tests (NATs) (Pabbaraju et al., 2008) . The xTAG RVP can detect all the respiratory viral targets included in the in-house NAT panel, which is used for detection of Influenza A, B viruses (IFVA, IFVB), parainfluenza virus types 1 to 4 (PIV 1-4), RSV, hMPV, and respiratory adenovirus types (ADV). Of the 1,530 samples tested, 532 were positive by xTAG RVP and 580 by in-house NATs for these targets. This gives the xTAG RVP a sensitivity of 91.2% and a specificity of 99.7%; in addition, xTAG RVP can detect picornaviruses (the in-house assays did not detect 88 picornaviruses) and coronaviruses and can subtype IFVA positives simultaneously. The xTAG RVP includes all the respiratory viral targets that are tested routinely for the diagnosis of acute respiratory tract infections; further, the technology is flexible and can easily allow for incorporation of other targets (e.g., human bocavirus) in the future.

The xTAG RVP assay was subsequently modified and was marketed as the xTAG RVP Fast assay, which has a simpler protocol; the results are obtained in a shorter time and handling of the amplified product is not required (amplified DNA is mixed with TAG primers specific to each viral target), which could be a potential contamination risk (Pabbaraju et al., 2011) . The respiratory samples were tested for a variety of respiratory viral targets by xTAG RVP and xTAG RVP Fast in parallel. The xTAG RVP was more sensitive than xTAG RVP Fast (88.6% versus 77.5%) for all the viral targets; in addition, some targets (influenza B virus, parainfluenza virus type 2, and human coronavirus 229E) were not detected using xTAG RVP Fast and, e.g., the sensitivity for detection of IFVB was very low (41.3%). Therefore, it is not suitable as the primary assay for the detection of IFVB.

In addition to respiratory viral diseases the MBMNA was successfully applied also for detection of viral pathogens causing acute viral gastroenteritis. Acute viral gastroenteritis is usually caused by four distinct families of viruses: rotaviruses, noroviruses, astroviruses, and adenoviruses (Liu Y. et al., 2012) . The authors focused on simultaneous detection of rotavirus A (RVA), noroviruses (NoVs), sapoviruses (SaV), human astrovirus (HAstV), enteric adenoviruses (EAds) and human bocavirus 2 (HBoV2). Altogether 140 fecal samples were tested using the MBMNA and RT-PCR in parallel. The specificity of MBMNA was equal to the conventional RT-PCR (>90%), but MBMNA was faster in terms of detection of different viral pathogens in one tube (Liu Y. et al., 2012) . The studies of (Hamza et al., 2014) were also directed to the detection of human enteric viruses (human adenovirus (HAdV), human polyomavirus (HPyV), enterovirus (EV), rotavirus (RoV), norovirus GI (NoVGI), and norovirus GII (NoVGII), but environmental water samples were tested (Hamza et al., 2014) . MBMNA provided high specificity and no cross-reactivity, but was not as sensitive as qPCR for the identification of viral contamination in river water samples. In contrast, all wastewater samples that were positive in qPCR were also positive by the MBMNA and the detection limit was higher than qPCR; MBMNA was as sensitive as qPCR for viral detection in wastewater samples. Therefore, MBMNA could be a reliable method for the simultaneous detection of viral pathogens, but only in wastewater. For detection of gastrointestinal pathogens xTAG R Gastrointestinal Pathogen Panel -GPP is commercially available [multiplex detection of various viral, bacterial and parasitic nucleic acids in human stool samples (Beckmann et al., 2014; Perry et al., 2014; Wessels et al., 2014; Zboromyrska et al., 2014) ]. In comparison to the two previous studies mentioned above only three enteric viruses (norovirus, rotavirus and adenovirus 40/41) can be identified by the GPP (See chapter 4.2).

Viruses such as human papillomaviruses (HPV) are also associated with oncogenesis. HPV belong to those viruses, which require simultaneous detection and typing to identify individual HPV types because the genotype determination is necessary for the investigation of epidemiology and behavior of individual HPV types. Therefore, DDH was designed for detection and genotyping of HPV using L1 consensus (primer systems, which can detect 10 to 100 molecules of HPV targets) resulting in the establishment of a method for simultaneous detection of 26 different HPV genotypes including 18 high-risk HPV and 8 lowrisk HPV genotypes (Jiang et al., 2006) . Subsequent analysis of the data showed that the 26-plex method precisely discriminated all 18 high-risk HPV targets and also 8 low-risk HPV targets. Another study focused on genotyping HPV also used specific probes targeting a region of the L1 gene (Zubach et al., 2012) . DDH was optimized for the detection and genotyping of 46 mucosal HPV types, which are associated with infections of the genital, anal, and oropharyngeal mucosae and the method enables a more comprehensive coverage of HPV types compared with the previously mentioned study, where only 26 types of HPV were genotyped. The DDH was more sensitive than the Linear Array (a leading commercial genotyping method) in terms of distinguishing positive/negative HPV samples, but less sensitive for detection of multiple HPV types; another limitation was the inability of the PCR system to amplify certain variants of HPV68. HPV genotype detection was by combined whole genome amplification and xMAP technology showed that this method is highly specific and sensitive (Lowe et al., 2010) . This approach is capable to identify all high risk HPV types with the analytical limit of detection 100 copies plasmid DNA.

Many viruses can cause infections with fatal consequences for human health, e.g., Hendra and Nipah viruses, which can infect cells of the central nervous system and may cause relapsing encephalitis (Clayton et al., 2013) , Ebola virus, which causes lethal hemorrhagic disease in humans (Takada and Kawaoka, 2001) or Menangle virus, which causes an influenzalike illness with a rash in humans (Bowden et al., 2012) ; these zoonotic viruses are linked to bats. The surveillance of zoonotic viruses in wildlife populations is necessary in order to monitor the risk of emerging infectious disease outbreaks. For the complex detection and genotyping of paramyxoviruses in Australian bats two bat virus panel assays (BVPA) for detection of paramyxoviruses in Australian bats (BVPA-1) and for paramyxoviruses and filoviruses in non-Australian bats (BVPA-2) were introduced (Boyd et al., 2015) . Examined RNA was extracted from the urine of bats and a total of 532 samples were tested in 11-plex BVPA-1 and 540 field 8-plex BVPA-2; both developed assays were proven to be reliable and accurate.

A number of pathogens, including viruses, are implicated in reproductive diseases of swine. (Chen et al., 2015) combined onestep asymmetric multiplex reverse transcription PCR (RT-PCR) with DDH for simultaneous detection of respiratory syndrome virus (PRRSV), porcine circovirus type 2 (PCV-2), porcine pseudorabies virus (PRV), classical swine fever virus (CSFV), and porcine parvovirus (PPV). All strains of these five viruses were accurately identified. The results showed that the combination of RT-PCR with the DDH assay is more accurate and specific than the other methods, e.g., conventional RT-PCR, and could be a useful tool in the diagnostics of swine diseases. MBMNAs could become very important for veterinary diagnostic testing and (Christopher-Hennings et al., 2013) reported the potential use of MBMNAs for detection of different pathogens in pigs using panels for the multiplex detection of swine pathogens (viruses and bacteria) in serum, lung, oral fluids, feces and spleen or liver.

Although direct diagnosis based on the detection of the nucleic acids of viral pathogens described above prevails, xMAP antibody-based tests for the detection and typing of viruses are also available. MBMI was used to develop a competitive immunoassay that measures HPV type 6, 11, 16, and 18 specific neutralizing antibodies (Opalka et al., 2003) ; this was later validated for use in epidemiology studies and clinical vaccine trials (Opalka et al., 2003; Dias et al., 2005) . MBMI was also compared with a Western blot assay for the detection of HIVspecific antibodies (Kong et al., 2016) . The microspheres were coupled with anti-p24 monoclonal antibody and with HIV antigens: gp41, p17, p24, p31, and p66 recombinant protein. The results of both methods showed that MBMI sensitivity was 82.7% and Western blot assay sensitivity was 74.7%. The MBMI was more efficient and precise for screening several parameters and based on the acquired results it was better in HIV diagnostics than Western blots. For the determination of antibodies against HCV in patient serum samples MBMI based on the antigenic properties of four recombinant proteins was designed . Only a small number of samples was tested and that is why the specificity and sensitivity were 100%, but in spite of this the MBMI has the potential to become a viable alternative to standard tests due to its excellent specificity and it may be used for screening of HCV infection. Detection of antibodies against several Epstein-Barr virus (EBV) antigens in nasopharyngeal carcinoma patients (NPC) showed the possibility of simultaneous detection of multiple markers using MBMI, which is not possible with ELISA, and because of the distinct EBV serology spectrum in individual NPC patients, the multiplexed microsphere assay has powerful potential to allow serological diagnosis of NPC in the future (Gu et al., 2008) . MBMI showed increased sensitivity and the possibility of quantifying antibodies, antigens, as well as other substances (e.g., hormones, cytokines, tumor markers, etc.), in contrast to conventional ELISA tests (duPont et al., 2005) .

The majority of applications for multiplex bacterial diagnostics are based on the detection of DNA. The most widely used approaches are based on the DDH, ligation assays or ASPE, but multiplex detection of bacteria may be performed as well using MBMI.

Direct DNA hybridization was used for the detection of pathogens causing foodborne diseases such as acute gastroenteritis and diarrhea, which are usually associated with ingestion of contaminated food. DDH was applied for the typing of 500 Salmonella isolates using the genes encoding the flagellar antigens H (fliC and fljB) (McQuiston et al., 2011) . Allele-specific probes for fifteen H antigens, 5 complex major antigens and 16 complex secondary antigens according to the Kauffmann-White serotyping scheme were designed. Comparison of DDH with traditional serotyping methods revealed that the DDH cannot completely replace these methods because unfortunately not all flagellar antigen types were detected. A similar DDH assay for the typing of Salmonella focused only on the most common six serogroups of Salmonella in the United States (B, C 1 , C 2 , D, E, and O13), as well as serotype Paratyphi A, using the rfb genes required for O-antigen biosynthesis in Salmonella (Fitzgerald et al., 2007) . In contrast with the previous study of McQuiston et al. (2011) , the authors showed that the DDH was more specific than traditionally used methods for typing of Salmonella.

In the previous sections, it was described how DDH can be used for typing of pathogens; however, in most cases DDH is used only for the detection of pathogens, as described below. attempted simultaneous detection of the enteric pathogens Aeromonas, Campylobacter jejuni/coli, Shigella, enteroinvasive Escherichia coli (EIEC), Vibrio, Yersinia and as well as Salmonella in fecal samples. However, there were some limitations to the method, which included the limited number of clinically significant pathogens or the inability to detect diarrheagenic E. coli, protozoa, or viruses. The full capacity of the DDH assay was utilized when the panel was expanded to include the most common bacterial/viral enteropathogens found in stool samples, such as Salmonella, Shigella, Vibrio, toxin B producer Clostridium difficile, Campylobacter, Clostridium perfringens, Yersinia enterocolitica, Aeromonas, Escherichia coli O157:H7, verocytotoxin-producing Escherichia coli and adenovirus, Group A rotavirus, norovirus GI and GII and astrovirus (Onori et al., 2014) . The results showed that the assay is rapid, sensitive, specific, and reliable for screening and for exploring the etiology of gastrointestinal infections. The sensitivity of MBMNA was demonstrated to be greater than the routine methods (76.3% versus 66.5%), with the exception of Salmonella sp. and toxigenic C. difficile where the adoption of multiplex PCR did not always result in a significant improvement of specificity. The causative agents were not found in 44 of 245 (18%) of the presumed infectious gastroenteritis cases, but this could be due to the limitations of the detection panel, which did not include allelespecific probes for detection of parasitic enteric pathogens or emerging viruses related to gastroenteritis. Also, using DDH, detection of pathogenic bacteria occurring in environmental samples and causing acute and often fatal diseases (Bacillus anthracis, Yersinia pestis, Francisella tularensis, and Brucella melitensis) was optimized in a multiplexed format to allow the maximum sensitivity and specificity (Wilson et al., 2005) . DNA was extracted robotically and in combination with DDH a rapid reliable screening approach was developed. Detection limits were from 100 fg to 10 pg starting DNA concentration when primer sets were multiplexed; in some cases the limits of detection were higher when primer sets were tested separately (range from 10 fg to 10 pg).

Besides the in-house assays developed for multiplex detection of bacteria described above, there are also commercial solutions based on xMAP technology for detection of the most common gastrointestinal pathogens and toxins. The xTAG R Gastrointestinal Pathogen Panel is a multiplex nucleic acid test designed for detection of various bacterial, viral and parasitic nucleic acids in human stool samples (Beckmann et al., 2014; Perry et al., 2014; Wessels et al., 2014; Zboromyrska et al., 2014) . The panel allows qualitative detection of Campylobacter sp., Clostridium difficile (toxin A/B), Escherichia coli O157, Enterotoxigenic E. coli (ETEC) LT/ST, Shiga-like toxin producing E. coli (STEC) stx1/stx2, Salmonella sp., Shigella sp., Vibrio cholerae, Yersinia enterocolitica, HAdV serotypes 40 and 41, NoV GI and GII, Rotavirus A, Giardia, Cryptosporidium and Entamoeba histolytica. The xTAG GPP was tested and compared with routine tests, which are used in clinical diagnostic laboratories for screening of 17 kinds of enteropathogens, e.g., qRT-PCR kit for detection of viruses, culture methods for detection of bacteria or microscopic examination for detection of parasites (Deng et al., 2015) . Samples with discordant results between the routine tests and xTAG GPP were tested by singleplex PCR and sequencing. The overall sensitivity of xTAG GPP was 96.3% and specificity was 99.8%. The sensitivity of xTAG GPP was 100% for all enteropathogens except Salmonella sp. (84.9%) and C. difficile toxin B (88.6%). The specificity was 100% for all targets except Salmonella sp. (99.2%), Shigella sp. (99.7%), C. difficile toxin B (99.2%), and norovirus GII (98.8%). xTAG GPP is also capable of detecting coinfections; 35 coinfections were detected using xTAG GPP, which is more than by the routine tests. However, the authors also reported some disadvantages as xTAG GPP failed to detect some important diarrheal pathogens (Aeromonas, Plesiomonas shigelloides) often detected by routine diagnostic tests; further, the detection of Salmonella exhibited low sensitivity (84.9%).

Ligation assays are also often used for multiplex detection of pathogenic bacteria. The main advantage over direct hybridization methods is the ability to simultaneously detect diverse signatures such as unique sequences, SNPs, indels and repeats (Song et al., 2010) . MOL-PCR was initially optimized for the detection of the biothreat agents Bacillus anthracis, Yersinia pestis, and Francisella tularensis . The pathogen-specific sets of MOLigo pair probes were designed and their specificity and sensitivity were tested using similar species of Bacillus anthracis, Yersinia pestis, and Francisella tularensis and dilutions of isolated DNA, respectively. MOLigo pairs, which showed the highest specificity and sensitivity, were selected for compilation of a final probe panel, which was validated on extracted DNA from infected rodent liver and spleen, human blood or pleural fluid spiked with pathogen DNA. Nine from 10 unknown samples were successfully identified using the final probe panel. The results also showed the ability of this method to simultaneously detect multiple different signatures (SNPs, indels and repeats). The versatility of MOL-PCR was utilized when simultaneous detection of Bacillus anthracis, Yersinia pestis, and Francisella tularensis was supplemented by characterization of antibiotic resistance (ciprofloxacin and doxycycline) of these bacteria based on SNP analysis (Song et al., 2010) . The allelespecific probes for detection and characterization of all the known resistance determinants performed well when tested individually, but multiplex use did not provide satisfactory results. Due to the ability to simultaneously detect diverse signatures such as unique sequences, SNPs, indels, and repeats, MOL-PCR can be used as a genotyping method as described below. A MOL-PCR-based 8plex SNP typing method for Mycobacterium tuberculosis complex (MTBC) based on two phylogenetically equivalent sets of SNP markers that are specific for the six main human-associated lineages of MTBC was introduced (Stucki et al., 2012) . MOL-PCR was compared with TaqMan qPCR and the obtained results showed that the sensitivity and specificity of both methods were similar (specificity 100%, sensitivity 98.6% MOL-PCR, 98.8% TaqMan) and that both methods were of comparative cost. MOL-PCR was ideal for classification of unknown isolates, while TaqMan qPCR was faster for confirmation of unknown isolates. MOL-PCR was also successfully used for genotyping of Bacillus anthracis in a 13-plex assay to score 13 phylogenetically lineagespecific canonical SNPs within the genome of Bacillus anthracis (Thierry et al., 2013) .

Allele-specific primer extension was applied for identification of bacteria (Lin et al., 2008) even though it is more commonly used for SNP genotyping. ASPE was used for the identification of Acinetobacter sp. and antimicrobial susceptibilities of the clinical Acinetobacter species isolates were also determined (Lin et al., 2008) . The 16S-23S rRNA gene intergenic spacer (ITS) regions of 13 distinct Acinetobacter species were amplified and then multiplex ASPE was performed. It was shown that this multiplex identification of Acinetobacter sp. is applicable also for determination of antibiotic resistance of the clinical Acinetobacter isolates. ASPE was compared with SBCE for identification of bacterial samples (Ye et al., 2001) and both methods provided similar results as they managed to correctly classify 17 bacterial species into 17 groups.

In addition to MBMNA also MBMI can be used for the direct multiplex detection of bacteria and their products (Dunbar et al., 2003) . In MBMI direct fluorescence (detection antibody that incorporates a fluorescent label) is used for detection of reaction or of emerging product in contrast to ELISA and, in addition, MBMI enables measurement of multiple analytes simultaneously. For this reason, MBMI is preferred because time for detection is reduced and also test sensitivity is increased (Jun et al., 2012) . Capture sandwich immunoassays (CS) were successfully applied for detection of organism-specific antibodies using microspheres coupled with antibodies for Salmonella, Campylobacter, Escherichia coli, and Listeria and it has been demonstrated that MBMI is a suitable method for multiplex detection of bacteria occurring in foodstuffs or for detection of Brucella sp. from milk using capturesensitive monoclonal antibodies for the lipopolysaccharide (LPS) O-antigen of Brucella sp. (Silbereisen et al., 2015) . MBMI was also applied to test bacterial contamination of foods through the detection of staphylococcal enterotoxin B (SEB) , staphylococcal toxin A (SEA), and toxic shock syndrome toxin (TSST) produced by various strains of Staphylococcus aureus (Simonova et al., 2014) using sandwich immunoassays in which microspheres were conjugated with specific antibodies. A similar approach was used for the detection of pneumococcal serotype-specific polysaccharide and C-polysaccharide (C-Ps) antigens from urine samples (Sheppard et al., 2011) . For the detection, MBMI was combined with the Binax NOW Streptococcus pneumoniae antigen detection kit. The specificity of MBMI was determined by testing 85 serotypes of S. pneumoniae and other strains of streptococci; 18 of the 26 non-pneumococcal serotypes gave C-P positive results, which showed that MBMI could be used for diagnosis of infection caused by S. pneumoniae only in combination with the Binax NOW assay.

Parasitic zoonoses are recorded worldwide and some of them have endemic character. Parasitic agents may pass from animals to humans in several ways, e.g., by direct contact, vector, consumption of raw or undercooked foodstuffs containing the infective stages or by infective stages released into environment (Hubalek, 2003) . In the context of animal health and human food consumption, a list of the top ten parasites has been defined by the UN's Food and Agriculture Organization (FAO) and World Health Organization (WHO) ( Table 3) . Although in the last decades a number of novel diagnostic methodological approaches has been developed, the current diagnosis of some parasitic diseases is still based only on a combination of clinical signs, anamnesis, and direct visual identification of parasitological objects (Anderson et al., 2015) . The most common conventional diagnostic methods, such as microscopic examination, biochemical assays or ELISA, are available, but they are laborious, time-consuming and in many cases not reliable (Navidad et al., 2013) . Improvements in this field are represented by molecular methods, including also routine PCR diagnostics, increasingly used for detection mainly of intestinal parasites, which are easy to recover from fecal specimens (Taniuchi et al., 2011) or potentially useful for other parasites found in secretions. With regard to the fact that parasites might exhibit very strictly confined localization within the host's bodyintracellular/extracellular or tissue/organ, sampling can be very problematic and it often leads to a false negative results.

Outbreaks of diarrheal diseases are caused by a wide range of pathogens, including parasites. Stool microscopy (detection of eggs, parts of bodies etc.) is the gold standard in the diagnostics of intestinal parasites. However, the presence of parasites in stool may vary and could be naturally low, requiring multiple sampling. In fact, up to 80% of all cases of diarrhea remain without confirmed etiology (Vernacchio et al., 2006) . Therefore, there is space for the development of more sensitive diagnostic assays (Taniuchi et al., 2011) , which should provide more precise determination. Among the modern molecular diagnostic methods qPCR assays are most frequently used for determination of intestinal parasites. In areas where co-infections are common (up to 22% of cases are caused by two or more pathogens) (Jansen et al., 2008; Friesema et al., 2012) , the application of multiplex assays is of great benefit. Several pioneering works have been published in relation to this topic. To date, in parasitology, improved multiplex qPCR assays were adapted to DDH, which enables parallel diagnosis of seven intestinal parasites (Taniuchi et al., 2011) ; separate reactions were optimized -3plex for protozoa (Cryptosporidium sp., Giardia intestinalis, and Entamoeba histolytica) and 4-plex for helminths (Ancylostoma duodenale, Ascaris lumbricoides, Necator americanus, and Strongyloides stercoralis). The final calculated sensitivity was 83% and specificity was 100%. The results of both DDH assays were equivalent or better in comparison to the parent multiplex qPCR. Moreover, this approach has been developed as a commercial diagnostic xTAG GPP tool-a 19-plex assay, which enables inter alia detection of the protozoa G. intestinalis, E. histolytica and Cryptosporidium sp. The overall performance of xTAG GPP compared with conventional methods (standard culture, microscopic examination, immunochromatographic tests, qPCR) showed a sensitivity of 94.5% (range 90 to 97%) and a specificity of 99% (range 98,5% to 99,9%) (Claas et al., 2013; Mengelle et al., 2013; Navidad et al., 2013) . If multiplexing more than 20 targets, the limit of detection might be reduced for individual targets when compared to single-target detection (Navidad et al., 2013) . However, the identification of multiple pathogens revealed that very often (in up to 65% of samples), the physicians do not request testing for the proper pathogen (Claas et al., 2013) . Therefore, multiplexing refines the diagnosis and contributes to the selection of a suitable treatment. It was mentioned above that microsphere-based assays can be arranged also as multiplex indirect immunoassays, although the conventional singleplex ELISA still represents the gold standard in serodiagnostics for screening of individual human/animal or higher numbers of samples at a population level (Ruitenberg et al., 1983; Nockler et al., 2000; Dubey et al., 2005) . Recently, some studies have been done in order to improve the potential of this serological method and to upgrade it to the multiplex level. These studies are mostly focused on parasites with the ability to migrate through the tissues of the host's body -where PCR based detection would not be reliable. In this context, the most studied group of parasites are representatives from the phylum Nematoda, including also the important human pathogens, the Trichinella sp. The larvae may infect humans during the ingestion of raw or undercooked meat, mainly pork (domestic pig, wild boar) and can induce disease, whose consequences can be fatal (Dupouy-Camet, 2000; Pozio and Murrell, 2006) . Inspection of meat for the most important species, Trichinella spiralis, is mandatory at slaughter (Anonymous, 2015) , but currently used methods like artificial digestion and microscopic examination of pooled meat samples (Nockler et al., 2000) are archaic and usually do not properly reflect the real infection. Therefore, serodiagnostic methods are considered as a possible alternative and xMAP technology in the form of ISA, using excretory/secretory (E/S) products, was also developed and tested. The effectivity of ISA was tested with T. spiralis-positive pig meat samples. The system was developed as a duplex assay (with Toxoplasma gondii), using goat anti-swine secondary antibodies against specific antibodies. The results of this study corresponded to the infection status of the animals with an assay sensitivity of 68% and specificity of 100% (Bokken et al., 2012) . When the immunoglobulin binding protein A/G (generic Ig-binding protein), which can be used in multiple species in contrast with goat anti-swine secondary antibody, was included, the results showed a similar specificity of 95%, but an increase in sensitivity from 88% for anti-swine antibody to 94% with protein A/G. The xMAP technology-ISA exhibited 87% sensitivity and 95% specificity in comparison with the commercial Pourquier ELISA, and 98% sensitivity and 95% specificity in comparison with the Safepath ELISA (van der Wal et al., 2013) .

With the rising popularity of MBMIs, ISA was also developed for other members of Nematodes, such as representatives from the genus Toxocara (Anderson et al., 2015) . The infection by these parasites is typically peroral at areas contaminated by embryonated roundworm eggs, e.g., sand from childrens' playgrounds. The recombinant T.canis and T. cati E/S antigens Tc-CTL-1 and Tc-TES-26 were used to detect toxocara-specific antibodies in sera from humans pre-diagnosed as positive for visceral and ocular larval migrans (VLM, OLM). The specificity of ISA was 94% for both sets of samples, but there were differences in the sensitivity, which was 99% for VLM and 64% for OLM samples. It was recorded that a combination of recombinant antigens improves sensitivity in comparison with conventional immunoassays (e.g., Western Blot, ELISA), which employ native E/S antigens isolated from larvae (limited availability) that also exhibit cross-reactivity with antibodies from other helminthic infections so reducing its usefulness in regions with poly-parasitism.

Within the unicellular parasitic protozoa ISA was tested in representatives from the genus Toxoplasma. Unlike T. spiralis, no such regulations for meat control exist for T. gondii, even though its prevalence is higher and health complications can be very severe. Recombinant tachyzoite surface protein (SAG-1) was used for simultaneous serological detection in a set with T. spiralis E/S (Bokken et al., 2012) . Similarly to T. spiralis, the results exactly reflected the load of infection; sensitivity was 86% and specificity was 96% for T. gondii. The obtained results repeatedly underline the potential of these assays for further implementation in routine diagnostic screening of a wide range of parasites.

As we have descibed, the ISA represents an improved methodological alternative to current serological diagnostics, enabling multiplex detection of pathogenic agents with higher sensitivity.

Traditional diagnostic methods for the identifications of fungal pathogens are mostly based on phenotype analysis of fungal cultures or detection of antigens (polysaccharides), but these approaches are time-consuming and not very accurate (Diaz and Fell, 2004; Bovers et al., 2007; Landlinger et al., 2009; Babady et al., 2011) . Rapid and correct identification methods are important for efficient therapy (Diaz and Fell, 2004) , however, available qPCR assays have various levels of sensitivity and specificity and often have a limited range, targeting only a few yeasts or mold species (Landlinger et al., 2009; Babady et al., 2011) .

The need for rapid and correct identifications of fungal pathogens was addressed by development of xMAP technology based detection methods (Diaz and Fell, 2004; Page and Kurtzman, 2005; Das et al., 2006; Bovers et al., 2007; Babady et al., 2011; Balada-Llasat et al., 2012; Farooqi et al., 2012; Landlinger et al., 2009) . Majority of xMAP applications for the multiplex detection and identification of fungal pathogens are based on the capture of fungal nucleic acid by DDH assays.

To perform rapid and accurate identifications of fungal pathogens in immunocompromised individuals, the DDH was designed detect a wide range of the most commonly occurring clinically relevant fungal pathogens including species of the genera Aspergillus and Candida and other important pathogens such as Cryptococcus, Fusarium, Trichosporon, Mucor, Rhizopus, Penicillium, Absidia, and Acremonium (Landlinger et al., 2009 ). The DDH was used mainly for identifications of fungi due to its ability to detect coinfections with multiple fungal species in patients and may contribute to improved diagnosis of invasive fungal infections.

Studies employing xMAP technology were developed and successfully used to identify individual fungal species within Candida sp. (Page and Kurtzman, 2005; Farooqi et al., 2012) , or Trichosporon sp. (Diaz and Fell, 2004) . In these studies, DDH assays for fast and accurate detection and identification of important fungal pathogens were developed. In another study, the xMAP technology was used for genotyping of human pathogenic Fusarium sp. (O'Donnell et al., 2007) . Fusaria were genotyped also by sequence analysis. The independent comparison of the results obtained via xMAP technology with results obtained via sequencing showed the xMAP incorrectly identified some of Fusarium isolates.

Besides the in-house assays described above, commercial kits have also been developed for the detection of fungal pathogens by xMAP, e.g., xTAG R Fungal Analyte-Specific Reagents (ASR) assay and the sensitivity and specificity of the assay were tested within identification of fungal isolates and positive blood culture bottles (Babady et al., 2011) . The Candida 7-plex assay was tested within 43 of Candida strains and 16 bacterial strains with no-cross-reaction with any of the bacterial strains. The sensitivity and specificity were 100%. Using 11-plex assay were tested 51 mold species and the assay correctly identified all species of Aspergillus, with 100% specificity and sensitivity except A. niger (0/8 isolates). Other molds were identify also with 100% specificity and sensitivity except Mucor (0/6 isolates) and Rhizopus (1/6 isolates). Besides the testing of fungal isolates also positive blood culture bottles were tested for the presence of Candida species using Candida 7-plex assay. The sensitivity and specificity of the assay was 100% for each species. The mold 11plex assay did not detect one Rhizopus species and the A. niger strains, so the results were similar as the previous mentioned results in the course of identification of fungal isolates.

In addition, ASR for identification of Candida species do not distinguish between members of Candida complexes, e.g., phenotypically indistinguishable groups II and III of C. parapsilosis (group I), which have been renamed Candida orthopsilosis and C. metapsilosis. Similarly, ASR for identification of A. fumigatus were unable to distinguish between members of the A. fumigatus complex. The results showed that xTAG R Fungal ASR assay could be used as an adjunct to culture. The mold 11-plex assay has been developed specifically for the detection of specific species of mold, which may be reason why Rhizopus, Mucor, and A. niger have not been identified. Due to the equal treatment of infections caused by genera Mucor and Rhizopus, it would be better to design a panel to detect the most common genera of fungi, and not to focus on the detection of particular species.

The results showed that the xTAG R Fungal ASR assay is an attractive alternative to reference methods, due to its speed and ability to simultaneously identify multiple fungal species (Balada-Llasat et al., 2012) .

DDH assay is able to not only identify the fungal pathogens, but it can be used for a genotyping of fungal pathogens. It was applied for identification of closely related pathogenic yeasts Cryptococcus neoformans and Cryptococcus gattii that may cause meningoencephalitis in immunocompromised individuals (Bovers et al., 2007) . Six haploid genotypic groups within these pathogens can be distinguished by several molecular methods e.g. PCR fingerprinting or intergenic spacer genotyping. Besides these haploid groups, hybrids have been described as well. AD hybrids are hybrids between the two varieties of C. neoformans and also hybrids between C. neoformans var. neoformans and C. gattii have been described. The DDH has been adapted for the detection of the genotypes within Cryptococcus neoformans and Cryptococcus gattii. The detection limit was calculated from 4 × 10 1 to 2 × 10 3 cells for the various specific probes for each of the six haploid genotypic groups. The results showed that DDH is highly specific method and it is possible not only identify cryptococcal isolates at the species and genotype levels but also allows identification of hybrid isolates that have two alleles of the specific probes region and also able to identify cryptococci in cerebrospinal fluid. However, the optimization of DNA extraction methods is needed before routine use in clinical laboratories.

Detection and identification of pathogens, as well as an understanding of pathogen variation, the pathogenesis of the diseases they cause, and timelines of infection and antimicrobial resistance, are all required in order to obtain the full picture of disease progression and to select effective cures for infected individuals or populations. As the amount of input data required for such decisions increases, so too does the number of tests that are required during laboratory examinations. The multiplex assays for the detection and typing of pathogens using xMAP technology are tools of choice as they are capable of providing all of the important information within a reasonable timeframe, and without excessive labor or costs. The major improvement of xMAP assays is that they add another dimension to the simple detection, which is represented by the simultaneous analysis of many targets within a single sample, and they therefore represent complementary tools to procedures for the detection and quantification of pathogens such as qPCR, culture, or ELISA assays. The significance of such a complex approach for the multiplex detection has grown in recent years, which is documented by the increase in published data and of application of the commercial assays in routine diagnostics.

",0.751066354819413
Applications of Next-Generation Sequencing Technologies to Diagnostic Virology,"Novel DNA sequencing techniques, referred to as ""next-generation"" sequencing (NGS), provide high speed and throughput that can produce an enormous volume of sequences with many possible applications in research and diagnostic settings. In this article, we provide an overview of the many applications of NGS in diagnostic virology. NGS techniques have been used for high-throughput whole viral genome sequencing, such as sequencing of new influenza viruses, for detection of viral genome variability and evolution within the host, such as investigation of human immunodeficiency virus and human hepatitis C virus quasispecies, and monitoring of low-abundance antiviral drug-resistance mutations. NGS techniques have been applied to metagenomics-based strategies for the detection of unexpected disease-associated viruses and for the discovery of novel human viruses, including cancer-related viruses. Finally, the human virome in healthy and disease conditions has been described by NGS-based metagenomics.","Novel DNA sequencing techniques, referred to as ""next-generation"" sequencing (NGS), provide high speed and throughput that can produce an enormous volume of sequences. The most important advantage provided by these platforms is the determination of the sequence data from single DNA fragments of a library that are segregated in chips, avoiding the need for cloning in vectors prior to sequence acquisition.

The first next-generation high-throughput sequencing technology, the 454 FLX pyrosequencing platform (http://www.454.com/), which was developed by 454 Life Sciences and later bought by Roche, became available in 2005. In early 2007, Illumina released the Genome Analyzer (http://www.illumina.com), developed by Solexa GA, and more recently, SOLiD was released by Applied Biosystems (http://www.appliedbiosystems.com). This field is in rapid expansion and novel and improved platforms are continuously being developed and released, like Heliscope by Helicos (http://www.helicosbio.com/), Ion Torrent PGM by Life Technologies (http://www.iontorrent.com/) and a real-time sequencing platform by Pacific Biosciences (http://www.pacificbiosciences.com/). While the platform developed by Pacific Biosciences, as well as other novel sequencing platforms, are referred as ""third-generation"" because they sequence processively single large DNA molecules without the need to halt between read steps, 454 pyrosequencing, Illumina GA and SOLiD methods represent the ""second generation"" systems, able to sequence populations of amplified template-DNA molecules with a typical ""wash-and-scan"" technique [1] . Given these criteria, Ion Torrent PGM and Heliscope sit between ""second-"" and ""third-generation"" technologies, since they do not completely fulfill the features assigned to each category.

These NGS methods have different underlying biochemistries and differ in sequencing protocol (sequencing by synthesis for 454 pyrosequencing, Illumina GA, Ion Torrent PGM and Heliscope, sequencing by ligation for SOLiD), throughput, and for sequence length (Table 1) . Thus, the SOLiD system may be more suitable for applications that require a very high throughput of sequences, but not long reads, such as whole genome re-sequencing or RNA-sequencing projects, while both 454 and Illumina provide data suitable for de novo assembly and the relative long length of 454 FLX (and its smaller version GS Junior) reads allows deep sequencing of amplicons, with applications in microbial and viral metagenomics and analysis of viral quasispecies, as described in this review. The technical features of NGS methods (reviewed in refs. [2, 3] ) will not be described in this review, which is focused on the diagnostic applications of NGS in clinical virology. 

NGS technologies are currently used for whole genome sequencing, investigation of genome diversity, metagenomics, epigenetics, discovery of non-coding RNAs and protein-binding sites, and gene-expression profiling by RNA sequencing (reviewed in refs. [2] [3] [4] [5] [6] ). Typical applications of NGS methods in microbiology and virology, besides high-throughput whole genome sequencing, are discovery of new microorganisms and viruses by using metagenomic approaches, investigation of microbial communities in the environment and in human body niches in healthy and disease conditions, analysis of viral genome variability within the host (i.e., quasispecies), detection of low-abundance antiviral drug-resistance mutations in patients with human immunodeficiency virus (HIV) infection or viral hepatitis, as outlined in this review article.

The human population is exposed to an increasing burden of infectious diseases caused by the emergence of new previously unrecognized viruses. Climate changes, globalization, settlements near animal and livestock habitats, and the increased number of immunocompromised people probably contribute to the emergence and spread of new infections [7] . In addition, several clinical syndromes are suspected to be of viral etiology, but the causing agent cannot be isolated and recognized by traditional culture and molecular methods. Thus, there is the need to improve methods for the identification of unsuspected viral pathogens or new viruses. Subtractive techniques, such as representational difference analysis or random sequencing of plasmid libraries of nuclease resistant fragments of viral genomes, have led in the past to the discovery of several viruses, including human herpesvirus type 8 [8] , human GB virus [9] , Torque Teno Virus [10] , bocavirus [11] , human parvovirus 4 [12] , WU polyomavirus [13] and KI polyomavirus [14] . These techniques are poorly sensitive and time-consuming, and thus are unsuitable for large scale analysis. For these purposes, NGS-based methods have been developed. However, traditional cloning and sequencing methods can be relatively simple and sensitive for the discovery of new viruses when used for the analysis of otherwise sterile samples, and may represent an alternative to NGS. One of these methods is termed VIDISCA (Virus Discovery cDNA Amplified Fragment Length Polymorphism Analysis) and may be applied to sterile specimens, such as cell culture supernatants [15] . In this method, samples are ultracentrifuged for viral particle enrichment and treated by DNase and RNase to digest away cellular nucleic acids. Capsid-protected viral nucleic acids are then purified, converted to double stranded DNA, digested with restriction enzymes and ligated to oligonucleotide adaptors, which are used as primer binding sites for comparative PCR [15] . This method was described originally in the context of the discovery of severe acute respiratory syndrome coronavirus (SARS-CoV) in 2004 [16] . Microarray-based diagnostic assays have also been used to characterize previously unknown viruses, such as SARS-CoVs [17] , but require information on the genome of the virus or closely related viruses that are under investigation [18] .

High throughput NGS techniques represent a powerful tool which can be applied to metagenomics-based strategies for the detection of unknown disease-associated viruses and for the discovery of novel human viruses [19, 20] . Compared with microarray-based assays, NGS methods offer the advantage of higher sensitivity and the potential to detect the full spectrum of viruses, including unknown and unexpected viruses.

One of the first applications of NGS for pathogen discovery was the investigation of three patients who died of a febrile illness a few weeks after transplantation of solid organs from a single donor and for whom conventional microbiological and molecular tests, as well as microarray analysis for a wide range of infectious agents, had not been informative [21] . In this study, RNA was purified from blood, cerebrospinal fluid and tissue specimens from transplant recipients and, after digestion with DNase to eliminate human DNA, RNA was reverse-transcribed and amplified with random primers. Amplification products were pooled and sequenced with the use of the 454 pyrosequencing platform. After subtraction of sequences of vertebrates and highly repetitive sequences, contiguous sequences were assembled and compared with motifs represented in databases of microbes, leading to the identification of putative protein sequences which were consistent with an Old World arenavirus.

Additional sequence analysis showed that it was a new arenavirus related to lymphocytic choriomeningitis viruses. Further serological and immunohistochemical analyses documented that the virus was transmitted through organ transplantation [21] .

A similar strategy, based on unbiased high-throughput sequencing using 454 pyrosequencing for the direct diagnosis of viral infections in clinical specimens, has been used in different diagnostic settings, such as the investigation of patients during seasonal influenza and norovirus outbreaks [22] , the identification of an astrovirus as a causative agent for encephalitis in a boy with agammaglobulinemia, after conventional methods had failed to identify an infectious agent [23] , and the identification of a hemorrhagic fever-associated arenavirus from South Africa (Lujo virus) [24] .

When implemented into virus-discovery methods based on shotgun sequencing, next-generation technologies greatly enhance turnaround time and sensitivity. For example, the 454 system was implemented into a virus discovery assay based on an improved version of the VIDISCA protocol to minimize rRNA contamination [25] . Likewise, the association of NGS techniques with rolling circle amplification (RCA), another method for virus discovery, could greatly increase its performance. RCA employs the PhiX29 polymerase to selectively amplify small double stranded DNA (dsDNA) molecules and is used to amplify circular genomes of DNA viruses and bacteria plasmids [26] . Recently, RCA led to the identification and whole genome sequencing of novel human papillomaviruses and polyomaviruses [27] , including human polyomaviruses 6 and 7 (HPyV6 and HPyV7), detected in cutaneous swab specimens of healthy persons [28] , and trichodysplasia spinulosa-associated polyomavirus (TSPyV), detected in skin lesions from immunocompromised patients [29] .

Besides 454 pyrosequencing, short-read-based metagenomic methods using the Illumina GA platform have also been used to detect unknown viruses in clinical specimens. The Illumina GA platform allowed to identify influenza A viruses from swab specimens and de novo assembly of its genome [30] [31] [32] . It also led to the detection of viral pathogens in nasopharyngeal aspirate samples from patients with acute lower respiratory tract infections [33] , such as a new enterovirus, named enterovirus 109 (EV109) detected in a cohort of Nicaraguan children with viral respiratory illness [34] .

A comparative study of the analytical sensitivity of the two platforms, 454 pyrosequencing and Illumina GA, for the detection of viruses in biological samples was done on a set of samples which were artificially spiked with eleven different viruses [35] . The Illumina method had a much greater sensitivity than 454, approaching that of optimized quantitative real-time PCR. However, at low viral concentration in the specimen, the number of reads generated by the Illumina platform was too small for de novo assembly of viral genome sequences [35] .

Vector-borne viruses and zoonotic viruses represent another important and challenging field for viral discovery. The feasibility of detecting arthropod-borne viruses was explored in Aedes aegypti mosquitoes experimentally infected with dengue virus and pooled with noninfected mosquitoes to simulate samples derived from ongoing arbovirus surveillance programs [36] . Total RNA was purified from mosquito pools, reverse-transcribed using random primers and subjected to 454 pyrosequencing, which led to the correct identification of infected mosquito pools [36] .

Another interesting strategy to discover arthropod-borne viruses exploits the property of invertebrates to respond to infection by processing viral RNA genomes into siRNAs of discrete sizes. A recent study on small RNA libraries sequenced by NGS platforms [37] showed that viral small silencing RNAs produced by invertebrate animals are overlapping in sequence and can assemble into long contiguous fragments of the invading viral genome. Based on this finding, an approach of virus discovery in invertebrates by deep sequencing and assembly of total small RNAs was developed and applied to the analysis of contigs (i.e., a contiguous length of genomic sequences in which the order of bases is known to a high confidence level) assembled from published small RNA libraries. Five previously undescribed viruses from cultured Drosophila cells and adult mosquitoes were discovered, including three with a positive-strand RNA genome and two with a dsRNA genome [37] . This strategy for virus discovery based on deep sequencing of small RNAs has been also successfully used in plant virology [38] .

Bats are reservoirs for emerging zoonotic viruses that cause diseases in humans and livestock, including lyssaviruses, filoviruses, paramyxoviruses, and SARS-CoV. In a surveillance study focused on the discovery of bat-transmitted pathogens, gastrointestinal tissue obtained from bats was analyzed by coronavirus consensus PCR and unbiased high-throughput pyrosequencing that revealed the presence of sequences of a new coronavirus, related to those of SARS-CoV [39] .

Computational subtraction analysis of data obtained using conventional shotgun sequencing methods has been used to identify viral sequences (e.g., HBV, HCMV, human papillomaviruses 18 and 16, HHV8, HCV, EBV and human spumavirus) in EST libraries derived from normal and cancerous tissues [40] and in post-transplant lymphoproliferative disorder tissue [41] . In these studies, computational subtraction analysis relied on sequence data gathered for other purposes as the yield of viral sequences was very low due to the predominance of human sequences. However, exploiting the great amount of sequencing data achievable by NGS methods, computational subtraction analysis could become a method of choice for viral discovery. This approach has been used for the discovery of a new polyomavirus associated with most cases of Merkel cell carcinoma (MCC) [42] . MCC is a rare and aggressive human skin cancer that typically affects elderly and immunosuppressed individuals, a feature which was suggestive of an infectious origin. RNA was purified from MCC samples and analyzed by 454 pyrosequencing. Digital transcriptome subtraction of all human sequences led to the detection of a fusion transcript between a human receptor tyrosine phosphatase and a Large T antigen sequence related to murine polyomaviruses. This sequence was used as starting point for whole genome sequencing and characterization of this previously unknown polyomavirus that was called Merkel cell polyomavirus (MCPyV). The presence of the virus in 80% MCC tissues but only in about 10% of control tissues from various body sites, including the skin, and the demonstration that, in MCPyV-positive MCCs, viral DNA was integrated within the tumor genome in a clonal pattern, strongly suggested the etiological role of the virus in the pathogenesis of MCC [42] .

In a NGS study of the skin virome of a patient with MCC in comparison with healthy controls [43] , another human polyomavirus strain was detected, which was nearly identical to the recently discovered HPyV9 polyomavirus [44] and closely related to the lymphotropic polyomavirus (LPV). Likewise, unbiased high-throughput sequencing or deep sequencing of amplicons generated with consensus primers targeting regions of the viral genome conserved within viral families, like the tumor-associated Polyomaviridae and Papillomaviridae, allowed the discovery and characterization of many new polyomavirus and papillomavirus genotypes in several animal species.

The Papillomaviridae family includes several viral species and at least 189 completely characterized papillomavirus types and putative new types are continuously found [45] . High throughput 454 pyrosequencing of amplicons generated by consensus PCR of a conserved region of viral genome was used to detect and genotype HPV in cervical cytology specimens [46] . The method allowed the detection of HPV types which were present in low amount in multiple infections and had the potentiality to detect a broad spectrum of HPV types, subtypes, and variants [46] . A similar approach was used to detect and genotype cutaneous HPV types in a large series of squamous cell carcinoma of the skin and other skin lesions [47] . Several different HPV types were detected, including novel putative cutaneous HPVs [47] .

Investigation of retrovirus and retroviral vector integration sites in host cell chromosomes is another field of viral oncology which received a great contribution from NGS technologies. The use of viral vectors that integrate in host genome for gene transfer may cause malignant transformation due to activation of host proto-oncogenes or inactivation of tumor-suppressor genes, as a consequence of viral vector integration within these genes [48] [49] [50] . Deep sequencing technology has been used to map the integration sites of retroviruses and HIV [51] , as well as retroviral and HIV-based vectors for gene therapy and cell reprogramming [52] [53] [54] . Deep sequencing methods for detection of retrovirus integration are based on 454 pyrosequencing of products of ligation-mediated PCR (LM-PCR) [55, 56] or linear amplification-mediated PCR (LAM-PCR) [57] . Both LM-PCR and LAM-PCR use restriction enzymes to fragment the DNA of interest containing proviruses. Then, digested DNA is ligated with a compatible linker and amplified by PCR using primers that anneal in the LTR and in the linker sequence. Nested primers containing linkers for the 454 protocol are then used for a second PCR, which is processed by 454 high-throughput sequencing. A LAM-PCR method without the use of restriction enzymes was also developed for high throughput sequencing [58] . Recently, a new method was developed for recovering sites of integrated DNA based on the bacterial transposase MuA. The transposase is used to introduce adaptors into genomic DNA to allow PCR amplification and analysis by 454 pyrosequencing. This method could avoid the bias associated with restriction enzymes and recovered integration sites in a near random fashion. It provided a measure of cell clonal abundance, which is crucial for detecting expansion of cell clones that may be a prelude to malignant transformation [59] .

The human microbiome is the entire population of microbes (i.e., bacteria, fungi, and viruses) that colonize the human body. Metagenomics refers to culture-independent studies of the collective set of genomes of mixed microbial communities and applies to explorations of all microbial genomes in consortia that reside in environmental niches, in plants, or in animal hosts, including the human body [60] [61] [62] . The ""metagenome"" of microbial communities that occupy human body niches is estimated to have a gene content approximately 100-fold greater than the human genome [63] . These diverse and complex collections of genes encode a wide array of biochemical and physiological functions that may be relevant in healthy and disease conditions.

Metagenomics strategies are generally based on whole genome shotgun sequencing of nucleic acids purified from a specimen. In case of bacteria metagenomics, analysis can be simplified by exploiting universal and conserved targets, such as 16S rRNA genes, which have both conserved regions that can be targeted by PCR primers, and intervening variable sequences that facilitate genus and species identification [60, 61] . At variance, no conserved ubiquitous viral sequences are available for broad amplification of viral genomes and methods to enrich samples with viral particles can only be used. In addition, viral metagenomics analyses, which have been applied so far mostly in environmental samples like fresh water, reused wastewater, and ocean water [64] [65] [66] [67] , have shown that many of the detected viral sequences are unique and represent unknown viral species. Thus, viral sequences may be missed even by shotgun sequencing [68] .

A recent study [69] developed a bioinformatic annotation strategy for identification and quantitative description of human pathogenic viruses in virome data sets and applied this strategy to annotate sequences of viral DNA and RNA (cDNA) extracted from sewage sludge residuals resulting from municipal wastewater treatment (biosolids), which were obtained by 454 pyrosequencing. In this experimental model, within the 51,925 annotated sequences, 94 DNA and 19 RNA sequences were identified as human viruses. Virus diversity included environmentally transmitted agents such as parechovirus, coronavirus, adenovirus and aichi virus, as well as viruses associated with chronic human infections, such as human herpesviruses and hepatitis C virus [69] .

In the diagnostic setting, metagenomic approaches could be used for systematic analysis of samples collected from patients with unexplained illness, especially in the context of outbreaks and epidemics [70, 71] . As mentioned in the above section, application of high throughput NGS methods in viral metagenomics can greatly enhance the chances to identify viruses in clinical samples, including viruses that are too divergent from known viruses to be detected by PCR or microarray techniques (reviewed in ref. [20] ). An attractive application of metagenomic approaches is the study of influenza, given the constant threat of antigenic drift and shift. Deep sequencing strategies can be used to monitor the emergence of mutations that confer virulence or resistance to antiviral drugs, to detect influenza viruses in clinical samples, and to identify viral quasispecies [22, 31, 32] . In addition, deep sequencing of clinical samples allows to identify and characterize not only novel pathogens but also the microbiota and host response to infection [32] .

The study of the human virome includes also the description of viral communities-including bacteriophages-in human body and their relationship with health and disease. Examples are the characterization of fecal viromes (mainly phages) and their relations with bacterial metagenome [72] and the characterization of the virome in the skin of healthy individuals [28] .

Like viral metagenomics, sequencing of full-length viral genomes is a difficult task due to the presence of contaminating nucleic acids of the host cell and other agents in viral isolates. In fact, preparation of a simple shotgun sequencing DNA library, the most comprehensive approach, or of a library of cDNA synthesized from RNA with random priming, results in a huge amount of host specific instead of a comprehensive representation of the viral sequences, even in the presence of a very high viral load [21, 31, 73] . Very high throughput sequencing techniques, such as SOLiD platform, could be used to obtain sufficient sequence coverage [74] , but the length of reads might be too short to allow de novo assembly of viral genomes and methods that provide longer reads, like 454 and Illumina technology, might be preferable [31, 32] . Several techniques have been used to enrich virions or viral nucleic acids from cell culture or from host tissue and fluids before extracting the genomic DNA/RNA, in order to limit the contamination from host nucleic acids. One of these methods is ultracentrifugation, but this procedure may be very time-consuming and laborious with uncertain outcome [75] . Other methods are based on enrichment of viral nucleic acids by using capture probes or PCR amplification targeting conserved genome segments [76, 77] or, vice versa, by depletion of host nucleic acids by probing total RNA with labeled host nucleic acid [78] . Other approaches could be enrichment of dsRNA virus genomes [79] or circular dsDNA viral genomes by RCA [28, 29] .

High mutation rates inherent to replication of RNA viruses create a wide variety of mutants that are present in virus populations, which are often referred to as quasispecies [80] . The diffuse, ""cloud-like"" nature of viral populations allows them to rapidly adapt to changing replicative environments by selecting preexisting variants with better fitness [81, 82] . Thus, many important virus properties cannot be explained by a mere consensus sequence, but require knowledge about the microvariants present in viral populations. These sequence variants may be critically relevant to viral evolution and spread, virulence, evasion of the immune response, anti-viral drug resistance, and vaccine development and manufacture.

The use of deep sequencing data for mutation analysis in viral genomes has required the development of computational methods for estimation of the quality of sequences and for error correction, algorithms for sequence alignment and haplotype reconstruction, statistical models to infer the frequencies of the haplotypes in the population, for comparative analysis and for their visualization [83] [84] [85] [86] .

Among RNA viruses, HIV quasispecies have been extensively investigated because of their relevance for vaccine design and response to antiviral drug therapy [87] . Within infected individuals, HIV is highly heterogeneous owing to rapid turnover rates, high viral load, and a replication mediated by the error-prone reverse transcriptase enzyme that lacks proofreading activity. High variability is also the consequence of recombination, which can shuttle mutations between viral genomes and lead to major antigenic shifts or alterations in virulence [88] . An example of application of NGS for analysis of HIV quasispecies is the use of massive parallel 454 pyrosequencing with the shotgun approach to characterize the full length genome of an HIV-1 BF recombinant and its quasispecies heterogeneity in a patient who died from multiorgan failure during seroconversion [89] . Another fascinating application of deep sequencing in HIV research is the use of the 454 pyrosequencing methods to analyze the variable regions of heavy and light chains of neutralizing antibodies against HIV in the blood obtained from HIV-1-infected individuals, in order to understand how broadly neutralizing antibodies develop [90] . But the most relevant application of NGS in HIV diagnostics is the detection of anti-viral drug resistant minor variants, which will be discussed in the next section.

Analysis of full-length viral genome and quasispecies was also applied to other RNA viruses. Deep sequencing with the Illumina platform on total RNAs extracted from the lung of a patient who died of viral pneumonia due to pandemic 2009 influenza A virus (A/H1N1/2009) revealed nucleotide heterogeneity on hemagglutinin as quasispecies, leading to amino acid changes on antigenic sites which could be relevant for antigenic drift [31] .

Mutations of human rhinovirus (HRV) genome were explored in a lung transplant recipient infected with the same HRV strain for more than two years [91] . Analysis of complete HRV genome sequences by both classical and Illumina ultra-deep sequencing of samples collected at different time points in the upper and lower respiratory tracts showed that HRV populations in the upper and lower respiratory tract were phylogenetically indistinguishable over the course of infection, likely because of constant viral population mixing. Nevertheless, signatures of putative adaptation to lower airway conditions appeared after several months of infection, with the occurrence of specific changes in the 5'UTR polypyrimidine tract and the VP2 immunogenic site 2 of HRV genome, which might have been relevant for viral growth at lower airway conditions [91] .

Populations of DNA viruses are considered less complex and variable when compared to RNA viruses. However, data from deep sequencing of DNA virus genomes have revealed that complex mixtures of viral genotypes may be present in infected subjects and that positive selection could have contributed to the divergence of different strains. This is the case of human cytomegalovirus (HCMV), which establishes lifelong latent infections in humans and may reactivate and cause severe life-threatening disease in immunocompromised patients. High intra-host variability of HCMV genome was demonstrated in lung transplant recipients by deep sequencing of the amplicons of three variable HCMV genes [92] and in neonates with congenital HCMV infection by deep sequencing of long range, overlapping amplicons covering the entire HCMV genome [93] . Since PCR amplification and sequencing can introduce errors in their own, which could be misinterpreted as mutation or polymorphisms, deep sequencing studies have to develop protocols and algorithms to estimate experimental error and to filter false positive results. In the studies reported here on HCMV genome variability, experimental error rate was estimated by using arbitrary criteria [92] or an algorithm based on experimental data obtained from deep sequencing analysis of a control HCMV genome cloned in a BAC vector [93] .

Deep sequencing showed also variability of herpes simplex virus 1 (HSV-1) genome and allowed to demonstrate virulence genes. Using Illumina high-throughput sequencing, genome sequences of both a laboratory strain (F) and a low-passage clinical isolate (H129) were obtained and compared with the available genome sequence of a more virulent isolate of HSV-1 (strain 17) [94] . The HSV-1 H129 strain, isolated from the brain of an encephalitic patient, is the only virus known to transit neural circuits exclusively in an anterograde direction [95] . Whole genome sequencing demonstrated many protein-coding variations between strains F and H129 and the genome reference strain 17 and some genes were proposed to be responsible of the anterograde mutant phenotype of strain H129, including the neurovirulence protein ICP34.5, while a frameshift mutation in the UL13 kinase could account for decreased neurovirulence of strain F [94] .

Deep sequencing by NGS techniques is being increasingly used in the clinical practice to detect low abundance drug resistant HIV variants and, with the recent availability of new drugs active against hepatitis C virus (HCV), also for the detection of HCV minor variants.

Conventional direct sequencing of RT-PCR products (referred to as ""population sequencing"") is the gold standard in HIV resistance testing and is used to detect drug-resistance mutations in the molecular targets of HIV-1 therapy, i.e., reverse transcriptase, protease, integrase, and V3 loop of the HIV env gene. A major limitation of direct PCR sequencing, however, is its inability to detect drug-resistant variants present in less than 20-25% of the heterogeneous virus population existing in a patient's plasma sample [96] . Several studies have shown that minor drug-resistant variants that are not detected by population-based sequencing are clinically relevant in that they are often responsible for the virological failure of a new antiretroviral treatment regimen [97] [98] [99] .

Clonal sequencing of RT-PCR products by 454 pyrosequencing offers the advantage of high sensitivity for minor variants and a relatively long sequence length that facilitates the characterization of the linkage amongst resistance mutations and avoids the risk to miss mutations due to sequence variation around the site under investigation. The application of 454 sequencing-based resistance testing in clinical setting, however, requires careful consideration of potential technical errors that can be introduced in the experimental protocol and in data analysis in order to discriminate between experimentally introduced errors and true variants [87, 100, 101] . Data analysis issues are discussed in Section 2.9.

Several studies that employed 454 pyrosequencing for deep analysis of mutations in HIV protease and reverse transcriptase genes demonstrated the accuracy of this technique in detecting all drug-resistance mutations identified by population sequencing, and the ability to detect low-frequency mutations undetectable by population sequencing [100, 102, 103] . In addition, several studies demonstrated that drug-resistance mutations detected by 454 had a significant impact on virological failure [103] [104] [105] [106] [107] while others did not find a strong association of low-frequency mutations with clinical responses [108, 109] . Deep sequencing using the 454 platform has been also applied to investigate drug-resistance mutations against the more recently approved integrase inhibitors and CCR5 antagonists.

Drug-resistance mutations to integrase inhibitors occur in the integrase gene. These mutations were detected by deep sequencing at very low levels if at all prior to initiating therapy [110] and could be selected by previous drug pressure [111] . Resistance to CCR5 antagonists, like maraviroc, occur by outgrowth of CXCR4-tropic HIV variants, i.e., viruses that use the CXCR4 coreceptor [112] or via mutations in the viral envelope protein [113] [114] [115] [116] . Coreceptor usage can be screened using phenotypic coreceptor tropism assays, based on recombinant virus technology, or genotypic tests, based on sequencing of the V3 loop of HIV env gene [117] . Phenotypic assays have good sensitivity and specificity, but they are time consuming, expensive, and require special laboratory facilities; thus they are not convenient as diagnostic tests in clinical practice. Genotyping methods based on population sequencing represent a more feasible alternative, but their sensitivity for the detection of minority variants is lower than phenotypic assay (about 10-20%) and this represents a problem, since the proportion of CXCR4-tropic HIV variants before initiation of therapy is generally very low. In addition, the algorithms used for interpretation of sequencing results may underestimate the impact of some mutations in viral tropism [118] . Deep sequencing by using 454 has been used in several studies [119] [120] [121] [122] [123] , including large clinical trials, to determine viral tropism and has been demonstrated to be comparable in sensitivity and specificity with phenotypic assays in detecting CXCR4-using variants. According to data reported to date, the clinical threshold for detection of CXCR4-tropic variants might range between 2-10% [118] . With this threshold, 454 pyrosequencing at ≥1% sensitivity for minority variants can represent a valuable diagnostic tool for viral tropism testing. In addition, deep sequencing of relatively long reads allows defining the contribution of multiple mutations in a single viral genome. This information could improve the performance of interpretation algorithms as compared with population sequencing.

Deep sequencing based on the 454 technology has been also applied for the detection of nucleoside and nucleotide reverse-transcriptase inhibitor resistance in HBV. The NGS method was more sensitive for the detection of rare HBV drug resistance mutations than conventional methods based on population sequencing or reverse hybridization [124, 125] . In addition, deep sequencing allowed to identify G-to-A hypermutation mediated by the apolipoprotein B mRNA editing enzyme, which was estimated to be present in 0.6% of reverse-transcriptase genes [124] .

Finally, with the availability of new drugs targeting HCV protease and polymerase, the experience of drug-resistance mutation and quasispecies analysis achieved with HIV is being translated to HCV. Also for HCV, deep sequencing technologies seem a promising tool for the study of minority variants present in the HCV quasispecies population at baseline and during antiviral drug pressure, giving new insights into the dynamics of resistance acquisition by HCV [126, 127] .

High throughput sequencing is being used to investigate the epidemiology of viral infections and viral evolution, addressing issues such as viral superinfection (e.g., HIV superinfection, which occurs when a previously infected individual acquires a new distinct HIV strain) [128] , tracing the evolution and spread of viral strains, such as the emergence, evolution and worldwide spread of HIV [88] , tracing the transmission of viruses among individuals [129] , or modeling the evolution of viruses within the host and the mechanism of immune escape, balanced with replication fitness, such as in the case of HIV and HCV infection [127, 130, 131 ].

Intrinsic genetic instability of RNA viruses may lead to the accumulation of virulent revertants during manufacture of live viral vaccines, requiring rigorous quality control to ensure vaccine safety. High throughput deep sequencing methods have been proposed as tools for monitoring genetic consistency of live viral vaccines. Deep sequencing was used to analyze lots of oral poliovirus vaccine and the detected neurovirulence mutations were identical to the mutation detected with the standard method based on PCR and restriction enzyme cleavage [132] . Patterns of mutations present at a low level in vaccine preparations were characteristic of seed viruses used for their manufacture and could be used for identification of individual batches [132] . Deep sequencing was also used to examine eight live-attenuated viral vaccines, i.e., trivalent oral poliovirus, rubella, measles, yellow fever, varicella-zoster, multivalent measles/mumps/rubella, and two rotavirus live vaccines [133] . The method allowed identification of, not only mutations and minority variants relative to vaccine strains, but also sequences of adventitious viruses from the producer avian and primate cells. The results were in agreement with those obtained by using a panmicrobial microarray [133] .

An aspect that should not be neglected when dealing with NGS data, is the bioinformatics analysis and issues concerning sequencing output. There are inherent strengths and weaknesses in the different platforms as reported in Table 1 . For example, 454 technology is well suited for small de novo sequencing projects and amplicon studies, given its read length output that presently reaches the average length of sequences produced with Sanger method. The main issue to be aware of concerns the homopolymer length, due to signal thresholding of the incorporated nucleotides. SOLiD platform is not presently suitable for amplicon studies due to the short read length, but exhibits an extremely high throughput capacity. Illumina has a superior read length and is not affected by homopolymers but, as SOLiD, shows low coverage of AT rich regions [134] . Other platforms present in Table 1 (with the exclusion of the GS Junior, which shares the same features of 454 FLX but has a lower throughput) are still in development and not yet evaluated in their diagnostic potential.

Besides the specific limits of the different platforms, other common issues should be taken into account and carefully considered. The first sources of problems are certainly chimerical sequences, point mutations and insertions/deletions which occur during reverse transcription, PCR amplification or sequencing itself. In addition, PCR amplification bias might impact the relative frequencies of viral variants. The process of ""data cleaning"" consists of three main steps: sequence filtering, alignment and error correction, for which a panel of methods has been proposed [84, 135, 136] . Briefly, the filtering phase removes the low-quality sequences from the dataset, while the error correction separates true variants from those due to experimental noise. This step is based on the idea that errors are randomly distributed with low frequency, while sequences with real mutations can be clustered and their abundance quantified. A cluster of reads presenting the same mutations represents a haplotype and the size of the cluster is the haplotype frequency. Global haplotypes are more difficult to be identified, since the reads must be assembled in larger contigs and a unique solution in aligning overlapping reads is not guaranteed. To this respect, the advantage of 454 platform for haplotype reconstruction studies is evident, thanks to its longer reads output.

As concerns the data analysis step, a multitude of software has been developed for very different applications of NGS. Nevertheless, if on the one hand this availability of methods greatly eases the task, on the other hand available algorithms for both genome assembly and amplicon analysis present limitations or drawbacks [137] which require custom made scripting and in-house resolution of bioinformatics problems caused by specific needs [46] . The direct consequence is that data analysis can be no more sustained by the wet-lab researcher alone, but requires the acquisition of computer skills and bioinformatics expertise.

Next-generation high throughput sequencing technologies have become available in the last few years and are in continuous development and improvement. They have been widely used in many projects, e.g., whole genome sequencing, metagenomics, small RNA discovery and RNA sequencing. Their common feature is the extremely high throughput data generation. As a result, new issues have to be addressed in order to exploit the full potential of these new instruments: firstly, the data analysis step has become very time consuming and requires a competent amount of manpower and expertise in bioinformatics; secondly, adequate computing resources are necessary to handle the data produced.

Diagnostic virology is one of the most successful applications for NGS and exciting results have been achieved in the discovery and characterization of new viruses, detection of unexpected viral pathogens in clinical specimen, ultrasensitive monitoring of antiviral drug resistance, investigation of viral diversity, evolution and spread, and evaluation of the human virome. With the decrease of costs and improvement of turnaround time, these techniques will probably become essential diagnostic tools in clinical routines.

",0.7508609000951524
Mapping road network communities for guiding disease surveillance and control strategies OPEN,"Human mobility is increasing in its volume, speed and reach, leading to the movement and introduction of pathogens through infected travelers. An understanding of how areas are connected, the strength of these connections and how this translates into disease spread is valuable for planning surveillance and designing control and elimination strategies. While analyses have been undertaken to identify and map connectivity in global air, shipping and migration networks, such analyses have yet to be undertaken on the road networks that carry the vast majority of travellers in low and middle income settings. Here we present methods for identifying road connectivity communities, as well as mapping bridge areas between communities and key linkage routes. We apply these to Africa, and show how many highly-connected communities straddle national borders and when integrating malaria prevalence and population data as an example, the communities change, highlighting regions most strongly connected to areas of high burden. The approaches and results presented provide a flexible tool for supporting the design of disease surveillance and control strategies through mapping areas of high connectivity that form coherent units of intervention and key link routes between communities for targeting surveillance.","networks, the regular and planar nature of road networks precludes the formation of clear communities, i.e. roads that cluster together shaping areas that are more connected within their boundaries than with external roads. Highly connected regional communities can promote rapid disease spread within them, but can be afforded protection from recolonization by surrounding regions of reduced connectivity, making them potentially useful intervention or surveillance units 6, 26, 27 . For isolated areas, a focused control or elimination program is likely to stand a better chance of success than those highly connected to high-transmission or outbreak regions. For example, reaching a required childhood vaccination coverage target in one district is substantially more likely to result in disease control and elimination success if that district is not strongly connected to neighbouring districts where the target has not been met. The identification of 'bridge' routes between highly connected regions could also be of value in targeting limited resources for surveillance 28 . Moreover, progressive elimination of malaria from a region needs to ensure that parasites are not reintroduced into areas that have been successfully cleared, necessitating a planned strategy for phasing that should be informed by connectivity and mobility patterns 26 . Here we develop methods for identifying and mapping road connectivity communities in a flexible, hierarchical way. Moreover, we map 'bridge' areas of low connectivity between communities and apply these new methods to the African continent. Finally, we show how these can be weighted by data on disease prevalence to better understand pathogen connectivity, using P. falciparum malaria as an example.

African road network data. Data on the African road network (ARN) were obtained from GPS navigation and cartography as described in a previous study 24 . The dataset maps primary and secondary roads across the continent, and while it does have commercial restrictions, it is a more complete and consistent dataset than alternative open road datasets (e.g. OpenStreetMap 29 , gRoads 30 ). Visual inspection and comparison between the ARN and other spatial road inventories validated the improved accuracy and consistency of ARN, however a quantitative validation analysis was not possible due to the lack of consistent ground-truth data at continental scales. Figure 1a shows the African road network data used in this analysis. The road network dataset is a commercial restricted product and requests for it can be directly addressed to GARMIN 31 . Plasmodium falciparum malaria prevalence and population maps. To demonstrate how geographically referenced data on disease occurrence or prevalence can be integrated into the approaches outlined, gridded data on Plasmodium falciparum malaria prevalence were obtained from the Malaria Atlas Project (http:// www.map.ox.ac.uk/). These represent modelled estimates of the prevalence of P. falciparum parasites in 2015 per 5 × 5 km grid square across Africa 32 . Additionally, gridded data on estimated population totals per 1 × 1 km grid square across Africa in 2015 were obtained from the WorldPop program (http://www.worldpop.org/). The population data were aggregated to the same 5 × 5 km gridding as the malaria data, and then multiplied together to obtain estimates of total numbers of P. falciparum infections per 5 × 5 km grid square.

Detecting communities in the African road network. We modeled the ARN as a'primal' road network, where roads are links and road junctions are nodes 33 . Spatial road networks have, as any network embedded in two dimensions, physical spatial constraints that impose on them a grid-like structure. In fact, the ARN primal network is composed of 300, 306 road segments that account for a total length of 2, 304, 700 km, with an average road length of 7.6 km ± 13.2 km. Such large standard deviations, as already observed elsewhere 23, 24, 34 , are due to the long tailed distribution of road lengths, as illustrated in Fig. 1c . Another property of road network structure is the frequency distribution of the degree of nodes, defined as the number of links connected to each node. Most networks in nature and society have a long tail distribution of node degree, implying the existence of hubs (nodes that connect to a large amount of other nodes) 21 , with the majority of nodes connecting to very few others. For road networks, however, the degree distribution strongly peaks around 3, indicating that most of the roads are connected with two other roads. The long tail distribution of the length of road segments, coupled with the peaked degree distribution, indicates the presence of translational invariant grid-like structure, in which road density smoothly varies among regions while their connectivity and structure does not. Within such gridlike structures it is very difficult to identify clustered communities, i.e. groups of roads that are more connected within themselves than to other groups. This observation is confirmed by the spatial distribution of betweenness centrality (Bc), which measures the amount of time the shortest paths between each couple of nodes pass through a road. The probability distribution of Bc is long tailed (Fig. 1d) , while its spatial distribution spreads across the entire network, with a structural backbone form, as shown in Fig. 1b. Again, under such conditions and because of the absence of bottlenecks, any strategy to detect communities that employs pruning on Bc values 35 , will be minimally effective.

To detect communities in road networks we follow the observation that human displacement in urban networks is guided by straight lines 36 . Therefore, geometry can be used to detect communities of roads by assuming that people tend to move more along streets than between between streets. We developed a community detection pipeline that converts a primal road network, where roads are links and roads junction are nodes 33 , to a dual network representation, where link are nodes and street junction link between nodes 37 , by mean of straightness and contiguity of roads. It is important to note here that the units of analysis are road segments, which here are typically short and straight between intersections, making the straightness assumption valid. Community detection in the dual network is then performed using a modularity optimization algorithm 38 . The communities found in the dual network are then mapped back to the original primal road network. These communities encode information about the geometry of road pattern but can also incorporate weights associated with a particular disease to guide the process of community detection.

Nodes in the dual network represent lines in the primal network. The conversion from primal to dual is done by using a modified version of the algorithm known as continuity negotiation 37 . In brief, we assume that a pair of adjacent edges belongs to the same street if the angle θ between these edges is smaller than θ c = 30°. We also assume that the angle between two adjacent edges (i, j) and (j, p) is given by the dot product cos (θ) = r i, j r j,p /r i, j r j,p , where r i, j = r j r i . Under these assumptions, the angle between two edges belonging to a perfect straight line is zero, while it assumes a value of 90° for perpendicular edges.

Our algorithm starts searching for the edge that generates the longest road in the primal space, as can be seen in Fig. 2a . Then, a node is created in the dual space and assigned to this road. Next, we search for the edge that generates the second longest road, and a new node is created in the dual space and assigned to this road. If there is at least one interception between the new road and the previous one, we connect the respective nodes in the dual space. The algorithm continues until all the edges in the primal space are assigned to a node in the dual space, as shown in Fig. 2b . Note that the conversion from primal to the dual road network has been used extensively to estimate human perception and movement along road networks (Space syntax, see 36 ) , which also supports our use of road geometry to detect communities.

Despite the regular structure of the network in the primal space, the topology of these networks in the dual space is very rich. For instance the degree distribution in dual space follows the power-law P(k) k −γ . This property has been previously identified in urban networks 33 and it is strongly related to the long tailed distribution of road lengths in these networks (see Fig. 1c ). Since most of the roads are short, most of the nodes in dual space will have a small number of connections. On the other hand, there are a few long roads (Fig. 2a ) that originate at hubs in the dual space (Fig. 2b ). Our approach for detecting communities in road networks consists then in performing classical community detection in the dual representation ( Fig. 2c) and then bringing the result back to the primal representation, as shown in Fig. 2d . The algorithm used to detect the communities is the modularity-based algorithm by Clauset and Newman 35 . The hierarchical mapping of communities on the African road network, with outputs for 10, 20, 30 and 40 sets of communities, is shown in Fig. 3 . The maps highlight how connectivity rarely aligns with national borders, with the areas most strongly connected through dense road networks typically straddling two or more countries. The hierarchical nature of the approach is illustrated through the breakdown of the 10 large regions in Fig. 3a into further sub-regions in b, c and d, emphasizing the main structural divides within each region in mapped in 3a. Some large regions appear consistently in each map, for example, a single community spans the entire north African coast, extending south into the Sahara. South Africa appears as wholly contained within a single community, while the horn of Africa containing Somalia and much of Ethiopia and Kenya in consistently mapped as one community. The four maps shown are example outputs, but any number of communities can be identified. The clustering that maximises modularity produces 104 communities, and these are mapped in Fig. 4 .

Even with division into 104 communities, the north Africa region remains as a single community, strongly separated from sub-Saharan Africa by large bridge regions. South Africa also remains as almost wholly within its own community, with Somalia and Namibia showing similar patterns. The countries with the largest numbers of communities tend to be those with the least dense infrastructure equating to poor connectivity, such as DRC and Angola, though West Africa also shows many distinct clusters, especially within Nigeria. Apart from the Sahara, the largest bridge regions of poor connectivity are located across the central belt of sub-Saharan Africa, where population densities are low and transport infrastructure is both sparse and often poor. The communities mapped in Figs 3 and 4 align in many cases with recorded population and pathogen movements. For example, the broad southern and eastern community divides match well those seen in HIV-1 subtype analyses 12 and community detection analyses based on migration data 27 . At more regional scales, there also exist similarities with prior analyses based on human and pathogen movement patterns. For example, the western, coastal and northern communities within Kenya in Fig. 4b , identified previously through mobile phone and census derived movement data 39, 40 . Further, Guinea, Liberia and Sierra Leone typically remain mostly within a single community in Fig. 3 , with some divides evident in Fig. 4c . This shows some strong similarities with the spread of Ebola virus through genome analysis 15 , particularly the multiple links between rural Guinea and Sierra Leone, though Fig. 4c highlights a divide between the regions containing Conakry and Freetown when Africa is broken into the 104 communities. Figure 3 highlights the connections between Kinshasa in western DRC and Angola, with the recent yellow fever outbreak spreading within the communities mapped. Figure 4d shows the'best' communities map for an area of southern Africa, and the strong cross-border links between Swaziland, southern Mozambique and western South Africa are mapped within a single community, as well as wider links highlighted in Fig. 3 , matching the travel patterns found from Swaziland malaria surveillance data 41 .

Integrating P. falciparum malaria prevalence and population data with road networks for weighted community detection. The previous section outlined methods for community detection on unweighted road networks. To integrate disease occurrence, prevalence or incidence data for the identification of areas of likely elevated movement of infections or for guiding the identification of operational control units, an adaptation to weighted networks is required. We demonstrate this through the integration of the data on estimated numbers of P. falciparum infections per 5 × 5 km grid square into the community detection pipeline. The final pipeline for community detection calculated a trade-off between form and function of roads in order to obtain a network partition.

The form is related to the topology of the road network and is taken into account during the primal-dual conversion. The topological component guarantees that only neighbor and well connected locations could belong to the same community. The functional part, on the other hand, is calculated by the combination of estimated P. falciparum malaria prevalence multiplied by population to obtain estimated numbers of infections, as outlined above.

The two factors were combined to form a weight to each edge of our primal network. The weight w i, j of edge (i, j) is defined as where m(r) is the P. falciparum malaria prevalence and p(r) is the population count, both at coordinate r. These values are obtained directly from the data. When the primal representation is converted into its dual version, the weights of primal edges, given by Eq. 1, are converted into weights of dual nodes, which are defined as where i represents the i th dual node and Ω i represents the set of all the primal edges that were combined together to form the dual node i (see Fig. 2a,b) . Finally, weights for the dual edges are created from the weights of dual nodes, by simply assuming

The dual network weighted by values of λ i,¯j was used as input for a weighted community detection algorithm. Ultimately, when the communities detected in the dual space are translated back to primal space, we have that neighbor locations with similar values of estimated P. falciparum infections belong to the same communities. For the example of P. falciparum malaria used here, the max function was used, representing maximum numbers of infections on each road segment in 2015. This was chosen to identify connectivity to the highest burden areas. Areas with large numbers of infections are often 'sources' , with infected populations moving back and forward from them spreading parasites elsewhere 6, 42 . Therefore, mapping which regions are most strongly connected to them is of value. Alternative metrics can be used however, depending on the aims of the analyses.

The integration of P. falciparum malaria prevalence and population (Fig. 5a ) through weighting road links by the maximum values across them produces a different pattern of communities (Fig. 5b) to those based solely on network structure (Fig. 3) . The mapping of 20 communities is shown here, as it identifies key regions of known malaria connectivity, as outlined below. The mapping shows areas of key interest in malaria elimination efforts connected across national borders, such as much of Namibia linked to southern Angola 43 , but the Zambezi region of Namibia more strongly linked to the community encompassing neighbouring Zambia, Zimbabwe and Botswana 44 . In Namibia, malaria movement communities identified through the integration of mobile phone-based movement data and case-based risk mapping 26 show correspondence in mapping a northeast community. Moreover, Swaziland is shown as being central to a community covering, southern Mozambique and the malaria endemic regions of South Africa, matching closely the origin locations of the majority of internationally imported cases to Swaziland and South Africa 41, 45, 46 . The movements of people and malaria between the highlands and southern and western regions of Uganda, and into Rwanda 47 , also aligns with the community patterns shown in Fig. 5b . Finally, though quantifying different factors, the analyses show a similar east-west split to that found in analyses of malaria drug resistance mutations 6, 48 and malaria movement community mapping 27 .

The emergence of new disease epidemics is becoming a regular occurrence, and drug and insecticide resistance are continuing to spread around the world. As global, regional and local efforts to eliminate a range of infectious diseases continue and are initiated, an improved understanding of how regions are connected through human transport can therefore be valuable. Previous studies have shown how clusters of connectivity exist within the global air transport network 49, 50 and shipping traffic network 50 , but these represent primarily the sources of occasional long-distance disease or vector introductions 1, 8 , rather than the mode of transport that the majority of the population uses regularly. The approaches presented here focused on road networks provide a tool for supporting the design of disease and resistance surveillance and control strategies through mapping (i) areas of high connectivity where pathogen circulation is likely to be high, forming coherent units of intervention; (ii) areas of low connectivity between communities that form likely natural borders of lower pathogen exchange; (iii) key link routes between communities for targetting surveillance efforts. The outputs of the analyses presented here highlight how highly connected areas consistently span national borders. With infectious disease control, surveillance, funding and strategies principally implemented country by country, this emphasises a mismatch in scales and the need for cross-border collaboration. Such collaborations are being increasingly seen, for example with countries focused on malaria elimination (e.g. 51, 52 ), but the outputs here show that the most efficient disease elimination strategies may need to reconsider units of intervention, moving beyond being constrained by national borders. Results from the analysis of pathogen movements elsewhere confirm these international connections (e.g. 6, 12, 41, 48 , building up additional evidence on how pathogen circulation can be substantially more prevalent in some regions than others.

The approaches developed here provide a complement to other approaches for defining and mapping regional disease connectivity and mobility 9 . Previously, census-based migration data has been used to map blocks of countries of high and low connectivity 27 , but these analyses are restricted to national-scales and cover only longer-term human mobility. Efforts are being made to extend these to subnational scales 53, 54 , but they remain limited to large administrative unit scales and the same long timescales. Mobile phone call detail records (CDRs) have also been used to estimate and map pathogen connectivity 26, 40 , but the nature of the data mean that they do not include cross-border movements, so remain limited to national-level studies. An increasing number of studies are uncovering patterns in human and pathogen movements and connectivity through travel history questionnaires (e.g. 41, 47, 55, 56 ), resulting in valuable information, but typically limited to small areas and short time periods.

There exist a number of limitations to the methods and outputs presented here that future work will aim to address. Firstly, the hierarchies of road types are not currently taken into account in the network analyses, meaning that a major highway and small local roads contribute equally to community detection and epidemic spreading. The lack of reliable data on road typologies, and inconsistencies in classifications between countries, makes this challenging to incorporate however. Moreover, the relative importance of a major road versus secondary, tertiary and tracks is exceptionally difficult to quantify within a country, let alone between countries and across Africa. Finally, data on seasonal variations in road access does not exist consistently across the continent. Our focus has therefore been on connectivity, in terms of how well regions are connected based on existing road networks, irrespective of the ease of travel. A broader point that deserves future research is that while intuition suggests a correspondence in most places, connectivity may not always translate into human or pathogen movement.

Future directions for the work presented here include quantitative comparison and integration with other connectivity data, the integration of different pathogen weightings, and the extension to other regions of the World. Qualitative comparisons outlined above show some good correspondence with analyses of alternative sources of connectivity and disease data. A future step will be to compare these different connections and communities quantitatively to examine the weight of evidence for delineating areas of strong and weak connectivity. This could potentially follow similar studies looking at community structure on weighted networks, such as in the US based on commuting data 57 , or UK and Belgium from mobile network data 58, 59 . Here, P. falciparum malaria was used to provide an example of the potential for weighting analyses by pathogen occurrence, prevalence, incidence or transmission suitability. Moreover, future work will examine the integration of alternative pathogen weightings. The maximum difference method was used here to pick out regions well connected to areas high P. falciparum burden, but the potential exists to use different weighting methods depending on requirements, strategic needs, and the nature of the pathogen being studied.

Despite the rapid growth of air travel, shipping and rail in many parts of the world, roads continue to be the dominant route on which humans move on sub-national, national and regional scales. They form a powerful force in shaping the development of areas, facilitating trade and economic growth, but also bringing with them the exchange of pathogens. Results here show that their connectivity is not equal however, with strong clusters of high connectivity separated by bridge regions of low network density. These structures can have a significant impact on how pathogens spread, and by mapping them, a valuable evidence base to guide disease surveillance as well as control and elimination planning can be built.

Results were produced through four main phases. Phase 1: Road network cleaning and weighted adjacency list production: the road cleaning operation aimed to produce a road network from the georeferenced vectorial network of roads infrastructure. This phase was conducted using ESRI ArcMap 10.4 (http://desktop.arcgis.com/en/ arcmap/) through the use of the topological cleaning tool. The tool integrates contiguous roads, removes very short links and removes overlapping road segments. Road junctions were created using the polyline to node conversion tool, while road-link association was computed using the spatial join tool. Malaria prevalence values were assigned to each road using the spatial join tool. The adjacency matrix output, containing also the coordinates for each road junctions, was extracted in form of text file. Phase 2: Conversion from the primal to the dual network: the primal network created in phase 1 was then used as input for a continuity negotiation-like algorithm. The goal of this algorithm was to translate the primal network into its dual representation (see Fig. 2a,b) . The implementation of the negotiation-like algorithm used the iGraph library in C++ (http://igraph.org/c/) on an octa-core iMac. The conversion took around 20 hours for a primal network with ~200 k nodes running. The algorithm works by first identifying roads composed of many contiguous edges in the primal space. Two primal-edges are assumed to be contiguous if the angle between them is not greater than 30° degrees. Because the dual representation generated by the algorithm strongly depends on the starting edge, we started by looking for the edge that produces the longest road. As soon as this edge was found, a dual-node was created to represent that road. Next we proceeded to look for the edge that produced the second longest road and create a dual-node for that road. We continued this process until every primal-edge had been assigned to a road. Finally, dual-nodes were connected to each other if their primal counterparts (roads) crossed each other in the primal space. Phase 3: Community detection: we used a traditional modularity optimization-based algorithm to identify communities in the dual representation of the road network. The modularity metrics were computed in R using the iGraph library (http://igraph.org/r/). To incorporate the prevalence of malaria, we used the malaria prevalence values as edge weights for community detection. Phase 4: Mapping communities. Detected communities were mapped back to the primal road network with the use of the spatial join tool in ArcMap. All maps were produced in ArcMap.

",0.7504698709855615
Web-based infectious disease surveillance systems and public health perspectives: a systematic review,"Background: Emerging and re-emerging infectious diseases are a significant public health concern, and early detection and immediate response is crucial for disease control. These challenges have led to the need for new approaches and technologies to reinforce the capacity of traditional surveillance systems for detecting emerging infectious diseases. In the last few years, the availability of novel web-based data sources has contributed substantially to infectious disease surveillance. This study explores the burgeoning field of web-based infectious disease surveillance systems by examining their current status, importance, and potential challenges. Methods: A systematic review framework was applied to the search, screening, and analysis of web-based infectious disease surveillance systems. We searched PubMed, Web of Science, and Embase databases to extensively review the English literature published between 2000 and 2015. Eleven surveillance systems were chosen for evaluation according to their high frequency of application. Relevant terms, including newly coined terms, development and classification of the surveillance systems, and various characteristics associated with the systems were studied. Results: Based on a detailed and informative review of the 11 web-based infectious disease surveillance systems, it was evident that these systems exhibited clear strengths, as compared to traditional surveillance systems, but with some limitations yet to be overcome. The major strengths of the newly emerging surveillance systems are that they are intuitive, adaptable, low-cost, and operated in real-time, all of which are necessary features of an effective public health tool. The most apparent potential challenges of the web-based systems are those of inaccurate interpretation and prediction of health status, and privacy issues, based on an individual's internet activity. Conclusion: Despite being in a nascent stage with further modification needed, web-based surveillance systems have evolved to complement traditional national surveillance systems. This review highlights ways in which the strengths of existing systems can be maintained and weaknesses alleviated to implement optimal web surveillance systems.","Despite medical advances and increased vaccine availability, emerging and re-emerging epidemics continue to pose tremendous threats, based on reported cases of severe acute respiratory syndrome, influenza A (H1N1), avian flu, Ebola virus, and the recent Middle East respiratory syndrome [1] . To avoid the repercussions of an epidemic, early detection and immediate response are emphasized to manage infectious diseases. Many online surveillance systems that function based on real-time data have been developed involving a wide range of technologies and data sources to prevent the occurrence of infectious diseases; these systems are continually being added to and evaluated [2] . Traditional passive surveillance systems typically rely on data submitted to the relevant public health authority by various healthcare providers [3] . This process is often expensive and inefficient, as substantial delays between an event and notifications are common, resulting in an incomplete account of disease emergence. Such limitations of traditional surveillance systems are a shared concern worldwide. The Internet has revolutionized efficient health-related communication and epidemic intelligence [4] . The increased frequency of Internet use for acquiring health information has contributed to the rise of web-based early detection systems for infectious diseases through various methodologies [5] . The principal concept is that diseaserelated information is retrieved from a wide range of available real-time electronic data sources, which play critical roles in the identification of early events and situational preparedness by offering current, highly local information about outbreaks, even from remote areas that have been unapproachable by traditional global public health efforts [6] . These systems not only monitor and predict disease outbreaks but also provide a user interface, and aid in visualization for an easier understanding and maneuvering of the operation. These new systems for early detection of epidemics are still in the nascent stage, but the concept and relevant promising mechanisms have been adopted and tested by the Centers for Disease Control and Prevention (CDC) with positive indications for efficiency and feasibility [7] . In fact, several web-based surveillance systems are affiliated with the CDC from which they are granted funding and technical assistance [8] .

Previous studies have suggested that these new systems exhibit remarkable potential for expansion and for enhancing the capacity of traditional surveillance systems for emerging infectious diseases [9] . It is of great importance to discuss the possible directions in which these new surveillance systems are headed in the context of public health by thoroughly examining areas of improvement for such systems. In addition, the absence of a system for predicting and monitoring epidemics in some countries with strong information communications technology (ICT) capability should command the attention of their national public health sectors, as there is an imminent need to implement such a mechanism. The objective of this systematic review was to investigate well-established web-based infectious disease surveillance systems that focus on infectious disease occurrence and the early detection of outbreaks. Our investigation can serve as an overview and starting point for readers interested in the topic and as a useful reference for the design of prospective infectious disease surveillance systems in countries that lack such tools.

A systematic review was performed and reported in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist (Additional file 1).

Literature from multiple journal sources was obtained by searching with relevant search terms, and appropriate articles on web-based disease surveillance systems were reviewed extensively. The literature search was conducted using the PubMed, Web of Science, and Embase databases. Articles written in English published between 2000 and 2015 were searched for a more refined outcome. The following key words were used in the search process: syndromic surveillance (""syndromic"" [All Fields] AND ""surveillance"" [All Fields]), digital disease detection (""digital"" [All Fields] AND ""disease"" [MeSH Terms] AND ""detection [All Fields]), biosurveillance (""biosurveillance"" [MeSH Terms]), infoveillance (""infoveillance"" [All Fields]), infodemiology (""infodemiology"" [All Fields]), online surveillance (""online"" [All Fields] AND ""surveillance"" [All Fields]), outbreak forecast (""outbreaks"" [All Fields] AND ""forecasting"" [MeSH Terms], and web surveillance systems (""web"" [All Fields] AND ""surveillance"" [All Fields] AND ""systems"" [All Fields]). The initial search strategy developed for PubMed was that some of the vague terms were re-sorted into ""medical subject headings"", which brought forth more specific and relevant results.

The first task was to systematically search the three databases PubMed, Web of Science, and Embase. Second, the 4,650 articles identified after the removal of duplicates were meticulously checked for relevant information on web-based infectious disease surveillance systems. Third, those web-based infectious disease surveillance systems which were mentioned in at least five studies were further considered. Lastly, all identified evidence was further complemented with the authors' expert knowledge and personal archives. The last step also included the consultation of the CDC website and the inclusion of the ""GET WELL"" system, which was only mentioned in four studies (see Fig. 1 ) and would have been omitted without this last step. Other web-based infectious disease surveillance systems that were mentioned in only a few studies and thus were not considered in this systematic review are as follows: Argus, Electronic Surveillance System for the Early Notification of Community-based Epidemics (ESSENCE II), International system for Total Early Disease Detection (InsTEDD). The studies included provided a comprehensive review for understanding existing web-based surveillance systems aimed at detecting infectious diseases early.

Typical terms associated with conventional systems have evolved following the emergence of new online-based infectious disease surveillance systems. The merging of public health and ICT has brought forth several recently coined terms and unprecedented word combinations, both of which are essential for understanding the fundamentals of the new disease detection systems. These new terms reflect the complexity of the convergence. The most commonly appearing terms and their descriptions are shown in Table 1 .

Across the three databases and the CDC website, 4,650 articles were collected, and duplicates were removed within the same database and across the different databases, resulting in 2,080 articles. Subsequently, these articles were further screened by assessing whether the title or abstract contained the exact search terms or if the content itself was relevant to the subject matter. After a meticulous assessment of full-text articles for eligibility, and exclusion of those with insufficient and inadequate information for analysis, 60 studies were filtered for the final qualitative analysis. Eleven web-based surveillance systems were analyzed, based on the selected literature, with regard to their development, various characteristics, and mechanisms, including their methods of data collection and delivery of service. The flow chart (see Fig. 1 ) illustrates the literature selection process for this systematic review.

As newly emergent and resurgent infections have progressively become a significant threat to the global community, a more systematic approach is needed to respond to these challenges [12] . Web-based reporting and surveillance systems first originated to strengthen global capacity for disease surveillance [20] . The forerunner was the Program for Monitoring Emerging Diseases (ProMED-Mail), which was established in 1994 under the auspices of the Federation of American Scientists, with the aim of rapidly disseminating disease-related information to a wide audience and allowing for informed discussion in real-time. However, it has been operated by the International Society for Infectious Diseases since 1999 [21] . Subsequently, the World Health Organization (WHO) established an effectively organized infrastructure called the Global Outbreak Alert Response Network (GOARN) for the very first time, which served as a ""network of technical partners and other networks with the capacity and expertise to contribute to an international coordinated response to outbreaks of epidemic-prone and novel infectious diseases"" [22] .

Following the information revolution and the rise of web 2.0, active and frequent use of the Internet triggered the creation of more surveillance systems [5] . While earlier network-based infrastructure focused on news reports as the primary data source, recently created surveillance systems use various sources for early warning systems, developed in several countries, which include query data from online search engines and social media such as Twitter [23] . Moreover, some Internet-based surveillance systems have been selected to be part of a national security system and are managed at the national level. Such a phenomenon is most often apparent in developed countries, as in the United States and Sweden. CDC funds feasible and effective surveillance systems to enhance the technical aspect, and the Generating Epidemiological Trends from Web Logs, Like (GET WELL) system has been officially accepted by the Swedish government and is in regular use at the Swedish Institute for Infectious Disease Control, providing a complementary aid to the daily surveillance performed by epidemiologists [24] . Over the last decade, these systems have progressed dramatically, as evidenced by the transformation in data collection and dissemination (Fig. 2) .

Web-based surveillance systems have been developed to monitor news reports and to rapidly spread information on disease outbreaks with the aim of detecting an infectious disease at the onset of the outbreak. Figure 3 shows the classification of standard disease surveillance systems. Event-based surveillance systems are based on the organized, rapid capture, and reporting of information about outbreaks or events that can be a risk to public health [25] [26] [27] . However, rather than relying on official reports, this information is retrieved directly from witnesses of real-time events or indirectly from reports transmitted through various communication channels, such as social media, and information channels including news media and public health networks [28] . A great deal of attention from the public, and media interest, are associated with an epidemic [29, 30] . Health information monitored via the Internet and social media is a pivotal part of event-based surveillance and is most often the source emphasized by many existing surveillance systems [18] . Event-based disease surveillance systems can be classified into three main categories of news aggregators, automatic systems, and moderated systems. Moderated systems function so that information is processed by human analysts or is processed automatically before being analyzed by human analysts [31, 32] . These systems screen for epidemiological relevance of the data extracted within the information prior to being presented to the user [26] . Examples of this system include ProMED-Mail, GPHIN, GOARN, and Bio-Caster. The process by which automatic systems collect data is complex; it adds a series of steps for analysis, but differs in the levels of analysis performed as well as in the scope of information sources, language coverage, speed of delivery, and visualization methods. EpiSPIDER, Health-Map, EpiSimS, MedISys, and GETWELL are examples of automatic systems [33] [34] [35] . Finally, news aggregators include Google Flu Trends, which collect reports and articles from sources screened by language or country; by such means users can easily access many sources via a common portal but they are required to view each article individually [26] .

Most new surveillance systems have been applied worldwide, as seen through the structured table of the systems The systematic process of data collection and analysis to detect and characterize disease outbreaks in humans and animals in a timely manner [10, 11] .

The process of gathering, integrating, interpreting, and communicating essential information that might be related to activity and threats to human, animal, or plant health [12] . Biosurveillance activities range from standard epidemiological practices to advanced technological systems, utilizing complex algorithms [13] . The focus is on the use of early disease indicators to identify outbreaks before definitive diagnoses are made [14] .

Information epidemiology; the science of the distribution and determinants of information in an electronic medium, specifically the Internet, or in a population, with the ultimate aim to inform public health and public policy [15, 16] .

Infoveillance Information surveillance; longitudinal tracking of infodemiology metrics for surveillance and trend analyses [16] [17] [18] .

Digital surveillance Attempts to provide knowledge of public health issues by analyzing health information stored digitally, as well as the distribution and patterns governing access to these data [18] .

Encompasses alerting public healthcare practitioners during the early phases of an outbreak, enabling them to promptly institute control measures and case finding and to ensure adequate access to treatment, thereby reducing morbidity and mortality [19] .

categorized according to the origin, area of service, language coverage, data source, data access, user interface and format and arte offered in different languages, except EpiSimS and GET WELL [36] . BioCaster, EpiSPI-DER, and HealthMap are disseminated on a geographical map. MedISys and ProMED-mail are disseminated through websites or news aggregators in the public, whereas GOARN and GPHIN are disseminated through a secured or restricted portal accessed by entities with monitoring responsibility, who respond to and mitigate emerging public disease threats [37, 38] . Influenzanet is a unique system, as it obtains data directly from the population; this participatory system monitors the activity of influenza-like illnesses in realtime with the aid of volunteers with certain symptoms and via internet questionnaires comprised of various medical, geographic, and behavioral questions [39, 40] . Table 2 below summarizes the various characteristics of 11 of the most often used and/or recognized web-based surveillance systems.

The development of and access to telecommunications, media, and the Internet marked the starting point for Restricted/ Subscription [22] Boolean and metadata query system [35] Website/e-mail alert [ An unprecedented increase in the number of Internet users was observed during this period, followed by growth of social network services and the introduction of big data. These changes were sufficient to spark integration between the ICT and public health issues, leading to the rise of web-based disease surveillance systems. The first systems were regarded as pilot trials at the exploratory level, and were often based at, or in cooperation with, universities or institutions (BioCaster, HealthMap, and GETWELL), non-governmental organizations (GOARN, MedISys, and ProMED-Mail) and a few governmental agencies (EpiSPIDER and GPHIN). Since the initiation of these web-based surveillance systems as trial programs, many have evolved and become renowned over the past few years. Several general trends are observed among the characteristics of the 11 web-based surveillance systems. Most of the web-based surveillance systems were first developed in North America, particularly the United States, with abundant infrastructure and technological resources, when integration of ICT and syndromic surveillance for early detection and response to diseases was at a preliminary phase. As time progressed, other regions, such as Asia and Europe, have caught up by launching similar but distinct web-based surveillance systems, spreading the notion of early detection of disease outbreaks by real-time scanning and collecting, and analyzing unstructured information from diverse internet sources [62] . English was the only language in service in the earlier systems but, subsequently, the collection and analysis of data began to be published in different languages based on the service area. The scope of data sources has also expanded as newer surveillance systems extract information not just from secondary news reports but also from social media, web search queries, and various organizations such as the CDC, Central Intelligence Agency, and the WHO.

The terminology has changed among the many elements of the web-based surveillance systems that have evolved and become sophisticated. The fusion of epidemiologic intelligence and ICT has produced newly coined terms that describe the core functions and characteristics of web-based surveillance systems. This new terminology is essential for depicting the underlying importance of digital technology as a public health tool. Future webbased surveillance systems will produce additional new terms to highlight the collaborative characteristics of these systems.

The best recognized use of novel technologies and health surveillance data together is that of estimating the range and magnitude of health problems in a community to rapidly detect the outbreak of an epidemic at its onset [63] . It is evident that web-based surveillance systems have huge potential to enhance traditional systems, as opposed to merely being an alternative, as they have added benefits and capacities, such as a large quantity of relevant data, increased accessibility, and timeliness [63, 64] .

Strengths and future challenges of newly emerging surveillance systems

Internet-based systems are intuitive, adaptable, inexpensive to maintain, and operate in real time [3] . Advanced computational capabilities involving Internet searches enable automated and rapid collection of large volumes of data, referred to as ""big data"", and provide the public with ""real-time"" detection and improved early notification of localized outbreaks [65] . In addition, a system based on web queries can easily be applied to various infectious diseases, as the underlying mechanisms are very similar [66] .

Some groups, such as the WHO, CDC, and other governmental and multi-lateral bodies, have begun to recognize the added value of these tools through the use of technologies, such as HealthMap and other new initiatives [52, 67] ; such acceptance serves as a valuable lesson for developing countries shaping the future of their public health systems. Developing countries that are particularly prone to the spread of infectious disease should seek ways to emulate the strengths of existing web-based surveillance systems and broaden the group of users directly accessing and utilizing such systems [68] .

However, the new Internet-based surveillance systems are not without limitations, thereby provoking skepticism. First, due to the unstructured nature of the data sources, interpreting the information may require highly complex techniques to effectively implement the system initially [69] . The recent closure of Google Flu Trend was partially due to its failure to provide a swift and accurate account of flu outbreaks [70] . Although the quantity of information was thought to be reliable for monitoring and predicting the occurrence of a flu outbreak, the lack of methodological transparency for data extraction, processing, and analysis led to inaccurate prediction in detecting an influenza outbreak [71] . Second, Internet use and health-seeking behavior vary among individuals, and between different sectors of the community and environment. Thus, the limited environments in which these tools are useful must be considered along with the demographics of the population [72] . Large discrepancies occur between availability of the Internet and active seeking of healthcare information that account for unequal use and access [73, 74] . Third, data sharing permits more and better quality data to be used to monitor public health and potential outbreaks [75] . However, use of data with precise information connected to individuals could be a privacy concern. Careful and appropriate decisions need to be made to avoid any further privacy intrusion on personal information. Last, forecasting health and disease-related phenomena is very likely to provoke accuracy issues because health fluctuates in every individual, and how people perceive their health status is very subjective. Although monitoring trends in disease outbreaks and health outcomes is possible, forecasting them is subject to false predictions. Thus, data sources must be evaluated extensively, particularly to identify gaps in coverage and false decisions [76] . The expectation now is that the accuracy of these systems will be enhanced through iterative procedures and that the scope of search-term surveillance will be more inclusive to other diseases [69] . The precedent of the Google Flu Trend failure illustrates the importance of a balance between traditional data and big data to maintain these systems. It is probable that future challenges will remain with regard to data integration, compatibility issues, and evaluating surveillance systems, all of which are underdeveloped and lacking in the current research. More research addressing these issues will be necessary.

Two major elements should be thoroughly considered when implementing a prospective web-based surveillance system. First, one of the potential problems in countries with a high Internet penetration rate is that many people share their personal experiences, perceptions, and distinct individual health conditions via social media, which may not always be a true reflection of the occurrence of a disease activity or an epidemic [3] . In other words, selfreporting and media-driven actions may be a chief confounder of web surveillance systems [3] . Thus, relying solely on data based on lay people's web queries and post frequency must take into consideration possible inaccurate interpretations.

The majority of the existing web-based surveillance systems work on the premise that disease incidence correlates with the frequency of information-seeking using specific terms [3] , which are query data most often analyzed in English. The primary language used to operate these web-based surveillance systems is also English, which limits the frequency of use and monitoring among many people worldwide, and can cause a compatibility problem if the same platforms are used in non-English speaking countries. Repercussions of the language barrier issue will likely affect the accuracy of detecting an outbreak. Several language-related intricacies, including cultural tone, language shifts, and the use of colloquialisms [3] are factors that cannot be easily recognized by technical aspects of web-based surveillance systems as opposed to traditional, conventional surveillance systems maneuvered by human analysts. This is another reason why data accuracy might be heavily affected and constitutes an area for improvement.

Traditional disease surveillance systems are feebly structured but at the same time require high management costs and excessively complex network operation. The most challenging task will be to implement a standardized web-based surveillance system that can be accessed and utilized universally and efficiently at low cost. In high-income, developed countries where the Internet penetration rate is high, the ""real-time"" feature of these web-based surveillance systems will overcome the limitations of traditional systems with regard to the speed of response and data dissemination. As well, the immediate effect of these systems in developing countries that lack technologies and an efficient public health system will be powerful and innovative. The introduction and amplification of these web-based systems in public health will remedy the shortcomings of traditional systems. Ultimately, the aim is to safely prevent the spread of an infectious disease at early onset by placing timeliness as the utmost priority, so that health consequences of a disease outbreak will be reduced significantly.

This review has several limitations despite employing a systematic review approach and aiming at providing a well-structured overview of web-based infectious disease surveillance systems. Due to limited article accessibility, the literature search was restricted to published articles from a limited number of selected sources. However, as a consequence, we cannot rule out a certain selection and reporting bias in our review. Nevertheless, the here reported work may serve as a good overview and starting point for readers interested in web-based infectious disease surveillance systems. Our hope is that future efforts will further complement and advance our work and provide a continuously updated, more comprehensive and at the same time more detailed picture of the currently existing web-based infectious disease surveillance systems.

Despite being in a nascent stage, with much modification needed, web-based surveillance systems demonstrate the capacity to complement national traditional surveillance systems [61] . However, the failure of Google Flu Trends shows that continued effort at the national level is required to develop more elaborate web-based surveillance systems. The aim of the present study was to systematically review a compilation of web-based infectious disease surveillance systems to provide the necessary groundwork for developing prospective surveillance systems. Future studies should be diversified and intensified, and involve an expanded scope of research, integration of a wider range of data sources, and the application of advanced methodologies.

",0.7494851549537191
Digital disease detection: A systematic review of event-based internet biosurveillance systems,"Background: Internet access and usage has changed how people seek and report health information. Meanwhile,infectious diseases continue to threaten humanity. The analysis of Big Data, or vast digital data, presents an opportunity to improve disease surveillance and epidemic intelligence. Epidemic intelligence contains two components: indicator based and event-based. A relatively new surveillance type has emerged called event-based Internet biosurveillance systems. These systems use information on events impacting health from Internet sources, such as social media or news aggregates. These systems circumvent the limitations of traditional reporting systems by being inexpensive, transparent, and flexible. Yet, innovations and the functionality of these systems can change rapidly. Aim: To update the current state of knowledge on event-based Internet biosurveillance systems by identifying all systems, including current functionality, with hopes to aid decision makers with whether to incorporate new methods into comprehensive programmes of surveillance. Methods: A systematic review was performed through PubMed, Scopus, and Google Scholar databases, while also including grey literature and other publication types. Results: 50 event-based Internet systems were identified, including an extraction of 15 attributes for each system, described in 99 articles. Each system uses different innovative technology and data sources to gather data, process, and disseminate data to detect infectious disease outbreaks. Conclusions: The review emphasises the importance of using both formal and informal sources for timely and accurate infectious disease outbreak surveillance, cataloguing all event-based Internet biosurveillance systems. By doing so, future researchers will be able to use this review as a library for referencing systems, with hopes of learning, building, and expanding Internet-based surveillance systems. Eventbased Internet biosurveillance should act as an extension of traditional systems, to be utilised as an additional, supplemental data source to have a more comprehensive estimate of disease burden.","The large-scale spread of infectious diseases has a significant impact on individuals and society [1] . Data systems are important to producing an efficient approach to prevent, detect, respond to, and manage infectious disease outbreaks of plants, animals, and humans [3, 4] . Due to the nature of epidemics, there is a vital need for timely data collection and processing [5] . Large quantities of infectious disease data are continuously compiled and analysed by various laboratories, health providers, and government agencies at local, national, and international levels with increasing complexity [5] . In practice, infectious disease data collection and analysis is E-mail address: jesse.oshea@yale.edu complicated and encompasses a multi-stage process with several stakeholders across many organisational boundaries [5] .

The main objective of infectious disease surveillance is to identify changes in incidence, either in the form of an acute outbreak or a change in long-term trends [6] . Epidemic intelligence includes all activities related to prompt identification of potential health hazards and their verification, assessment and investigation to enable public health control recommendations [7] . Epidemic intelligence incorporates two components: an indicator-based component and an event-based component [7] . The indicator-based component refers to structured (or formal) data collected through routine surveillance systems, such as the number or rates of cases based on standard case definitions, and the computation of indicators upon which abnormal disease patterns to investigate are detected [7, 8] . The goal of indicator-based surveillance is to find increased numbers or clusters at a specific time, period, and/or location that may indicate a threat [9] . Statistical methods set against thresholds of increased cases or clusters are essential to determining potential health effects [9] . In some cases, non-specific syndromes are monitored as markers for specific diseases, termed 'syndromic surveillance'.

Traditional indicator-based surveillance systems are based on the obligatory reporting of certain diagnosed diseases to a central health agency. Most of these systems depend on data from physician visits and laboratory confirmations, which can be costly and require a formal public health structure (see Fig. 1 ) [10] . Though this data is typically very accurate; data gathering can be slow [10, 11] . Substantial lags, sometimes weeks or months, between an event and its notification are common; a consequence of late or failed reporting and the hierarchical structure of these systems [11] .

The 'event-based component' of epidemic intelligence refers to unstructured data gathered and collected from sources of intelligence of virtually any nature [12] . Thereby, the detection of public health events is based on the capture of ad-hoc unstructured reports issued by formal or informal sources [12] . Rather than relying on formal official sources, information is received directly from the witnesses of real-time events or indirectly from reports sent through different communication channels, such as social media or established alert systems, and information channels, such as news, public health networks, and nongovernmental organisations [12] .

The availability of health-related information on the Internet has changed how people seek information about health [11] . The United States alone generates eight million search queries for health-related information daily [11] . The analysis of digital data, such as Google search queries, has been used to monitor communicable and non-communicable diseases, as well as mental health, illegal drug use, health policy impact, and behaviours with potential health implications [11] . In addition, social media is an easily approachable, highly cost-effective and interoperable system, that provides real-time online data with high geographical resolution that can be systematically mined, aggregated and analysed to inform public health agents [13] .

Event-based Internet surveillance, also known as digital surveillance, could improve both the sensitivity and timeliness of detection of health events [8] . Event-based Internet biosurveillance systems are systems that use information on events impacting human health or the economy from Internet sources, instantaneously incorporating diverse streams of data [14] . They contain the event-based component described above, but with the Internet as the means for data sources, processes, and dissemination and analysis of health information stored digitally. The data reported is aggregated and visualised in real-time, which enables immediate feedback to the users, the public, and officials [15] . Several surveillance systems use non-structured, event-based, digital data, such as the Global Public Health Intelligence Health Network (GPHIN), which detected severe acute respiratory syndrome (SARS) more than two months before the first publications by the World Health Organization (WHO) [11] .

Event-based surveillance systems either can broadly be categorised as news aggregators, automatic systems, or moderated systems [9] . News aggregates collect articles and news from sources, aggregates them, either by location or topic, and filters them. The result is provided as an Rich Site Summary (RSS) feed. Automatic systems further advance systems like news aggregators by adding a series of steps of analysis [9] . Non-moderated systems can search the web and display new articles without time delay in an unbiased manner than moderated systems [16] . Often, the data is not structured or collated, and therefore epidemiologists must spend more time and energy determining their relevance to a specific situation of interest [9] . Moderated systems rely on some component of human input or analysis, either solely, or after data is first processed automatically [9] . Moderated systems offer a chance of screening for relevance by public health practitioners before it is disseminated [9] . Thereby, moderated systems might show less irrelevant news items, fewer false positives than the non-moderated systems, but are subject to moderator bias [16] . Systems without human moderation often focus on data sources that already have been validated and, therefore, do not help aid those interested in the early warning and alert potential for unknown or new outbreaks or diseases [9] .

Until now, a comprehensive systematic literature review on event-based Internet biosurveillance surveillance systems, including unevaluated systems, participatory surveillance systems, social media, and mobile applications, has not been completed. A summary of previous studies can be found in Table 1 . Therefore, the purpose of this study is to systematically review and update the current state of knowledge on event-based Internet biosurveillance systems by identifying all of these systems, including current functionality, with hopes to aid decision makers with whether to incorporate new methods into their existing surveillance programmes and provide researchers a catalogue of systems.

A preliminary literature review was conducted through Google Scholar with general topic terms such as 'digital disease' and 'detection' and 'surveillance' and by author to identify scope, previous work, and to aid with keyword generation. Keyword generation was adapted from previous systematic reviews with the addition of supplementary terms from preliminary search results. A list of keywords can be found in Appendix A (in Supplementary material). A literature search was conducted in July 2015 by the author through the databases PUBMED; SCOPUS; and Google Scholar from May 2011 to July 2015. The search was a combination of all the key terms generated by using Boolean functions of 'and' and 'or.' Grey literature results relevant to event-based Internet biosurveillance systems; including mobile applications and participatory systems; was perfomed via Google and Google Scholar (not time restricted); which also included patents. Finally; when a system was mentioned in any of the above capacities; a general Google query with the system's name; or creator of the system; and the first 1000 entries were analysed by relevance in order to further discover details and functionality of systems.

The electronic databases were selected based on their relevance to the subject matter and previous reviews. Reference lists from the selected literature were examined to include articles of relevance. Titles were screened first, then abstracts, based on inclusion and exclusion criteria. Questionable eligibility based on title and abstract would then be read in full and judged for eligibility. Duplicates were eliminated and irrelevant articles were excluded from the review. Process for the article selection and screening can be found in the flowchart, Fig. 2 , which is similar to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram methodology. Extraction criteria were used to collect comparable data on each system. For eligibility, inclusion and exclusion criteria were created. These can be found in Appendix A (in Supplementary material) and were inspired and adapted from Velasco et al. [9, 17] . The publication type included journal articles in addition to conference abstracts and presentations, letters to the editor, commentaries, and patents. The articles were then categorised for classification. These categories are 1) 'background' (which are articles not directly describing an event-based surveillance system, but rather surveillance sys- Velesco et al . [9] 1990-early 2011 To review event-based surveillance systems to 2011 and uncover which have been evaluated and which are being utilised by national surveillance systems. [21] 13 event-based systems were identified, and 10 out of 13 evaluated. [21] Gajewski et al. [14] 1994-2012 To systemically assess electronic event-based biosurveillance system evaluations to identify uncertainties about current systems and guide development to exploit web-based information. 30

Identified 11 electronic event-based biosurveillance systems that have been evaluated. 30 tems in general) or 2) 'system' (articles describing at least one event-based surveillance system). Those placed in 'system' were further distinguished by either 'indicator-based systems' or 'eventbased systems'. 'Event-based systems' was also further subdivided if it was a participatory surveillance system. Participatory surveillance systems enable the public to report directly on diseases through the Internet, such as through crowdsourcing [15] . These systems encourage the regular, voluntary submission of syndromic, health-related information by the general public using technology like computers, tablets, or smartphones [15] . 

The systematic review, covering over 28,830 articles, discovered 50 event-based Internet systems, including subsystems, described in 99 articles (see Fig. 2 for the flowchart of these results and methodology). Thirty-two of the 50 systems have been evaluated in the literature in some capacity. Table 2 includes a list of all identified systems.

The data extractions of individual systems and analyses that are more detailed can be found in Appendix B (in Supplementary mate- rial), which should serve as a database for other researchers in this field for future studies.

The systematic review found no systems that can only be classified as news aggregators, although most utilise them, such as HealthMap. Of the 50 systems, nine (38%) automatic systems and 31 (62%) moderated systems.

The systematic review revealed four types of coordinating organisations: 1) university based or in collaboration with a university (n = 24, 48%) 2) government agency (n = 15, 30%) 3) NGO or non-profit based (n = 3, 6%) and 4) private corporation based (n = 8, 16%).

Of all the systems, 13 (26%) were not currently online, meaning likely no longer in existence, and 37 (74%) are fully functioning and online. Google Trends is no longer publishing their system's findings, but rather going to provide the data to a select number of systems that utilise their data. It is unclear if this will affect systems that utilise Google Trends' estimates.

Each of the systems has a different purpose and aim, but three overarching themes can be illuminated: 1) to improve and enhance early detection (n = 45, 90%) 2) to improve communication or collaboration between actors, users, and parties involved (n = 3, 6%) 3) to supplement other existing systems (n = 2, 4%). EpiSPI-DER supplements the Program for Monitoring Emerging Diseases (ProMED-mail) and GeniDB supplements BioCaster. Many systems are a blend of these three themes.

Eight (16%) of the systems can be classified as prototypes (experimental or beta), whereas 42 (84%) can be classified as fully developed.

The geographic scope of the system varies. A system could operate and cover one particular area of a nation, the entire nation, continent, or the globe. Further, coverage may be restricted or confined to a particular region or state (n = 28, 56%). Of the total systems, 22 (44%) monitor internationally. The following countries had systems based in the country: United States (n = 23, 46%), European Union (n = 16, 32%), Canada (n = 3, 6%), Japan (n = 2, 4%). 6 (12%) are based elsewhere, such as Australia, Brazil, Singapore, Mexico, and Thailand.

Thirty four (68%) systems collect or disseminate their data only in one language, with the majority being English (n = 21), whereas 16 (32%) are multilingual-with the highest being Medical Information System (MedISys) with 43 languages.

Of the 20 (40%) that focus only on one disease type, 17 of which monitor influenza-like-illness. Thirty (60%) focus on multiple infectious diseases and types, with the highest being HealthMap with over 170 disease types.

Access levels vary from system to system, partly because of the scope of the system and the intended audience. Systems are freely available to the public (n = 31, 62%), paid subscriptions (n = 1, 2%), free subscriptions (n = 5, 10%), and restricted access (n = 12, 24%), either to the coordinating entity or denied to the outsiders of the intended jurisdiction such as the European Union within a closed network. MedISys offers multiple access levels, freely available and restricted.

While it is important to offer freely accessible information, some sensitive information (personal data or other confidential data) is often filtered in specific ways among public health officials with specific restricted access. GPHIN has restricted access for organisations with an established public health mandate, with access varying according to factors like the organisation's size and number of users [9] .

Data collection and acquisition is different for each system. However, general categories exist: 1) systems that collect information directly from RSS feeds or mailing lists (n = 13), 2) those that mine social media (n = 13), 3) those that mine search engine queries (n = 3), 4) those that collect data from both formal and informal sources (n = 4), and 5) those that are crowdsourced (or participatory), where data is collected or submitted by users (n = 19). Many systems are combinations of the above, however most systems incorporate the use of RSS feeds and news aggregators. Data processing is either automated or moderated, as divided in the system category section.

For example, MedISys monitors at least 50,000 articles per day, mostly through RSS feeds. These types of systems all use text-mining technology to extract relevant data and most have sophisticated algorithms for processing, filtering, and classifying relevant disease information [9] . The text extraction process utilises document heuristics, an experience-based method for machine learning that is applied to the information to enable an intelligent, and more accurate, decision about its relevance [9] . Monitoring is improved over time, as the heuristics learn as their output is confirmed against a set threshold for the epidemiological attributes of extracted health events [9] . Prior to returning the extracted information, the system aggregates the extracted events into outbreaks, across several documents and sources [9] .

Some systems, such as HealthMap, relieve noise by integrating data from an assortment of online sources that have been moderated already (see Appendix B (in Supplementary material)) [9] . HealthMap's informal source feeds vary, such as using ProMed-Mail and official-validated outbreak RSS feeds and alerts. These sources feed into a classification engine, such as a parser, which utilises the information to generate disease and location output codes [9] . Systems like HealthMap then filter the articles into a category and then store them in a database [9] .

Systems may disseminate data by four identified trends: 1) via a display onto a map interface or as a time series graph (n = 32, 64%) 2) through a secured or restricted portal (n = 8, 16%) 3) to a website or newsgroup (n = 8, 16%) and 4) as a dashboard, where the dashboard is an interactive multi-tooled system, sometimes with plug-and-play features added (n = 2, 4%). HealthMap is an example of data results being illustrated through a map interface.

The review emphasises the importance of utilising both formal and informal sources for timely and accurate infectious disease outbreak surveillance, cataloguing all event-based Internet biosurveillance systems. The review covers 50 systems, ten of those may be classified as subsystems with similar characteristics, or part of a larger umbrella organisation, with a detailed analysis of 15 attributes of each system.

North America and Europe are leading in event-based Internet biosurveillance systems, whereas Africa, Asia, Australia, and South America possess little or no such systems to monitor their epidemic threats. There lies an opportunity for these systems in varying income countries to aid with emerging diseases in vulnerable countries or encompass people who may not have access to healthcare systems.

There is a rise in the private sector as the creator or coordinator for the system. Perhaps there is profit making potential in this field that has yet to be seen, or perhaps technology can excel more quickly when left with reduced bureaucracy. Coincidentally, a large amount of systems are not currently online or ceased to function. Various reasons can be postulated, such as funding issues, lack of manpower, little utilisation or lack of results.

Event-based Internet biosurveillance is a recognised effective approach for infectious disease detection. Inevitably, the number of online data sources will increase, and as it does, event-based Internet biosurveillance will increasingly become more important in infectious disease surveillance. These systems are attractive from a logistic, economic, and epidemiologic viewpoint [19] . They are intuitive, flexible, function close to real-time, and many are freely available [19] . Once they are established, they are relatively inexpensive to run and sustain [19] .

Many of the systems allow citizens to report public health events via social media platforms or electronic communication channels independently of governments [14] . Therefore, these systems do not rely on the formal healthcare system or hierarchal organisation structures to provide, analyse, or disseminate data, or to advise the international community of emerging infectious disease concerns. Governments are no longer in sole control of their public health information, making it substantially harder to hide or delay outbreak or event reports [14, 20] . However, the same aspects of event-based Internet biosurveillance systems that make them an important new surveillance tool also may makes them a less reliable tool [14] . Since public health professionals do not verify some sources of data, these systems are prone to noise and false alarms [14, 21] . Further, there is difficulty differentiating signal from noise. Sometimes, they lack specificity and results from these systems differ from official sources [14, 21] .

Social media data, and other online data, are often closed to the public or agencies, but companies may pay large amounts of money to acquire user information [22] . To further facilitate meaningful data mining in social networking, more open-source data or increased data sharing is required [22] . In addition, regulations are needed in social media as a data source, and all online data sources, to ensure good governance of the data and that individuals' privacy is not violated.

More synergy among systems is needed. Systems often operate in silos and may also compete with one another, especially those that are run by a private corporation. Now that the systems exist, ways to complement one another should be researched, such as integrating indicator-based systems to these new digital systems to enhance accuracy and surveillance, thereby creating a hybrid disease surveillance system.

The systematic review has several limitations. The scope of the study excluded bioterrorism and animal related systems, so potential relevant systems may have been neglected or not included. Data on some systems was not publicly available, due to restricted access, thus complete information or key factors may be missing from the attribute list. Further, there is a possibility of systems existing outside of academic literature or search engine realms, such as those developed by military or closed environments. Additionally, there is inconsistent application of the term 'surveillance' here and throughout the literature. Some system information provided suggests that the system is a monitoring system rather than a surveillance system, and many articles did not contain enough information to correctly distinguish between the two, reflecting a lack of overall surveillance theory in public health. Further, as this review was limited to articles in English, the results may be biased towards including systems from English-speaking areas.

Ongoing evaluation, validation and verification of event-based Internet biosurveillance systems with epidemiological and clinical data by users, developers, and agencies will greatly increase the robustness of these systems for infectious disease detection and monitoring [23] . Yet, the willingness to integrate these systems into public health surveillance programmes is rooted in the effectiveness studies and evaluations, but such effectiveness evaluation studies can only be proved through integration-a circular dilemma [9] . Big Data has the chance to revolutionise infectious disease outbreak detection and management. If we, as a society, are going to allow this to occur, we must collaborate as academics, health professionals, and civil society, to achieve it.

None.

All authors have made substantial contributions to all of the following: (1) the conception and design of the study, or acquisition of data, or analysis and interpretation of data, (2) drafting the article or revising it critically for important intellectual content, (3) final approval of the version to be submitted.

What was known before this study?

• Innovative event-based Internet biosurveillance systems are being utilised, assessed, and popularized with varying sources of data. • Previous reviews focused primarily on systems that have been evaluated in the literature and did not include new surveillance systems such as participatory systems.

What did this study add to our body of knowledge?

• The current review contributes with an extensive overview of all event-based Internet biosurveillance systems, including those that use social media, participatory surveillance, mobile applications, and those which have not been evaluated yet. • Results yielded the largest catalogue of systems, with 15 attributes described for each system, and each using different technology and data sources to gather data, process, and disseminate data to detect infectious disease outbreaks. • The review points out several areas in need of more research such as ongoing evaluation, validation, and verification of systems and meaningful data mining and data sharing to facilitate synergy.

",0.7494474790586406
New Technologies for Monitoring Marine Mammal Health,"Omics for Marine Mammal Health Chapter | 11 293 genes. The sequencing of the entire DNA of an organism is called a whole genome sequence (WGS). A WGS project involves the sequencing of DNA, the assembly of that sequence to create a representation of the original chromosome, and the annotation and analysis of that representation, with an emphasis on significance and function. It aims at the collective characterization of coding and noncoding DNA regions, their structure, function, and evolution. It reveals the complete DNA makeup of an organism, enabling the better understanding of the variations both within and between species, allowing an accurate differentiation between organisms. Comparisons of genome sequences from different individuals have led to the establishment of copy number variation with over 10 million single-nucleotide polymorphisms (SNP). SNPs can provide a genetic fingerprint for use in identity testing and are found to be involved in the etiology of many diseases. Several studies have demonstrated the diagnostic utility of WGS, for example, in mutation detection (Lupski et al., 2010; Herdewyn et al., 2012; Bae et al., 2014) or in the identification of clinically relevant variants in ∼40% of pediatric populations with autism (Weedon et al., 2014) and ∼60% of those with intellectual disability (Gilissen et al., 2014).","Traditionally, biology was a descriptive discipline, and that is how the biology of marine mammals has also been for a long time. In his work Historia Animalium, Aristotle was the first to make many pertinent observations about dolphins, including the fact that they would bear their young alive, suckle them, breathe air, and communicate by underwater sounds. From his descriptions and writings, we know that in the 4th century BC there were dolphins in the Mediterranean and porpoises in the Black Sea. Accurate natural history observation on the biology of marine mammals expanded throughout the time associated to morphologic descriptions in the pre-1900s and followed by description of behavior and distribution during the times of hunting and whaling activities. Studies of life history patterns, habitat use, and behavior in captivity or in nature started only in the second half of the last century, slowly combining aspects of mammalogy, ethology, ecology, conservation, evolutionary biology, and, finally, molecular biology.

Molecular biology is the field of biology that studies the composition, structure, and interactions of the most important macromolecules for each living organism, such as nucleic acids (DNA and RNA) and proteins that carry out the biologic processes essential for each cell's functions and maintenance. The DNA contains the genes that determine how the individual organism will be. The therapeutic control of a medical condition is generated from the basic concept that genes make proteins that catalyze a biochemical reaction and control the phenotype of the organism. Thus, the understanding of DNA and the identification of gene and protein defects responsible for a specific disease is very important in maintaining the health of an organism.

The method of dissecting biologic systems into their constituent parts, known as reductionist method, has been very useful in explaining the chemical basis of numerous living processes and has been largely responsible for the amazing progress seen in biology during the past five decades or so. However, biologic systems are extremely complex and have emergent properties that cannot be explained, or even predicted, by studying a gene at the time. New experimental techniques for investigating the unique complexity of biologic systems that results from the diversity of interactions and regulatory networks were, all of a sudden, a necessity.

Recent developments in high-throughput nanotechnologies and bioinformatics have enabled the examination of biologic systems in remarkable detail, providing the data that molecular biologists need to simulate the behavior of complex biologic networks and systems. We can now monitor thousands of molecules simultaneously and generate real-time pictures of any biologic system in any condition. The opportunity to assess the expression of hundreds to thousands of genes, proteins, or metabolites simultaneously has been made possible by the development of the ""omics"" technologies.

Omics technologies aim at understanding a complex system, considered a whole. Therefore, in a specific biologic sample, the universal detection of genes is called genomics, of the totality of the mRNA is called transcriptomics, the study of the entire set of peptides and proteins is called proteomics, and that of all the intermediate products of metabolism is called metabolomics. Taking advantages of the new technologies, many other applications can be added to the list of these major omics technologies, generated by the necessity of deepening information on a specific mechanism or/and at a specific level.

The integration of all the omics technologies is characterized by the generation of an enormous amount of data that can be interpreted only by the involvement of many scientific disciplines (e.g., biology, computer science, engineering, bioinformatics, physics) . The result aims at deciphering systems in their complexity, at predicting how they can change over time and conditions, and at proposing solutions to health and environmental issues ( Fig. 11 .1).

Genomics is an area that concerns the sequencing and analysis of an organism's genome. The cost of genome sequencing has gone down drastically from the 3 billion dollars used for the first human genome sequencing in the beginning of this century to about 3 thousand dollars in the year 2016 (and probably the half of that by the end of 2017), resulting in the establishment of the size and sequence of the genome of over a thousand species of organisms. Such knowledge of genome sequence has been useful in many ways. For example, we know that the minimum number of genes to sustain life by a bacterium is only 260. It also led to the surprising understanding that humans carry only 20,000 genes, against the previous guess that humans may have up to 100,000

Transcriptomics is the study of the transcriptome: the complete set of transcripts in a cell and their quantity in a specific developmental stage or physiologic condition. Transcriptomics studies RNA in any of its forms: mRNA, rRNA, tRNA, and other noncoding RNA produced in one or a population of cells. The term can be applied to the total set of transcripts in a given organism, or to the specific subset in a particular tissue. Currently, there are two key techniques used for transcriptomic analysis: microarrays, which quantify a set of predetermined sequences, and RNA sequencing, which uses high-throughput sequencing to capture all sequences.

Differently from the genome, the transcriptome can vary with external environmental conditions. Generally transcriptomic studies are referred to expression profiling studies, examining the expression level of mRNAs in a given cell population. Transcriptome analysis allows us to understand the expression of a genome at the transcription level, which provides information on gene structure, regulation of gene expression, gene product functions, and genome dynamics. Understanding the transcriptome is essential for interpreting the functional elements of the genome and revealing the molecular constituents of cells and tissues, and also for understanding development and disease. Transcriptome analysis can be carried out at the resolution of single cells, a powerful strategy to connect gene expression networks, cell lineage, and phenotype of individual cells and to study complex disease such as cancer, as well as other biologic phenomena such as tissue regeneration, embryonic development, and immune response (Liu and Trapnell, 2016; Kanter and Kalisky, 2015) . Comparison of transcriptomes allows the analysis of interspecies differences (Shay et al., 2013) , the identification of genes that are differentially expressed in distinct cell populations, across tissues and individuals (Mele et al., 2015) , in response to an injury (Khan et al., 2017) or to different treatments (Datta et al., 2016) . Transcriptome sequencing can evaluate absolute transcript levels of sequenced and unsequenced organisms, detect novel transcripts and isoforms, and reveal sequence variations and splice variants. Affordable and fast, it is the most informative assay to start with, offering an overview of the expressed genes to guide subsequent analyses (carried through proteomics, metabolomics, and other methods).

The combination of genomics and transcriptomics is called functional genomics. Functional genomic studies aim at deciphering the connection between phenotypes and genotypes and have brought a revolution in the fields of medicine and modern biology.

Proteomics is the study of the proteome, the totality of the proteins in a cell, tissue, or organism and their identity, their biochemical properties, and functional roles. Differently from genomics and transcriptomics, propelled by advancement in sequencing technologies, proteomics has been driven by advances in mass spectrometry (MS) and other techniques that allow the analysis of a large number of protein samples at low cost (e.g., 2D gel electrophoresis). Proteomics is the study of how protein quantities, modifications, and structures change during development and in response to internal or external stimuli. The proteome of an organism is much larger and complex than its genome. Contrarily from the genome, the proteome changes constantly: DNA in organisms is essentially constant throughout their lives, while the kinds and amounts of proteins that are synthesized at any instant are subject to much variation. Proteins are continuously made, modified, and eliminated; many genes encode for more than one version of a protein, and a protein can be modified differently in response to cellular stimuli. The field of proteomics investigates which proteins are expressed at what stages in an organism's life and exactly how and why these proteins are expressed. Protein controls the structure and function of a cell by facilitating all biochemical reactions. Proteomics contributes to improve biomarker translation to modern medicine (Rifai et al., 2006; Veenstra, 2007) . Biomarkers are measurable characteristics that reflect physiologic, pharmacologic, or disease processes and can be used to screen an individual for diagnostic and therapeutic purposes. Proteomic-based approaches for biomarker investigation can be employed in different aspects of medicine, to better understand pathways affected in a disease, to identify individuals at a high risk of developing the disease, and to identify individuals who are most likely to respond to specific treatment (Guest et al., 2013) . The application goes beyond medicine: biomarkers may, in fact, be the best approach to identify an early response to contaminants (Broeg et al., 2005) and are very sensitive for identifying an organism's stress, although it is not always clear the origin of the stress, and there are often multiple stressors present (Smit et al., 2009) . Biomarker responses can be measured in organisms collected from or deployed in field sites to integrate the effects of chemical and nonchemical stressors, reducing the need for complex laboratory exposure scenarios.

The integration of proteomics with genomics and transcriptomics is called proteogenomics. Posttranscription regulation, protein half-life, and posttranslational modification are examples of what defines the proteome dynamics and cannot be deduced from data of functional genomics; consequently, proteomics is a crucial, corresponding methodology to both transcriptomics and genomics.

Metabolomics refers to the systematic identification and quantification of the metabolome of a biologic system (cell, tissue, organ, biologic fluid, or organism) at a specific point in time. Metabolome refers to low molecular weight organic molecules, intermediates, and products of metabolism, such as hormones, other signaling molecules, and secondary metabolites: like the transcriptome and the proteome, the metabolome is dynamic. Metabolomics is a powerful approach because metabolites and their concentrations directly reflect the underlying 296 SECTION | II Effects of Toxicological and Cumulative Stress biochemical activity and state of the cell, thus representing best the molecular phenotype. Mass spectrometry (MS) and nuclear magnetic resonance (NMR) spectroscopy are the techniques most often used for metabolome profiling.

One of the applications of metabolic profiling studies is in the toxicology field. Metabolomics can detect the physiologic changes caused by toxic insult of a chemical (or a mixture of chemicals) especially of urine or blood plasma samples. The observed changes can be often related to specific lesion in liver or kidney.

A wider application of metabolomics is in the characterization of the interactions of organisms with their environment (environmental metabolomics) (Samuelsson and Larsson, 2008; Viant, 2008) . A study can involve individuals to populations, and it can be related to the traditional fields of ecophysiology and ecology, and from an instantaneous effect to those over evolutionary time scales, enabling studies of genetic adaptation (Bundy et al., 2009 ). Metabolomics has a number of advantages over the other omics approaches. The metabolome is the final product of gene transcription, and therefore, changes in the metabolome are amplified relative to changes in the transcriptome and the proteome (Urbanczyk-Wochniak et al., 2003) . The metabolome contains the smallest domain (made up of approximately 5000 metabolites vs. 100,000 proteins in the proteome and 20,000 expressed genes in the genome), but it also contains the most diverse biologic molecules, which can make it more physically and chemically complex than the other ""omes.""

Different from the omics technologies listed so far describing the ""ome"" representing the totality of the principal macromolecules presents in all living organisms, epigenomic studies the modifications of the DNA that do not change its sequence, while changing the way the DNA sequence gives its instructions. The epigenome is made up of chemical compounds and proteins that can attach to DNA and can turn genes on or off, controlling the production of proteins in precise cells. These modifications are sometimes passed on from cell to cell as cells divide (mitosis) and can be passed down from one generation to the next (meiosis); hence, they are heritable. The epigenome can also change throughout a person's lifetime. Adjustments to next-generation sequencing protocols to enrich for the DNA regions carrying modifications (e.g., bisulfite treatment or antibody-based methods) are applied to the study of epigenetic processes. One type of modification is called DNA methylation, where methyl groups attached to the bases of the DNA molecule turn genes on or off. A second type is called histone modification, where chemical tags are attached to the histones (DNAwrapping proteins that enable the DNA to be wound up into chromosomes), giving information about regions in the DNA that should be used or ignored.

Usually, the modifications occur as a natural process of development and tissue differentiation, but they can be altered in response to environmental exposures or disease. Epidemiological evidence increasingly suggests that early life exposures to dietary and environmental exposures can have a profound effect on the epigenome, resulting in birth defects and diseases developed later in life (Dolinoy et al., 2007; Jirtle and Skinner, 2007) . The normal role in development and differentiation of epigenetic regulation can be misdirected, leading to a number of diseases including cancer (Jones and Baylin, 2007; Robertson, 2005) . The epigenetic alterations are more readily reversible than genetic events, offering potential for the development of therapies (Reamon-Buettner and Borlak, 2007) .

Toxicogenomics is the application of genomic technologies (genome sequence analysis, gene expression profiling, proteomics, metabolomics) to study the adverse effects of environmental stressors and toxicants on the organisms and the environment health. The application of transcriptomics, proteomics, and metabolomics enables the study of adverse effects of xenobiotic substances in relation to structure and activity of the genome to understand the role of gene-environment interactions in disease (Heijne et al., 2005) . This area promises to have a large impact on many other scientific and medical disciplines, as scientists can now generate complete descriptions of how components of biologic systems work together in response to various stresses, drugs, or toxicants (Gomase and Tagore, 2008) . Due to the rapid advent in genomics technologies and attention to ecologic risk assessment, the term ""ecotoxicogenomics"" has recently emerged to describe integration of omics technologies (i.e., transcriptomics, proteomics, metabolomics, and epigenomics) into ecotoxicologic fields. Ecotoxicogenomics is defined as study of an entire set of genes or proteins expression in nontarget organisms that is important in responses to environmental toxicant exposures, offering benefit in ecologic risk assessment (Kim et al., 2015) . While the availability of genomic information about nonmodel organisms is expanding, the application of ecotoxicogenomics to a variety of organisms becomes a powerful tool for evaluating the effects of chemicals on the entire ecosystems (Iguchi et al., 2006; Watanabe and Iguchi, 2003) .

Many marine mammal species and populations, considered to be most vulnerable to human activities, are endangered or threatened, or designated as depleted worldwide (Endangered Species Act; International Union for Conservation of Nature Species Programme). Marine mammals cannot escape the legacy of the global decline of the ocean health due to anthropogenic impacts such as overfishing, coastal habitat destruction, deep sea mining, oil and gas exploration, release of chemical contaminants, and pollutants from industrial applications (Desforges et al., 2016; Lane et al., 2015; Schwacke et al., 2012; Van Bressem et al., 2009) . Noise can also critically impact marine mammal behavior and fitness (Blair et al., 2016; Ellison et al., 2012; Peng et al., 2015) . Moreover, marine mammals are also constantly exposed to natural factors; biotoxins from harmful algae can produce mass mortalities that have devastating effects on population dynamics both in the short and long term (Van Dolah, 2000) . Among the consequences, there is an increase in reports of diseases, such as metabolic disorder, opportunistic infections, and population changes in growth, reproduction, and survival (Di Guardo et al., 2011; Gulland and Hall, 2007) . Current understanding of the long-term effects of these factors, alone and in combination, has been limited by the lack of comprehensive methodologies and by the protected status of the animals.

The potential of the application of omics technologies to marine environmental science and especially in environmental risk assessment is recognized by many researchers and official organizations (Bozinovic and Oleksiak, 2011; Kim et al., 2015; Kumar and Denslow, 2017; Veldhoen et al., 2012) . The application of the omics technologies holds promises toward a significant progress in the understanding of marine mammal health and physiology challenged by a marine environment subject to continuous changes.

The pregenomic era was characterized by the Human Genome Project (HGP), an international research effort to determine the sequence of the human genome and to identify the genes that it contains. The first draft was released in 2001 (Lander et al., 2001) , and the project was finally completed in 2003, 2.5 years ahead of time, and also significantly under budget, thanks to the advancement in sequencing technology and reduced cost. By the time the HGP began in the late 1990s, the highest estimates put the number of human protein-coding genes at 100,000, and since then the number has continued to shrink. However, at present, the final number of true protein-coding genes in the reference genome lies between 19,000 and 20,000, accounting for 1.5% of the 3 billion base pairs representing the entire genome. Of course, this information led to many unanswered questions that characterize what is presently called ""the postgenomic era."" The focus is on the discovery and explanation of all the functional elements encoded within the genome sequence, and the comparison of related genomes has emerged as a powerful instrument for genome interpretation. In an effort seeking the identification of functional elements that are conserved across mammals, a project funded by the National Human Genome Research Institute, the bottlenose dolphin has been chosen as one of 24 animals whose genome has been sequenced as part of the comparative genomic annotation (Lindblad-Toh et al., 2011) . The first marine mammal genome to be sequenced was sequenced at 2× coverage, with its first version publicly available in 2010. In a 2× coverage, each genomic base is represented in roughly two sequence reads (""2×"" redundancy), leaving many gaps in the final sequence due to statistical fluctuations in read placement, biases in preparative libraries, and difficulties leading to a low-quality assembly. The low coverage has significant effects on the subsequent analyses: the absence of a protein-coding gene, or a disruption of its open reading frame, may represent a deficiency of the assembly or may represent a real evolutionary gene loss. Moreover, low depth can introduce sequence errors that can be propagated, leading to wrong conclusions of a study (Green, 2007) . However, a low-redundancy genome, such as the 2× dolphin genome sequenced, is useful to obtain biologic information: partial sequences of most genes and other evolutionarily conserved segments average estimates of mutation rates, as well as a comprehensive assessment of interspersed repeat content including the identification of lineage-specific families. A comparison of about 10,000 protein-coding sequences from the bottlenose dolphin genome with nine other amniotes genomes documented rates of synonymous substitution in the dolphin lineage that were significantly lower than other mammals and equivalent to that of humans and elephants. The dolphin lineage exhibited evidence of positive selection of multiple genes associated with the nervous system, metabolic processes, and glycemic regulation, and others possibly linked to cetacean specializations such as deep diving, blubber, and fat storage. In addition, the dolphin lineage showed a significant increase in selection on genes expressed in the mitochondrion in comparison with other mammalian genomes (McGowen et al., 2012) . A deeper analysis using 11,838 high-quality orthologous gene alignments selected from the dolphin and four other terrestrial mammalian genomes identified genes that had undergone positive selection that are significantly enriched in the categories of lipid transport and localization, ATPase activity, sense perception of sound, and muscle contraction, all areas that are potentially related to cetacean adaptations (Nery et al., 2013; Sun et al., 2013) . The dolphin genome has been used to obtain information on the independent evolution of echolocation in bats and cetaceans (Parker et al., 2013) . More information has been obtained, increasing the coverage of the bottlenose dolphin genome; the analysis revealed that parallel substitutions are widespread in marine mammals, but also that while convergent phenotypic evolution can result from convergent molecular evolution, in the evolution process are more often used different molecular pathways to reach the same phenotypic outcome (Foote et al., 2015; Zhou et al., 2015) . Among the whales, a comparative genomic analysis identified the expansion in the whale lineage of gene families associated with stress-responsive proteins and anaerobic metabolism, whereas gene families related to body hair and sensory receptors were contracted (Yim et al., 2014) . Further insights into the genomic basis of aquatic adaptations in marine mammals that can be linked to their physiology and health may rely on functional or genomic analyses of noncoding regions, which will be soon achieved thanks to the latest release of novel genomes, first among all the bottlenose dolphin genome with 114.5× coverage. From the current annotation report, the bottlenose dolphin genome seems to contain about 17,000 protein-coding genes in 2.1 billion base pairs. The current coverage is 300 SECTION | II Effects of Toxicological and Cumulative Stress about 95% versus the 86% of the previously implemented version. The advancement in sequencing technologies, together with the reduced cost, has made possible the sequencing of genomes of several other marine mammals in the last few years. To date, the genomes of several marine mammal species have been annotated and released by the National Center for Biotechnology Information (NCBI) Eukaryotic Genome Annotation Pipeline: five belonging to the order of Cetartiodactyla (Balaenoptera acutorostrata scammoni, minke whale; Lipotes vexillifer, Yangtze River dolphin; Orcinus orca, killer whale; Physeter catodon, sperm whale; Tursiops truncatus, bottlenose dolphin), three belonging to the order of Carnivora (Leptonychotes weddellii, Weddell seal; Ursus maritimus, polar bear; Odobenus rosmarus divergens, Pacific walrus), and one belonging to the order of Sirenia (Trichechus manatus latirostris, Florida manatee).

Transcriptomics in marine mammal science is unquestionably the omics approach more represented in the literature. The reasons are several, the first being the cost of the experimental procedure: it is extremely less expensive to sequence only the small coding fraction of the genome instead of the whole with best coverage and resolution. Moreover, transcriptomic analysis does not necessarily need a reference genome. Therefore, the transcriptomic approach has been in use for many years now, refined by the advancement in sequencing technologies. Transcriptomic analyses aid discovery of novel gene functions and the connection of molecular and physiologic responses to a large scale of stimuli (ecologic, anthropogenic). The principal methods used are the gene expression microarrays and the more recent RNA-seq. With gene expression microarrays, thousands of genes are analyzed simultaneously in any given sample, thus obtaining a lot of information about the physiologic systems and the impact of environmental challenges. In 2007, the first microarray for marine mammal studies was developed. It was a species-specific cDNA microarray containing 1395 unigenes selected from targeted cloning and T and B cells cDNA libraries features (Mancia et al., 2007) . The first microarray was useful for stress response and immune function studies in wild dolphins (Mancia et al., 2008 (Mancia et al., , 2010 . The blood transcriptome reveled that (1) the dolphin immune system mechanisms have high similarity to those of humans and other terrestrial mammals; (2) the immune system of the dolphins, resident inhabitants of coastal locations of temperate waters worldwide, reflects the environmental condition in which they live; and (3) the sampling method impact on downstream analysis underlines the need for species-specific baseline data. The same microarray was applied to screen a different tissue in a study focused on the study of the vitamin D3 pathway on cell cultures from dolphin skin; the analysis showed the importance of nonclassic functions of vitamin D3, such as its role in innate immunity, similar to what has been demonstrated in other mammals (Ellis et al., 2009) .

In 2014, a much more comprehensive system, a species-specific oligo microarray, containing 24,418 unigene sequences from cDNA libraries of seven different tissues from bottlenose dolphin was generated (Mancia et al., 2014) . The microarray was used to screen wild animal blood transcriptomes and was effective in the differentiation of populations of dolphins inhabiting different geographic locations and by the effects of environmental contaminants on dolphin health (Mancia et al., 2014 . Dolphins inhabiting the coastal waters of Georgia in the United Statees, known to be heavily contaminated by Aroclor 1268 (Kucklick et al., 2011) , an uncommon polychlorinated (PCB) mixture, displayed variation in expression of genes involved in xenobiotic metabolism, development/differentiation, and oncogenic pathways . The same microarray was also applied to evaluate the skin tissue as a source of information, giving the minimal disruption inferred from the sampling methods (e.g., dart biopsy and/or stranding events). The skin was useful to evaluate the activation of an immune response to the exposure of contaminant of emerging concerns, such as bisphenol A and perfluorooctanoic acid (Lunardi et al., 2016) and also to establish baseline health parameters for investigations on contaminant exposure or health status. Gene expression was greatly impacted by season, with one-third of all the genes on the array varying between winter and summer, highlighting the need for creating a baseline for natural variability for a better investigation of the effects of a stressor (Van Dolah et al., 2015) .

Despite its ability to interrogate the expression of thousands of genes, the microarray can still lack information, due to the incorrect abundance of some of the transcripts. The most recent RNA sequencing (RNA-seq) method, in which the cDNA made from the RNA sample is directly sequenced through highthroughput DNA sequencing, provides a less biased evaluation of the transcriptome. RNA-seq analyses are very sensitive and offer the advantages of detecting all the unique sequences and of quantifying levels of RNAs expressed at a very low level. Most importantly, the reduced cost of RNA-seq provides an efficient approach to generate sequences for functional genomics analyses in a nonmodel organism with unsequenced genomes using a de novo assembly procedure (Gui et al., 2013) . Gui et al. (2013) characterized the leucocyte transcriptome of the Indo-Pacific humpbacked dolphin, now an endangered species because of the dramatic decline in population size of the past decades. The dataset provides a substantial genomic-level resource for the endangered species, while the identification of genetic markers and genes involved in immune system response and adaptive evolution can be useful in understanding the molecular mechanisms of various pathways in cetaceans.

Proteomic analyses have been demonstrated to support and validate studies on marine mammal health. One of the first proteomic studies on dolphins was a screening of the dolphin skin proteome by two-dimensional polyacrylamide gel electrophoresis (2D-PAGE), to compare the proteomic profile of skin tissue samples with that of skin-derived cultures. Results demonstrate that samples share distinct similarities sustaining the validity of the use of epidermal cell lines 302 SECTION | II Effects of Toxicological and Cumulative Stress for the study of the interaction occurring between dolphins and the environment (Yu et al., 2005) . Another example was a study investigating a dolphin condition similar to human metabolic syndrome using serum lipid profiles between two groups of dolphins, where high or low insulin were analyzed. The study found differences in phospholipid fatty acids in the two groups, which may play a role in the susceptibility to or development of an insulin-resistant-like state: the proteomic analysis of dolphin serum showed correlation of changes in the fatty acids with an insulin-sensitizing phenotype (Sobolesky et al., 2016) .

An approach to direct, noninvasive, health assessments of wild marine mammals used metabolomics to profile exhaled breath metabolites, providing a first library of volatile and nonvolatile compounds in cetacean exhaled breath. Dolphin breath contains a large variety of low-abundance metabolites, many of which are common with those found in human breath and considered indicative of human health status. Providing a link between dolphin breath and certain health conditions or exposures, the monitoring of exhaled breath metabolome content provides useful diagnostic information that can be used by veterinary personnel and conservation managers in their decision-making processes (Aksenov et al., 2014; Zamuruyev et al., 2016) .

Diseases of wild marine mammals can be difficult to diagnose because of their protected status and their unknown life history. A study using tools relying on cross-hybridization between closely related species presented the blood transcriptome of the California sea lion (Zalophus californianus) as a diagnostic classifier for rapid diagnosis and treatment of infection, disease, intoxication, or other causes of compromised health status in stranded animals (Mancia et al., 2012) . The results achieved with this work indicate that cross-species microarray technologies, using a selected gene set for microarray design and analysis could be of use to wildlife managers and veterinarians in handling species with little genomic data. The California sea lion, a protected marine mammal inhabiting the western coast of North America, is a good sentinel species for coastal habitats, and the most common cause of stranding is reported to be domoic acid toxicosis, caused by the ingestion of domoic acid, a potent neurotoxin produced by diatom species of the genus Pseudo-nizschia. Human domoic acid intoxication cases present with abnormal behaviors, even seizures and epilepsy linked to hippocampal lesions, sharing the neuropathology profile observed in the California sea lions exposed to domoic acid, in particular in the recently discussed link between hippocampal lesions and epilepsy (Buckmaster et al., 2014; Ramsdell and Gulland, 2014) . The blood transcriptomic analysis showed enrichment of pathways consistent with that neuropathology profile, confirmed by the analysis of the proteome of cerebrospinal fluid from intoxicated animals . Proteomic analysis of blood indicated that several apolipoprotein E charge forms decreased in domoic acid-intoxicated sea lions and may be important in the progression of domoic acid toxicosis .

The transcriptomic and the proteomic approaches taken, combined with machine learning approaches, performed as a robust and accurate tool to diagnose domoic acid toxicosis, demonstrating promise for future studies combining additional variables in multidimensional space to create robust classifiers (Mancia et al., 2012; Neely et al., 2015) .

Omics studies on California sea lion (family: Otaridae) were the first attempt to combine a global-scale analysis to the health status of marine mammals belonging to the order of Carnivora, mostly related to the possibility of screening a high number of unhealthy animals. In fact, data analysis was guided and/or supported by evaluation obtained by clinical observations and health parameters measurements carried out on the high number of unhealthy and ill cases that seasonally were hospitalized and rehabilitated at the Marine Mammal Center, in Sausalito, CA. Lately, the attention seems to be focused on different species, pinnipeds belonging to the family of the Phocidae, and to the understanding of the mechanisms activated by environmental chemical or biologic stress. Transcriptomics was used to evaluate brain tissues from stranded harbor seals (Phoca vitulina) to understand the cause of death. The study identified pathways involved in innate and adaptive immunity in samples with a phocine herpesvirus (PhV-1) brain infection and found a strong upregulation of fatty acid metabolic genes in animals that did not die from viral infection. Although the cause of the dysregulation of fatty acid metabolism in the brains of these harbor seals is unknown, it may be correlated to exposure to toxins or nutrient depletion (Rosales and Vega Thurber, 2016) . A de novo assembly of the blubber transcriptome in ringed seal (Pusa hispida) was used to identify molecular indicators of PCBs exposure. Transcript levels for gene targets were found to be correlated with increasing levels of blubber PCBs, linking an energy metabolism imbalance to the toxicity of the chemicals (Brown et al., 2017) . Transcriptome analysis of muscle tissue of another member of the family Phocidae, the northern elephant seal (Mirounga angustirostris), provides a resource for a deeper investigation of the alteration of molecular pathways triggered by environmental stress, including modifications of metabolic and immune signaling as well as muscle tissue maintenance (Khudyakov et al., 2015a ). An example comes from a study of the cellular responses to hypothalamic-pituitary-adrenal axis stimulation by measuring corticosteroid hormones, metabolites, and muscle gene expression before, during, and following administration of exogenous adrenocorticotropic hormone (ACTH); results suggests a compensatory, tissue-sparing mechanism used to maintain cortisol and aldosterone sensitivity while avoiding deleterious long-term consequences of stress (Khudyakov et al., 2015b) . Reference sequences specific to elephant seals were also obtained from the blubber tissue and used to profile the transcriptomic response to hypothalamic-pituitary-adrenal stress axis activation and to identify tissue-specific molecular markers of stress in the pinniped fasting-adapted species (Khudyakov et al., 2017) .

The completion of the genome draft of the Florida manatee (Trichechus manatus latirostris) in 2012 made immunogenetic exploration of the Sirenia order more feasible. The Florida manatee is one of the most endangered marine mammals in American coastal waters, continuously facing demographic challenges due to anthropogenic activities and stochastic factors. Brevetoxicosis and papillomavirus are disease-associated agents that have been described in their populations (Bossart et al., 2002; Walsh et al., 2015) , in addition to several parasites including trematodes, nematodes, and coccidians (Bando et al., 2014) . Discovering the genetic details of their immune system is an important step in the development of manatee-specific tools for monitoring health. The genomic scaffolds of the Florida manatee were used to characterize the organization and expression of the immunoglobulin heavy chain locus. The potential segmental diversity and constant region isotypic diversity described may be related to the mechanisms of defense against infectious disease in their environment.

The number of advantages using the new omics technologies constantly increase, but there are also limitations and challenges that need to be considered. With the completion of the human genome, the relationship between one or more genes to a specific disease and the importance of the interaction with complex environmental factors became clear, making the understanding of the disease process more challenging. Moreover, knowledge of physiologic processes such as SNPs, epigenetic modifications, and posttranslational modifications increased the complexity of host-disease interactions. Combining the study of multiple genes, proteins, and metabolites, the multiomics approach proved to be the correct one to understand the global picture of how each disease affects its host. Each omics study provides information between disease and nondisease, describing alterations of genes (transcriptomics), proteins (proteomics), or metabolites (metabolomics). Thanks to the advent of the bioinformatics with the development of computational and statistical techniques, these large datasets can be combined to offer an integrated view of how an organism may react to a disease, leading to potential biomarkers that can be used in the future for therapeutic applications.

However, technical difficulties, expensive instruments, cost versus benefit uncertainty, and clinical stringency make the use of omics still limited. In marine mammal studies, these limitations are associated with limited sample availability and genetic heterogeneity. Beside these restrictions, the massive amount of data generated from each omics study with often small sample size complicates the data interpretation and restricts the clinical value of the results. While using experimental replication and validation methods as well as reference samples and baseline data will improve confidence in the reliability of the results, there are parameters that need to be well evaluated. The small sample size used in the experimental setting caused by the protected status of marine mammals and by the cost of the techniques/sample can be a source of false discovery. Solutions to maximize the value of an omics study rely on repeated observations over long period of times, the examination of multiple tissues or body fluids correlated to the disease, the use of more than one technique to validate results, and the integration of the results with the biology or pathology of disease. However, different types of high-throughput technologies, each simultaneously collecting a large set of molecular data, used to collect information on the molecular components of biologic systems need to be integrated and analyzed. Taken together, the complexity of biologic systems, the technological limits, the large number of biologic variables, and the relatively low number of biologic samples make data integration of multilayer datasets one of the most relevant problems computational scientists are addressing nowadays.

Mammals have returned to the water in at least seven separate lineages: Cetacea, Sirenia, Desmostylia (extinct order), Pinnipedia, Ursus maritimus (polar bear), Enhydra lutris (sea otter), and Thalassocnus spp. (aquatic sloths, extinct genus). Some of these lineages have retained most of their terrestrial form while spending most of their time in the water, while others have changed their morphology dramatically to spend the entire time in the water. Clades of extant marine mammals seem to have originated at two discrete times. Cetacea and Sirenia originated during the early Eocene (50 million years ago, MYA), a time of high productivity in aquatic environments, with warm, broad, shallow seas and abundant resources to exploit, whereas Pinnipedia originated during the Oligocene (35 MYA), when productivity of the oceans, like today, was more concentrated around areas of upwelling. Both Cetacea and Sirenia were fully aquatic by the end of the Eocene, while Pinnipedia are semiaquatic animals (Gingerich et al., 1983; Lipps and Mitchell, 1976; Uhen, 2007) . Each of these evolutionary histories is different from the others. Despite the finding that these adaptations evolved in mosaic patterns, they all have aquatic characteristics in common, and different morphologic solutions to aquatic conditions were achieved separately in each clade. The transition from terrestrial animals to fully aquatic animals took about 12 MY, and eventually, they diversified into the species we know today: the genus Tursiops, which bottlenose dolphins belong to, first appeared in the fossil record about 5 MYA.

While some mammals were evolving in the oceans, others, on land, evolved into the first primates, humans ancestors. The human lineage split from the modern chimpanzees and bonobos around 7 MYA, with the early hominins (humanlike primates) that were our direct ancestors. Current estimates of the similarity between the DNA sequences of both the human and chimpanzee genome range between 95% and 99% (Varki and Altheide, 2005) .

The Earth is old and so is life: while the Earth formed 4.5 billion years ago, the oldest known fossils are around 3.5 billion years old. In this time scale, the evolution of marine mammals and primates are relatively recent events. Many

studies have compared primates and cetaceans in the perspective to elucidate the social evolution of highly intellectual mammals in terrestrial and aquatic environments. Despite a deep evolutionary divergence, adaptation to physically dissimilar environments, and very different neuroanatomic organization, some primates and cetaceans show striking convergence in social behavior, artificial language comprehension, and self-recognition ability (Marino, 2002; Yamagiwa and Karczmarski, 2014) . Thanks to next-generation sequencing and the availability of the genomes, we can now correlate the knowledge on the observations made on the ecology, social relationships, behavior, in the molecular mechanisms operated by genes, proteins, and metabolites. Using the current genomic available data, we can look briefly at the identity, at transcripts and protein level, between humans and marine mammals. In Table 11 .1, coding regions of the Cetacea, the bottlenose dolphin (Tursiops truncatus), the minke whale (Balaenoptera acutorostrata scammoni), the killer whale (Orcinus orca), the sperm whale, (Physeter catodon), and the Yangtze River dolphin (Lipotes vexillifer) are compared to those of humans and to those of the closest relatives in the Cetartiodactyla order. Coding regions of the Pinnipeds, the Weddell seal (Leptonychotes weddellii), and the walrus (Odobenus rosmarus divergens) are compared to those of humans and to those of the closest relatives in the Carnivora order. Coding regions of the Florida manatee (Trichechus manatus latirostris) are compared to those of humans and to those of the closest relatives in the Afrotheria superorder. As expected, the similarity is higher between closely related species within the same order and superorder. But the results from alignment of sequences of human and marine mammals is still very high, at least in the coding regions analyzed. Unquestionably, more of the genome needs to be understood and analyzed in both humans and marine mammals, but the similarity that we can already observe is an important instrument for basic research and translational science. Marine mammals are used as sentinels for ocean and human health (Bossart, 2011) , a choice driven by the characteristics shared with the human species (e.g., mammals with long life spans, long-term coastal residents, feed at a high trophic level). Marine sentinels allow the characterization and management of potentially negative impacts linked to the environment degradation that can affect animal and human health associated with the oceans. Marine mammals have also developed unique adaptations, some of which are species-specific, to live their entire life, or most of it, in the aquatic environment (e.g., deep long dives, swimming, thermoregulation, echolocations). These fascinating differences with land mammals can sometimes be turned into powerful tools to understand and treat complex pathologic processes in human medicine.

Marine mammals are top predators that are essential for the health and function of the oceans, too often affected by various factors that can be detrimental Sets of transcripts and proteins were retrieved from Entrez, aligned to the genome by Splign (transcripts) or ProSplign (proteins) and passed to Gnomon, NCBI's gene prediction software. The sequences used for the alignments are reads from RefSeq or from the GenBank database. The number in the report is the sum of multiple GenBank queries. The query of GenBank should retrieve the list of organisms with txid91561 for the Cetartiodactyla, txid311790 for the Afrotheria, and txid33554.

to their populations. Forty-four percent of stranded marine mammals die from unknown causes (Gulland and Hall, 2007) . Disease is a major cause of marine mammal population decline, and the role of the microbiome in disease has generated considerable interest. The etiology of stranding events still remains poorly characterized, but high-throughput sequencing technology can identify and yield new insights into the virome and microbiome for disease identification and surveillance. The microbiome acts strongly and significantly in maintaining host health with a vital role in disease manifestation and immune system function. Members of the microbial community can directly influence the progression of a disease and modulate the host's immune system regulation and response, making the host's microbial partners essential to immune system function (Maynard et al., 2012) . A promising approach for pathogen identification in stranded marine mammals is the use of metagenomics, the characterization of the collective genome of microorganisms isolated from an organism using high-throughput sequencing technologies. Metagenomic studies do not require prior information about the disease agents and allow detailed comparisons of health and disease, identifying new insights into the virome and microbiome of wildlife. Metagenomic studies of marine mammals focused on the viral and microbial community of many tissues and body niches have displayed a big diversity of the microbiota according to the organ type and may be used as a baseline survey for comparison with samples from stranded animals during unexplained disease outbreaks (Godoy-Vitorino et al., 2017) . A viral metagenomic study to investigate potential viral pathogens associated with a mortality event of captive California sea lions identified a novel species-specific anellovirus (ZcAV) (Ng et al., 2009) . A similar study on lung samples of the Pacific harbor seals (Phoca vitulina richardsii) revealed another novel seal anellovirus (SealAV), which clusters phylogenetically with anelloviruses from California sea lions and domestic cats (Ng et al., 2011) . The description of ZcAV and SealAV in the lungs of pinnipeds suggests that anellovirus infections may be common and play a role in marine mammal health and disease. A different metagenomic study used nextgeneration sequencing to get a comprehensive view of the fecal viral populations from wild and temporarily captive California sea lions, reporting previously uncharacterized viruses, including astroviruses, picornaviruses, bocaviruses, and sapoviruses (Li et al., 2011) . A metagenomic survey of viromes from feces of the Subantarctic fur seal (Arctocephalus tropicalis) and south American fur seals (Arctocephalus australis) used next-generation sequencing to explore the viral diversity of southern hemisphere marine mammals (Kluge et al., 2016) . A study of the bacterial communities in hundreds of samples from different body sites in healthy dolphins and sea lions showed a highly diverse bacterial taxonomic composition, which varies according to body site and host species (Bik et al., 2016) . These findings provide species-specific databases that can be compared to later virome surveys and microbiota to detect alterations associated with changes in marine mammal health or population size.

Metagenomic studies in cetaceans have also indicated how the cetacean microbiome is affected by human-related bacteria (Godoy-Vitorino et al., 2017) . This is particularly relevant since many human infections have a zoonotic, i.e., wild or domestic animal, origin. The rise in zoonotic diseases is driven by a complex interplay of environmental, ecologic, and epidemiologic factors. Therefore, the identification of pathogens in marine mammals may also moderate disease outbreaks and prevent zoonotic transmission (Delwart, 2012) . Currently, there are about 15 known zoonotic marine mammal pathogens (Waltzek et al., 2012) . The bacterial pathogen that causes tuberculosis, Mycobacterium tuberculosis, was introduced to the Americas via pinnipeds (Bos et al., 2014) . The influenza A virus, a global human threat, is present in cetaceans and pinnipeds and has been shown to be transmitted from seals to humans (Geraci et al., 1982; Reperant et al., 2009; Webster et al., 1981) . Most recent emerging diseases have been associated with host switches, including severe acute respiratory syndrome coronavirus, H5N1 avian influenza, Hendra virus, Nipah virus, and acquired immunodeficiency syndrome (Woolhouse and Gowtage-Sequeria, 2005) . The risk of being injured or acquiring zoonotic diseases is highest in marine mammal researchers, rehabilitators, trainers, veterinarians, and volunteers, but there also numerous recreational activities now permitting contact with these animals. Subsistence hunters such as whalers and sealers, as well as human rescuers during marine mammal stranding events, are at risk of disease acquisition through their direct physical contact with infected marine mammals or through the ingestion of marine mammal food products (Hunt et al., 2008; McLaughlin, 2004; Webster et al., 1981) . Luckily, so far the majority of zoonotic marine mammal diseases have resulted in localized skin infections in man that resolved spontaneously or with appropriate medical therapy. However, other marine mammal zoonoses, if left untreated, could induce life-threatening systemic diseases that could pose public health risks.

As the number of zoonotic diseases rises, the identification of pathogens in marine mammals has become an indicator of environmental health. The list comprises bacterial, viral, and fungal infections producing seal finger, brucellosis, leptospirosis, mycobacteriosis, mycoplasmosis, influenza, lobomycosis, and blastomycosis (Waltzek et al., 2012) . As the closest oceanic relatives of humans, marine mammals are sentinel species for both human and ocean health, and they are long-lived, top predators, inhabiting the same inshore ecosystems utilized by man (Bossart, 2011) .

Medical research has necessitated the integration of the omics and computational biology data to diagnose, interpret, and prognosticate human disorders, even with the current more comprehensive knowledge of the human physiology and molecular mechanisms of diseases. Omics technologies, including genomics, transcriptomics, proteomics, metabolomics, and epigenomics have transformed human medical research in the last decade, but there are still unresolved underlying mechanisms in human disease. The exploitation of marine mammals' unique adaptations through omics technologies can give valuable insights into uncertain human biomedical conditions. An example is the astonishing wound healing mechanisms observed in dolphins. In humans, the healing of tissue wounds is often associated with infection and results in scars. Zasloff, a professor of immunology at the Georgetown University, observed the clinical course of the recovery of two dolphins showing shark's bites about 30 cm in length and 3 cm in depth (including the blubber layer and the underlying muscle). He reported that, during the first day postinjury, blubber from surrounding tissues had already migrated over the open wound surface. On the second day, newly generated tissues were described, which would gradually fill the wound from its base, restoring the original volume (Zasloff, 2011) . The wound healed completely in 4 weeks and without any infections, which is remarkable considering the analogies between their immune system and that of terrestrial mammals (Beineke et al., 2010; Mancia et al., 2007) . Zasloff suggested a role of the components of the dolphin blubber during the healing process. Stem cells present in the blubber could have a role in this remarkable wound healing process. The composition of dolphin's blubber is in fact different from other marine mammals: the isovaleric acid is higher in concentration, being 2%-5% of total fatty acid in bottlenose dolphin, while there is no detection in several species of whales (Koopman et al., 2003) . The isovaleric acid accumulates in the blubber and does not get burned for fuel during times of starvation; moreover, it seems to have antimicrobial activity (Hayashida-Soiza et al., 2008) . A multilayered omics approach describing transcripts, proteins, and metabolites present in the blubber after a serious injury may unravel the mechanisms involved in the healing process and in the protection from infection. Advances in the comprehension of the mechanisms controlling the healing process could lead to the improvement in the control of wound healing in humans and terrestrial mammals, leading to new therapies: regenerative medicine is an emerging field, with numerous open trials in the veterinary and human fields.

Another example is given by the recent discoveries of the unique dolphin metabolism that can aid research for human metabolic syndrome and diabetes. People affected by metabolic syndrome can develop not only type II diabetes but also cardiovascular disease and the possibilities to have strokes. Like humans, dolphin can develop metabolic syndrome, characterized by elevated insulin, glucose, triglycerides, and ferritin (Venn-Watson et al., 2011 . Fatty liver disease has been found both in wild dolphins and dolphins under human care, suggesting that dolphins are susceptible to metabolic disorders. A study compared a wild population of dolphins to one kept under human care, which have higher annual survival and lower mortality rates. A deep study into these animals' diets as a possible risk factor for longer life and metabolic disease highlighted the potential benefits of C17:0, the margaric or heptadecanoic acid, a saturated fatty acid also present in bovine milk fat. High levels of C17:0 on erythrocyte membranes or plasma phospholipids have been identified as protective factors against development of metabolic syndrome, type 2 diabetes, and associated inflammation (Venn-Watson et al., 2015) . In a parallel study supported by proteomic analysis, differences in serum lipid profiles between two groups of dolphins, with high or low insulin, found differences in phospholipid fatty acids. Shifting the dolphin diet to fish rich in odd chain saturated fatty acids, such as C17:0, resulted in increased serum levels of the insulin-sensitizing hormone adiponectin and serum sphingosines consistent with an insulinsensitizing phenotype (Sobolesky et al., 2016) . A better understanding of the networks activated by the mechanisms regulating adiponectin in dolphins could aid research of metabolic syndrome and diabetes affecting human populations.

Each year, millions of individuals die or become ill because of conditions or diseases that reduce the oxygen supply to hypoxia-sensitive tissues such as the brain. Hypoxia is also involved in and is the cause of many neuronal disorders in humans, for example, Alzheimer disease (Peers et al., 2007) , Parkinson disease (Speer et al., 2013) , and cerebral ischemia (stroke). Acute metabolic insults like stroke have an especially devastating impact, which is mostly impossible to repair. By contrast, brains of diving mammals tolerate extended periods of systemic hypoxia without damage. There are several behavioral, anatomic, and physiologic adaptations that are associated with the remarkable dive capacity of many marine mammals (Davis, 2014) that may be partly explained on the genetic level by specific substitutions within the coding sequences (Foote et al., 2015) or by selective gene duplications and losses (Yim et al., 2014) . Different studies used transcriptional analysis of the brain of the hooded seal (Cystophora cristata) and showed differential regulation of specific genes that may have a central role in the protection of the diving brain. In large parts, the seal brain responds to the hypoxic challenge in a similar way as the brain of other mammals, which includes the upregulation of typical stress proteins like cytokines and immediate early genes. However, the genes involved in the energy metabolism seem to be a specific response of the seal brain to hypoxia. These genes may also be suitable drug targets for human neuronal disorders associated with hypoxia (Fabrizius et al., 2016; Hoff et al., 2017) .

A new era of discoveries in the marine mammal field began in 2011, with the completion of the first marine mammal genome, 8 years after the first wholegenome assembly of the human genome. Since then, the bottlenose dolphin genome has been greatly implemented thanks to supplement of new highthroughput sequences data and to the newer high-coverage version of the genome. To date, genomes from nine species of marine mammals have been sequenced, annotated, and are publicly available at the NCBI database. Genome-guided transcriptome assembly as well as de novo transcriptome assembly are providing novel insights into the mechanisms underlying the variety and diversity of marine mammals, revealing novel genes, functions, and connections of molecular and physiologic mechanisms related to their adaptations, health, and disease.

But the study of the genome and of the information that is encoded within the full DNA sequence of an organism will not unlock the code of life. The genotype is not the only thing responsible for the final physical makeup of the organism. The study of the epigenome with the genome-wide mapping of DNA methylation, histone modifications, nucleosome positioning, and three-dimensional architecture and the integration of genome and epigenome, of the RNA information from coding (mRNA) and noncoding RNA (epigenetic-related RNAs, such as micro RNA, short interfering RNA, piwi-interacting RNA, and long noncoding RNA) are necessary to understand how environment and genetic inclinations can intertwine in the complexity of cell biology. The network and pathways observed can in predictive models identify potential risks and propose solutions, making the new technologies relevant as prognostic and diagnostic tools.

The exploitation and combination of the new advanced omics techniques and computational methods will finally allow the discovery of a constantly changing environment thanks to the understanding of the interaction between marine mammals, humans, and the oceans.

",0.7491657980094879
BMC Medical Informatics and Decision Making Automated real time constant-specificity surveillance for disease outbreaks,"Background: For real time surveillance, detection of abnormal disease patterns is based on a difference between patterns observed, and those predicted by models of historical data. The usefulness of outbreak detection strategies depends on their specificity; the false alarm rate affects the interpretation of alarms.","The release of anthrax in 2001, the Severe Acute Respiratory Syndrome (SARS) outbreaks in China, Hong Kong and Toronto in 2002, and the emergence of new diseases such as West Nile virus have underscored the need for automated, real-time detection of outbreaks. Several such detection systems have been deployed in recent years at the hospital [1, 2] , city [3] [4] [5] , regional [6] [7] [8] and national [9] [10] [11] levels. Many systems use time series algorithms to detect aberrant conditions, such as CuSUM [12] [13] [14] , variants of the Serfling method [3] , multiresolution waveletbased models [15] , and trimmed seasonal models [16] .

An outcome of any of these statistical methods -whether or not there is an alarm on any given day -is uninformative without an estimate of the likelihood that an alarm signals a true outbreak. This likelihood depends in part on the specificity of the detection method, equal to the proportion of non-outbreak days for which no alarm is raised. The specificity is related to the false alarm rate by the simple equation false alarm rate = 1 -specificity.

Even small changes in the specificity of the detection method may have a large impact on the likelihood of a true outbreak. Despite the importance of knowing the specificity, analysis of the specificity of outbreak detection algorithms has been rudimentary, and it is common practice to report one average value of specificity that is assumed to reflect the true specificity on any day of the year or week. Implicit in this is the assumption that the specificity is constant as a function of time. If this assumption is incorrect -if instead the specificity of an outbreak detection system is a function of time that deviates significantly from its average value -then on any given day, a public health practitioner cannot know the specificity of the system or the related probability that there is a disease outbreak, and therefore cannot respond appropriately to alarms.

The sensitivity of a method, or proportion of outbreaks detected, is negatively associated with its specificity. Unlike the specificity, however, it cannot be evaluated from non-outbreak data. This is because in addition to its dependence on the specificity, it also depends on the characteristics of an outbreak, including its duration and magnitude. Hence the trade-off between sensitivity and specificity must be carefully considered in the context of the outbreak type of interest to ensure that both fall in a useful range.

We sought to characterize changes in the specificity of alarms produced by standard time series outbreak detection methods as a function of time. We further explored how these changes affect the sensitivity of detection methods to several outbreak types. We introduced a statistical technique that allows us to model properties of time series not captured by traditional models, developing an outbreak detection strategy with constant specificity that may be used by public health practitioners for biosurveillance.

Data were collected retrospectively in the emergency department (ED) of an urban pediatric tertiary care teaching hospital. All patients with respiratory presenting complaints seen in the ED between August 1, 1992 and July 30, 2004 were included in the study. The data were divided into a six-year training period, and a test period consisting of the final six years. ED chief complaints were selected at triage from among a constrained list, and classified as respiratory or non-respiratory using a previously validated method [17] . The study was approved by the institutional review board.

During the study period, approximately 137 patients were seen each day in the ED. The number of daily visits for respiratory complaints varied from 2 to 78. The mean number of respiratory visits was 21.05, and the standard deviation was 9.03 (see figure 1 ). These data and other hospital visit data time series have previously been shown to depend significantly on the day of the week and the season of the year [16, [18] [19] [20] .

We implemented five traditional time series models used for outbreak detection: a simple autoregressive model, a Serfling model, the trimmed seasonal model, a waveletbased model, and a generalized linear model. In addition, we introduced a model of both the expectation and the variance based on generalized additive modeling techniques. The input to each algorithm was a time series of historical daily ED respiratory visit counts, and each returned a threshold number of visits for the day immediately following the historical period. An alarm occurred when the actual number of visits exceeded the threshold.

The autoregressive model predicted the number of ED respiratory visits using linear regression on the number of visits during the previous seven days:

Emergency department visits for respiratory presenting com-plaints, August 1, 1992 -July 30, 2004 1993  1994  1995  1996  1997  1998  1999  2000  2001  2002  2003  2004 Visits where E t is the predicted number of visits on day t, V t-k is the actual number of visits on day t -k, and the coefficients a k were fitted by least squares regression using training data.

The Serfling method and its variants have been extensively used for surveillance of influenza and other diseases [3, 21, 22] . Our implementation modeled the number of daily visits using linear regression on sine and cosine terms having yearly periodicities to capture seasonal effects, categorical variables for the day of week, and linear and quadratic terms. Under this model, the predicted number of visits on day t was where dow(t) is the day of the week from 0 to 6, doy(t) is the day of the year from 1 to 365, and the Kronecker delta function δ x,y is equal to 1 when x = y and 0 otherwise. To calculate the day of the year during leap years, each day after February 28 was treated as though it occurred on the previous day.

The trimmed seasonal model is used in the AEGIS system [23] for statewide real-time population health monitoring, and was implemented as previously described [16] . Beginning with training set data, the average number of visits was calculated and subtracted from the data. From this, the average for each day of the week was calculated and again subtracted. To remove seasonal effects, the average for the day of the year was calculated after excluding the highest and lowest 25% of values for each day of the year, and again subtracted from the data. A first-order autoregressive, first-order moving average (ARMA) model was then fitted to the errors. The predicted number of visits E t was calculated by summing the overall average, the average for the day of the week, the average for the day of the year, and the ARMA prediction for day t.

The wavelet-based model was patterned after the wavelet anomaly detector developed by Zhang et al. [15] . The method used the number of daily visits in a training set, V 1 , V 2 , ..., V t-1 , to produce a prediction for day t. It consisted of the following steps:

1. A low-frequency wavelet component of the visit signal having periodicity of more than 32 days was calculated. This period was selected by Zhang et al. because it removes seasonal effects while preserving higher-frequency information, and because it is a power of 2, which is mathematically convenient for wavelet analysis. We used the Haar wavelet in our implementation of the model [24] .

2. This low-frequency baseline was subtracted from the original signal, producing a residual for each day in the training set.

3. The predicted number of visits on day t was the value of the low-frequency component on the previous day.

Daily alarm thresholds for the autoregressive, Serfling, trimmed seasonal, and wavelet-based models were calculated as the sum of the expected number of visits and a multiple λ of the standard deviation of the model residuals on the historical training data. The value λ of was an adjustable parameter that affected the specificity of each model.

The generalized linear model consisted of a Poisson distribution function, an identity link function, and a linear predictor that included day of the week, month of the year, holiday and linear trend terms:

where dow(t) and δ x,y are described in equation 2, moy(t)

is the month from 1 (January) to 12 (December), and I holiday (t) is an indicator function equal to 1 if day t is a holiday, and 0 otherwise. An alarm sounded if the value of the cumulative distribution function of a Poisson random variable with mean E t exceeded the desired specificity. This model was found by Jackson et al. [18] to have superior sensitivity to a variety of outbreak types compared to several control-chart and exponential weighted moving average models.

In addition, we developed and implemented a novel method for outbreak detection that captures changes in the ED visit standard deviation, as well as in the expected number of visits. In contrast to previous surveillance models, which assumed that the variance is constant or proportional to the mean, it did not assume a functional form for the variance. Instead, the dependence of both the mean number of visits and the variance was modeled explicitly. In other applications, several statisticians have modeled the variance as a function of the same or additional covariates used to model the mean using iterative successive relaxation procedures (see, for example, [25] and [26] ). We employed a simplified procedure involving two distinct models: an expectation model of the daily expected number E t of respiratory ED visits, and a variance model of the daily variance of respiratory ED visits.

The number of daily visits is then modeled as a Gaussian

with mean E t and variance . Both components are generalized additive models (GAM's): nonparametric extensions of linear regression models having several variants depending on the choice of smoothing technique, the procedure used to find estimates of the nonparametric functions for multivariate models, and the number of degrees of freedom for each covariate [27, 28] .

The GAM of the expectation accepted historical daily visit counts as input, and modeled them as a function of linear time to capture a long-term trend, the day of the year to account for seasonal trends, and the day of the week:

No smoothing was performed for the day-of-week term, since many replicates were available for each day of the week. A Gaussian kernel smoother was used for the trend term, and a Gaussian kernel smoother with circular boundaries was used for the day-of-year term since the day is a periodic covariate. Although a Gaussian was selected for its ease of interpretation, in general the choice of kernel function has little effect on the model compared to the choice of bandwidth [27] . Optimal bandwidths of the two Gaussian smoothers were estimated by a two-step procedure. First, to optimize the bandwidth of the day-ofyear Gaussian, the mean predictive squared error (PSE) on a training set consisting of the first six years of ED visit data was calculated for a range of bandwidths using 10fold cross-validation for a model containing only the dayof-week and day-of-year covariates. The bandwidth minimizing the mean PSE was chosen, corresponding to a Gaussian distribution with a standard deviation of five days. Next, the bandwidth of the kernel used for the trend term was chosen by using 10-fold cross-validation to estimate the mean PSE on the training set of a model containing all three covariates for a range of trend bandwidths, using the previously determined optimal bandwidth of the day-of-year kernel. The minimizing bandwidth was again chosen, corresponding to a standard deviation of eight days. Because the model contained multiple nonparametric functions, an iterative backfitting procedure was used to estimate each until the model converged [27] .

The residuals of the expectation GAM on the historical data were squared and used as the input to the variance GAM. This GAM was also a function of linear time, day-ofyear, and day-of-week variables:

The Gaussian smoothers were chosen to minimize the PSE on the training data set using the same procedure as above. The optimal smoothers corresponded to Gaussian distributions with standard deviations of 6 and 253 days for the day-of-year and trend terms, respectively.

To set the alarm threshold for a given day, a composite expectation-variance model consisting of the two GAM's was trained on the previous six years of data. The alarm threshold for the next day was calculated as the sum of the expected number of ED visits, as predicted by the expectation GAM, and a multiple λ of the expected standard deviation of ED visits, as predicted by the variance GAM:

The value of λ was an adjustable model parameter.

All models were implemented using the Matlab software package, Version 7.0.1 [29] . The Matlab system identification, statistics and wavelet toolboxes were used for the wavelet, generalized linear, and expectation-variance models.

We used the expectation-variance model to generate alarm thresholds for each day during the test period from August 1, 1998 to July 30, 2004 , which comprised the last six years of historical data. All of the available data could not be used for testing because a training period was required. To predict each threshold, the model was trained on the previous six years of data, ending the day before the day to be predicted, and was blind to the actual number of ED visits on the prediction day. The backfitting procedures to estimate the model successfully converged for each day of the study period. The model predictions for both the expected number of patients and the variance were always positive numbers throughout the study period. The average absolute predictive error was approximately four patients during the study period. 

The predicted threshold for day T was E T + λ·σ T .

Alarm thresholds for each day of the test period and each desired specificity were similarly calculated for the autoregressive, Serfling, trimmed seasonal, and wavelet models. The alarm threshold for the generalized linear model was the largest integer A t for which the cumulative distribution function of a Poisson random variable with mean E t was at most s. With the exception of wavelet model thresholds, all alarm thresholds were calculated using the six years of visit data immediately preceding the prediction day. The wavelet model requires a training period having length equal to a power of two, so 2048 days of training data were used.

To determine whether a given model at a particular mean specificity had constant specificity as a function of the day of the week, we tabulated the proportion of alarm and non-alarm days at that mean specificity by day of the week. A chi-square analysis was performed under the null hypothesis that all days of the week had an equal fraction of alarm days. A p-value less than 0.05 indicated that the specificity was dependent on the day of the week. To determine whether the specificity was constant as a function of month and year, we performed similar chi-square analyses after tallying alarm days by month of the year and by calendar year of the study, respectively.

In order to ascertain the sensitivity of the models to outbreaks, we superimposed three synthetic outbreaks on the test data set: a flat outbreak of five additional patients per day for seven days, a linear outbreak which increased from one to five patients over five days, and a spike outbreak of 10 additional patients in one day. For each model, each outbreak type, and each day of the test period, we created a new semisynthetic data set by adding an outbreak beginning on that day to the original data set. We then made an alarm threshold prediction for each of the outbreak days, and for each desired specificity between 0.01 and 0.99, based on training using the semisynthetic data set.

The actual mean specificity for one model at each desired input specificity was determined by running the model on the historical data set. Specificity was estimated by calculating the fraction of days without alarms for each day of the week, month of the year, or calendar year. Sensitivity calculations used the results of applying each of the models to the semisynthetic data sets. The sensitivity was cal-culated as the fraction of outbreaks for which there was at least one alarm day. Exact 95 percent binomial confidence intervals were calculated for each estimate of sensitivity and specificity. Timeliness of detection was evaluated for each method by calculating the mean lag in days between the start of a flat outbreak and the first alarm sounded. Missed outbreaks, for which no alarms were sounded on any day of the outbreak, were excluded from timeliness calculations. An alarm sounding on the first outbreak day corresponded to a lag of zero. Timeliness calculations were calculated at the benchmark specificity values of 0.85 and 0.97.

To compare the outbreak detection performance of the expectation-variance model with the traditional models, receiver-operator (ROC) curves were constructed for all models. ROC curves show the dependence of the mean sensitivity on the mean specificity, and the area under the ROC curve is an indicator of overall performance. The area was estimated by the trapezoidal method.

As suspected, the specificity of the five standard models was not constant over time. Hypothesis testing indicated that the specificity of the Serfling, trimmed seasonal and generalized linear models varied with the study calendar year and study month (p < 0.05) over a range of mean specificities between 0.50 and 0.99. The autoregressive model demonstrated a variable specificity with the study month and day of the week (p < 0.05) for the same range of mean specificities, and the wavelet model had variable specificity (p < 0.05) on all three time scales (figure 2). Several trends in the specificity were apparent when the analysis was limited to particular values of mean specificity. For example, at a mean specificity of 85 percent, corresponding to approximately one false alarm each week, the autoregressive, Serfling, trimmed seasonal and wavelet models had highest specificity in June and July and low specificity during the winter months. The specificity of the autoregressive and wavelet models was highest in the middle of the week and lowest on Sunday, and the Serfling, trimmed seasonal and generalized linear models had higher specificity during certain study years (figure 3). Similar trends were observed at other mean specificity values, including 0.90, 0.95, and 0.97 (data not shown).

By contrast, the expectation-variance model specificity was constant as a function of the study year, study month, and the day of the week. Hypothesis testing resulted in a p-value above 0.05 for the entire range of input specificities on all three time scales, indicating that there was no evidence to suggest that the specificity was non-constant on any time scale (figure 2).

The expectation-variance model usually outperformed traditional approaches in terms of sensitivity. The area under the expectation-variance model ROC curve was equal to or greater than that of the five comparison models for all three outbreak types (table 1) .

The expectation-variance method also performed well in terms of earliness of detection. At a benchmark mean specificity of approximately 97 percent, it detected a sevenday outbreak consisting of five additional patients each day with a shorter lag than the autoregressive, Serfling, trimmed seasonal, and wavelet models (table 2). The expectation-variance model also had earlier detection than these models at 85 percent specificity (data not shown).

The sensitivity of outbreak detection depends on the size and shape of an outbreak, as well as on the amount of noise in the ED utilization signal. Thus even when the specificity is held constant, it is natural for the sensitivity to vary with the season, day of the week, and trend. The ED visit signal had the least noise in the summer and the most noise in the winter ( figure 4) . Hence the signal-to-noise ratio was highest in the summer for any fixed type of outbreak, and the sensitivity of any reasonable detection strategy should theoretically be greater during the summer than in the winter. Summer and winter ROC curves for the expectation-variance and five comparison methods confirmed that summer sensitivity was greater than winter sensitivity when the specificity was held fixed (figure 4 insets). However, at mean specificity values of 85 and 97 percent, plots of sensitivity over time for the autoregressive, Serfling, trimmed seasonal and wavelet models showed a paradoxical increase in sensitivity to synthetic outbreaks during winter months compared to summer months ( figure 4) . These seemingly contradictory results occurred because the mean specificity of these four comparison models was not the actual specificity during either Average specificity trends over time Figure 3 Average specificity trends over time. Average specificity for each calendar year, month, and day of week for the five comparison methods during the study period. Data shown were recorded for each model implemented at 85% mean specificity. Similar trends were observed for all methods at 97% mean specificity (data not shown). the summer or winter. The specificity was significantly higher during the summer, corresponding to a shift to the left along the summer ROC curve and a concomitant decline in summer sensitivity. The opposite occurred in winter. This anomaly was corrected by the expectationvariance model (figure 4), since it operated at the same specificity during all seasons. The generalized linear model exhibited variable specificity by month, but its specificity was not highest during the summer months ( figure  3) , and hence it also had greater summer sensitivity than winter sensitivity.

We found that the specificity of outbreak detection was not constant for five traditional algorithms. This is important because having a standardized interpretation of the statistical characteristics of an outbreak detection test, including the specificity, aids public health practitioners in making rational decisions regarding resource allocation in the event of an alarm. The positive predictive value (PPV) of an alarm, the probability that an alarm signals a real outbreak, bears directly on the priority and extent of response required. The PPV is related to the specificity by the equation

where p is the prior probability of an outbreak. Because the specificity of an alarm strategy affects its PPV, it is crucial to have an accurate estimate of the specificity on any particular day. Even small differences in the specificity may have a great impact on the PPV; an alarm strategy at 95 percent specificity may have a PPV nearly twice as high as the same strategy at 90 percent specificity, depending on the nature of the outbreak considered and the sensitivity of the system. A public health practitioner responding to an alarm in the first case may wish to devote twice as many resources to investigating the alarm than in the second case.

The specificity also affects the overall cost associated with a surveillance model. Let c TP , c FP , c TN and c FN denote the costs associated with true positive alarms, false positive alarms, true negatives, and false negatives, respectively. Then the expected total cost of an alarm strategy on a given day is a weighted sum of these costs:

Lowering the specificity contributes to the cost due to fruitlessly investigating more false positive alarms, reflected in the third summand of the equation. At a specificity of, for example, 99%, one can expect to experience a false alarm every 100 outbreak-free days. Lowering the specificity to 97% increases the false alarms to approximately once per month. The cost equation can also be used to compare two alarm methods, A and B. Strategy A is more cost-effective than strategy B if and only if the expected cost of A is less than that of B:

PPV sensitivity sensitivity specificity Thus the greater the accuracy in the estimates of the specificity and sensitivity of each method, the prior probability of an outbreak p, and the costs of each scenario, the more accurately a public health department can compare the cost-effectiveness of the various available surveillance methods.

It may be desirable under certain conditions to have nonconstant specificity. For example, one may wish to adjust the specificity so that the PPV is constant as a function of the day of the week, season, and trend. Alternatively, a high profile event may merit special attention, requiring lower specificity surveillance to increase the sensitivity to outbreaks. The expectation-variance model is preferable to traditional models in these situations because its specificity is known more reliably than that of traditional models. Therefore the specificity can easily be adjusted with time according to public health needs. By contrast, current models operate with unknown specificity, and adjusting an unknown quantity presents a difficulty.

To understand the inability of traditional models to maintain constant specificity over time, it is useful to recast the outbreak detection problem in terms of percentiles instead of means. A perfect outbreak detection model operating at a specificity of 0.95 would output an alarm threshold equal to the 95th percentile for each day, above which an alarm would sound. More generally, a perfect model at specificity would model the kth percentile.

The autoregressive, Serfling, trimmed seasonal and wavelet models assume that the data have normally distributed errors with constant variance. They thus make a first approximation to this percentile by modeling the mean, to which a constant (which depends on k) is added. One problem with this approach is that the ED utilization signal is heteroscedastic -that is, its variance is not constant as a function of time ( figure 5 ). In practical terms, this means that the kth percentile is sometimes farther from the signal mean than at other times. Hence it cannot be captured by adding a constant value to the mean. The result is that during periods of greatest ED utilization variance, such as the winter months (figure 5), the alarm thresholds of these traditional models underestimate the kth percentile, leading to a decreased winter specificity ( figure 3) . Conversely, all four models overestimate the alarm threshold during the summer months, when the ED utilization variance is lowest. In fact, neglecting the dependence of the ED visit variance on the day of week, day of year, or long-term trend when determining the alarm threshold introduces some degree of systematic error in the alarm threshold, although it may not be of k 100 Seasonal sensitivity trends Figure 4 Seasonal sensitivity trends. Average sensitivity for each month of the study period for the autoregressive (left), trimmed seasonal (center), and expectation-variance (right) models when applied to data containing a superimposed spike outbreak of 10 additional patients during one day. Data shown were collected at a mean specificity of 97%. The sensitivity of the trimmed seasonal and autoregression models is higher during the winter than during the summer. Sensitivity is higher during the summer than during the winter for the expectation-variance model. July receiver-operator (ROC) curves lie below February ROC curves for all three models (insets). Similar trends were observed for flat and linear outbreaks. Although the generalized linear model does not assume that the variance is constant, it does assume that the data are Poisson distributed, and consequently that the signal variance is equal to the signal mean. However, the actual signal variance is greater than the mean; the ratio ranges from approximately one to more than three during the calendar year ( figure 5 ). The result is that during periods of high relative signal variance, the specificity of the method is also relatively high. For example, in October, both the ratio of signal variance to signal mean (figure 5) and the specificity (figure 3) are high.

Changes in specificity may also result from systematic errors in the expected number of ED visits predicted by the algorithms. For example, our implementations of the wavelet and autoregression models do not take into account day-of-week effects on the number of ED visits. Hence during high-volume days, such as Sundays, these models underestimate the expected number of visits. This in turn lowers the alarm cutoff value and the specificity compared to low-volume days such as Wednesdays. The Serfling model constrains the seasonal effects of ED utilization to a sine wave. However, the normal seasonal pattern of respiratory visits includes a spring increase that coincides with the allergy season (figure 5), which cannot be captured by a sine curve. This causes a May dip in the specificity of the Serfling model ( figure 3 ).

In addition to the approach considered here, it may be possible to apply a generalized additive or other model to the squared residuals of a traditional algorithm. A model for the alarm threshold would then be constructed in a similar manner to the expectation-variance model. Because the specificity is affected by systematic errors in both the mean and the variance, it would be necessary to apply a statistical test to ensure that the specificity was constant.

The expectation-variance model is a general time series method which could be applied to surveillance of other syndromes and populations. Implemented here in Matlab, it could easily be imported to other platforms, and it requires minimal additional computational resources for public health departments collecting surveillance visit data. It does, however, have several limitations. While useful for modeling syndromes that are predictable functions of the trend, season, and day-of-week covariates, such as respiratory or gastrointestinal illnesses, it would have limited utility compared to simpler models for rare or sporadically occurring syndromes. The present study has evaluated the specificity, sensitivity, and timeliness of detection using a training set containing six years of data. However, this much historical data is not always available for model training. Although the algorithm is easily adapted to shorter training sets, future work is needed to assess its performance with such sets. Like other detection methods, the training data must be free of an outbreak of interest in order for the specificity estimates to be accurate. Thus the training set used in the present study would be useful for detecting anthrax, other bioterrorism events, or large influenza outbreaks due to changing viral strains, but not for reliably detecting yearly average influenza outbreaks present in the data. Like other time series methods, the model also does not take advantage of geospatial information or data streams containing different types of data.

A more subtle limitation of the expectation-variance model is that its output is a binary variable -the absence or presence of an alarm. Kleinman et al. [30] proposed an approach to temporal and spatial surveillance which instead provides the probability that an observed event would be expected in the absence of an outbreak. This approach represents a shift from statistical testing to more detailed statistical modeling techniques [31] . Although the current implementation of our method is binary, it can easily be converted to a ""modeling"" approach. For example, a graph of the specificity as a function of the alarm threshold corresponds to a predicted cumulative distribution function of the number of visits on any given day.

In addition to the limitations of the model, our study is limited in its analysis of sensitivity to various outbreak types. The sensitivity depends on the time series of additional outbreak patient visits, of which an infinite array of possibilities exist. In the absence of outbreak data capturing the essential features of the many diseases and syndromes that may be monitored, we have used synthetic outbreaks having simple functional forms or ""canonical shapes"" [32] . This makes comparisons between types of outbreaks easy to interpret. Alternatively, the response to one or more known outbreaks may be evaluated [18, 33] . This approach has the advantage that the outbreaks are inherently realistic, since they are instances of true outbreaks. However, they may be highly irregular and dominated by stochastic effects. Indeed, there is no guarantee that they bear resemblance to future outbreaks of the same or other diseases. The present study offers the promising conclusion that the expectation-variance model has good comparative sensitivity for a limited number of artificial outbreaks, but more detailed study in the context of outbreaks of interest would be necessary to conclude that the model is preferable to previous models for real-world surveillance.

The interpretation of alarms using current outbreak detection strategies is difficult because the specificity is extremely variable. The fluctuations in specificity are due to changes on the same time scales in the variance of the ED utilization signal. Unlike previous models, the model developed here accounts for changes with time of not only the expected number of ED visits, but also of the variance of the number of visits. It is our hope that this provides a useful method for achieving a signaling strategy with known, constant specificity, enhancing the ability of public health practitioners to interpret the meaning of an alarm.

SW participated in the study design, carried out the study, and helped to draft the manuscript. JB, BB and KM participated in the design of the study and helped to draft the manuscript. All authors read and approved the final manuscript.

",0.7471331337036421
Establishment of CDC Global Rapid Response Team to Ensure Global Health Security,"The 2014-2016 Ebola virus disease epidemic in West Africa highlighted challenges faced by the global response to a large public health emergency. Consequently, the US Centers for Disease Control and Prevention established the Global Rapid Response Team (GRRT) to strengthen emergency response capacity to global health threats, thereby ensuring global health security. Dedicated GRRT staff can be rapidly mobilized for extended missions, improving partner coordination and the continuity of response operations. A large, agencywide roster of surge staff enables rapid mobilization of qualified responders with wide-ranging experience and expertise. Team members are offered emergency response training, technical training, foreign language training, and responder readiness support. Recent response missions illustrate the breadth of support the team provides. GRRT serves as a model for other countries and is committed to strengthening emergency response capacity to respond to outbreaks and emergencies worldwide, thereby enhancing global health security.","T he need to detect and respond to disease outbreaks before they spread has long been recognized as a priority because uncontained outbreaks can rapidly proliferate into international emergencies (1) (2) (3) . A jarring example was provided by the 2014-2016 Ebola virus disease (Ebola) epidemic in West Africa, in which ≈29,000 cases were identified and ≈11,000 patients died (1, 4) . Although most cases occurred in 3 countries, imported and locally transmitted cases were confirmed in 7 others, including the United States (5) . This experience highlighted needs for improved international collaboration and coordination and stronger national response capacity to rapidly detect and control major health threats at their source to ensure global health security (3, (6) (7) (8) (9) (10) .

The 2005 International Health Regulations (IHR 2005) , adopted by the World Health Organization, dictate that all member states should be prepared to detect and respond to public health threats and emergencies (11) . However, by 2012, <20% of countries reported full compliance with IHR 2005 (12) . To accelerate progress, several member states and international partners launched the Global Health Security Agenda, which outlines specific actions that countries can take to meet IHR 2005 requirements (6, 7, (13) (14) (15) . The US Centers for Disease Control and Prevention (CDC), in coordination with other US government agencies and global partners, is using its expertise and the Global Health Security Agenda framework to assist partner countries and strengthen global health security (16) .

CDC has a long history of responding to global public health emergencies, including polio and severe acute respiratory syndrome. It is internationally recognized for its expertise in disease detection, investigation, diagnosis, monitoring, and control, as well as management of public health emergencies (16) . Several groups within CDC work closely to identify and respond to public health threats. The Global Disease Detection Operations Center (GDDOC) is dedicated to the detection and monitoring of global public health events of international importance (17). GDDOC links external requests for assistance with the appropriate disease-specific CDC subject matter experts, who respond frequently to domestic and international outbreaks of diseases in their program domains. GDDOC also serves as an agency liaison to the Global Outbreak Alert and Response Network (GOARN) and supports the mobilization of subject matter experts through GOARN. In the field, responders work closely with governments and partners, including within Incident Management System structures or health clusters when established. Although mobilized CDC responders do not provide medical care, such activities are coordinated with organizations providing patient care.

Before the Ebola epidemic, when response operations exceeded subject matter expert program capacity, surge staff from the Epidemic Intelligence Service and other CDC programs were engaged and coordinated by the CDC Division of Emergency Operations (DEO). For larger, complex public health responses, the CDC director can authorize the activation of an agency-level Incident Management System, supported by the CDC emergency management subject matter experts in DEO and ordinarily based in the CDC Emergency Operations Center (EOC) (18). DEO also provides logistical and other support to response operations funded by GDDOC without activating the Incident Management System. At the time of the Ebola epidemic, CDC lacked a formal pool of on-call, trained responders who could rapidly mobilize for extended periods and in large numbers.

In July 2014, CDC activated its Incident Management System in response to the Ebola epidemic; as the largest agencywide response ever, it tested the limits of the agency response capacity (19) . During July 9, 2014-March 31, 2016, ≈4,000 CDC staff participated in the response in Ebola-affected countries; in countries at high risk for Ebola introduction; from CDC headquarters in Atlanta, Georgia, USA; or through other partner organizations (1). By March 31, 2016, CDC had supported ≈2,000 mobilizations of 1,400 personnel providing wide-ranging technical support, for ≈80,000 person-days of mobilization time (19) (20) (21) (22) (23) .

The size, scale, severity, and duration of the Ebola response highlighted key challenges to the efficiency and effectiveness of international emergency response efforts (Table) . Specifically, greater support from the international community was needed because of limited national capacity of affected countries to detect and respond to the outbreak, fundamental aspects of IHR 2005, and the diminishing healthcare capacity over the course of the epidemic (1). Despite CDC experience regularly providing assistance for smaller, shorter outbreaks, sustaining support over 21 months proved difficult. Because of limited CDC presence before the epidemic, weak or underdeveloped relationships with governments and partner organizations in affected countries hindered response coordination. Short mobilizations (typically 30 days) and frequent staff rotation in the field also disrupted development of long-standing relationships and continuity of response. However, longer mobilizations of such a large workforce could hamper staff members' regular duties, potentially affecting other CDC programs (1, 19, 20) . Additional challenges included identifying staff with the appropriate technical skills and foreign language abilities who were mentally and emotionally prepared for the austere conditions and ready and available to mobilize (19, 20) .

The challenges observed during the Ebola response underscored the need for a cadre of highly trained and experienced personnel who can rapidly mobilize to respond for extended periods (20) . To address these challenges, CDC established the Global Rapid Response Team (GRRT). We describe the establishment of GRRT, team structure, main activities, case studies, and lessons learned.

Before the Ebola epidemic ended, CDC began investing in its capacity to rapidly respond to public health emergencies. GRRT strategy Limited in-country capacity to detect and respond to disease outbreaks (1) Support the development of national outbreak detection and response systems Wide range of technical expertise required to address needs of a large outbreak response (1) Recruit team members with a wide range of technical expertise and experience Train responders in multiple technical areas for high-risk diseases Establishing working partnerships with governments and partner organizations for more efficient coordination (1, 19, 20) Train responders on working with partner organizations, incident management systems, cultural sensitivity, and foreign languages Recruit dedicated, ready responders who can mobilize for up to 6 mo for stronger partner relationships and improved coordination Short mobilizations (traditionally 30 d) and frequent rotation of staff disrupted continuity of response activities (19, 20) Recruit dedicated responders who are available and ready to mobilize for up to 6 mo if needed Expand the typical mobilization length of those in leadership roles Develop best practices and systems for information management in field response Responder preparation and readiness (19) Strengthen safety, security, and responder wellness training through a GRRT orientation Support continuous learning by offering frequent technical trainings on priority topics Track responder international travel-related mobilization requirements, training, and clearance compliance Obtain supervisor preapproval for mobilizations during on-call months Identifying appropriate responders (19) Roster GRRT responders and tracking skills and experience to match staffing needs Limited foreign language capacity (20) Develop a program to develop and validate foreign language capacity Logistical support for field efforts (19) Roster a group of dedicated and surge logisticians who can mobilize to provide support directly to responders in the field or coordinate with Atlanta-based logistics personnel to provide support *GRRT, Global Rapid Response Team.

In June 2015, CDC launched GRRT to address many of the challenges recognized during the Ebola response and to support other countries when their national response capacity is overwhelmed. Housed within the Emergency Response and Recovery Branch (ERRB), Division of Global Health Protection, at the CDC Center for Global Health, GRRT is an agencywide asset mandated to strengthen emergency response capacity. GRRT stands ready to provide technical and nontechnical support for public health responses worldwide; it is the result of collaboration across CDC.

GRRT comprises a small group of dedicated responders and a large group of agencywide surge staff. This model enables effective response to common events with a small number of experts while the team prepares for larger, rare events that necessitate substantial response. A total of 18 dedicated responders with public health emergency response expertise can immediately mobilize and remain in the field for extended periods. Included on this Atlantabased team are multilingual epidemiologists with expertise in public health and humanitarian emergencies, logisticians who support GRRT activities and coordinate with DEO during a response, highly experienced team leaders, and support staff. Outside Atlanta, 1 regional emergency advisor in West Africa is tasked with engaging national, regional, and global partners to build capacity to detect and respond to health threats in the region. This group of dedicated responders answers the need to improve response time for emergencies, establish stronger long-standing relationships with governments and key partners, and reduce disruption to the continuity of response activities from staff turnover in the field. GRRT surge capacity comprises >400 CDC staff members from around the agency; the goal is to support an emergency response with up to 50 staff members on short notice. Nearly 40 of the surge staff members routinely respond to humanitarian emergencies and build public health capacity as part of their regular duties in ERRB. They provide expertise in nutrition, emergency preparedness, surveillance, mental health, reproductive health, water, sanitation, and hygiene. The remaining surge staff vary widely in technical, language, and leadership skills and experience levels. They were recruited from 15 CDC centers, field personnel staff with state and local health departments, and overseas offices. International experience of the surge staff is a median of 2 years (mean 5 years), totaling 1,577 years combined. More than half have emergency response experience and ≈13% report having expertise in >1 foreign language. The most common occupations are epidemiologist, health scientist, public health advisor, and health communicator; surge staff have experience in nearly 30 different occupational areas.

Balancing the need to mobilize large numbers of agency staff, thereby possibly hindering their regular duties, with the need to ensure that existing programs maintain their operations is challenging (19) . To address both needs, surge staff are on call 2 months each year for emergency mobilizations. The assignment of these on-call months is determined by staff availability (avoiding months in which regular duties or personal needs require the staff to be in the home office) while evenly distributing the technical skills, foreign language, and experience levels across months. The resulting roster lists at least 50 surge staff with a similar distribution of skills and experience who are on call for mobilization each month.

Requests for assistance come from within CDC and from external partners. After receipt, requests are evaluated to determine the appropriate response mechanism. Requests meeting specific criteria are addressed through standard response mechanisms (e.g., GDDOC or subject matter expert mobilizations). GRRT reviews requests that do not meet the criteria or exceed capacity of other CDC groups. Decisions to respond are based on, among other considerations, the urgency, public health impact, and availability of appropriate staff to fill the request. After the decision to respond is made, responders are selected according to their skills, experience, and availability.

From September 1, 2015, through December 31, 2016, GRRT responders were mobilized 291 times for 10,148 person-days to work in 35 countries, territories, and the CDC EOC (Figure) . Most of the mobilization time was spent responding to outbreaks of Zika virus infection (65.0%), yellow fever (9.4%), Ebola (4.3%), cholera (3.9%), polio (0.5%), and measles (0.5%). The remaining time went to natural disasters (Hurricane Matthew [12.8%] and wildfires in Indonesia [3.2%]). Responders aligned themselves with existing response activities, working directly with ministries of health, the World Health Organization (WHO), CDC country offices, and other partners.

In addition to response activities, GRRT collaborated with ministries of health and external partners, such as the Africa CDC and the West African Health Organization, to assess and build national and international capacity to detect and respond to health threats, improving IHR compliance (320 person-days mobilized; median mobilization length 9 days). Activities included supporting the WHO Joint External Evaluations (24), developing rapid response team guidance, and facilitating response-related trainings.

Within CDC, GRRT works to build a sustainable, trained workforce. GRRT has designed a comprehensive training curriculum for surge staff that includes safety, security, soft skills, and technical training. GRRT increases responder readiness for rapid mobilization by defining and tracking training and logistical criteria. Continuing education is provided monthly for additional training opportunities beyond the baseline training received during a 1-day orientation. These trainings are hosted by subject matter expert groups throughout the agency and feature a combination of scientific topics, role-specific technical content for the field, interpersonal skills, and situational awareness updates depending on current emergency context. GRRT is also developing training focused on the principles of field team leadership in international response; the aim is preparing leaders to apply Incident Management System principles during mobilization while navigating the nuances of international field response. To enhance the agency's foreign language capacity, GRRT provides foreign language training opportunities online and in classrooms. Efforts to standardize foreign language testing are under way.

To illustrate the breadth of GRRT's response work and its influence on agency response capacity, we describe selected responses to the Zika virus epidemic, urban outbreaks of yellow fever, and Hurricane Matthew in Haiti. Case studies demonstrate GRRT ability to support large complex outbreak responses, fill response needs when CDC expert capacity is strained, and manage smaller responses without EOC activation.

In May 2015, an outbreak of Zika virus disease was reported in Brazil. In October, unusually high rates of birth defects, particularly microcephaly, were reported in areas with Zika virus transmission (25) . By January 2016, Zika virus had spread to 14 countries and territories in Latin America and the Caribbean, and CDC activated an Incident Management System to respond to the outbreak (26) . To support the response and address external requests for assistance, GRRT coordinated closely with subject matter experts, GDDOC, and DEO.

The complex Zika virus response, with its expansive affected geographic area and multidisciplinary technical needs, tested the CDC emergency response capacity soon after the Ebola experience. Investigations into the modes of transmission, birth defects associated with infection, and effective interventions required subject matter experts in vectorborne diseases, maternal and child health, reproductive health, and birth defects. Laboratorians strengthened Zika virus testing capacity and improved existing diagnostic tools. Health communication specialists developed messages in multiple languages for varied audiences, balancing the relatively mild symptoms of infection experienced by most persons with the devastating consequences of infection during pregnancy (26) . GRRT supported the agency response by mobilizing 117 responders to 9 countries and territories for 151 mobilizations and 6,597 person-days. A total of 69 mobilizations and more than half of the response time (3,556 person-days) were in the CDC EOC, where responders worked in Incident Management System leadership positions and as subject matter experts. The GRRT primary focus is international response, and responders are trained to work within varying cultural and environmental conditions outside the continental United States; however, the needs for assistance resulted in ≈90% of GRRT response time occurring in affected US territories and freely associated states. The GRRT roster, searchable by technical and language skills, facilitated the rapid identification of appropriate responders to fill response needs, particularly for speakers of Spanish and Portuguese, key languages in many of the affected areas. Although WHO declared the end of the emergency in November 2016 (27) , GRRT will support CDC Zika virus response activities until no longer needed.

In January 2016, the Angola Ministry of Health alerted WHO of an urban outbreak of yellow fever in Luanda Province (28) . Because of active cross-border travel in the region, yellow fever cases spread to neighboring Democratic Republic of the Congo (DRC). In March 2016, the DRC Ministry of Health notified WHO of another yellow fever outbreak.

The CDC GDDOC closely monitored the evolution of the outbreak and coordinated mobilization needs with GRRT. Traditionally, the CDC response to a request for support would be led directly by subject matter experts; however, at the time, these experts were already fully engaged in the CDC Zika virus response and had limited capacity to lead another vectorborne disease response. Therefore, GRRT, in close coordination with GDDOC and anchored by expert guidance from CDC subject matter experts, contributed to the requested technical assistance and surge presence in the field.

During April-November 2016, GRRT mobilized 15 responders to Angola for 742 person-days and 7 responders to DRC for 211 person-days. Responders, working closely with expert guidance from headquarters, provided epidemiologic and management support to country ministries of health; led the interagency Incident Management System in the field on behalf of WHO; led field investigations and epidemiologic surveillance activities; and supported logistical needs, border health assessments, and a mass vaccination campaign. Four responders were mobilized to WHO headquarters to coordinate with and support the WHO yellow fever outbreak response. By August 2016, the last confirmed cases of yellow fever were reported, and the disease did not spread to additional countries. The last GRRT mobilization ended in November 2016.

The yellow fever response highlighted the benefits of agency surge capacity, particularly when specialized technical expertise is needed for multiple responses in multiple locations. The response also underscored the benefits of accurately identifying responders with highlevel foreign language fluency but demonstrated the need to strengthen language capacity. Fluent speakers of Portuguese and French were identified for mobilization to Angola and DRC, respectively. However, because insufficient numbers of Portuguese speakers were available, fluent Spanish speakers partially filled the language gap.

On October 4, 2016, Hurricane Matthew, a category 4 storm, made landfall in southwestern Haiti, causing major damage and flooding, killing at least 540 persons, and displacing ≈175,000 persons (29,30). Torrential rains washed away roads, bridges, and crops, threatening food security, water safety, telecommunication capabilities, and medical services (29). The hurricane devastated healthcare facilities, including 46 cholera treatment centers (29), and disrupted key public health programs.

After the 2010 earthquake in Haiti, GRRT surge staff, particularly ERRB responders, had experience in Haiti, and a field response was coordinated with the CDC Haiti Country Office. Because the CDC EOC was already coordinating 3 simultaneous activations for Ebola, Zika virus, and polio, GRRT and ERRB implemented the Incident Management System in the field and in ERRB workspace at CDC headquarters. Simultaneously, the CDC National Center for Environmental Health activated an Incident Management System to coordinate the domestic response for the expected effects to the US coastline. To foster coordination within the agency, both activations, outside of the CDC EOC, were supported by DEO in the early phases of the response.

GRRT mobilized the first wave of responders to Haiti 2 days after the hurricane struck. In total, GRRT mobilized 31 responders to Haiti, 26 members to the Atlanta-based Incident Management System structure, and 2 liaisons to the US Agency for International Development Office of Foreign Disaster Assistance and the Pan American Health Organization. In total, 1,302 person-days were spent responding to Hurricane Matthew.

GRRT responders supported the response in a diversity of roles. Early in the response, while physical access to affected areas was still limited, GRRT members organized a rapid phone assessment to provide critical information on the current needs of affected populations. CDC responders partnered with the Haiti Ministry of Health to investigate cholera cases, assess damage to healthcare facilities, and reestablish affected disease surveillance systems. Atlanta-based support staff mobilized to the CDC Haiti Country Office to support the Incident Management System structure, enabling the Haiti-based staff to fulfill their regular duties. At CDC headquarters, responders worked as Incident Management System staff coordinating the agency response and information managers for the CDC Haiti Country Office.

The Hurricane Matthew response demonstrated successful coordination of international and domestic response activities across the agency without burdening EOC staff. The GRRT/ERRB Incident Management System deactivated in November 2016, and the last mobilization for the Hurricane Matthew response ended in December 2016. An after-action review was conducted to evaluate the response and improve GRRT processes for future activations.

The lessons learned from the Ebola epidemic forced many national and international organizations to reevaluate their emergency response capacity and processes. At CDC, these lessons contributed to the development of GRRT, a cadre of highly trained and experienced staff members and resources that provide response and surge capacity for CDC international emergency response operations. GRRT dedicated response staff enable rapid and longer mobilizations to establish and sustain working relationships with governments and partner organizations and to improve continuity of response activities. The large roster of >400 team members fosters a diversity of skills and experiences, and tracking of team member profiles facilitates matching technical skills and language capacity with response needs. GRRT support for CDC staff preparation and deployment readiness improves the speed at which qualified responders can be mobilized. GRRT capacitybuilding activities support countries' progress toward IHR 2005 compliance, particularly around workforce development, personnel deployment, and emergency operations, in alignment with DEO and subject matter expert activities for other action packages.

Despite progress, several challenges remain. The Zika virus and yellow fever responses highlighted the need for strengthened language capacity. GRRT language training and targeted recruitment of highly proficient staff aim to address this gap; other language training options are being explored. CDC response capacity can be developed further by providing additional disease-specific technical training, particularly for high-risk pathogens and epidemic-prone diseases that may warrant a large-scale response. This training will build disease-specific response capacity and enable a limited set of subject matter experts to guide response activities in multiple areas, as was seen during the yellow fever response.

Moving forward, GRRT continues to evolve and seek new ways to improve international response capacity in coordination with international partners. Ongoing identification and rostering of responders with appropriate technical and language skills to fill response needs is critical for rapid response. The GRRT surge capacity roster will need to be maintained to keep responder information current and replenished with future qualified staff. CDC response mechanisms can be further improved through continued coordination with agency emergency response personnel and streamlined mobilization processes. To ensure a cohesive approach, GRRT will continue coordinating with external partners during emergency responses by identifying clear roles and responsibilities for staff (20) . In addition, GRRT will continue supporting Global Health Security Agenda activities; building local, national, and regional response capacities; and supporting WHO, GOARN, and other international partners in global efforts toward development of international and regional public health rapid response teams. The lessons learned from the establishment of GRRT at CDC can serve as a model for the creation of similar response units in other countries.

The CDC GRRT was established to address lessons learned during the 2014-2016 Ebola epidemic. Since June 2015, GRRT has been actively engaged in strengthening agency and partner emergency response capacity by developing a capable emergency workforce. However, continuing these activities and sustaining the momentum of global health security requires ongoing resources to ensure that GRRT is ready to respond to future health threats. CDC is one of many global organizations that respond to outbreaks and emergencies; no one organization alone can effectively control global health threats. As the international emergency response community coordinates to build capacity around the world, GRRT will work diligently so that disease threats are rapidly detected, responded to, and controlled at their source, thereby ensuring global health security.

",0.7470167590982019
Characterizing Influenza surveillance systems performance: application of a Bayesian hierarchical statistical model to Hong Kong surveillance data,"Background: Infectious disease surveillance is a process the product of which reflects both actual disease trends and public awareness of the disease. Decisions made by patients, health care providers, and public health professionals about seeking and providing health care and about reporting cases to health authorities are all influenced by the information environment, which changes constantly. Biases are therefore imbedded in surveillance systems; these biases need to be characterized to provide better situational awareness for decision-making purposes. Our goal is to develop a statistical framework to characterize influenza surveillance systems, particularly their correlation with the information environment. Methods: We identified Hong Kong influenza surveillance data systems covering healthcare providers, laboratories, daycare centers and residential care homes for the elderly. A Bayesian hierarchical statistical model was developed to examine the statistical relationships between the influenza surveillance data and the information environment represented by alerts from HealthMap and web queries from Google. Different models were fitted for non-pandemic and pandemic periods and model goodness-of-fit was assessed using common model selection procedures. Results: Some surveillance systemsespecially ad hoc systems developed in response to the pandemic flu outbreakare more correlated with the information environment than others. General practitioner (percentage of influenza-like-illness related patient visits among all patient visits) and laboratory (percentage of specimen tested positive) seem to proportionally reflect the actual disease trends and are less representative of the information environment. Surveillance systems using influenza-specific code for reporting tend to reflect biases of both healthcare seekers and providers. Conclusions: This study shows certain influenza surveillance systems are less correlated with the information environment than others, and therefore, might represent more reliable indicators of disease activity in future outbreaks. Although the patterns identified in this study might change in future outbreaks, the potential susceptibility of surveillance data is likely to persist in the future, and should be considered when interpreting surveillance data.","The threat of pandemic influenza has led to extensive efforts to strengthen the global influenza surveillance [1, 2] , including the development of novel syndromic surveillance systems intended to identify potential outbreaks and track influenza in the population. Some focus on identifying influenza-like-illness (ILI) in clinical and other settings, while others search the Internet to identify disease outbreaks that might not have been recognized by the authorities [3, 4] . Having found high correlations with traditional surveillance systems and noting the benefits of timeliness and low cost, Internet-based surveillance systems have been widely recognized as important supplementary data sources for influenza surveillance [5] .

Infectious disease surveillance is a process; the data available for analysis reflects not only disease status in the population (the signal) but also other non-random factors (the noise). Our research has shown that decisions made by patients, healthcare providers, and public health professionals about seeking and providing healthcare and about reporting cases to health authorities are all influenced by the information environment, which we define as the information the population is exposed to through media, the Internet, social networks, and so forth. And since the information environment changes constantly, surveillance data systems that depend on decisions by patients and health professionals are likely to be biased, possibly in different ways [6, 7] . Epidemiologists and public health practitioners typically recognize these potential biases qualitatively, and present their analysis of the available data with appropriate caveats. Public health practitioners and clinicians are also aware of the surge in medical resource utilization caused by the ""worried well"" in response to media coverage of disease outbreaks [8] . Awareness of these biases is often lost, however, at higher levels [9] .

Some researchers assume that data on the information environmentsuch as Google searches and Twitter feedscan be used as proxies that directly estimate disease transmission in the population as long as the ""signal"" can be separated from the ""noise"" (trends in the data reflecting public awareness rather than disease transmission per se) [3, 10, 11] . Others simply view information environment data as a direct proxy for disease transmission. Surveillance systems that are fast, inexpensive, decentralized, automated, and utilize the power of information technology seem to satisfy the need for a magic bullet in the digital era. Whether such systems work as expected, or are another example of ""big data hubris"" [12] , remains an open question.

To address this issue in a rigorous way, the objectives of this study are to (1) develop a method to characterize the relationship between surveillance data and the information environment, (2) identify surveillance systems that more closely reflect actual disease trends rather than the information environment, therefore useful for tracking, and (3) understand the implications of the fact that some surveillance systems are more correlated with the information environment. In particular, we developed a Bayesian hierarchical statistical model that allows us to examine the relationship between surveillance data and information environment more formally than our previous analyses [6, 7] . This methodological paper is for public health surveillance specialists to better understand and improve the performance of data systems.

Our analysis uses influenza surveillance data and information environment proxy data (e.g. Google search and HealthMap) from Hong Kong during the pre-pandemic (2007) (2008) and pandemic (June -November 2009) periods. Rather than thinking of influenza-related web queries and news being direct indicators of disease transmission in the population, we view them as indicators of the information environment. We built a Bayesian hierarchical statistical model to estimate the correspondence between individual surveillance data and the information environment proxy data. Although not employed in this analysis, the model has the potential to incorporate epidemiological expertise through informed prior distributions. The findings have enabled us to understand how each surveillance system is related to the information environment and disease status, which should eventually help public health practitioners interpret the influenza surveillance data for situational awareness purposes, as well as prioritizing resources to different surveillance systems given the specific decision-making needs.

The local public health agency in Hong Kong -Centre for Health Protection (CHP), conducts surveillance monitoring influenza-like-illness among a network of over 40 private general practitioners (GP) and 64 public sentinel general out-patient clinics (GOPC). The out-patient ILI surveillance network reports the proportion of outpatients with ILI [Fever (>38°C, oral or equivalent) AND Cough or sore throat] on weekly basis [13, 14] . During the Influenza A (H1N1)pdm09 virus infection (pH1N1) outbreak, eight designated flu clinics (DFC) were operated from June 13 th 2009 to May 23 rd 2010 [15] , while the general out-patient clinics data were interrupted [16] . Hospital Authority, a public-sector organization that oversees all public hospitals, manages over 95% of the in-patient care [17] . The weekly numbers of hospitalization with principle diagnosis of pneumonia and influenza (P&I; ICD9 480-487) and influenza (ICD9 487) were obtained from the Hospital Authority for age groups of 0-4,5-14,15-64 and ≥65 years. Respiratory specimens from sentinel out-patient clinics, general practitioners are routinely submitted to the Public Health Laboratory Centre (PHLC) of CHP for sentinel surveillance purpose, while specimens from other outpatient healthcare facilities and hospitals are routinely submitted to the PHLC for diagnostic purposes [13] . Weekly data on numbers of specimen received, numbers of specimen tested positive for influenza and proportion of specimen tested positive were obtained from PHLC for 1997-2009. Reliable estimates of weekly incidence rate of pH1N1 for age groups (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46) (47) (48) (49) (50) (51) (52) (53) (54) (55) (56) (57) (58) (59) were constructed based on serological data and hospitalization data [18] . Summary of data sources are provided in Table 1 .

We used data from Hong Kong for this analysis because of the following reasons. First, Hong Kong has a population of high density and mobility. The heterogeneity in the disease transmission dynamics and the informational environment is less likely to be attributable to the geographic variability. Therefore, the influenza surveillance systems are presumably monitoring a relatively homogenous population under the same informational environment, which is an important presumption for this modeling approach. Also, in addition to the normal winter-spring peak, Hong Kong usually experiences a summer peak in July to August [19] , which potentially doubles the data volume. Since 2004, the Hong Kong Centre for Health Protection (CHP) has been monitoring influenza activity using multiple surveillance systems summarized in a weekly surveillance dashboard. These include sentinel surveillance system based at Accidents and Emergency Department of public hospitals, private medical practitioners, general outpatient clinics, traditional Chinese medicine practitioners, childcare centres and kindergartens and residential care homes for the elderly, as well as more traditional laboratory and influenza-related hospitalization surveillance systems. Given the variety of data types available, as well as the availability of reliable estimate of incidence rate based on serological study during pandemic period as gold standard [18] , Hong Kong is chosen for this project.

Two data sources were identified as proxies for the information environment: HealthMap and web queries using Google search engine. HealthMap is an online information aggregator and surveillance platform that identifies, characterizes, and maps events of public health and medical importance in real-time [20] . For this project, the first author manually coded Chinese language feeds using data filtered by disease category and geographic location. All feeds in both English and Chinese categorized as respiratory illness and related to Hong Kong area were verified for the correctness and completeness based on the original sources. Among 4,695 feeds that were extracted from the earliest date available, 2,166 feeds posted between January 1 st , 2007 and November 28 th , 2009 were extracted and tabulated for this study. The inclusion and exclusion criteria for HealthMap alerts data can be found in Additional file 1: Table S1 . Given the bilingual cultural environment in Hong Kong, we developed an original search matrix covering the disease and behavior related indicators in both English and Chinese. The ""seed"" for keywords are first identified based on literature review [11] . Each seed keyword is then assessed for correlated search terms through the Google Correlates tool [21, 22] . Additional keywords are then identified through snowballing, until no more new keywords are shown in the correlation list. By using Google Insight for Searchnow rebranded as Google Trendsthe search volume index is retrieved for each of the keyword individually for Hong Kong. The search volume index, also called the ""interest over time"" on the new Google Trends website, is the search volume of the individual search term divided by its maximum search volume during the user specified period [23] starting from 2004 to the latest available data.

Among the 144 original keywords through Google Correlate, 44 are in English and 100 are in Traditional Chinese, among which 74 are not disease related. 18 keywords have no search results from Google Trends due to the lack of search volume. Among the remaining 52 keywords, 20 are available on weekly basis and 32 are available on monthly basis. For keywords that only have monthly data, weekly estimates are estimated using the monthly value assuming the level stays the same through the month (Figure 1 ).

All search index data are standardized. And for quality control purposes, the ""Related terms"" shown on the Google Trends website for each search keyword is also examined and documented, since some keywords are significantly correlated with non-disease related terms. In Additional file 1: Table S2 , all keywords are listed under seven categories.

For influenza surveillance data, some researchers believe the flu activity in tropical areas like Hong Kong is present all year round [24] . To comply with the general terminology of flu surveillance, we still use the terms flu season and non-flu season, but define flu season as the mostly likely time period when flu activity peaks every year, based on literature review and official definition [19] . January to March and July to August are then defined as flu season, while the remaining periods are referred as non-flu season. The sole exception is the summer of 2007 when the flu activity in multiple data streams started peaking before July; the flu season is therefore readjusted to begin at week 25 

Bayesian hierarchical modelling framework allows for decomposing complex problems into subset of simpler problems governed by simple rules of probability [26] . Hierarchical modeling has many advantages such as allowing for multiple sources of data and accounting for parameter uncertainty; in particular, the Bayesian framework allows for the ability to consider scientifically meaningful structures and a priori knowledge regarding the model parameters.

Given the availability of reliable estimates of incidence rate of influenza during the pandemic period, we applied two models in this study.

For pandemic period, a model is constructed based on the hypothesis that, the values reported by each surveillance system reflect both actual disease status and public awareness. The information environment input can influence the level of actual disease trends being captured through a multiplier θ j,t , and the level of public awareness imbedded in the surveillance systems φ j,t for each surveillance system j at time t.

For data as proportions in the pandemic model (P model) model we have… Data model:

Process model:

Parameter model:

For data as counts in P model we have… Data model:

Process model:

In P model, X t denotes as the estimated influenza incidence rate of the whole population; Y j,t refers to the data from surveillance system j at time t. The log of Y j,t follows a normal distribution with mean of μ j,t and variance of σ 2 j . μ j,t has two components: θ j,tdenoted as ""completeness"", describes the component of actual disease trends that surveillance system j captures at time t which is further regressed on predictive variables of the information environment proxy data (k p l−1;θ;t ); φ j,tdenoted as ""excess"", estimates the component in the surveillance data that cannot be fully explained by the actual disease trends, and is regressed on another set of information environment predictors. β j,t,m and α j,t,n are the coefficients for the information environment proxy data (k p ) during the pandemic period. The parameter model, as stated in equation (5) and (6) , apply to both models for data as counts and proportions.

Due to the lack of estimated incidence rate during the non-pandemic period, a different model is constructed to assess the statistical relationship between surveillance data and the information environment. A linear regression model is fitted as follows:

Data model:

Process model:

Parameter model

where Y j,t refers to the data from surveillance system j at time t; s refers to either flu season or non-flu season; and np denotes non-pandemic period. Log(Y j,t ) follows a normal distribution with mean μ j,t and constant variant σ 2 j . k np i;t (i = 1,2,3) are the three information environment indices during non-pandemic period as described in Table 2 . ρ s,j,t,i is the indicator of the correspondence between the expected values of the log of the surveillance data μ j,t and each of the k terms.

Non-informative priors were used for both pandemic and non-pandemic models. A normal distribution with mean of 0, variance of 100 (or precision of .01) was used as prior distribution for α j,t,n , β j,t,m and ρ s,j,t,i . A Gamma distribution with mean 1 and variance of 10 or Gamma (.01, .01) was used as prior for τ 1=σ 2 j .

We conducted variable selection using a commonly used measure called the Deviance Information Criterion (DIC) as our primary method [27] . To confirm the DIC results, we also looked at other methods, such as singular value decomposition (SVD) and root-mean-square error (RMSE), which both agree with the DIC results. The predictor variables selected for the final model are listed in Table 2 . Further information on model selection is available in the Additional file 1: Table S4 .

Exploratory data analysis was conducted using Stata [28] and R (the Comprehensive R Archive Network) [29] . Both NP and P model are implemented in OpenBUGS [30] , an open-source software package for performing Bayesian inference using Gibbs sampling. In OpenBUGS, precision τ (1/variance) is used to define the distributions; the posterior results are also the estimation for the precision.

Model implementation includes the choice of priors, initial values, sampling procedures, and so forth. Non-informative priors were used for both pandemic and non-pandemic model. A normal distribution with average of 0, variance of 100 was used as prior for α j,t,n , β j,t,m and ρ s,j,t,i . A Gamma distribution (.01, .01) which has mean 1 and variance 100 was used as prior for σ 2 j . Using Markov Chain Monte Carlo (MCMC), we estimated the posterior distributions and reported means and standard deviations as well as 95% credible intervals (CIs) for each of the posterior distributions. In order to make sure the algorithm is robust to the choice of initial values and convergence is achieved, we used three sets of initial values. It should be noted that OpenBUGS has a feature to generate random initial valuesit is advised that the user chooses the initial values carefully, as randomly generated initial values may result in epidemiologically unreasonable prior densities or unreasonable collection of values for the posterior or the likelihood function, or very slow convergence for the algorithm. The initial values used in the model, along with the OpenBUGS code, are included in Additional file 2. For each parameter, 500,000 iterations are conducted; the posteriors are calculated after the first 5,000 iterations are discarded. Three chains converge quickly for both modelsusually stabilizing after 500 iterations. 500,000 iterations with three initial chains take about 3,000 seconds for the NP model and 15,000 for the P model. No thinning is applied in either model because autocorrelation is negligible.

To present these results graphically with 95% credible intervals (CIs), we estimate the posterior distribution of each coefficient for the correspondence between individual surveillance system and the information environment proxy data. Following standard practice in the Bayesian literature, we use the term ""significant"" when the 95% CI does not include zero, indicating evidence of a statistical correlation between the surveillance system and the information environment proxy data. For instance, in Figure 2A , flu-HA has a CI that is entirely above zero, which suggests a positive correspondence with one of the information environment proxy data streams -Google search for seasonal flu term. On the other hand the data for %ILI visits at general practitioners shows a lack of statistical correspondence with seasonal flu term searches because the CI includes zero.

To examine the sensitivity of the model to the choice of prior, models are run with three sets of hyper-parameters, as listed in Additional file 1: Table S7 . No evidence showing sensitivity issues about the choice of hyper-parameters was observed.

In the pandemic model we characterized the relationship between surveillance data and the information environment proxy data by estimating the ""completeness"" and ""excess"" in each surveillance data stream (Table 3 ). Figure 3 illustrates how biases can be introduced into the surveillance systems by drawing in not only non-flu patients but also increased numbers of flu patients, who otherwise might not present themselves to any of the surveillance systems. The proportion of the infected population captured by the surveillance systemsdefined as ""completeness""is usually not constant but fluctuates. A surveillance system might only capture a small proportion of the actual infected population. However, as long as the ratio stays constant and independent from the information environment, it can be a useful for tracking disease in the population. Out of eleven surveillance data streams, fiveflu-HA, percentage of ILI visits at designated flu clinics, notifiable infectious disease reporting, number of specimen tested positive, number of specimen receivedhave consistently significant coefficients for all four k's ( Figure 2 ). In other words, the proportion of actual infected cases being captured by these surveillance systems is strongly affected by the information environment. If the coefficient has a positive CI such as Google search term for seasonal flu term (Figure 2A) , it suggests more infected patient would be captured by the surveillance systems when web searching for ""flu"" and ""influenza"" increases. When the coefficient has a negative CI, such as Google search terms for flu medications ( Figure 2C ), it suggests that web searching related to flu medication is inversely correlated with the portion of flu infected population being captured. The category ""medications"" includes over-the-counter medications for ILI, which can be an indicator of self-diagnosis and self-treatment. Self-treatment behavior might be related to the decision of not seeking medical attention from healthcare practitioners, and therefore inversely correlated to the flu hospitalization, general practitioner patient visits, and so forth.

The remaining six surveillance data seriespercentage of ILI visits at general practitioners, fever surveillance at residential homes for the elderly, P&I-HA, P&I-HA(0-15 yr), P&I-HA(65+ yr), and the percentage of specimen tested positivehave more than one insignificant coefficient for the correspondence with the information environment proxy data (Figure 4 ). When the coefficient is insignificant, such as Google search term for seasonal flu (Figure 2A) , only general practitioners, residential homes for the elderly and P&I-HA(65+ yr) data series have coefficients that are insignificant, that is, are tracking the actual disease trends at a constant rate unrelated to the information environment. 

▪ Data as counts and as a proportion Log Y j;t À Á ∼N μ j;t ; σ 2 j μ j;t ¼ P s;j;t;1 þ P s;j;t;2 ⋅k np 1;t þ P s;j;t;3 ⋅k np 2;t þ P s;j;t;4 ⋅k np ""Excess,"" parameterized as φ j,t , is meant to account for the variability in the surveillance data that cannot be explained by incidence rate only. In the original conceptual model, φ j,t describes the non-flu cases captured in the surveillance systems. However, since the model itself does not have any mechanism to distinguish flu cases from non-flu cases, φ j,t is then interpreted as the overall biases in the surveillance systems due to public awareness of the disease. The parameters α j,t,2 to α j,t,7 , which can be interpreted as reflecting excess reported cases, are mostly significant for %ILI visits at designated flu clinics, notifiable infectious diseases reporting, the number of specimens received, and the number of specimens tested positive. In other words, these surveillance data streams are more likely to show an increase parallel to the information environment. On the other hand, general practitioners and residential homes for the elderly data series (Figures 5 and 6 ) are less likely to reflect excess cases. Among the hospitalization data series, P&I-HA(65+ yr) has insignificant coefficients for total number of alerts, number of unique alerts, and number of healthcare facilities alerts on HealthMap as well as Google search terms for pandemic influenza, suggesting that they are unrelated to the information environment. On the other hand, flu-HA has significant coefficients for almost all k's ( Figure 6 ). Posterior distributions for all coefficients can be found in Additional file 1: Table S8 .

During the flu season all surveillance data streams except the P&I HA (65+ yr) and residential homes for the elderly have significant coefficients for the public awareness index ( Figure 7C ), or in other words, are highly correlated with the public awareness of respiratory diseases. For sentinel surveillance systems at general out-patient clinics and general practitioner and children-specific surveillance systems at kindergarten/daycare centres and hospitals (P&I-HA (0-15 yr)), the differences between flu season and non-flu season are significant, which suggests these surveillance systems behave differently with respect to public awareness in the flu and non-flu seasons. The non-flu index coefficient is significant for flu hospitalization, number of lab tested positive specimens, percentage of specimens tested positive and percentage of ILI-visits at general practitioners during the non-flu season, which suggests these surveillance data streams are influenced by the information environment related to other diseases such as common cold when flu activity is low. During the flu season, however, the coefficient for nonflu index is insignificant for all surveillance systems ( Figure 7A ), which suggests none of the surveillance data streams is influenced by non-flu related information environment.

When the non-pandemic period data is divided into 2007 and 2008, and the model fit separately for each period, two sets of posteriors -2007 (week 1-52) and 2008 (week 53-104) are compared. For public awareness index coefficient, the heterogeneity between 2007 and 2008 is less obvious during the flu season as compared to non-flu season ( Figure 8B ). In other words, the correspondence between the surveillance data streams and public Figure 4 Posterior distributions of ""Completeness"" coefficient β j,t,m (m = 2,..,5; j = 4,…, 9) in ""completeness"" parameter θ j,t (multiplier for the estimated incidence rate) as measure of correspondence between surveillance and the information environment proxy data for surveillance systems that are less correlated with the information environment during the pandemic period. β2 is the coefficient for Google search index of seasonal flu terms; β3 is the coefficient for Google search index for symptoms; β4 is the coefficient for Google search index of medications; β5 is the coefficient for Google search index of non-flu terms.

awareness is relatively stable from year 2007 to 2008 when flu transmission is active. For the non-flu index ( Figure 8A ), during the flu season, P&I-HA, P&I-HA(0-15 yr) and lab (#spm) have significantly different coefficients, which suggests when the flu activity is high, the increased reports of P&I-HA and pediatric P&I-HA, plus the number of specimens sent to the lab is correlated with the information environment related to non-flu respiratory diseases. As shown in Figure 9 , when comparing the four hospitalization data streams side by side, flu-HA looks more similar to P&I-HA(0-15 yr), and P&I-HA is more similar to P&I-HA(65+ yr). Figure 6 Posterior distributions of α j,t,m (m = 2,..,7; j = 4,5,6) in ""excess"" parameter φ j,t as measure of correspondence between surveillance and the information environment proxy data for surveillance systems that are less correlated with the information environment during the pandemic period. α2 is the coefficient for total number of alerts at HealthMap; α3 is the coefficient for the total number of unique alerts at HealthMap; α4 is the coefficient for number of healthcare facilities related alerts at HealthMap; α5 is the coefficient for %RSV from virological surveillance; α6 is the coefficient for Google search index of authority; α7 is the coefficient for Google search index of pandemic influenza terms. Figure 5 Posterior distributions of α j,t,m (m = 2,..,7; j = 1,…,11) in ""excess"" parameter φ j,t as measure of correspondence between surveillance and the information environment proxy data during the pandemic period. α2 is the coefficient for total number of alerts at HealthMap; α3 is the coefficient for the total number of unique alerts at HealthMap; α4 is the coefficient for number of healthcare facilities related alerts at HealthMap; α5 is the coefficient for %RSV from virological surveillance; α6 is the coefficient for Google search index of authority; α7 is the coefficient for Google search index of pandemic influenza terms.

Posterior distributions for all coefficients in NP model can be found in Additional file 1: Table S10 .

In their efforts to develop new methods for influenza surveillance, researchers have considered many different data sources, most of which already exist in electronic form. Some derive from traditional surveillance approaches while using influenza-like-illness (ILI) and other data that do not require laboratory diagnosis. Others, such as the Global Public Health Intelligence Network (GPHIN) and HealthMap, search results from the Internet and other media sources via automated algorithms to identify disease outbreaks that might not have been recognized by the authorities [3, 4] . Google Flu Trends uses influenzarelated search queries to model flu activity [31] , while some other studies try to capture ILI through microblogging platforms such as Twitter [32] . New terms such as ""Internet-based surveillance"", ""digital disease detection"", and ""inforveillance"" have been introduced to describe such public health surveillance practices [20, 10] .

Recent studies have found a high correlation between syndromic surveillance and traditional influenza surveillance data [33] [34] [35] [36] [37] [38] . Internet-based surveillance such as Healthmap and Google Flu Trends have claimed success in capturing pandemic flu outbreak [3, 4] and tracking flu activity [11] days to weeks ahead of standard Centers for Disease Control and Preventions (CDC) systems. With its advantages of timeliness and low cost, Internet-based surveillance systems have been widely recognized as important supplementary data sources and widely used as the baseline standard for evaluating new influenza surveillance systems [5, [39] [40] [41] .

Internet-based surveillance data, however, reflects both a ""signal"" reflecting actual disease trends and ""noise"" caused by changes in public awareness. How to accurately and effectively separate the ""signal"" from the ""noise"" becomes one of the biggest challenges in analysing internet-based surveillance data. Some researchers have developed natural language processing algorithms to classify this information automatically [34, 42] , some use crowd-sourcing platforms to engage Internet users in tagging data manually [43] , and some use both [44] . The curated data, which is thought of as reflecting actual disease status, is then compared to the traditional surveillance data and tested for correlations [31, 32, 34, 35] . This approach, however, does not prove the validity of the new surveillance method since both data streams may reflect the same information environment, therefore be biased in the same way. For instance, Google Flu Trends, which had been performing well in tracking CDC surveillance data, dramatically over-estimated the flu activity in the United States in 2012-13 flu season [37] . The overestimation might be due to the extensive media coverage of flu during the winter holiday season [45] , but raises the question of how well Internet-based surveillance systems reflect flu activity per se, rather than other factors such as public awareness. Figure 9 Comparison of posterior distributions for non-flu index, illness index and public awareness index as measure of correspondence between surveillance and the information environment proxy data for hospitalization data series during the non-pandemic period. A) flu-HA; B) P&I-HA; C) P&I-HA(0-15 yr); D) P&I-HA(65+ yr).ρ2 is the coefficient for non-flu index, ρ3 is the coefficient for illness index, and ρ4 is the coefficient for public awareness index.

Our analysis uses a Bayesian hierarchical statistical model to estimate the correspondence between individual surveillance data and the information environment proxy data. The model structure is developed based on an understanding of disease surveillance being a process rather than a direct reflection of disease status per se. The statistical model does not directly describe disease transmission dynamics, and the goal is not to estimate parameters or flu activity level. Rather, as a characterization tool, this analysis reveals how surveillance systems ""behave"" differently under changing information environments. Similarly, the purpose of model fitting is not to identify the perfect model with the best fit for the data. Rather, the goal is to find a model that captures the relationship between the surveillance systems and the information environment that is consistent with epidemiological expertise and practitioners' understanding of the actual disease process, and thus one that is likely to be applicable in the future.

Among all the influenza surveillance data that we studied, we found some surveillance systems that more consistently corresponded to the information environment proxy data than others. The level of correspondence with the information environment is associated with certain characteristics of the surveillance data. General practitioner (%ILI-visit) and laboratory (%positive) seem to proportionally reflect the actual disease trends and are less representative of the information environment. Surveillance systems using influenza-specific code for reporting tend to reflect biases of both healthcare seekers and providers. This pattern is what we would expect to see if the information environment were influencing the observable data.

Characterization of surveillance systems using the pandemic model When looking at ""completeness"" only, three types of surveillance systems show a certain level of stability in the changing information environment. Surveillance data in percentages such as percentage of specimen tested positive, percentage of ILI visits at general practitioners and percentage of fever at residential homes for the elderly tend to have insignificant CIs for ""completeness."" Surveillance systems that use less specific diagnostic and reporting codes such as ""pneumonia and influenza hospitalization"" and ""fever at residential homes for the elderly"" are also less likely to be influenced by the search index and the media coverage. Surveillance systems monitoring the elderly tend to be less susceptible to the information environment as compared to those monitoring children, which can be observed by the comparison between P&I-HA(0-15 yr) and P&I-HA(65+ yr) (Figures 4 and 5) . For surveillance systems that meet more than one criterion, the correlation with the information environment is weaker than those only meet one.

Surveillance data represented as percentages seem to be less correlated with the information environment, perhaps because the nominator and denominator change in the same direction in response to the information environment. Reflecting general practitioners' role as the gatekeeper of the healthcare system, general practitioner visits are predominantly influenced by only one layer of decision-makingpatients seeking medical attention. Since patients usually do not have the ability to distinguish influenza from other viral respiratory infections themselves, flu and non-flu infections may be just as likely to be presented to general practitioners. This pattern might not hold during the early stage of a pandemic, when the spread of novel influenza virus may not keep up with the spread of awareness, possibly leading to a negative correlation between the percentage of ILI-related general practitioner visits and the information environment. However, due to insufficient data volume, the model failed to run when segmenting the pandemic period into the early (summer) and late (fall) stage.

Percentage of specimen tested positive, on the other hand, is often used as the ""gold standard"" for influenza surveillance. As a surveillance system with a specific case definition based on confirmed virological testing, as well as being in a percentage format, the percentage of specimens tested positive is likely to provide the most reliable estimates of flu activity. However, an individual case has to go through at least two layers of decision-makingpatient's decision on healthcare seeking and physician's decision on sampling and diagnosis. Thus, it is possible that when the physicians are ""sensitized"" by the media and official guidelines, they may actively look for cases that fit the clinical definition of influenza and sample them for laboratory testing. This effect is more obvious in the count data for flu-HA, but may also influence the percentage of specimen tested positive as shown in Figures 2, 4 

Another pattern is the difference between influenza specific and non-specific surveillance systems. Flu hospitalization data seem be more correlated with the information environment as compared to pneumonia and influenza together. As discussed above, once sensitized, physicians are more likely to take samples from patients, and to use diagnostic codes that are specific for influenza, especially in the subpopulations that are considered to be more vulnerable to the pH1N1 virus during the flu pandemic. During the early stage of the pandemic, children and young adults were considered to be more susceptible to the novel influenza virus, which might contribute to the observation in our study that the pediatric P&I-HA tends to be more correlated with the information environment compared to the elderly.

We also observed a difference in the level of correspondence to the information environment between surveillance systems that monitor elderly versus other age groups. One possible explanation is disparities in information literacy and access to computers and the Internet among different age groups. Google searches are likely to be driven by subpopulations of specific demographic characteristics and socio-economic status, such as young and middleaged people who have easy access to digital devices as an information portal, as compared to the elderly who live in residential homes. Although children may have limited information literacy and access, their parents are likely to take immediate action in response to the information related to children's health.

As for the ""excess"" parameter, φ j,t , ILI visits at general practitioners, percentage of specimen tested positive and percentage of fever at the residential homes for the elderly show the least significant correlation with the information environment proxy data, including the number of total HealthMap alerts, unique alerts, healthcare facilities related alerts, lab(%RSV), search index for authorities and pandemic flu terms ( Figure 6 ). The lack of significant correlation might be due to the percentage format of these data streams, since data in counts usually have significant CIs. The coefficients for the search terms for pandemic influenza are all positive for the surveillance systems represented as counts, which suggests a positive correlation between the biases in those surveillance systems and public awareness of pH1N1.

In general, the fewer layers of decision-making, the less correlated the surveillance system is with the information environment. The traditional ""gold standard"" surveillance systems, such as hospitalization and virologic surveillance, are subject to the biases introduced by healthcare professionals. The more specific and ad hoc the diagnostic and reporting codes are, the more likely it is influenced by the information environment. Surveillance data in percentage format tends to capture actual disease trends in constant ratio, less influenced by the information environment than data in counts.

For the non-pandemic period we developed three indices to describe the relations between surveillance data and information environment proxy dataan actual disease status indicator (illness index) plus public awareness of both influenza and other viral respiratory diseases (public awareness index and non-flu index). Since we are most interested in the correspondence between the surveillance data and the public awareness index, the posterior distribution of the public awareness index coefficient is compared among different surveillance data streams, segmented by flu/non-flu season and year.

The public awareness index used in the NP model is a collection of search keywords and categories of HealthMap alerts that are most likely to be associated with public awareness of influenza outbreaks, such as search volume for influenza outbreaks and the number of alerts of school-based outbreaks. Surveillance systems are in general more correlated with public awareness during the flu season as compared to the non-flu season. When observing increasing flu activity in the community or from news media, one may get sensitized and tend to seek medical attention when feeling sick. The exceptions are two surveillance systems that monitor predominantly the elderlyflu surveillance at residential homes for the elderly and P&I-HA (65+ yr) ( Figure 7B ). These two surveillance systems are relatively less correlated with the public awareness in most cases, and show more stability from year to year ( Figure 8B ). During the non-flu season, the majority of surveillance systems seem not to be influenced by public awareness except for the flu associated hospitalization, number of specimens tested positive, percentage of specimens tested positive, and P&I-HA(0-15 yr) ( Figure 7B ), which can also be observed in the year to year comparison graph ( Figure 8B) .

Beyond comparing the correlation between surveillance and information environment proxy data individually, we also made an exploratory effort to assess the similarity among different surveillance systems by using the characterization tool and the identified evidence of potential biases in clinical practice. When comparing the four hospitalization data streams side by side, we observed that flu-HA looks more similar to P&I-HA(0-15 yr) ( Figures 9A and C) , while P&I-HA is more similar to P&I-HA(65+ yr) ( Figures 9B and D) . The patterns are consistent in flu and non-flu season and in different years (Figures 8 and 9) , and correspondent to the pandemic model. In the pandemic model, we observed that flu-HA is more correlated with the information environment than P&I-HA, while pediatric P&I-HA also seems to be more correlated with the information environment than the P&I-HA for the elderly. Also, when replaced the incidence rate of all-age with 5-14 yr, data fits better for flu-HA, general practitioner ILI-visits, notifiable infectious disease reporting and P&I-HA(0-15 yr) (Additional file 1: Table S9 ). During the pandemic flu outbreak, since the children and young adults were considered to be at higher risk than the elderly, physicians may tend to order more laboratory testing for pediatric patients [7] . During the non-pandemic period, the same clinical practice might still prevail. Since pediatric mortality is a reportable condition, clinicians are more likely to order laboratory tests and use a specific diagnostic code if the test results are positive. The elderly patients, however, who usually have non-specific clinical manifestation for respiratory diseases and lower viral loads [46] , are less likely to be sampled, less likely to have a positive result if tested, and usually given a less specific diagnostic code such as ""pneumonia and influenza"".

It is worth noting that the data volume for flu-HA and pediatric P&I-HA are both relatively low during the non-pandemic period, which might also contribute to the similarity between the two data streams. Also, there is a much lower Google search volume and fewer HealthMap alerts in 2007 compared to 2008, possibly due to the introduction of smartphone and rapid growth of the Internet itself from 2007 and onwards [47] .

Before the 2009 pandemic flu outbreak, the age-stratified flu-HA was not collected in Hong Kong, therefore it is difficult to test our hypothesis of the biases in clinical practice. The implications of this finding are (1) flu hospitalization might not be representative for all age groups, and (2) it is important to collect age-stratified flu hospitalization data, not only for monitoring the susceptibility of the subpopulation, but also for assessing potential biases in practice.

This study also has implications for the use of information environment data for disease surveillance. Advances in information technology have made a wide range of data available to public health researchers and practitioners, offering the promise of improving current surveillance systems, generating more sensitive and timely warnings of disease outbreak or providing more accurate estimates of disease transmission. For this potential to be realized, however, the characteristics of these new data sources must be understood before they are used in sophisticated statistical models. Olson and colleagues have suggested, for instance, that Google Flu Trends' impressive retrospective correlation with ILI surveillance data may be a product of over-fitting by ""fishing"" through numerous search term combinations as part of data mining. Moreover, Google Flu Trends' tendency to miss the beginning of an outbreak and its poor accuracy at the local level also limits its application in providing early warning and situational awareness [37] . The importance of understanding the nature of the data and the environment in which the data is generated may be overshadowed by researchers' and practitioners' enthusiasm for data availability (i.e. ""big data"") and purely statistical patterns (often ignoring confounding variables or underlying processes).

The approach we used in this study is limited by availability of data, which influenced how we evaluate the model and interpret the results. Since the reliable incidence rate estimate is not available before the 2009 pH1N1 outbreak, we developed different model for non-pandemic period, the results of which are, to some extent, consistent with what have been found in the pandemic model. Google search volume for some keywords in Hong Kong is not large enough to generate a search index; or sometimes is not of the same time resolution as the weekly surveillance data. More than half of the search keywords, for instance, are only available on monthly basis. Also, we have not exhausted all the possible combinations for keywords, HealthMap alerts count, and different time lag.

Given the noisy data and the lack of disease transmission mechanism, our search for the best fitting model might have led to over-fitting. The selection and aggregation of predictors, therefore, is guided by both practical knowledge and model performance comparison, in order to achieve a balanced model version that is of relatively good fit and meaningful for practitioners to interpret. For instance, the predictors are grouped in a relatively arbitrary manner, but the selection process for the pandemic model was blinded from the results of posteriors for each parameter before the final model version was selected.

In this study, we estimated the correspondence of multiple influenza surveillance data streams with indicators of the information environment, and the results suggest that most influenza surveillance data, to some degree, reflect public awareness as well as actual disease status. For instance, individuals who are aware of the on-going transmission of influenza are likely to search for information for prevention and self-diagnosis purposes, and may tend to seek medical attention once feeling sick. Thus, although it has not been recognized and studied systemically, many influenza surveillance systems may reflect changes in the information environment as well as actual disease trends. And although the data we analysed are all from Hong Kong, the underlying mechanisms are not specific to that region, so the problem may be widespread. Indeed, Zhang and colleagues and Stoto found similar patterns using less formal methods in the United States [6, 7] . Some surveillance systems seem to represent public awareness more than actual disease status. In particular, ad hoc surveillance systems set up during the early ascertainment of pH1N1 outbreaksuch as the walk-in clinics for ILI, making pH1N1 as a new condition for notifiable infectious diseaseare more correlated with the information environment than other surveillance systems that we identified in Hong Kong. Such results help us better understand and characterize influenza surveillance systems, which can be used in data interpretation, resources allocation, new surveillance systems design

",0.7452772050510649
Digital epidemiology and global health security; an interdisciplinary conversation,"Contemporary infectious disease surveillance systems aim to employ the speed and scope of big data in an attempt to provide global health security. Both shifts -the perception of health problems through the framework of global health security and the corresponding technological approachesimply epistemological changes, methodological ambivalences as well as manifold societal effects. Bringing current findings from social sciences and public health praxis into a dialogue, this conversation style contribution points out several broader implications of changing disease surveillance. The conversation covers epidemiological issues such as the shift from expert knowledge to algorithmic knowledge, the securitization of global health, and the construction of new kinds of threats. Those developments are detailed and discussed in their impacts for health provision in a broader sense.","The term digital epidemiology is in this special compilation defined by Marcel Salathe as epidemiology that uses data that was generated outside the public health system, i.e. with data that was not generated with the primary purpose of doing epidemiology (Salathe 2018) . Arguably a narrow definition, we will use this conceptualization as the starting point for our conversation. The so defined digital epidemiology promises faster detection of disease outbreaks and improved surveillance as well as reduction in administrative and financial burden, among other things. At hand in the following conversation is less the question if those promises are kept. Instead we are interested to reflect epistemological/methodological, ethical/legal, social/political, and organizational aspects and implications corresponding to the promise of digital epidemiology. What will be the relationship of traditional and digital epidemiology? Will a possible change influence the scope of Public Health and Global Health? Tim Eckmanns, Henning Füller and Stephen Roberts discuss political implications of digital epidemiology.

Digital infectious disease early detection systems such as the ProMed-mail, Global Public Health Intelligence Network (GPHIN), HealthMap, the now closed Google Flu Trends or the syndromic surveillance system ESSENCE are central elements of global public health surveillance. However, with increasingly digitalized (algorithmic) global public health surveillance systems and related data-driven epidemiological analyses (e.g., Digital Epidemiology and other research methodologies), there seem to emerge epistemological shifts, as well as methodological ambivalences and diverse social and political effects.

You, Henning and Stephen, both work from a social (or rather political) science perspective on the societal implications of Digital Epidemiology, which is shaped by multiple imperatives, e.g., of 'global health security' as well the potentials of big data.

Over the past two decades, I would argue, we have seen an unleashing of the algorithm across practices of health security and surveillance. Algorithmically-guided infectious disease surveillance systems have proliferated across global health geographies, seemingly in response to a series of interconnected and complex transformations within global health governance (GHG), as well as the practice of international relations and international security. We have seen the rise of a seeming 'epidemic of epidemics' from the late twentieth century onward, including the emergence of HIV-AIDS, novel strains of avian and swine influenza, SARS, Ebola, MERS, the Zika virus, and the re-emergence of cholera, polio and multi-drug resistant Tuberculosis across low and middle income countries (LMICs) clinical and public health surveillance practices with their routinised processes of data collection, analysis, and dissemination from national health institutes have increasingly fallen out of pace with the capacity to timely identify the globalised spread of novel and re-emergent pathogens.

Correspondingly, the rise of the digital era, resultant from technological interconnectivity and innovation, has generated infinite, voluminous and diverse data at a rate never feasible in history. Between 2016 and 2018, 2.5 quintillion bytes of data have been produced continually, every day in the mere span 24 months (IBM 2018). Celebrated for the capacity to connect the operational 'dots' between these seemingly unintelligible and largely unstructured streams of data in the surveillance and identification of infectious disease outbreaks, the algorithm has emerged as salient and novel technology of security in the pre-emption of pandemic threats in the twenty-first century.

To firstly illustrate this shift, in late November 2002, the Global Public Health Intelligence Network (GPHIN), a semi-automated online health surveillance system, which piloted the use of retrieval algorithms to filter international media sources, identified the early reporting of a form of atypical pneumonia circulating in Guangdong Province, China. The 'algorithmic gaze' of GPHIN identified the origins of the severe acute respiratory syndrome (SARS) in advance of 3 months of traditional public health and governance authorities. More than a decade following the rapid spread of SARS, HealthMap, an online health surveillance system, identified again, via algorithmic processing of digital data streams, the emergence of a mysterious hemorrhagic fever occurring in Macenta, Guinea. HealthMap critically captured and presented strategic epidemic intelligence detailing the emergence of the Ebola Virus Disease (EVD) on 14 March 2014, 9 days in advance of the official notification of outbreak by the Guinean health authorities.

Digital epidemiology -from expert knowledge to 'knowledge without truth'

The increasing integration of algorithmically-driven infectious disease surveillance systems contemporary logics of health security are critical and significant for a number of reasons (Roberts and Elbe 2017) . First, reflective of a growing recourse to the harnessing of novel information sources to contain pandemic illness, the WHO, via the revision of the International Health Regulations (2005) , has clearly authorised the collection, assessment and utilisation of non-governmental sources of epidemic intelligence and data (Article 9.1), without prior clearance of member-states. (World Health Organization 2008) In this regard, the algorithm emerges a new purveyor of varied, voluminous and expedited data sources to be leveraged in the risk assessment of future infectious disease threats. Epistemically, what we can see is how the centralisation of the algorithm within security technologies such as digital disease surveillance systems re-contour previous relations and understandings of knowledge production, the practice of surveillance and the regulation of pandemic risk. The cultivation of knowledge to address the contingent within past 'regimes of truth' were largely sustained, as illustrated by Foucault by the 'avalanche of statistical numbers' (Hacking 1982) . Increasingly however, within these contemporary security technologies, the 3 Vs of Big Data (volume, variety, and velocity) are now being mined, scanned, and reassembled via algorithmic processing of data to produce findings and alerts on the next pandemic. Information and 'truths' about the physical world and the contingent threat of infectious disease are increasingly extracted in the forms of signals and signs of the realm of the digital, and no longer solely generated from statistical processes via human analysis.

Furthermore, as the conceptual work of Antoinette Rouvroy (2011 Rouvroy ( , 2013 Rouvroy ( , 2015 , has demonstrated, algorithms have emerged within health surveillance technologies as purely 'rational' or 'objective' instruments of forecasting, indifferent to the causes of phenomena and seeking only to accrue maximal reservoirs of data to address that which constitutes the contingent or the uncertain. What this means therefore is information and knowledge generated by these algorithmic techniques now appear to bypass the traditions of human assessment, analysis, hypothesis, testing and trial which were essential to the statistical calculation of the contingent. Rouvroy has referred to this dissemination of this new form of understanding future-situated uncertainty as ""knowledge without truth"", represented in the context of this discussion by disease tracking systems including GPHIN and HealthMap, which have, with upward intensity sought to apprehend infinitely expanding data sources through an intensified recourse to algorithmic-suffused disease surveillance. What is absolutely vital to emphasise here is that amid the widespread deployment of big data analytics and increasingly sophisticated algorithms for tracing the next outbreak, little critical assessment has been formulated by global health security theorists and practitioners on the ramifications 'digital' turn of health surveillance and the implications of big data and algorithmic surveillance practices on individuals, populations and states.

Thus, these continued shifts towards employing advanced algorithms to make sense of unprecedented amounts of information (Leese 2014) , across practices of contemporary disease surveillance must be continually matched with equally robust interrogations of the unforeseen or unprecedented implications of securitization by algorithms in the realms of ethics, law, politics and society.

Thanks, Stephen for underlining the function of algorithms in current approaches of disease surveillance in global health policies. In addition, I pose that the rationale of an 'emerging diseases world view' (King 2002) is similarly influential for public health surveillance on a domestic scale. Especially in the US, systems of syndromic surveillance have been explicitly employed to answer the challenges of the 'next pandemic' with a new algorithmic form of public health monitoring. Going a bit into the details of one specific example of Syndromic surveillance, I want to illustrate the problem of ""knowledge without truth"" Stephen mentioned above. The argument is that those systems 'call back' in several ways, influencing both truth claims and practices of public health provision. My empirical example is a study on the use of the ""Electronic Surveillance System for the Early Notification of Community-based Epidemics"" (ESSENCE) in the U.S. National Capitol Region, an application of syndromic surveillance that received considerable attention as a pilot project (Füller 2018) . Technically the ESSENCE system provides the server infrastructure to draw together diverse data-sources that are considered indicative for public health. Its 'syndromic' approach consists in the integration of several so called surrogate data, signals of diseases or public health problems generated before a confirmed medical diagnosis (Velasco et al. 2014 ). In the case of ESSENCE, such surrogates are for example emergency department chief complaints, daily over-the-counter sales of the two big pharmacy chains CVS and Rite Aid, reports on absenteeism data gathered from public schools and others. ESSENCE claims to provide an unmatched situational awareness partly due to the near real-time nature of those data (collected and reported at least daily). Given the amount and unstructured nature of this data, the system employs algorithms to continually search the gathered data-stream for unusual patterns and a GUI to visualize and map resulting alerts. If there is an unusual coocurrence of for example the sale of headache pills and school absenteeism in a region, the system will flag out a warning. Importantly, the base for this pattern recognition are at no times diagnosed health problems but assumptions generated through the association of different data sets. Eventually the system promises to automatically provide an early notification of any unusual public health event before it has been medically diagnosed (Fearnley 2008) .

The turn towards infection control and surveillance in public health and the introduction of syndromic surveillance systems have both been contested early on and from several vantage points (Reingold 2003) . The focus here is to point out the performative character of technologies and their related practices in altering the goals and modes of public health provision.

My argument centers on the fact that the system is constantly producing health related truth claims. Whether it is just quietly monitoringas it does most of the timeor in the rare cases that it is flagging out a public health emergency, the system claims a certain truth about the health of the monitored population. In both cases, the algorithmically produced knowledge becomes performative in different ways. Both forms of truth claims illustrate the 'knowledge without truth' problematic Stephen already mentioned.

On the one hand, those systems introduce a new expectation and a demand to constantly assure the normal state of affairs. New technologies of surveillance are employed to be able to illustrate an absence, to be able to constantly assure that there is nothing to worry about, as Kezia Barker argues (Barker 2014) . In order to be aware of unusual events, resources, work and infrastructure are invested to extensively monitor the routine state of public health. But this additionally generated knowledge does not provide a qualified, actionable truth about the state of public health. Trying to see short-term events, those systems measure against the baseline of the 'normal'. In its usually quiet mode of monitoring, the systems make the implicit claim about a 'normal' , 'well functioning' , 'unproblematic' state of public health, ignoring any long-term and structural health issues.

On the other hand, In the case of actually flagging out an incidence, automated monitoring systems such as ESSENCE are problematic in their rendering of disassociated facts into medical truths. The threat of an emerging public health event is especially burdening for the executive branch of the local state. Decision-makers are pressured to act early, at best before the expected cascading of an infection gets out of control. This expectation makes it tempting to base a decision on the syndromic signals as they are readily available and -through the included mapping tool -often clearly localized. While those signals are explicitly handled as an additional but clearly undiagnosed source of information among epidemiologists and public health experts, for the executive branch they have a tempting appeal of providing a near real-time situational awareness and as such an actionable grasp on the emerging public health event. Importantly, using ESSENCE as a base for decision-making approaches the signal as if it was an authoritative medical fact instead of just an indicator for the clustering of certain syndromes. The danger of misinterpretation as the algorithmically generated knowledge travels contexts may result in wrongly employed public health interventions with negative social effects. Besides the problem of false positives prevalent to those systems (Fearnley 2008) the system always suggests a spatialized source of the problem that may or may not be medically justified. Employing public health interventions based on those seemingly objective and localized realities can easily mean the wrong allocation of scarce resources and attention or effect an unjustified stigmatization of a 'problematic' area.

From my perspective, as a medical infectious epidemiologist and public health expert who advises on the development of new surveillance systems and who constantly needs to be aware of their effectiveness as well as the consequences of their use, Stephen and Henning's analyses offer extremely important contributions on how to think about and evaluate increasingly digitized health-and infectious disease control. To add to this, I would, in the following, like to make a few further comments about the epistemic and political aspects of the digitization of infection control. In particular, I am able to speak to activities and experiences at Germany's national public health institute, the Robert Koch Institute (RKI), and to those at the World Health Organization (WHO), where I was within the framework of the West African Ebola outbreak (2013 to 2016) (Owada et al. 2016) .

First of all, I agree with Stephen's analysis that there is the risk, as a result of the successive propagation of algorithmic approaches and technologies for infectious disease control, an epidemiology traditionally based on diagnostic findings and controlled statistical processes is becoming increasingly marginalized and, in parallel, the necessary verification loops are being replaced in favour of 'Big Data' ideologies and trends of Dataification. 1 In this context, it seems to me that widespread assumptions that advance the idea that a digital, unofficial infectious disease surveillance and monitoring is quicker than traditional, official information and reporting systems need to be modified. It is true in retrospect that existing digital systems and their associated early warnings could have been faster if their first signals had been correctly named or interpreted at an early stage. At the same time, however, it is mostly ignored that even official state authorities often have knowledge about specific events at relatively early stagesonly that they either initially withhold such information or distribute it in other ways according to the official information/notification systems, e.g. the example of Stephen, the authorities of Guinea were aware that there was something going on, but they waited with the reporting. So have I experienced it at the WHO: few countries directly provided all available information to the organization. It can also be observed again and again that official information either minimizes or plays to the media or other entities in a targeted way. In this context, non-state surveillance platforms such as ProMED or HealthMap, for example, should be commended especially for their dimension of political transparency, as they put pressure on governments not to keep information from the public as much as possible. At the same time, however, the increasingly digitized identification, analysis and distribution of epidemiological indications of infectious disease these platforms enable not only leads to increasing likelihood of false positives, but also to specific problems of an immediate, uncontrollable communication of risk. The danger of panic and the great effort required to avoid panic are to be feared.

Henning provides very important information in this regards. He describes that specific public health actors (here: local health authorities) may be compelled to equate technologically-generated signals with epidemiologically certified public health events, and, on the basis of these unproven indications, initiate public health measures. Further, this is also a problematic development from the perspective of resource retention in an already thinly-resourced public health service. Early responses and over-reactions from political decision-makers or the media are to be feared in equal measure. In the broader context of the focus and framework of a 'Global Health Security' , such potentially exaggerated perceptions and reactions are tied to perceptions of elevated threats of infectionwhether from (quasi) natural or man-made infection (e.g., in the context of war or incidents of terror)and, consequently, to urgent demands for comprehensive and constant attention, outbreak detection, and further crisis/disaster preparedness measures.

As a result of this, infectious disease epidemiology is increasingly being, in my humble opinion, in an irritating way integrated into the national and international security architectures. So it was during the West African Ebola outbreak in Sierra Leone and Liberia, two of the three hardest hit countries, that the military was constantly present in the planning of public health measures. E.g. in one situation in Sierra Leone I remember this resulted in prioritizing quarantining over other public health measurements like community engagement. Quarantining is not per se negative but in this particular case turned out very ambivalent as the measure evoked strong resistance among the population and potential new infected individuals increasingly were actively hidden as a consequence. Also in non-outbreak times, the cooperation between security forces and public health entities is becoming increasingly narrow. This can be seen, for example, in the Global Health Security Initiative, which addresses both the biological threats of pandemic flu and possible threats from chemical or radio-nuclear terrorism.

These perspectives as well as the social aspects and subsequent costs of a digitized infection control should be discussed. They are closely linked to the imperatives of constant monitoring and early detection, as well as the similar focus of a 'Global Health Security'. As such, they should be considered with the view not only to the (not new) anticipated restrictions or marginalizations associated with classical, structural and also socially-reformed, areas of public health, as well as to further possible negative costs resulting from of a 'securitized' public health. I would like to hear from you -Stephen and Henning -especially with regard to these broader health and social policy debates, from your social science perspectives, what is your understanding of 'Global Health Security' in general and of the 'preemptive security logic' , which is often discussed in this context, especially?

Henning I would suggest to understand ""Global Health Security"" as a set of preferences and truth claims that are currently framing our understanding of health issues of international relevance. This understanding results from a perspective, that interrogates threat discourses and related policies as a structured but contingent formation of problem descriptions. Problems do not exist 'naturally' but they have to be articulated and put on the agenda in a process of social interaction. This approach draws back to Michel Foucault and his proposal to acknowledge a power/knowledge nexus in general and specifically the power effects of truth claims. According to this, articulating and framing an issue are powerful ways to predetermine the range of thinkable approaches and solutions. By using the term ""Global Health"", policy-makers, non-governmental actors and academic observers are drawing together several health problems into a common frame, but also marking this frame as a field of intervention and claiming its relevance. The contours of this frame are still blurry and there exist numerous approaches to define ""Global Health"" (Brown et al. 2006; Farmer et al. 2013; Fassin 2012) . There is no accepted definition and ""Global health […] is more a bunch of problems than a discipline"". (Kleinman 2010) The ongoing emergence of a problem field ""Global health"" is an interesting moment then, where new truth claims are put forward and a new understanding of related issues such as 'health' and 'the global' are formed. Those newly related ideas are powerful as they are confining the agenda setting and plausible goals and methods of intervention.

Approaching ""Global health"" from this angle, what is striking from the outset is a strong undercurrent of security. The recent surge of ""global health"" can be attributed to a confluence of two separate discourses. On the one hand, globalization is increasingly narrated as a health risk. An ""emerging diseases"" discourse paints the picture of a global spread of infectious diseases due to unparalleled levels of global connectivity and frequency of global travel (Barrett et al. 1998 ). On the other hand, the concept of national security is being reimagined, facing a new multi-polar and complex world order. Today, in order to achieve national security, one has to look beyond military dominance and to take societal issues such as health, poverty but also climate change as security threats into account (Redclift and Grasso 2013) . For example in the US, facing the threat of bioterrorism, public health has become a concern for the Department of Homeland Security and international infection control resurfaced as a security issue. Both the fear induced by 'globalisation of disease' and the rethinking of national security are underlining a new relevance of global health issues. The resulting tremendous development in global health policies and programs accordingly are often following a security rationale (Genest 2015) . One example is the newly installed global health surveillance mechanisms and the revised international health regulations (IHR) (Fidler 2005) . The recent conception presents global health as part of a security problem rather than as a humanitarian issue.

This securitization of 'Global health' has already been described in some detail (Cook 2010; King 2002; Pereira 2008) . Here I want to underline the corresponding shift in the perception of threats and its implications. Current problems of Global health security are often depicted as essentially incalculable. Emerging diseases, acts of intentional Bioterrorism, food security in an increasingly global connected distribution system, antimicrobial resistant agents, − more than ever we now seem to be confronted with ""unknown unknowns"". We not only do not know when those events will happen, but we even do not know what the threat is exactly. The reformulated International Health Regulations (IHR) tellingly have shifted from monitoring a fixed catalogue of diseases to the obligation to warn about anything unexpected. According to the IHR, the national health agencies have now to signal any unspecific ""public health emergencies of international concern"" (World Health Organization 2008) to the WHO. This specific perception of ""Global health security problems"" as incalculable threats calls for a certain pre-emptive and outbreak-oriented intervention.

The implications of the employed ""preemptive security"" logic have been detailed in critical security studies (de Goede and Randalls 2009; Lakoff and Collier 2010; Massumi 2007; Caduff 2015) . As those studies have shown, preemption often demands the extension of (technological) surveillance and orients efforts towards the event and away from structural conditions. Comparable tendencies have been shown for current ""Global health"" policies, for example an orientation towards containment of an event rather than the search for a broader structural prevention (Rushton 2011) .

To sum it up, I would argue that Global Health is currently presented as a problem and has been put on the political agenda in a way that calls for a very specific answer in the form of a ""preemptive security logic"". Firstly, the underlying truth claims about the problems to solve frame the emerging field of Global health partly as security issue. Secondly, the incalculability problem evoked in many threat discourses of current Global health thinking demands a certain security rationale. The problem of an unknown unknown has to be dealt with preemptively. This way of presenting the problem of Global health then implicitly constraints plausible interventions. Approaching health as a security issue does often not tackle the actual problems of health on the ground. For example, this approach inclines to invest scarce resources into monitoring and surveillance rather than education and local health infrastructure. In order to reach the goal of more substantial health policies it is important to be aware of this securitization bias in the current problematization of Global health.

Building further on excellent points articulated by Henning, this epistemic shift in government and politics towards 'global health security' has been resultant, as I argue, from significant larger geopolitical transformations, and new reconsiderations of security perspective, in a post-Cold War era of rapidly proliferating non-traditional security challenges, which extend beyond traditional security correlations of the state/military, are transnational or global in scope, and again, to underscore the centrality of Henning's earlier points, which cannot be prevented entirely, only addressed through coping mechanisms and the development of techniques of preemption and forecasting (Caballero-Anthony 2010).

The rise of global health security and its securitizing processes have transformed the ways in which international relations and global politics are understood, orientated and practiced. In 2000 the United Nations Security Council (UNSC) adopted Resolution 1308 (UNSC 2000) which emphasised that the current HIV/AIDS pandemic, if unchecked, posed a risk to international security and stability, marking the first time in which a health threat was discussed before the UN body mandated to maintain international peace and security (Fidler 2005) . 14 years following the seminal Security Council resolution on HIV/AIDS, the United Nations launched its first and only to date, military mission to combat the spread of an infectious disease outbreak. Known as the United Nations Mission for Ebola Emergency Response (UNMEER), the first ever UN emergency health mission sought to contain the spiraling West African Ebola outbreak following the UN Security Council Resolution 2177, which determined that the ongoing outbreak in West Africa 'constituted a threat to international peace and security' (UNSC 2014), and we can understand these grand transformations within global politics and international relations as permeated by emergent logics to preempt both occurring public health emergencies and also probable future pandemics.

Contrastingly, for critical theorists, global health security has emerged as a concept which denotes a novel biopolitical project, or rather, the appearance of a new governmental problem in public health: how to effectively manage 'emerging infectious diseases' at a global scale (Lakoff 2015) . Contemporary global health systems are therefore problematized not only by the rapid emergence of pathogens on a global scale, but the risk posed by these circulating pathogens are no longer calculable using tools of risk assessment, which are based on patterns of historical incidence (ibid). In this regard, global health security rationalities, I assert, galvanise and accelerate the facilitation and development of novel techniques and practices of anticipatory or preemptive security, which emphasise the real-time, continuous and cost-effective surveillance of potential disease outbreak and public health emergencies.

Increasingly, in an era of innumerable digital data sources, the preemption of health risks are managed and analysed via an assemblage of innovative and evolving surveillance practices which combine multiple data sources and disease-tracking techniques, enacted at local, regional and global levels. Syndromic surveillance platforms, and digital epidemic intelligence systems including ProMED-Mail, GPHIN, HealthMap, Bio-Caster, EpiSPIDER, and the now-defunct Google Flu Trends can thus be conceptualised as new governmental technologies of overarching global health security practices, developed and installed around yet unforeseen events in order to halt or preempt the 'sudden, circular bolting' of pandemic phenomena (Foucault 2007) .

Collectively then, in my view and building upon the expert points provided by Henning, processes of securitization of global health and the rise of preemptive security logics have advanced calls for the deployment of novel security technologies and surveillance apparatuses over the past two decades. These calls have been met with the re-drawing of disease surveillance operations and the launching of new technologies which now seemingly patrol digital datascapes in the surveillance of potential public health emergencies. Such novel technologies constitute critical components of an evolving ensemble of new governing practices, knowledges, techniques and rationalities of health security, increasingly influenced by digitised, automated and computerized algorithms. .

As components in an emergent socio-technological apparatus of security for the strengthening of global health governmentalities, it is also crucial to consider the ways in which these expanding digital syndromic surveillance systems re-contour previous understandings of the temporalities, form and practice of preemption in the identification of forthcoming pandemics. Firstly, the rise of syndromic surveillance technologies for the forecasting of probable disease outbreaks, departs significantly from previous methodologies to identify and further preempt pathogenic threats. As seen with the steady integration of algorithmic programming over the past two decades from ProMED-mail, to GPHIN, and to HealthMap, syndromic surveillance technologies increasingly draw upon and aggregate open-source data pulled via algorithmic processing from the realm of the digital to inform contemporary practices of health security in the non-digital/physical world. Within the politics of preemption, this marks a novel transition towards the harnessing of infinite online data sources, afforded by increasingly sophisticated algorithms to identify unusual data correlations or patterns indicative of a potential disease outbreak. In turn, this represents a process that is distinct and divergent from previous methodologies of health surveillance which utilised clinical and laboratory testing, analysis, observation, and the collation of statistics in order to render visible and intelligible, occurring or emergent infectious disease outbreaks. In the new era of digital disease surveillance, the data warehouse emerges alongside the traditional clinic as a new critical site of surveillance and zone of security praxis in the preemption and surveying of disease risk.

Further to this, novel techniques to preempt looming pandemic threats via these digital syndromic surveillance systems now also correspond with new problematizations of data and knowledge forms in the securitization of uncertain [pathogenic] futures. Unlike previous systems of infectious disease surveillance which were routinely marked by an incompleteness of data in which to understand forthcoming pandemic risks, the deluge of 'Big Data' of the early twenty-first century has now reversed this problematization of data.. Contemporary digital disease surveillance systems and the practice of health security are no longer hindered by a scarcity of data but rather burdened by an excess of infinitely generating, unstructured and diffuse streams of digital data. In order then to preempt and track the emergence of disease outbreaks in a present world that is submerged in data sources, digital disease practices must navigate, as Matteo Pasquinelli (2015) writes, 'vast data oceans' to detect that which constitutes the anomaly, be it common patterns of behaviours in social media, buying or selling tendencies in stock markets, the oscillation of temperatures in a specific region, or suspicious keywords in disease surveillance networks (ibid). Again, in this new practice of 'navigating vast data oceans' , the digital algorithm emerges once more as a strategic, pragmatic and celebrated technology of government with the capacity to apprehend, process and project new insights of disease patterns from troves of digital data which manifest beyond human cognitive and analytic capacities.

Thus, the politics of preemption in the present era of elevated pandemic threat are intimately intertwined with expanding recourses to apprehending Big Data sources and employing algorithmic processing techniques to produce advanced alerts, indications and insights of potential pathogenic uncertainties.

Indeed, during several critical public health emergencies over the past two decades, a combination of Big Data sources and algorithmic techniques produced meaningful and advanced insights into emergent public health emergencies, including during the early and critical stages of the emergence of severe acute respiratory syndrome (SARS) in China and Ebola in Guinea. However, the success and rise of the algorithm in these health histories should not distract from the imperative for continued meaningful-and indeed critical investigations and interrogations of emergent digital disease surveillance practices which utilize diffuse Big Data sources and processing of such data streams via algorithm.

Algorithms are not only famously opaque, but have also been shown to be cantankerous, if not delicate technologies, illustrated famously by a false reporting of a cholera outbreak in the United States by Google in 2007, as a result of Oprah Winfrey picking Love in the Time of Cholera as book of the month in her book club (Simonsen et al. 2016 ). However, as technology and innovation advance, algorithms are getting smarter, more insightful and more precise, but the growing commonplace of these knowledge producing machines with intensifying technical complexities makes the monitoring and regulation of these data-processing technologies ever the more urgent and vital.

The ascendancy of the era of Big Data and the rise of digital disease surveillance systems have afforded unprecedented new opportunities towards the enhancement and bolstering of disease detection capacities in an era increasingly preoccupied with the emergence of future security challenges-among them pandemic illness. The objective of this discussion has been to provide an overview and highlight the potential gains and benefits yielded by these new data sources and processing techniques, while also emphasising that key ethical, legal, political and societal concerns abound and must not be sidelined in contemporary efforts to accrue maximal data reserves and to effectively track and detect the next pandemic before it occurs.

Tim Dear Stephen, dear Henning, thank you very much for this inspiring conversation. Again, it made clear the necessity of an interdisciplinary and social sciences inspired debate about contemporary epidemiology and public health.

For me three insights emerge. First of all, the gains in timeliness and scope of digital epidemiology come at the cost of providing a different type of knowledge. The information provided through such systems is not the same as the traditional expert knowledge based on human assessment, analysis, hypothesis, statistical testing and trials but an algorithmic 'knowledge without truth'. The status of this knowledge may not be totally clear in all the different contexts where it is used. This may result in ill-informed decision making.

A driving force for the demand of digital epidemiology is a reformulated conception of global health. A common thread running through the diverse debates about global health policies today is the issue of security. This securitization of global health does frame current policies.

Specifically, threats to global health are increasingly identified as incalculable emergencies (unknown unknowns). This results in a demand for preemptive ways to act on those emergencies before they have evolved. This preemptive security logic also fosters an unlimited big data surveillance as a practice of 'navigating vast data oceans'.

For sure these points need further critical examination. Thus I am looking forward to future interdisciplinary exchange and discussion. 

",0.7450276444639551
"Establishing benchmarks and metrics for disruptive technologies, inappropriate and obsolete tests in the clinical laboratory","Benchmarks and metrics related to laboratory test utilization are based on evidence-based medical literature that may suffer from a positive publication bias. Guidelines are only as good as the data reviewed to create them. Disruptive technologies require time for appropriate use to be established before utilization review will be meaningful. Metrics include monitoring the use of obsolete tests and the inappropriate use of lab tests. Test utilization by clients in a hospital outreach program can be used to monitor the impact of new clients on lab workload. A multi-disciplinary laboratory utilization committee is the most effective tool for modifying bad habits, and reviewing and approving new tests for the lab formulary or by sending them out to a reference lab.","Laboratory test overutilization is estimated to represent 2.9% to 56% of all laboratory tests internationally. Efforts have been made to reduce the demand for or utilization of these over utilized tests [1] [2] [3] [4] [5] [6] . The most efficient outcomes have involved the formation of a laboratory utilization committee [2, 6] or a laboratory formulary committee [5] based on the hospital pharmacy and therapeutics committee's organizational structure. This committee evaluates the clinical value of laboratory tests using an evidence-based review of the appropriate medical literature. This same literature is reviewed by numerous professional specialty medical organizations as well as healthcare insurance carriers to determine what tests or procedures should be performed and reimbursed. The conclusions based on these reviews need to be updated on a regular basis.

""The quality of guidelines is only as good as the published studies on which they are based"" [7] . Often relevant studies evaluating laboratory tests demonstrate negative findings and are not published [7, 8] . This phenomenon is referred to as positive publication bias or publication bias. Tzoulaki et al. [8] demonstrated publication bias during a review of reports evaluating emerging cardiovascular biomarkers. Therefore, misinterpretation is a potential impact of failing to publish studies with negative results during a review of evidence-based literature. Readers beware.

Tests may be obsolete and should be retired from clinical use, while others may be inappropriately used for specific disease categories. The playing field is not level. There are at least six newer game-changing disruptive technologies being evaluated [9] [10] [11] which will result in modifications of clinical practice and laboratory testing modalities. These newer disruptive technologies may replace obsolete or inappropriate tests. Lab utilization benchmarks and metrics are under continuous flux as a consequence. In the case of evolving newer technology, it is imperative to explore their impact early in their development to anticipate and monitor their impact on laboratory testing.

The three authors have reviewed the current literature related to laboratory test utilization with an emphasis on where do the definitions of obsolete or inappropriate test utilization originate. We evaluated whole genome sequencing, next generation sequencing and proteomics as examples of high impact disruptive technologies that generate large quantities of data that need software to reduce to clinically useful results. Practical examples of obsolete and inappropriate tests are reviewed as potential metrics to monitor improvement in test utilization. Another useful metric is test utilization by clients in a hospital outreach program which can be used to monitor the impact of new clients on laboratory workload. Finally, the result of published data from the work of laboratory utilization committees is summarized.

Benchmarks and metrics for laboratory utilization will be reviewed for three disruptive technologies as well as obsolete and inappropriately used tests.

Medical practice as well as pathology is in the midst of the rapid development of at least six major game-changing disruptive technologies. They include genetics, proteomics, digital pathology, informatics, therapeutic pathology and in vivo diagnostics [9] [10] [11] . All six of these disruptive technologies share similar issues like resolution of best applications for routine clinical use, paucity of evidence-based outcome literature for review, education of practitioners and physician users of the clinical information generated and software to convert big databases the method generates into clinical useful information [9] [10] [11] . The utilization of these techniques will increase as these barriers or obstacles to clinical use are overcome.

An example of a disruptive technology is next generation sequencing or massively parallel sequencing [12] [13] [14] . This technique is currently not cleared by the U.S. Food and Drug Administration [13] . It has been used to generate genome wide sequences and one of the authors (FLK) has had his genome sequenced at the CLIA approved laboratory at Illumina (San Diego, CA). The results revealed 3.23 million variants compared with the reference method and 20,426 of these variants were in the exome or in the coding elements.

The study interrogated 344 genes causally associated with 140 conditions as recommended by the American College of Medical Genetics. In that limited number of genes, 1,254 variants were detected and classified as clinically significant (0), carrier status (1), variants of unknown significance (255), likely benign variants (356) and benign variants (642). The definition of these variants calls and the failure of this technique to detect deletions, insertions, interspersed repeats and tandem repeats (repeats adjacent to each other like triplet repeats [15] ) may lead to inappropriate interpretation of the results and expensive follow up clinical and laboratory evaluation. For example, a clinically significant pathogenic variant reported in at least 3 unrelated cases with control data may be found in additional genome studies in other populations [16] to be a benign variant that is also found with a new variant which contains the mutation that leads to the most significant deleterious effect on gene function. The software application for variant significance assignment, like DataGenno [17] , will need to be up-to-date with the latest genotype/phenotype associations to prevent false positive findings and inappropriate follow-up testing.

In 2009 the highest rate of reported cancers was prostate, lung and bronchus and colon and rectum for men with female breast replacing prostate for women in the U.S. [18] . The annual incidence rate was 459 cases per 100,000 individuals. Comprehensive sequencing of numerous human cancers have revealed driver genes, 2 to 8 such genes per tumor, which alter intracellular signal transduction pathways related to the cells future death or survival and/or genome maintenance [19, 20] . There are at least 10 FDA approved cancer therapies based on the inhibition of these tumor-activated intracellular pathways [19] . For example, the BRAF kinase inhibitor, Vemurafenib, has shown a response rate in 50% of patients with metastatic melanoma that have the BRAF valine to glutamic acid mutation at codon 600 (V600E) [21] . This V600E mutation is associated with aggressive clinical course in patients with thyroid papillary microcarcinoma [22] . In one study of a hybrid score composed of one molecular diagnostic (V600E) and 3 histopathologic parameters were used to predict this tumor's clinical course with a sensitivity of 96% and specificity of 80% [22] .

The selection of the correct molecular diagnostic tests for specific tumors is aided by published guidelines. Immunohistochemistry detection of estrogen and progesterone receptors in breast cancer from American Society of Clinical Oncology and College of American Pathologists [23] and selection of lung cancer patients for EGFR and ALK tyrosine kinase inhibitors from the International Association for Study of Lung Cancer, Association for Molecular Pathology and College of American Pathologists [24] .

Whole genome sequencing of a tumor will provide access to all known and unknown variants related to the tumor's survival skills [25] . The development of software [26] which will convert the patient's raw genome sequence into a medically relevant assessment of therapeutic targets and drug metabolism based on the tumor's body site will be very useful. From this genome analysis, the clinician wants to know what anticancer drug or drugs will this patient respond to as well as the dose.

MALDI-TOF (Matrix Assisted Laser Desorption Ionization-Time of Flight) spectroscopy is a relatively new technology to the Clinical Microbiology laboratory. Pathogen identification has always relied on visual and biochemical interrogation where the summary of results may point to a specific identification (genus and species) or sometimes to at least the genus level. Visual and biochemical results can sometimes yield variable results meaning in some cases the ID may change depending on the result. The use of MALDI-TOF allows the clinical microbiology laboratory to identify bacteria once an isolate has been cultured potentially without performing any biochemical testing [11, 27, 28] . The implications are quicker pathogen identifications to clinicians and the potential to affect antibiotic treatment before susceptibility results are available. The ability to obtain a quicker answer will disrupt the testing workflow and require a re-evaluation of that workflow to optimize the use of MALDI-TOF and antibiotic susceptibility testing [11, 27, 28] .

Benchmarks and subsequent metrics for monitoring laboratory test utilization have been developed by professional subspecialty medical organizations in the format of recommendations and guidelines [29] . Examples include guidelines for hypothyroidism in adults from the American Association of Clinical Endocrinologists and the American Thyroid Association [30] , definition of myocardial infarction from the American College of Cardiology Foundation and American Heart Association [31] , definition of diabetes mellitus from the American Diabetes Association [32] , pharmacogenetics as well as follow-up testing for metabolic diseases identified by expanded newborn screening using tandem mass spectrometry from the National Academy of Clinical Biochemistry [33, 34] , and use of bone metabolic markers from the Japan Osteoporosis Society [35] .

Thirty-five of these specialty societies have joined the Choosing Wisely project organized by the American Board of Internal Medicine. Societies are asked to provide five specific, evidence-based recommendations on when tests and procedures may be appropriate or inappropriate for patient care (www.choosingwisely.org).

A review of the lists from 26 specialty societies revealed 135 recommendations. Laboratory tests were referenced in 25 items or 18.5% of the total. Only one organization, American Society of Clinical Pathology, had a list of 5 laboratory test-related recommendations [36] . Kale et al. [37] reviewed the national annual savings if outpatient visits to the primary care physicians did not include unnecessary or inappropriate laboratory tests including CBC ($32.7 million), urinalysis ($3.3 million) and basic metabolic panel ($10.1 million). Those three procedures yield an annual cost savings of $46.1 million compared to the elimination of inappropriate Pap tests at an annual savings of $47.8 million. These figures illustrate the magnitude of healthcare savings by implementing simple laboratory test ordering practices which reduce duplication and/or inappropriate testing. Collaboration by subspecialty medical societies in disruptive technology development and improvements in routine clinical laboratory test utilization will be a fertile area for the development of benchmarks and metrics for future laboratory test utilization.

The appropriateness of laboratory tests and the appropriate utilization of laboratory tests are always important for patient care, but require increased scrutiny in the era of containment of healthcare costs. Objective criteria for the judging of appropriateness of tests and their utilization have not been universally developed or applied, so it is not always easy to define these terms [38] . Insurance companies are recognizing the medical and the financial burden of unnecessary testing and are taking action. Many companies have posted information on their websites defining obsolete and unreliable lab tests which are readily accessible on the internet, including Aetna [39] , United Healthcare [40] and AmeriHealth [41] .

One criterion for judging the appropriateness of a test is to determine if it is obsolete. The definition of ""obsolete"" as noted in the Merriam-Webster on-line dictionary at http://www.merriam-webster. com/dictionary/ is ""no longer in use or no longer useful"". Synonyms include: antiquated, archaic, dated, démodé, demoded, fossilized, and kaput. Over time, with advances in medical technology, laboratory tests become outdated. Although it is difficult to remove a test from a laboratory's formulary, there are good reasons to do so. Reasons include the availability of a more sensitive, specific, or accurate test or new guidelines recommend the elimination of a test with the replacement of another.

There are tests in Clinical Pathology that must be considered for obsolescence. These include T3 uptake and lactic acid dehydrogenase (LDH) isoenzymes in the clinical chemistry lab and bleeding time in hematology. Obsolete tests in the microbiology laboratory include bacterial antigen detection tests, Group B Streptococcus antigen (GBS) testing and HIV-1 Western blot.

T3 Uptake and Free Thyroxine Index (FTI) are still ordered by physicians, despite the fact that alternative tests have been available for many years. T3 Uptake is an old test designed with a purpose of indirectly measuring free thyroxine (T4). It was developed before the availability of direct assays able to accurately measure free T4 levels [42] . Standardization of free T4 assays has been reported using the time-consuming equilibrium dialysis in combination with isotope dilution-liquid chromatography/tandem mass spectrometry [43] . T3 Uptake is an assessment of unsaturated (unbound to thyroxine) thyroid binding proteins in serum and is used with total T4 to calculate FTI. The FTI is obtained by multiplying the (Total T4) times (T3 Update). There is no longer a need to estimate free T4 when there are assays for the direct measurement of free T4 in every laboratory. Supporting evidence for the obsolescence of T3 Update and FTI has been available for decades [44] . Current guidelines for the diagnosis and management of hypothyroidism [30] , hyperthyroidism [45] , and thyroid disease in pregnancy [46] , no longer include the assessment of T3 Update or FTI.

The analysis of lactic acid dehydrogenase (LDH) Isoenzymes by electrophoresis has been utilized as an aid in the diagnosis of myocardial infarction. With the development of a more specific test for myocardial damage, troponin, there is little use for this insensitive and time-consuming electrophoretic assay. Current guidelines clearly establish that the preferred marker for cardiac injury is troponin [31] .

Bleeding time is a crude test of hemostasis (the arrest or stoppage of bleeding). It is an indication of how well platelets interact with blood vessel walls to form blood clots. Indirectly assesses platelet function. It is performed by making a small incision on the skin and measuring, in seconds, the time taken for bleeding to stop. The test was designed to assess platelet function or exclude von Willebrand Disease. This test is labor intensive, invasive, poorly reproducible, and insensitive [36] . Historically it was performed because screening tests with a higher sensitivity for platelet dysfunction and von Willebrand disease (vWD) were unavailable. Bleeding time has been replaced by instrumentation that can assess platelet function in whole blood by aggregation studies [47, 48] . Available instrumentation includes the PFA-10 (Platelet Function Analyzer, Siemens USA), the VerifyNow (Accumetrics), the Plateletworks (Helena), the IMPACT (Diamed) and the thromboelastograph (TEG) (Haemonetics). Initial tests for a bleeding disorder rule out more common causes of bleeding. These tests include complete blood and platelet counts, PTT, PT, and possibly fibrinogen level or thrombin time. Additional tests for von Willebrand Disease (vWF: Antigen, ristocetin cofactor activity. Factor VIII clotting activity) can confirm the disease [48] .

Bacterial antigen detection tests should be considered obsolete. They have historically been used as an adjunct to other laboratory tests for the diagnosis of bacterial meningitis. The test's purported advantages were the rapid detection of H. influenza, N. meningitides, S. pneumoniae, and S. agalactiae. Overall, the sensitivity is essentially the same as that of a Gram-stained smear of a cyto-centrifuged CSF specimen [49, 50] . With the advent of vaccines to H. influenza type b and N. meningitides (A, C, Y, and W-135) the antigen testing is even less useful. The literature confirms that the use of direct antigen testing from the CSF is neither sensitive nor specific [49, 50] . More importantly, the Gram stain and cultures still need to be performed regardless of the initial antigen test result. Based on the data reviewed, our laboratory has discontinued this testing in-house.

This is an example of testing that was removed from the market based on recommendations from the Centers for Disease Control (CDC) stating that the rapid antigen detection tests for GBS are not sensitive enough to replace the culture based prenatal screening or to use in place of the risk-based approach when culture results are unknown at the time of labor [51] . Because of the poor performance of the rapid antigen testing, CDC has recommended intrapartum chemoprophylaxis antibiotics be administered to women who have certain risk factors [51] . Our laboratory looked at internal data for our patient population and found the sensitivity of the rapid antigen test that was being used was 28%. Forty-one patients were missed on the rapid antigen test were detected only by culture. Data from the literature show an average sensitivity of 25.7% among various labs surveyed [52] . This is an example where CDC recommendations and assessment of testing performance within your laboratory supports moving a test into obsolescence.

The HIV-1 Western Blot (WB) has been a constant as one of the confirmatory tests for HIV antibody testing. However, the WB is moving its way into obsolescence as newer generation antigen/antibody and molecular assays become part of the new HIV testing algorithms. HIV-1 WBs have always had issues with indeterminate results due to a myriad of factors (false positives and/or lack of specificity, kit design, etc.), which required either re-testing at a later time and/or molecular testing for HIV-1 nucleic acid [53] . The CDC/APHL, WHO, and France each have different interpretation criteria for defining a positive confirmatory result which indicates a lack of standardization for defining a patient as positive for HIV-1 [54] . The immune response to HIV-2 is well known to produce antibodies that cross react on the HIV-1 WB which could lead to a false positive HIV-1 result [54] . The lack of improvements or advancements of the WB compared to other antibody based assays has been nicely shown by Masciotra et al. [55] . They demonstrate that the rapid HIV tests actually detect HIV-1 antibodies several days earlier before the WB becomes positive [55] . The introduction of 4th generation antigen/antibody testing has allowed clinicians to detect reactive patients earlier in their disease course compared to 3rd generation testing effectively narrowing the window of serological detection by approximately 4-8 days [56] . Because of these new developments, CLSI and APHL have recommended new algorithms that incorporate antigen/antibody combination tests, rapid tests, and molecular testing [57, 58] . The WB should be a test that will be obsolete as laboratories adopt the newer algorithms for HIV testing.

In addition to obsolete tests, tests may be inappropriately used. Inappropriate utilization includes the failure to follow current practice guidelines for the diagnosis and management of disease, thus ordering the incorrect test, panel of tests, or algorithm of tests; ordering tests too frequently or lack of medical rationale for the test. The problem and the resolution need to be reshaped as a way to ""improve patient outcomes and lower costs"" [59] .

The acknowledgement of the laboratory test utilization problem has been known and published for more than 2 decades [1-6,60] Studies have been done to estimate and document the percentage of unnecessary tests [37, 61] .

A typical scenario for inappropriate test ordering occurs with thyroid function testing in the diagnosis and management of hypothyroidism. The guidelines are clear on the appropriate tests [30] . Thyroid stimulating hormone (TSH) is regarded as the best screening test, followed by free thyroxine (free T4) if the TSH is abnormal. Additional tests are often ordered including total T4. There is no need for a total T4 measurement if a free T4 is provided. Furthermore, adding a total T4 level may confuse the diagnosis if changes in binding proteins via disease or drug therapy result in a total T4 that is inconsistent with other test results.

Tests may be ordered inappropriately or at other times, the wrong test is ordered. Vitamin D testing is known to result in both inappropriate and erroneous ordering [62] . The correct test for the routine assessment of vitamin D status or deficiency is 25-hydroxy vitamin D. The test 1,25-dihydroxy vitamin D is often mistakenly ordered. The dihydroxy form of vitamin D is occasionally ordered in patients with kidney disease (decreased levels are one of the earliest changes to occur in persons with early kidney failure). However, most of the orders for 1,25-dihydroxy vitamin D are simply erroneous. Tests for both vitamin D2 and D3 are unnecessary for the assessment of vitamin D status. There is no need to differentiate between the D2 and D3 forms other than in the research setting.

Viral cultures have traditionally been the gold standard for virological detection in the clinical microbiology laboratory. Direct viral detection direct from patient samples have also been utilized with monoclonal antibodies to detect viral antigen(s) with the intent of obtaining a result in a timelier manner than viral cultures. Molecular technologies have now become established in the clinical microbiology laboratory. With the increased sensitivity and specificity [63] and almost always a shorter turn-around-time, it is reasonable to ask ""Why do viral cultures?"" [63, 64] .

One example of molecular testing that helps to answer this question are panels developed for respiratory viruses. There are several commercial companies that offer their version of an RVP (Respiratory Virus Panel). Our laboratory offers a RVP assay that detects 20 viral targets. Not only are results obtained sooner than traditional virology testing, but our molecular RVP offers greater sensitivity and specificity than our prior Direct Fluorescence Assay (DFA) that was sent out to a reference laboratory. Because of the increased sensitivity, the RVPs have allowed us to document co-viral infections, which have not been fully appreciated before with DFA or culture based testing [64] . There have been reports of increased severity of disease, as well as reports of decreased severity of disease in the setting of co-viral infections [65] . Much more research must be done to understand the potential interactions of different respiratory viruses in the setting of a respiratory infection. Our laboratory publishes a ""Virogram"" during the respiratory virus season and related viral coinfections (Figs. 1 and 2 and Table 1 ). The ""Virogram"" charts the respiratory virus prevalence among the patient populations tested to give an idea what is circulating in the community.

Another example of molecular testing replacing viral cultures is the detection of Enterovirus (EV) from CSF. EV PCR has been shown to have greater sensitivity over culture [66] . A study (unpublished) performed within our institution looked at the utility of an in-house EV PCR and its affect on length of stay (LOS) and cost on 20 EV PCR negative and 20 EV PCR positive patients. Those with an EV PCR negative result had an average LOS of 2.1 days greater than those that were EV PCR positive and had an estimated $187,992 of additional cost related to in-patient care.

Viral cultures still have importance in growing the virus for the purposes of subtyping, identifying new strains, or antiviral testing. However, these should remain in specialty or research labs and not be routine for clinical diagnostic testing.

PCR testing for the causative agent of Lyme Disease may initially make sense, but the life cycle of B. burgdorferi is such that PCR detects Borrelia DNA in the blood in less than half of patients that are in the early acute stage of disease when the characteristic erythema migrans is present [67] . Therefore, PCR is not recommended as a first line test for making the initial diagnosis of Lyme disease. A review article showed that the median percent sensitivities of PCR testing from blood, skin biopsy, CSF, and synovial fluid are 10-48%, 64-76%, 23-73%, and 66-83%, respectively [67] . The current recommendation for diagnostic testing involves a two-tiered algorithmic approach involving antibody testing [68] . There may be clinical utility for PCR, but only if the serology testing is negative or inconclusive and clinical history and symptoms strongly suggest Lyme disease.

Hospitals have introduced outreach programs that market laboratory services to physician offices, nursing homes, and other hospitals [69, 70] to increase test volumes and reduce unit costs per test. This effort generates increased laboratory test utilization which should be monitored by average number of specific tests ordered per subspeciality physician per month ( [69] , Table 40 .10). The data is collected from patient requisitions that are processed each day in the accessioning area of the outreach specimen receiving area. The sales force will introduce outreach administration to the projected number of new client accounts to be opened each month. The impact of the laboratory can be estimated by multiplying the volume of a specific test per physician for an office practice of multiple physicians. These volumes will estimate the utilization of tests per laboratory section and predict the future need for additional analyzers and/or personnel to perform the laboratory test procedures as the outreach client number increase. Utilization data varies by medical subspeciality. For example, urology had 94 requisitions/physician, 1.4 procedures/requisition and 132 procedures/physician with the PSA being the highest test volume. Compare that to the nursing homes with 189 requisitions/physician, 3.6 procedures/requisition and 688 procedures/physician with electrolytes being the highest test volume [69] . This data is used primarily for planning the best strategies to absorb the increased workload new clients will bring to the outreach business. It can also be used to monitor the use of obsolete and inappropriate lab tests to develop educational efforts to improve test utilization practices by subspeciality among outreach clients.

An excellent review of laboratory test utilization, understanding the many factors involved, and steps to implement changes are provided in a recent publication [4] . The author provides insight and guidelines to assist the laboratory in initiating improvements in laboratory utilization. Ultimate goals include: developing and adopt more-effective testing algorithms, reducing testing costs, use new technologies cost-effectively, and shorten the time to diagnosis.

Interventions for hospitals and laboratories focus on changing physicians' test ordering behavior and include:

1. Eliminating obsolete tests and modifying requisition forms. The laboratory can alter test-requisition forms to steer clinicians in the right direction. One such option is an ""out of sight, out of mind"" approach in which certain tests simply don't appear on the menu. 2. Assisting in the education to promote appropriate lab testing can be part of a hospitals continuing medical education (CME) program for clinicians through grand rounds, newsletters, and CME lectures. 3. Reinforcing positive changes by auditing clinicians' use of new protocols and offering feedback. 4. A two tier review process for molecular send out tests is a useful tool for pathologists to learn and advice on the wisdom of molecular assays [71] .

Finally, all hospitals should implement a laboratory utilization or formulary committee to help in overseeing testing and promoting good testing practices, similar to pharmacy and therapeutics committees. This approach has been met with great success [2, 5, 6] . The use of molecular methods for respiratory virus testing has allowed us to detect patients with more than 1 virus present. The presence of multiple viruses within a patient sample is currently underappreciated and the effect on the overall disease presentation is currently unknown. 

",0.7439368584572518
Automatic online news monitoring and classification for syndromic surveillance,"Syndromic surveillance can play an important role in protecting the public's health against infectious diseases. Infectious disease outbreaks can have a devastating effect on society as well as the economy, and global awareness is therefore critical to protecting against major outbreaks. By monitoring online news sources and developing an accurate news classification system for syndromic surveillance, public health personnel can be apprised of outbreaks and potential outbreak situations. In this study, we have developed a framework for automatic online news monitoring and classification for syndromic surveillance. The framework is unique and none of the techniques adopted in this study have been previously used in the context of syndromic surveillance on infectious diseases. In recent classification experiments, we compared the performance of different feature subsets on different machine learning algorithms. The results showed that the combined feature subsets including Bag of Words, Noun Phrases, and Named Entities features outperformed the Bag of Words feature subsets. Furthermore, feature selection improved the performance of feature subsets in online news classification. The highest classification performance was achieved when using SVM upon the selected combination feature subset.","Syndromic surveillance is concerned with the continuous monitoring of public health-related information sources and early detection of adverse disease events [48] . Syndromic surveillance systems aim to provide effective prevention, detection, and management of infectious disease outbreaks, whether naturally-occurring or caused by bioterrorism attacks. The Centers for Disease Control and Prevention (CDC) defines syndromic surveillance systems as those that ""collect and analyze morbidity, mortality, and other relevant data and facilitate the timely dissemination of results to appropriate decision makers"" [5] .

Increasing globalization, combined with accelerating population mobility and more frequent travel, has made the prevention and management of infectious disease outbreaks a growing concern in public health [19] . Outbreaks of various diseases routinely threaten the public health of the world's populations. Recent outbreaks include: the anthrax attacks of 2001, the outbreaks of severe acute respiratory syndrome (SARS) in Asia in 2003, and the continuous avian flu outbreaks in recent years. These outbreaks have highlighted the need of syndromic surveillance systems which can detect and monitor an outbreak and minimize associated morbidity and mortality [5, 9] . These days, important public health related news is increasingly available on the World Wide Web in electronic form, and has been shown to be a useful data source for syndromic surveillance [14] . However, the volume of news is very large and there is a question on how to most effectively use this kind of information for syndromic surveillance to accurately detect the signals indicative of disease outbreaks.

Syndromic surveillance systems that include a classification component can facilitate follow-up analysis and outbreak detection [48] . However, to our knowledge, there is currently no automatic online news monitoring and classification system specifically designed for specific infectious diseases. It is also not clear what kind of document representation approach and machine learning algorithm perform best on online news classification for syndromic surveillance. This study is aimed at designing and examining automatic online news monitoring and classification methods for syndromic surveillance and global situational awareness.

The remainder of this article is organized as follows. Section 2 provides an overview of literature concerning syndromic surveillance, news-based syndromic surveillance systems for infectious diseases, online data acquisition, text document representation, and feature selection used in text classification. In Section 3, we describe our research questions. In Section 4, we outline our architecture for automatic news monitoring and classification, after which we present our experiments and the results on foot-and-mouth disease (FMD) related online news in Section 5. Finally, we describe our conclusions and future directions in Section 6.

Early knowledge of a disease outbreak plays an important role in improving response effectiveness [35] . Syndromic surveillance, as a public health surveillance approach, is employed to systematically collect, analyze, and interpret ""syndrome""-specific data for early detection of public health aberrations [48] . Syndromic surveillance is concerned with continuous monitoring of public health-related information sources and early detection of adverse disease events. It has attracted significant attention in recent years [48] .

Collecting data is a critical early step when developing a syndromic surveillance system. Data sources used in syndromic surveillance systems are expected to provide timely pre-diagnosis health indicators and are typically electronically stored and transmitted [48] . Different types of data used for syndromic surveillance typically include: emergency department (ED) visit chief complaints, ambulatory visit records, hospital admissions, over-the-counter (OTC) drug sales from pharmacy stores, triage nurse calls, 911 calls, work or school absenteeism data, veterinary health records, laboratory test orders, and health department requests for influenza testing [25] . Table 1 lists the major data sources used for syndromic surveillance [48] .

As shown in Table 1 , public news reports and bulletins, published on the Internet to the public, are among the major data sources used for syndromic surveillance. Nowadays, Internet-based resources such as discussion sites and online news centers have become invaluable sources for a new wave of syndromic surveillance systems [7] . The World Health Organization (WHO) relies on those informal sources for about 65% of their outbreak investigations [18] . Currently, nearly all major outbreaks investigated by the WHO are first identified through online sources [14] . The earliest descriptions of the SARS outbreak in Guangdong Province, south China, came from informal online reports [7] . In this study, we chose the online public sources, specifically the online news reports and bulletins, as syndromic surveillance news sources.

ProMED-mail, Argus, MiTAP and HealthMap are major news-based syndromic surveillance systems for infectious diseases. Table 2 lists the data sources and domain for each system.

The Program for Monitoring Emerging Diseases (also known as ProMED-mail, http://www.isid.org/) is an electronic outbreak reporting system that monitors infectious diseases globally. It is one of the largest publicly available emerging disease and outbreak reporting systems in the world. Originally founded in 1994, and a program of the International Society for Infectious Diseases (ISID, http://www.isid. org/) since 1999, ProMED-mail is distributed without a fee to more than 40,000 e-mail subscribers in over 165 countries, and the site gets roughly 10,000 hits a day [31] . ProMED-mail provides up-to-date information on human diseases, zoonotic diseases and diseases that affect sources of human nutrition (both plants and livestock animals) [26] . Each day, ProMED-mail's editor, assisted by five associates and about 25 scientific experts, cull through the dozens of e-mailed reports of mysterious outbreaks sent in from experts and amateur disease watchers throughout the world. The ProMED-mail team gathers and posts, from newspapers, health department alerts, government reports and other sources, the information for threats to public health that official syndromic surveillance systems may not yet be circulating [31] . In 2003, ProMED-mail was the first to report the disease that turned out to be SARS. In 2006, ProMED-mail's prompt reporting on a cattle die-off in northeastern Kenya that turned out to be the first outbreak of Rift Valley fever in nearly a decade enabled officials to contain the virus [31] .

However, one problem for ProMED-mail is that in an attempt to display only the most relevant information, all submissions have to be manually processed by a group of experts and volunteers. This approach is limited by the number of staff available to process the volume of reports submitted throughout the world in multiple languages [44] .

In contrast, Argus (http://biodefense.georgetown.edu/projects/ argus.aspx) is an automatic disease surveillance system. It is one of the ongoing projects of the Division of Integrated Biodefense (DIB) at Georgetown University. The purpose of Project Argus is to create and implement a global biological event detection and tracking capability that provides early warning alerts [44] . The Argus analytic team consists of multilingual analysts that utilize state of the art online media processing software designed in collaboration with the MITRE Corporation [44] . Argus currently covers 34 languages, and manages between 2200 to 3300 active case files with update report threading for approximately 175 countries and over 130 disease entities [44] . The Argus team has developed a social disruption model to enable rapid detection and assessment of biological threats that may require swift intervention by the international public health community [46] . Social disruption is a deviation from a routine daily activity that can be tracked and used in lieu of direct reporting of diseases [45] . With over 200 social disruption parameters, the model was created by conducting more than 60 in-depth retrospective case studies for socially disruptive biological events affecting animals and humans, and has been operationally validated against over 20,000 prospective biological events detected and tracked to-date [45] . Another well known syndromic surveillance system is MiTAP, which was developed by the MITRE Corporation as an experimental prototype using human language technologies to monitor infectious disease outbreaks. MiTAP aims at providing timely, multilingual, global information access to analysts, medical experts and individuals involved in humanitarian assistance and relief work [12] . The system collects, annotates and categorizes documents from multiple open news sources, including foreign sources both in English and native languages that MiTAP translates into English before processing [12] . MiTAP architecture has three phrases: (1) information capture, (2) information processing, and (3) user interface [12] . The information capture process supports Web sources, electronic mailing lists, newsgroups, news feeds, and audio/video data. The information processing is carried out by the Alembic natural language analyzer [2, 3] using machine-learned rules and WebSumm [27] to generate a summary for each document. MiTAP consists of a number of different user interfaces to the processed data. The core MiTAP system provides two methods of access: via newsgroups and a web-based search facility. The user-based design approach and the integration of human language components have made the MiTAP system a success [12] .

Healthmap (http://www.healthmap.org/en) is also a major newsbased syndromic surveillance system. It is an Internet-based alert and mapping surveillance system which integrates outbreak data from a variety of electronic sources: online news wires, Really Simple Syndication (RSS) feeds, expert-curated accounts (such as ProMEDmail), and validated official alerts (such as WHO) [6] . Data are acquired automatically every hour and characterized via text mining to determine the disease category and location of the outbreak [6] . Currently alerts are geocoded to the country scale with province-, state-, or city-level resolution for select countries. Once processed, the outbreak data is visualized on an interactive world map for userfriendly access to the original reports. Users can choose among various information sources. In addition, users can look at all diseases, or the ones they are most interested in. The alerts then appear as flags on the map. By clicking on each of them, users can get the headline and a link to the full story.

However, all the systems shown in Table 2 consider a large number of general infectious diseases, and thus none of them specifically focus on or provide a large portion of information on a particular infectious disease. They do not provide the functionality of classifying the outbreak news of a particular infectious disease into different topic categories either. In this study, we proposed a general framework for building a domain specific news monitoring system for syndromic surveillance, and a classification component to automatically classify the related online news into different topic categories.

With the rapid growth of the Internet, users are often faced with information overload and find it difficult to search for relevant and useful information on the Web. To alleviate the problem, Web crawler programs are often used. These programs exploit the graph structure of the Web by starting at a seed page and then following the hyperlinks within it to attend to other pages. This process repeats with the new pages offering more hyperlinks to follow, until a sufficient number of pages are fetched or a certain higher level objective is reached [34] . To search for important information in a specific domain, vertical search engines are often used by keeping indexes only in that domain [10] . The Web crawler programs used in vertical search engines are selective about the pages fetched and ensure as best as possible that these are relevant to the initiating topics. In Web crawler programs, adding intelligence (heuristics) into crawling strategies can help improve the performance of collecting relevant and important Web pages [34] . Table 3 shows various studies on different intelligence (heuristics) including: best first search, genetic algorithm, reinforcement learning, evolutionary algorithm, Neural Networks, Naïve Bayesian, SVM, PageRank algorithm, etc.

In order to reduce the complexity of text documents (collected by the crawling programs) and make them easier to handle, full text documents have to be transformed to document vectors which describe the contents. A simple way to transform a text document into a feature vector is by using a Bag of Words representation, where each feature is a single token [23] . In the Bag of Words representation, the semantically empty stop-words need to be removed and the remaining terms are used as the textual representation. Such subset of terms building upon Bag of Words can get the main concepts of an article [32] . The Bag of Words representation has been widely used because of its simple nature and ability to produce a suitable representation of the text [38] . However, sometimes the word-based representation of content is imprecise. It suffers from noise issues associated with seldom-used terms as well as problems of scalability where immense computational power is required for large datasets [37] . An improved representational system which addresses a majority of these shortcomings is Noun Phrases. This representation retains only the nouns and noun phrases within a document and has been found to adequately represent the important article concepts [42, 43] . As a consequence, this technique uses fewer terms and can handle article scaling better than Bag of Words [37] . A third representational technique is Named Entities, an extension of Noun Phrases. It functions by selecting the proper nouns of an article that fall within well-defined categories. This process uses a semantic lexical hierarchy [41] as well as a syntactic/semantic tagging process [28] to assign candidate terms to categories. Selected categorical definitions are prescribed by the Message Understanding Conference (MUC-7) Information Retrieval task, and they encompass the entities of date, location, money, organization, percentage, person, and time. This method allows for better generalization of previously unseen terms [37] .

No matter which representation approach is used, a typical real world textual dataset usually has a large number of features. However, not all the features are necessary to learning the concept of interest. Many of them may be noisy or redundant and feeding all these features into a model often results in over fitting and poor predictions [29] . Therefore feature selection that aims at identifying a minimalsized subset of features relevant to the target concept can be applied [13] . The objective of feature selection is threefold: improving the prediction accuracy, providing faster and more cost-effective prediction, and providing a better understanding of the underlying process that generated the data [22, 43] . A feature selection method generates different candidates from the feature space and assesses them based on some evaluation criterion to find the best feature subset [22] . Each feature selection method contains two parts: the evaluation criterion [34] Compared various machine learning algorithms and the generation procedure of candidates. The evaluation criterion is used to assess the goodness of features or feature subsets, and the generation procedure determines how to explore different candidates to find the optimal ones. Graph-based search algorithms are often used to find the optimal features [22] . Text classification assigns category information to documents which are characterized by a set of features. The dominant approach in text classification is based on machine learning techniques. It is an inductive process that automatically builds a classifier by learning the characteristics of the categories from a set of pre-classified documents [40] . The advantages of the machine learning approach include effectiveness, considerable savings of expert labor, and straightforward portability to different domains [40] . A number of studies have documented the relative merits of text classification algorithms. Well known algorithms with good performance reported in the literature include: K-nearest neighbour (KNN), learn Bayesian net (LBN), Naïve Bayesian (NB), support vector machine (SVM), etc. Using feature selection in text classification can help to select a condensed subset of more relevant features from the initial set to classify future documents [24] . In general, feature selection improves the performance of text classification by offering more concise and precise feature representations of documents [39] .

From prior research, we identified several research gaps. While there are some well known automatic news-based syndromic surveillance systems for general infectious diseases, none has been specifically designed for a particular infectious disease. For classifying online news articles, it is not clear what kind of document representation approach works best, and which machine learning algorithm yields better classification results for syndromic surveillance. Finally, in building an automatic online news classification component for syndromic surveillance, it is yet to be known how feature selection improves performance.

From the research gaps identified above, we formulated the following research questions:

1. How can we monitor online news reports and bulletin notification for syndromic surveillance? 2. Can the features generated by combining the Bag of Words, Noun

Phrases and Named Entities approaches outperform those only from the baseline Bag of Words representation in domain specific online news classification for syndromic surveillance? 3. Can feature selection help improve the performance of domain specific online news classification for syndromic surveillance? 4. Which machine learning algorithm performs better for domain specific online news classification for syndromic surveillance?

To answer the research questions, we developed the system architecture as shown in Fig. 1 , which contains three components: (1) data acquisition, (2) document representation and feature selection, and (3) classification and evaluation. These components are described in the following sub-sections.

In this component, Web crawler programs are developed to spider news articles from the Internet.

The crawler programs are set up to monitor important infectious disease news sources on the Internet. In this study, we use two sets of important FMD news sources containing 27 and 100 news websites respectively identified by the domain experts in the FMD Lab at UC-Davis. Detailed information about these news websites are described in Section 5.1. After collecting the news as HTML pages, we use keyword filtering to filter out unrelated news, thus only keeping FMD related news. The keywords used in this study are listed in Table 4 (see Section 5 for detailed description). We then store all the FMD related news into a local database. This process follows how the domain experts in the FMD Lab at UC-Davis collect and monitor FMD news from the Internet. However, the difference is that here we use automatic ways instead of the manual collecting and filtering done by the FMD Lab.

In this study, since our focus is not to examine Web crawler programs, we do not add heuristics to our crawler programs. In the future, we could add and compare different heuristics to see whether they can improve the performance of the Web crawler programs.

In document representation and feature selection component, news documents are transformed from full text versions to document To conduct feature selection, we choose Correlation-based Feature Selection (CFS) as the evaluation criterion, and Best First Search as the generation procedure. CFS is a widely used feature evaluation criterion. The advantage of CFS is that it evaluates the group of attributes together rather than individually [15] [16] [17] . CFS uses a subset evaluation heuristic which assigns high scores to subsets containing attributes that are highly correlated with the class and have low inter-correlation with each other. Hall and Holmes [17] compared the performance of six feature evaluation methods including Correlation-based Feature Selection (CFS), Information Gain Attribute Ranking (IG), Relief (RLF), Principal Components (PC), Consistency-Based Subset Evaluation (CNS), and Wrapper Subset Evaluation (WRP) in the classification on 15 standard machine learning data sets from the University of California, Irvine (UCI) collection. The study showed that CFS performed consistently well on different data sets and suggested that CFS is a good overall performer. In addition, CFS chose fewer features and therefore performed faster. Since we did not intend to compare different feature evaluation methods, we chose CFS for our feature evaluation.

Best First Search is often used as a feature generation procedure. It searches the space of attribute subsets by greedy hill climbing augmented with a backtracking facility. It does not just terminate when the performance starts to drop but keeps a list of all attribute subsets evaluated, sorted in order of the performance measure, so that it can revisit an earlier configuration [47] . With a good evaluation criterion, Best First Search can drastically reduce the amount of searching needed, thus performing very quickly.

In the classification and evaluation component, machine learning techniques are used to conduct the online news classification task, and the performances are evaluated based on standard machine learning evaluation metrics: accuracy, precision, recall, and F-measure.

Due to their good performances as reported in the literature, we chose four widely used classification algorithms to perform news classification: K-nearest neighbour (KNN), learn Bayes net (LBN), Naïve Bayesian (NB), and support vector machine (SVM). First introduced into text classification by Joachims [20] , SVM achieves the best performance for various text classification tasks [1, 33, 49] . In this study, we compared SVM with the other three algorithms to see whether it can achieve relatively higher performance on online news classification for syndromic surveillance.

To conduct evaluation, accuracy measures the overall correctness of classification:

accuracy ¼ number of all correctly classified news items total number of news items Precision, recall, and F-measure evaluate the correctness for each class. Specifically, precision indicates the correctness of classification, and recall indicates the completeness of classification. F-measure is the harmonic mean of precision and recall. precisionðiÞ ¼ number of correctly classified news items for class i total number of news items classified as class i recallðiÞ = number of correctly classified news items for class i total number of news items in class i F−measureðiÞ = 2 × precisionðiÞ × recallðiÞ precisionðiÞ þ recallðiÞ

In this study, we chose foot-and-mouth disease (FMD) related online news to conduct our experiments. FMD is one of the most devastating diseases of farm animals. It occurs throughout the world and is a significant hazard to agriculture. The 2001 epidemic in the UK led to the loss of six million livestock [4] . The total costs arising from this outbreak have been put at no less than 9 billion (http://www. fmd.brass.cf.ac.uk/). Global situational awareness can play a critical role in warning of potential and imminent outbreaks.

The FMD Lab at UC-Davis (http://fmd.ucdavis.edu/) has developed models and systems for global FMD surveillance, including the FMD BioPortal Web-based system developed jointly with the Artificial Intelligence (AI) Lab at the University of Arizona. They have also been gathering and processing FMD-related news from the FMD World Reference Laboratory, the OIE, the FAO, etc., and classifying the news into different categories. The news is collected daily, weekly or biweekly, according to the update frequencies of different websites. They also search Google and Yahoo everyday for FMD-related news in four languages by using the keywords shown in Table 4 . They perform all news gathering and classification work manually. This manual work is time consuming and labor intensive, and can also lead to information loss. However, to our knowledge, there is currently no automatic news monitoring and classification system specifically for FMD.

To monitor the FMD related news from the Internet, we developed Web crawler programs for two sets of important FMD news sources identified by the domain experts from the FMD Lab at UC-Davis. The first set, as shown in Table 5 , contains 27 online news sources that fall into four categories: news websites, government websites, international organizations, and research labs. Although all of them provide FMD outbreak related news, the news websites and government websites mainly focus on the social and economic consequences of an outbreak and general information, and the international organizations and research labs emphasize epidemiological reports and analysis. The second set of important FMD news sources are identified from a collection of 2832 pieces of important FMD news manually gathered from the Internet by domain experts from the FMD Lab at UC-Davis. These news items, reported between October 6, 2004 and January 20, 2007, come from 878 different websites including the 27 sources listed in Table 5 . As shown in Fig. 2 , only four out of the 878 websites have more than 50 news items each in the collection. They are: (1) Agrolink (www.agrolink.com.br) with 191 news items, (2) Allafrica (http:// allafrica.com) with 144 news items, (3) OIE (www.oie.int) with 141 new items, and (4) Cattlenetwork (www.cattlenetwork.com) with 95 news items. 564 other websites have only one news item each. Among all the 878 websites, there are 100 websites, each of which has more than 5 news items in the collection. These 100 websites form the second set of sources used in FMD news monitoring.

Once online news are collected as HTML pages from the two sets of important news sources using the Web crawler programs, we use keyword filtering to identify all the FMD related news. The keywords used are listed in Table 4 . Till now, we have gathered more than 180,000 FMD related news documents from the 27 sources in the first set and more than 650,000 FMD related news documents from the 100 sources in the second set.

In order to examine the news classification component, we conducted experiments on a testbed containing 1674 pieces of FMD related news. They were culled from the collection containing 2,832 important FMD news articles gathered by the FMD Lab from the Internet. We use them as the testbed for our news classification component because these 1674 news articles have category information assigned by domain experts and therefore can be used to conduct the proposed evaluation metrics. The category assigned to each news item by domain experts is used as the golden standard for evaluation.

There are three categories assigned to the test data: (1) FMD outbreak related news, (2) FMD control program related news, and (3) FMD social, economic and general information. The classifiers we built are based on these categories.

In this section, we list the three sets of hypotheses we tested in this study. All the hypotheses are obtained based on our literature review and the research gaps and questions we identified. These hypotheses are all focused on the automatic online news classification component.

For domain specific online news document representation, we aimed to compare the combined feature subsets with the Bag of Words feature subsets. As discussed in the literature review, since Noun Phrases and Named Entities features can capture more syntactic and semantic information with less noise than the basic Bag of Words features, we posit that using all three types of features can achieve better performance than using only the Bag of Words features in domain specific online news classification. The specific hypotheses tested are as follows:

H1. The combination of Bag of Words, Noun Phrases and Named Entities features outperform the baseline Bag of Words features in domain specific online news classification (regardless of whether or not feature selection is conducted).

H1a. FeatureBFS-Comb N FeatureBFS-BW.

H1b. FeatureSFS-Comb N FeatureSFS-BW.

For domain specific online news feature selection, we aimed to compare the feature subsets created by conducting feature selection with the baseline feature subsets without conducting feature selection. As discussed in the literature review, feature selection can often improve the text classification performance. Therefore, we posit that conducting feature selection can improve the performance of domain specific online news classification. The specific hypotheses tested are as follows:

H2. The selected features developed by conducting feature selection outperform the baseline features in domain specific online news classification (regardless of whether or not the three representation approaches are combined).

H2b. FeatureSFS-Comb N FeatureBFS-Comb.

For domain specific online news classification, we aimed to compare support vector machine (SVM) with the other three widely used machine learning algorithms. SVM is often reported to have the best performance in text classification, although K-nearest neighbour (KNN), learn Bayesian net (LBN) and Naïve Bayesian (NB) also perform well in general. Therefore, we posit that SVM outperforms the other three algorithms in domain specific online news classification. The specific hypotheses tested are as follows:

H3. Support vector machine (SVM) outperforms K-nearest neighbour (KNN), learn Bayesian net (LBN) and Naïve Bayesian (NB) for the selected features developed by conducting feature selection in domain specific online news classification.

H3a. SVM N KNN on FeatureSFS-BW.

H3b. SVM N KNN on FeatureSFS-Comb.

H3c. SVM N LBN on FeatureSFS-BW.

H3d. SVM N LBN on FeatureSFS-Comb.

H3e. SVM N NB on FeatureSFS-BW.

H3f. SVM N NB on FeatureSFS-Comb. 

As mentioned before, our experiments are based on the automatic online news classification component. We tested the classification performance of different feature subsets on different machine learning algorithms as summarized in Table 6 . The four classification algorithms we used, including KNN (k = 10), LBN, NB, and SVM are from the Weka Data Mining Package [147] . For each classifier, we used 90% of the news articles in the testbed for training, and predicted the class labels of the remaining 10% news articles as testing. We repeated this process one hundred times by randomly splitting the testbed for statistical analysis. For each feature subset with each classification algorithm, we report in Table 6 the accuracy, the average precision, the average recall, and the average F-measure values on the three FMD news categories. The values in bold fonts denote the best performances.

In our experiments, the classifier using SVM upon feature subset FeatureSFS-Comb achieved the best performance. It achieved the highest accuracy of 77.04%, the highest average precision of 77.10%, the highest average recall of 77.05%, and the highest average F-measure of 77.08%. We can also see from Table 6 that in most cases, the two combination feature subsets worked better than the two Bag of Words feature subsets. Specifically, in most cases, FeatureBFS-Comb had higher evaluation values than FeatureBFS-BW, and FeatureSFS-Comb had higher evaluation values than FeatureSFS-BW. By conducting feature selection, the average accuracy of Bag of Words features increased from 70.66% to 73.27%, and the average accuracy of the combination features increased from 72.13% to 74.96%.

The errors in classification were mainly caused by such news that has information related to more than one category. For example, some news articles mentioned both FMD outbreaks which belong to category 1, and plans on how to control the outbreaks which belong to category 2. Some other news articles first talked different control programs on FMD which belong to category 2, and then compared them in terms of their social and economic influences which belong to category 3. In this case, to decide which category a news article belongs to, the domain experts from the FMD Lab at UC-Davis read the content to see which topic the large portion of the article is about. However, for automatic classifiers, the results sometimes were inconsistent with the golden standard provided by domain experts.

Furthermore, in order to test our hypotheses, we conducted pair wise single-sided t tests on accuracy and average F-measure. The p values and results for the tests of our hypotheses are presented in Table 7 , where p values with ⁎ and ⁎⁎ indicate significant differences at the levels of α = 0.05 and 0.01, respectively. The underlined p values indicate that the results contradict the hypotheses. Overall, most of our hypotheses were supported by our experiments. For Hypothesis 1a, FeatureBFS-Comb significantly outperformed FeatureBFS-BW on accuracy for each of the four classification algorithms. Although Hypothesis 1a on the average F-measure was not confirmed for KNN (p = 0.0550 N 0.05), it was confirmed for the other three algorithms with p values less than or equal to 0.001. For Hypothesis 1b, FeatureSFS-Comb significantly outperformed Fea-tureSFS-BW with most p values less than or equal to 0.001, except for the accuracy of KNN (p = 0.1260 N 0.05). Overall, the combination features achieved better performance than Bag of Words features, because the combination features capture not only the common words but also some important noun phrases and named entities.

For Hypothesis 2a, FeatureSFS-BW significantly outperformed FeatureBFS-BW with most p values less than 0.0001, except for the average F-measure of SVM (p = 0.4968 N 0.05). For Hypothesis 2b, six out of the eight p values were less than 0.0001, although it was not confirmed on the average F-measure for SVM (p = 0.1499 N 0.05). Overall, the feature subsets conducted feature selection outperformed the baseline feature subsets.

For Hypothesis 3, all six sub hypotheses were confirmed for accuracy, and four of them were confirmed on the average F-measure. Overall, SVM was the best performer with selected feature subsets. The feature subsets FeatureBFS-BW, FeatureBFS-Comb, FeatureSFS-BW, and FeatureSFS-Comb contain 1473 features, 2130 features, 48 features and 56 features respectively. As shown above, FeatureSFS-Comb performed best in FMD news classification. The feature subset size was reduced significantly by using CFS + Best First Search described before. With fewer features, the performance speed of FMD news classification increased dramatically.

From our experiment results, we can see that, overall, the combination of Bag of Words, Noun Phrases and Named Entities features outperform the baseline Bag of Words features in online FMD news classification; feature selection can improve the classification performance; and SVM achieves overall better performance than the other commonly used classification algorithms. The best performance was achieved using SVM upon the selected feature subset of the combination features, i.e. FeatureSFS-Comb.

In Table 8 , we list the 56 features in FeatureSFS-Comb in alphabetical order. The bold fonts indicate some important Noun Phrases and Named Entities features that did not appear in FeatureSFS-BW (48 Bag of Words features). We believe it is because of those features that FeatureSFS-Comb achieved overall higher performance than FeatureSFS-BW in our experiments. As shown in Table 8 , those important and semantically meaningful features include: animal health service, livestock production, mad cow disease, mass vaccination, mass vaccination campaign, negative results, susceptible animals, the embargo, the FMD, the herd, the outbreak, the outbreak of foot and mouth disease, to control, vaccination campaign and vaccine doses.

Increasing globalization, population mobility and travel frequency have made the prevention and management of infectious disease outbreaks a growing concern in public health. Syndromic surveillance can protect the public's health against infectious diseases by providing effective prevention, detection, and management of infectious disease outbreaks. Among the major data sources used for syndromic surveillance, online news is an important one. Some well known automatic news-based syndromic surveillance systems exist for general infectious diseases. However, to our best knowledge, none of them specifically focus on or provide a large portion of information on a particular infectious disease.

In this study, we have described a general framework for building an infectious disease specific news monitoring and classification system for syndromic surveillance. Our experimental study is based on FMD news. For FMD news monitoring, we have set up Web crawler programs to collect news articles from important online FMD news sources. We then use keyword filtering to identify FMD related news. For FMD news classification, we compared the performance of different feature subsets on different machine learning algorithms. The results showed that the combined feature subsets including Bag of Words, Noun Phrases, and Named Entities features outperformed the Bag of Words feature subsets. Furthermore, feature selection improved the performance of feature subsets in FMD news classification. The highest classification performance was achieved when using SVM upon the selected combination feature subset (FeatureSFS-Comb).

This study has made several contributions. First, we propose a general framework for building an automatic, infectious disease specific news monitoring and classification system for syndromic surveillance, which is becoming increasingly critical for protecting the public's health but has received little investigative attention. The well known existing news-based syndromic surveillance systems are based on general infectious diseases instead of a specific disease. Our approach focusing on automatic monitoring and classifying online news for a specific disease provides an informative point of departure for continued research. With such a framework, we can automatically gather a particular infectious disease related news from the Web and organize the information into different categories, thus providing accurate and timely information to health providers. Therefore, we believe such a unique framework is important and beneficial to researchers interested in syndromic surveillance, especially on infectious diseases. Second, the techniques used to develop the framework are based on the algorithms which have consistently reported the best performances. In order to identify what kind of document representation approach works best for classifying online news articles, which machine learning algorithm yields better classification results, and how feature selection improves the classification performance for syndromic surveillance, we examine different high-performance algorithms. None of the techniques have been previously used in the context of syndromic surveillance on infectious diseases. In addition, we demonstrate the viability of using our proposed framework to automatically monitor and classify FMD related online news. The automatic approach is much more advantageous than the labor-intensive and time-consuming way of monitoring and classifying FMD related online news manually done in the FMD Lab at UC-Davis; therefore, alleviates the burden caused by the manual work of the FMD Lab. Besides FMD, this general framework can be applied to automatically monitor and classify online news related to other particular diseases. Thus, the current study provides important guidance and implications for future infectious disease specific classification component development for syndromic surveillance systems.

This study has some limitations that can also be explored further. For the news monitoring component, we first used Web crawler programs to collect all the news Web pages from the online sources and then used keyword filtering to identify the FMD related news from the large collection. However, adding intelligence (heuristics) into crawling strategies could be more effective and efficient than keyword filtering in collecting relevant and important FMD news Web pages. To evaluate our news classification component, we used a set of FMD related online news provided by the FMD Lab as our testbed instead of using the news gathered by our monitoring component, because this set of FMD news has the category information assigned by domain experts; thus, can be used to calculate the evaluation metrics. However, these news items may be cleaner with fewer noises than the news gathered by the automatic monitoring component. Thus, this may lead to better classification results in our experiment. In the future, we will use the news gathered by the Livestock production 38 Stricken automatic news monitoring component to test our classification component. In addition, we only focus on English news sources in this study. However, due to the globalization and mobility nature of infectious diseases, some important and timely news, especially the outbreak news may be reported in other languages. Future research could extend to incorporate multilingual processing component to deal with important news sources in other languages. However, even in its current state, the empirical evidence clearly indicates that our proposed framework for building an automatic, infectious disease specific news monitoring and classification system for syndromic surveillance represents a solid foundation upon which to build more advanced automatic, infectious disease specific syndromic surveillance systems.

",0.7435538768824571
Rapid Diagnostic Testing for Community-Acquired Pneumonia Can Innovative Technology for Clinical Microbiology Be Exploited?,"Two nonsynchronous events have affected the management of community-acquired pneumonia (CAP): spiraling empiricism for CAP and the ""golden era"" of clinical microbiology. The development of broad-spectrum antibiotics has led to widespread empiric use without ascertaining the etiology of the infecting microbe. Unfortunately, this approach clashes with the second event, which is the advent of molecular-based microbiology that can identify the causative pathogen rapidly at the point of care. The urinary antigen is a most effective rapid test that has allowed targeted therapy for Legionnaire disease at the point of care. The high specificity (> 90%) allows the clinician to administer appropriate anti-Legionella therapy based on a single rapid test; however, its low sensitivity (76%) means that a notable number of cases of Legionnaire disease will go undiagnosed if other tests, especially culture, are not performed. Further, culture for Legionella is not readily available. If a culture is not performed, epidemiologic identification of the source of the bacterium cannot be ascertained by molecular fingerprinting of the patient and the putative source strain. We recommend resurrection of the basic principles of infectious disease, which are to identify the microbial etiology of the infection and to use narrow, targeted antimicrobial therapy. To reduce antimicrobial overuse with subsequent antimicrobial resistance, these basic principles must be applied in concert with traditional and newer tests in the clinical microbiology laboratory.","I n this issue of CHEST (see page 1576), Shimada et al 1 have presented a systematic review and metaanalysis of the urinary antigen test for Legionella, which is one of the most successful diagnostic aids for infectious diseases today. It is a rapid test (results can be seen within 15 min of testing) that can have an impact on the point of care of one of the most common and lethal infectious diseases facing mankind, community-acquired pneumonia (CAP).

CAP remains a hot clinical topic. Emerging pathogens, such as the severe acute respiratory syndrome (or SARS) coronavirus and avian influenza A (H5N1 virus) coupled with the emerging resistance of microbes to both antibacterial and antiviral agents have made this infection a constant challenge. However, the following two important events have transpired and come to a climax in the past few years: (1) the development of antimicrobial agents directed against the most common and most feared pathogens of CAP; and (2) the advent of innovative technology for the rapid identification of microbial pathogens for CAP, one of which is the urinary antigen test for Legionella. Both events are favorable to patient care, but, paradoxically, they are nonsynchronous. CHEST Commentary pharmaceutical industry. The advent of the quinolones and respiratory tract macrolides has led to a tremendous reduction in the morbidity and mortality from the once-feared Legionnaire disease because of the high mortality seen in the original American Legion outbreak. As a result, the physician-clinician has the capability to initiate empiric therapy that covers the most likely pathogens of CAP with only one or two broad-spectrum antibiotics. The empiric broad-spectrum therapy eradicates the offending pathogen and assures a favorable outcome.

Pneumonia is the most common infectious cause for hospital admission, with notable morbidity, and the prescription of active antibiotic therapy has a salutary impact on outcome. The situation often is sufficiently urgent that antibiotic therapy only should be administered based on suspicion of diagnosis. So, the results of laboratory tests, if they are to have clinical relevance, must be rapidly available at the point of care.

The polymerase chain reaction methodology was developed successfully in the 1980s, and automated instrumentation that harnessed the power of molecular biology has been applied successfully to laboratory testing. Molecular-based tests finally have moved from the research bench to the clinical diagnostic laboratory and now are becoming commercially available. Complex instruments can survey an entire pantheon of pathogens, including viral pathogens and the classic bacterial pathogens of CAP. Moreover, the results are available within a few hours of obtaining a patient specimen. This technical advance should allow targeted antimicrobial therapy because numerous pathogens of CAP now can be identified at the point of care. It appears that the long-anticipated ""golden era"" for clinical microbiology finally can have an impact on medical practice for CAP.

Disappointingly, a few studies have shown that knowledge of the identity of the offending pathogen has less of an impact on outcome than anticipated. Targeting the pathogen with a narrow-spectrum antibiotic after laboratory results have become available often is not exploited; rather, therapy with the broad-spectrum agent (or agents) merely is continued by the physician. 2, 3 As a result, an unanticipated and insidious deemphasis of the use of clinical microbiology has oc-curred, and therein lies the asynchronicity. For example, the Infectious Diseases Society of America and American Thoracic Society guidelines 4,5 no longer recommend sputum culture, Gram stain, blood culture, and Legionella testing as a routine necessity. Disappointingly, these microbiology tests instead are listed as an option in these guidelines. So, the most important functions of a clinical microbiology laboratory have been usurped in the stampede for empiric antibiotics.

Quality assurance guidelines 6 now require that therapy be initiated within 6 h of the diagnosis of CAP, placing pressure on physicians to provide immediate antimicrobial therapy at the point of care. This has also led to the unanticipated practice of administering empiric, broad-spectrum antibiotics to patients who do not have pneumonia. 7, 8 Antimicrobial resistance is the expected consequence. A requisite waiting period required for specimen collection, delivery to the laboratory, and microbiology processing has become a major impediment to targeted antimicrobial therapy. This deemphasis of clinical microbiology has led to budgetary cuts for the laboratories and the loss of trained personnel. Moreover, many laboratories now are being moved from the proximity of the patient to a location outside of the hospital. The physical separation produces a disconnect in the collaborative nature of infectious disease management and clinical microbiology.

The urinary antigen for Legionella was developed in the late 1970s 9,10 and is one of the most successful of the rapid diagnostic microbiology tests. Unfortunately, the test only can reliably detect one species, Legionella pneumophila, and only one serogroup. Nevertheless, it has significant advantages over direct fluorescent antibody testing for Legionella, antibody serology, and culture, including its relatively low cost and rapid performance. Most importantly, the results affect management at the point of care. As a result, the urinary antigen test now is the most common method used to make the diagnosis of Legionnaire disease. A dramatic increase in the proportion of cases of Legionnaire disease diagnosed as a result of the urine antigen test has occurred worldwide. In Europe, only 15% of cases were diagnosed with use of the urine antigen test in 1995 compared with Ͼ 90% in 2006. 11 Among the 10,753 cases of Legionnaire disease reported to the Centers for Disease Control and Prevention between 1980 and 1998, a significant increase was seen in the proportion of patients with a positive urine test; diagnosis as a result of urine antigen testing in-creased from 0% to 69% over this period, and the proportion likely is higher today.

Simultaneously, the use of Legionella culture and direct fluorescent antibody testing has notably declined. 12 For example, only 5% of travel-associated cases of Legionnaire disease were diagnosed by use of culture compared with 89% diagnosed by the use of urine antigen testing. 13 Among health-care-associated cases of Legionnaire disease reported in the United States, 76% were diagnosed through the use of the urine antigen test, and only 3% through culture. 14 The test has evolved from a radioimmunoassay to an enzyme immunoassay to an immunochromatographic test (Binax NOW urinary antigen test; Binax; Portland, ME). As the metaanalysis by Shimada et al 1 has shown, among the three immunochromatographic assays, the immunochromatographic test (Binax NOW urinary antigen test; Binax) had a sensitivity and specificity of 91.8% and 100%, respectively, compared with 71.2% and 96.6%, respectively, and 31.5% and 98.9%, respectively, for the other two tests. It is important to note that with these different formats, substantial differences in sensitivity and specificity would be expected.

The metaanalysis by Shimada et al 1 gives a good overview of the advantages and weaknesses of the Legionella urinary antigen, and a number of points are worth reemphasizing. An advantage of the urinary antigen test is its simplicity of collection and the rapidity of the test results. These tests are much less influenced by prior antibiotic therapy than sputum culture. Because the specificity is high (99.1%, as shown in the metaanalysis), the message of relevance to the clinician is that when a test result is positive, initiating anti-Legionella antibiotic therapy is imperative. However, as the metaanalysis 1 also has shown, the sensitivity is not as high (74%), even with the more sensitive commercially available tests. At least one-quarter of patients with confirmed legionellosis can have a negative test result. The sensitivity also is affected by the course of the infection. In milder cases, the disease may not be sufficiently advanced such that the amount of antigen in the urine is not detectable. Although the Centers for Disease Control and Prevention reported 12 that the urine antigen test is more sensitive than a culture, we found 15 that when performed properly with selective and dye-containing media, culture is useful as a ""gold standard.""

More than 90% of cases of Legionnaire disease in patients with CAP are caused by L pneumophila, and 50% to 90% of these are caused by L pneumophila serogroup 1. 12, 16 The next most common species obtained from patients with CAP are Legionella micdadei and Legionella longbeachae. Surprisingly, L longbeachae is common in Australia and New Zealand for reasons that are uncertain. Conse-quently, the urinary antigen is less valuable in these countries. Other serogroups, especially in hospitalacquired infections, also can cause disease, particularly L pneumophila serogroup 6. The urinary antigen is insensitive for these other species and serogroups, so the diagnosis of legionellosis due to other serogroups of L pneumophila or other species depends on the recovery of the organism from the culture or from serologic antibody tests.

Another consequence of the deemphasis of clinical microbiology is the decreased use of Legionella culture. Too many hospitals outsource these tests to reference laboratories, which causes an unacceptable delay in reporting results. All too often, clinicians order a urine antigen test without submitting or requesting a sputum culture. Both the urine antigen test and the Legionella culture should be performed on site for maximal effectiveness. Urine antigen test results should be available in 3 h instead of 3 days to allow for the opportunity to initiate targeted anti-Legionella therapy quickly. The ease of performing the ICT card-type urine test makes it ideal for use in EDs, long-term care facilities, and physician offices.

We believe that culture also should be readily available within the hospital if the hospital water supply is colonized with Legionella, and 70% of hospitals in the United States may well be colonized. 17 Culture allows the isolation of the organism so that molecular fingerprinting can be performed. Microbiology laboratories should freeze these isolates or forward them to a public health laboratory in anticipation of epidemiologic investigation. The simple practice of holding respiratory specimens in the refrigerator for 7 days allows the microbiology laboratory to retrieve the respiratory specimen if or when the urine antigen test result is positive. One reason that so many cases of Legionnaire disease (including the Pittsburgh pneumonia agent or L micdadei) were seen in Pittsburgh, PA, was because of the widespread availability of culture capability in Pittsburgh community hospitals as well as in academic tertiary care centers. Thus, more cases of Legionnaire disease were diagnosed annually in the Pittsburgh area than in most states. 18 Given the recent increase in the incidence of Legionnaire disease in the United States and Europe, 19 -21 there is motivation to reverse this trend and provide public health officials with the opportunity to determine the source of exposure. The isolate cultured from the patient is useful for diagnosis, case finding, and identification of the source.

The article by Shimada et al 1 concluded with a plea for more high-quality studies. However, in our opinion, more studies of statistical rigor are not the key. The key is to resurrect the use of culture (or to develop rapid diagnostic tests that can identify all the Legionella species and serogroups) and to strengthen the role of the clinical microbiology laboratory. Although dramatic progress has been made in molecular biology, with increasing roles for nucleic acid amplification, sequencing, and typing methods, there remains a need for validation of these newer tests. The results of molecular diagnostic assays must be correlated with clinical findings and culture results.

Physicians must resurrect the application of the basic principles of infectious diseases, which are to identify the etiology of CAP and then use narrow, targeted therapy. To do so, the full clinical microbiology armamentarium with the newer molecular tests must be exploited. The return of the sputum Gram stain, sputum culture, and blood cultures also would provide information that allows the use of penicillin for pneumococcal pneumonia. 22 With the advent of sensitive rapid tests for viral pathogens (especially influenza), empiric antibacterial agent therapy, which is ineffective while promoting drug resistance, can be replaced by effective antiviral therapy. If there truly is to be a golden era of clinical microbiology and a reduction in antibiotic misuse and overuse, we will need to be smarter in the use of all of the weapons at our disposal, including traditional tests and the newer molecular-based tests.

",0.7433018846194855
Assessing factors critical to smart technology applications to mobile health care − the fgm-fahp approach,"Smart technologies present numerous opportunities for enhancing mobile health care. However, some concerns regarding the viability of smart technology applications must be addressed. This study investigated these concerns by reviewing the current practices of smart technology applications to mobile health care. As a result, five factors critical to the applicability of a smart technology to mobile health care are identified, and the fuzzy geometric mean-fuzzy analytic hierarchy process (FGM-FAHP) approach is proposed to assess the relative importance levels of the identified factors. The experimental results showed that the three most critical factors identified include: (a) the relaxation of the related medical laws; (b) unobtrusiveness; and (c) the precise need and situation of a user. Accordingly, approximately 44%, 26%, and 15% of the budget should be allocated to the realization of the three critical factors, respectively. In addition, the challenges involved and opportunities for enhancing the effectiveness of existing applications are discussed.","Smart technologies enable a mobile user to apply intelligent sensors and actuators to detect and respond to ambient conditions unobtrusively, and the results of the detection can be transmitted wirelessly to a backend service center [1] . Today, smart technology applications have become more and more prevalent in mobile health care [ 2 , 3 ] . For example, Baek et al. [4] designed a smart diagnostic chair that could measure unconstrained heart rate and blood pressure without the direct contact between embedded instruments and bare skin of the users. According to the survey by Baig and Gholamhosseini [5] , no less than fifty smart health monitoring systems have since been invented at the time of their survey. These smart health monitoring systems automate patient monitoring tasks and facilitate workflow management. A smart surveillance system to detect abnormal motions of a user in emergency situations has been devised by Jung and Chung [6] . As noted by Natarajan et al. [7] , smart sensors adopted by a mobile health care system could be connected via an Internet of things (IoT), thereby facilitating the aggregation of various information sources to judge the health problem of a user accurately.

Some smart technologies are dedicated to specific groups or situations. For example, an app was designed in Kaewkungwal te al. [8] to plan the schedule for providing antenatal care (ANC) and E-mail address: tolychen@ms37.hinet.net expanded program on immunization (EPI) services to mothers and children in an under-served region. Hirata et al. [9] and Mutiara et al. [10] equipped a smart cane with proximity sensors to detect the fall of an elderly person. The sudden movement of such an individual or an increasing distance detected between the user of a smart cane and the cane could be interpreted as a possible fall.

Purportedly, three major drivers of mobile health care in today's era of digital health transformation include: (a) the high expenditure on health care and the need for disruptive solutions; (b) the rapid and ongoing growth of wireless connectivity; and (c) the need for more precise and individualized health care [11] . So far, most applications of smart technologies to mobile health care have been intended to improve disease management (such as monitoring the health/disease status, adjusting the medication dosage, and increasing adherence to prescribed medication) and to change health behaviors [ 3 , 12 ] . For a mobile health care service to be successful, five tasks need to be fulfilled, including allocation concealment, allocation sequence determination, blinding, complete follow-up, and the measurement of effects [3] .

This study aims to identify factors critical to the applicability of a smart technology application to mobile health care. This is an urgent and critical topic for the following reasons. First, the worldwide market for mobile health care is predicted to grow from an estimated 134 billion in 2016 to 280 billion by 2021 [13] . The market scale is large with numerous opportunities for related industries and companies. Additionally, the Affordable Care Act (ACA) and related effort s in the United States are moving health https://doi.org/10.1016/j.hlpt.2020.02.005 2211-8837/© 2020 Fellowship of Postgraduate Medicine. Published by Elsevier Ltd. All rights reserved. services from fee-for-service (FFS) models to managed care, valueadded payment or value-based (VB) models that may be built on smart technology applications [14] . More recently, advances in cloud computing and big data analytics [15] have enabled the analysis of massive data collected from mobile users via smart technologies in real time. With use of earlier technological applications quickly becoming obsolete, this is a critical step to achieve more precise recommendations for individualized care services.

This study also compares the relative importance levels of the identified critical factors. To this end, the fuzzy geometric meanfuzzy analytic hierarchy process (FGM-FAHP) approach is proposed. The FGM-FAHP approach aggregates multiple experts' judgments on the relative importance levels of critical factors and derives the weight for each. These weights provide valuable information to manage activities on the realization of the critical factors under limited resources [16] . In addition, a judgment modification mechanism is also incorporated in the FGM-FAHP approach, so that an expert can modify his/her judgment subjectively if that deviates considerably from the consensus, which was not considered in [17] . Further, experts can be of unequal authority levels, so that the decision-making process can be led by an authoritative professional, which is another novelty of this study when compared to earlier studies [17] [18] [19] .

The remainder of this paper is organized as follows. The next section introduces and categorizes smart technologies and discusses how they can be applied to mobile health care. Various factors critical to the applicability of a smart technology to mobile health care are then identified. The third section details the FGM-FAHP approach to assess the relative importance levels of the identified critical factors. The fourth section presents the results of applying the FGM-FAHP approach to assisting the government of a region in allocating its budget among smart technologies for mobile health care. The fifth section discusses the challenges involved and opportunities for enhancing the effectiveness of existing smart technology applications to mobile health care. The last section makes concluding remarks.

A scoping literature review has been performed with relevant citations sourced from Google Scholar from published works over the last ten years being reviewed and clustered into three themes:

(a) smart technologies; (b) smart technology applications to mobile health care; and (c) the digital transformation of health care using smart technologies.

Van Doorn [20] classified smart technologies and systems into four categories, as shown in Table 1 .

Obviously, not all of existing smart technologies are applicable in a mobile setting. Wu et al. [21] applied the technology acceptance model (TAM) and the theory of planned behavior (TPB) to sort out key factors influencing the adoption of smart technologies for mobile health by care professionals. They concluded from a questionnaire survey that perceived service availability (PSA) and personal innovativeness in information technology (PIIT) were the most critical factors. Other researchers such as Demirkan [22] , Chen & Chiu [23] , and Chen [17] have similarly encouraged using smart technology applications for mobile environments, all of which has been summarized in Table 2 .

Based on the scoping review of recent publications, not only should a smart mobile health care technology be interconnected, unobtrusive, and adaptable, but it also must be characterized as dynamic, embedded, and intelligent [24] . Table 3 shows how these various requirements are being met by a selection of existing smart technologies that are beginning to be diffused in developed and developing countries.

Among them, smart mobile services meet all the requirements. Smart phones, smart watches, and smart connected vehicles also meet most of the requirements. Such results explain why smart phones and smart watches are the most widely applied smart technologies to mobile health care, and why the interest in smart connected vehicles is increasing. Conversely, the interest in smart glasses is decreasing, mostly because smart glasses are obtrusive. Smart clothes are also facing a decline in interest owing to the same reason.

Among smart technological devices widely adopted for mobile health care services, smart phones are clearly the most prevent [25] . The sensors on a smart phone assist the mobile health care of users in a variety of ways [26] : Amin et al. [28] , Banos et al. [35] • Single sensor function, for example, the GPS receiver may be used to inform the location of a user [27] , and the camera to transmit a visual of the injured part, or to enable videoconferencing with a distant doctor. • Multi-sensor collaboration, for example, whether an individual has fallen may be ascertained by jointly applying the accelerometer, compass, and gyroscope on one's smart phone. Even so, the data collected by various sensors may be multifaceted with the ongoing challenge of aggregating them [28] . • Multi-device collaboration, for example, a smart phone may be coupled with another smart device, typically a smart watch [ 29 , 30 ] , smart glasses [31] , or smart clothing [32] , to compensate for the inability to measure some health conditions.

Altogether, a mobile health care app supports a wide variety of useful and meaningful functions. Table 4 presents a taxonomy of such functions as noted in previous research.

Generally speaking, the majority of people who use mobile health care apps are patients and consumers. In contrast, health care professionals rarely use such apps that may be considered to be inaccurate and unsafe for providers to rely upon in administering care [ 33 , 34 ] . As of June 2017, approximately 250 free healthcare-related apps with four stars or higher ratings by their users on Google Play [36] . The apps may be classified into five major types:

• Apps that use smart phone devices to monitor, measure, track, or calculate the health conditions (such as blood pressure, heart rate, and blood oxygen) of mobile users. • Apps that introduce or recommend healthy habits, activities, sports, food, nutrition, stores, clinics, and others to users [27] . Such apps are usually coupled with a navigation service like Google Maps. • Apps that provide health-related tips, knowledge, Q&A, guides, advice, reports, and other services to users. • Apps that help users to book a clinic or order medicine. This type pf apps are always accompanied with a navigation service. • Others.

The percentages of the five types of apps are compared in Fig. 1 . Over the years, the rapid proliferation of smart technological apps has resulted in several bundles of such apps trending towards new ways of delivering mobile health care services. In the past, apps and online social networks were disjointed ways to support mobile heath care [37] . Recently, a number of mobile health care apps have supported the forming of online social networks [38] . Some of these apps are based on the co-work of participants, while others are peer-led (i.e., led by people who have more experience or professional knowledge). In contrast, some apps for online social networking, such as Facebook and Line, have been applied to support mobile health care. People share health care related information and discuss with each other in such online social networks. In addition, rich multimedia, virtual reality, and augmented reality are also incorporated into a mobile health care app to provide vivid experiences of distant health care delivery [39] . In the past, it was pads or smart glasses that were usually adopted to support such functions [ 40 , 41 ] . However, advances in smart phone technologies and the several advantages of a smart phone (e.g., light weight, unobtrusiveness, etc.) have made it a more suitable alternative in recent years. Users with visual impairments usually rely on voice-based instructions. However, this is not possible in some situations such as meetings, crowded places, and noisy environments. To overcome this challenge, Porzi et al. [42] combined the use of a smart watch and a smart phone to develop a gesture recognition system for visual impaired users. When the smart watch detects a gesture made by the user, it will then transmit a signal to the smart phone to activate certain devices such as the screen or a camera. Where the camera is used to capture an image, the image could then be compared with several predefined images, representing certain risk, a specific location, or a facility. After that, the result is transmitted to the user via smart phone vibrations.

Another innovative means of applying smart technologies for mobile health care delivery is the work of Doukas et al. [43] , which emphasizes on affording caregivers with enhanced accessibility and availability of a mobile healthcare information management system. Here, the authors established a system in which the health records and medical images of a patient may be placed on a cloud so that the patient data can now be ubiquitously accessed by the patient's attending doctor via a smart phone.

Wile et al. [40] used the accelerometer on a smart watch to record the re-emergent postural tremors (4-6 Hz) of a patient with Parkinson's disease and differentiate them from essential tremors (5) (6) (7) (8) . Several decision rules have been established for one to make meaningful use of the wave forms of the recorded results. Reasoning, or more specifically, pattern analysis [40] and/or pattern recognition [42] , plays a key role in the continuing use and successful adoption of a smart watch application.

Linner et al. [44] identified an increasing shortage of caregivers in highly industrialized countries, particularly for elderly people. This problem can be overcome by using service robots. For example, Care-O-bot 4 is a service robot composed of a transportation unit, a telepresence unit, a telemanipulation system, and a twoarmed structure. The robot can deliver food or drinks, assist with cooking, and clean [45] . Instead of developing a more sophisticated service robot, Linner et al. [44] employed multiple cooperating service robots (i.e., a service robot system). Various types of service robots were selected to fit the layout and furniture of a room and to compensate for each other. Moreover, sensors were installed throughout the room to detect signals for activating these service robots. Advantages of applying robotic technologies to health care in serving as flexible physical assistant substitutes have been summarized in Linner et al. [44] . Importantly, their work showed how robotic technologies can extend human intelligence, improve situation awareness, and offer novel methods of intuitive man-machine interaction.

However, according to the review results by Kumar et al. [46] , although the applications of smart technologies to mobile health care were innovative and appealing, their efficacy still lacked sufficient evidence. In addition, a major barrier hampering the existing applications of smart technologies is the related laws that restrict the possibility of distant medical care [47] .

Mobile health care relies heavily on long-distance data transmission. The digital nature of smart technologies confers an advantage in this regard. For example, a traditional electromyogram (EMG) sensor for monitoring muscle and neural activities is usually analog [48] , which is not suitable for long-distance data transmission. To address this issue, a number of digital EMG sensors have been invented [49] .

A digital sensor can achieve a high accuracy. For example, Wile et al. [40] used a smart watch to detect the re-emergent postural tremors of a patient with Parkinson's disease. The experimental results obtained using a smart watch were comparable to those obtained using an attached analog accelerometer. In addition, the use of a smart watch increased the ease and portability of routine measurement, as well as patients' willingness to perform it.

In telehealth (or telemedicine), smart technologies can often aid in digitalizing the physician-patient interactions, so that many routine encounters such as contact, care, advice, reminders, education, intervention, monitoring, and remote admissions may be done more efficiently and at much lower costs even when the two parties are distant apart. Telehealth applications are particularly useful when an epidemic is highly infectious. For example, in the current China-led coronavirus situations [50] , it would be much safer for would-be ""quarantined"" or ""potentially infected"" patients to interact with their doctors virtually than face-to-face [51] .

In addition, advances in three-dimensional (3D) printing (or additive manufacturing) technologies are also conducive to the digital transformation of medical services such as transplantation. Traditionally, the organ required by a patient has to be prepared and transported to the hospital where the patient is. Now, the 3D file of the required organ can be transmitted via the Internet to the 3D printer of the hospital to print without transportation [52] .

The prevalence of the Internet has raised many opportunities for the digitization of health care services. Without the Internet, digital medical records must be transported physically. By contrast, placing medical information on a cloud makes it easier to share such information across regions [43] . Today, cross-border transportation has become more and more convenient. As a result, the global spread of disease has become faster. Easily and quickly sharing the medical information at hand is a viable way for different regions to cooperate in fighting the same disease.

To determine the relative importance levels of critical factors, the FGM-FAHP approach is proposed in this study. This approach comprises two phases: FGM for aggregating the pairwise comparison results by all experts (Phase 1), and FAHP for deriving the weights of critical factors (Phase 2), as described below. 

In Phase 1 of the proposed methodology, we first gather P experts from the related fields are gathered. In the extant literature, the value of P ranges from 3 to 30 [53] . With more experts, more diversified judgments may be sourced. However, it also becomes difficult for experts to reach a consensus. Smart technology applications for mobile health care is a relative new issue. It is not easy to gather a number of qualified experts for this issue. In addition, involving many experts may not be conducive to reaching a consensus.

Each expert compares the importance levels of critical factors in pairs. Only positive comparisons are accepted, i.e., a critical factor is more important than another to some degree. The accepted ratio is 0.5. Assuming the relative importance of critical factor i over critical factor j compared by domain expert p is indicated with ˜ r i jp ; i , j = 1-9; i = j ; p = 1-P . The following linguistic terms are provided to the experts to choose from [54] :

As important as: ˜

Slightly more important than: ˜

Considerably more important than: ˜

Extremely more important than: ˜

Absolutely more important than: ˜ r i jp = ( 7 , 9 , 9 ) All parameters and variables in the proposed methodology are given in or approximated with triangular fuzzy numbers (TFNs). In addition, after ˜ r i jp is chosen, its counterpart ˜ r jip is set as

(1)

to be consistent with the following FAHP operation and avoid the unnecessary increase in the uncertainty. Then, the comparison results by the experts are aggregated using FGM:

where ˜ r i j is the aggregation result of the relative importance of critical factor i over critical factor j . ω p is the authority level of expert p ; ω p ∈ R + . ˜ r i j is no longer a TFN, but can be approximated with a TFN as ˜ r i j ∼ = r i j1 , r i j2 , r i j3

where

Subsequently, the fuzzy pairwise comparison matrix ˜ R can be constructed based on the values of { ˜ r i j }. Finally, experts whose judgments are too far away from the aggregated result will be asked to modify their judgments. The authority level of an expert who refuses to modify his/her assessment will be adjusted until the distance between each expert's judgment and the aggregated result is small enough.

FAHP [55] is subsequently applied to derive the weight of each critical factor. Let ˜ R be the fuzzy pairwise comparison matrix

The fuzzy maximal eigenvalue ( ˜ λ) and the corresponding fuzzy eigenvector ( ˜ x ) of ˜ R satisfy the following equation:

where (-) and ( ×) denote fuzzy subtraction and multiplication, respectively; I is the identity matrix.

α-cut operations can be applied to solve for ˜ λ and ˜

x ,

and

α = 0-1. If they are approximated with TFNs, then ˜ λ ∼ = min λ (0) , λ (1) , max λ (0) (11) and ˜ x ∼ = min x (0) , x (1) , max x (0) .

Theorem 1. ˜ x can be approximated as [ 55 ] ˜

˜ x determines the importance (weight) of critical factor i .

Proof. (13) is a fuzzy weighted average (FWA) problem that can be solved by applying α-cut operations:

Therefore,

where 

Theorem 2 is proved. The fuzzy consistency ratio ( CR ) can be used to assess whether there is enough consensus among the experts:

where RI is the random consistency index (see Table 5 ) [56] . It is expected that CR ≤ 0.1 [56] .

The FGM-FAHP approach has been applied to assist the government of a region in allocating its budget for developing smart technologies for mobile health care. To this end, six experts (including a professor researching smart technology management, two health care professionals, an assistant professor of health policy and management, and two professors of technology management) were asked to perform pairwise comparisons of factors critical to the applicability of a smart technology to mobile health care. From the scoping review, five critical factors have been identified as relevant in deploying smart technologies within a mobile health context: Table 6 .

The pairwise comparison results were mapped to the corresponding TFNs that were then aggregated using FGM. The authority levels of the experts were 1.0, 1.25, 1.25, 1.25, 1.0, and 1.0, respectively. The results are summarized in Table 7 .

Consistency improved after aggregating these experts' judgments, as illustrated in Fig. 2 . In addition, taking ˜ w 1 as an example, the aggregation results were compared with those derived by the experts in Fig. 3 . Obviously, the results derived by Expert #2 were farthest from the aggregation result. Therefore, Expert #2 was asked to modify his pairwise comparison results subjectively by considering the aggregation results (i.e. Table 7 ). After consideration, Expert #3 made the following modifications: The aggregation results after modification are shown in Table 8 . 

Based on the experimental results, the following observations can be made:

• The relaxation of the related medical laws may be considered the most important factor, followed by unobtrusiveness, and then the correct identification of the need and situation of a user. • The derived fuzzy weights could be used to guide the government of a region in allocating its budget, as shown in Fig. 4 . It is noted that the values shown in this figure are flexible, since all weights are given in TFNs. • The fuzzy consistency ratio was CR = ( 0 . 089 , 0 . 756 , 6 . 182 ) (23) which revealed that there was some consensus among experts since CR could be less than 0.1. However, the upper bound of the fuzzy consistency index was high, showing that there was considerable uncertainty behind such a judgement. Given that The results of pairwise comparisons.

p Pairwise Comparison Results 1 (i) ""Unobtrusiveness"" is considerably more important than ""supporting online social networking"".

(ii) ""The relaxation of the related medical laws"" is slightly more important than ""unobtrusiveness"". (iii) ""Unobtrusiveness"" is extremely more important than ""the size of the health care market"". (iv) ""Unobtrusiveness"" is considerably more important than ""the correct identification of a user's need and situation"".

(v) ""The relaxation of the related medical laws"" is considerably more important than ""supporting online social networking"". (vi) ""Supporting online social networking"" is considerably more important than ""the size of the health care market"".

(vii) ""The correct identification of a user's need and situation"" is slightly more important than ""supporting online social networking"".

(viii) ""The relaxation of the related medical laws"" is considerably more important than ""the size of the health care market"".

(ix) ""The relaxation of the related medical laws"" is slightly more important than ""the correct identification of a user's need and situation"". (x) ""The correct identification of a user's need and situation"" is extremely more important than ""the size of the health care market"".

2 (i) ""Unobtrusiveness"" is slightly more important than ""supporting online social networking"".

(ii) ""The relaxation of the related medical laws"" is considerably more important than ""unobtrusiveness"". (iii) ""Unobtrusiveness"" is considerably more important than ""the size of the health care market"". (iv) ""Unobtrusiveness"" is slightly more important than ""the correct identification of a user's need and situation"".

(v) ""The relaxation of the related medical laws"" is considerably more important than ""supporting online social networking"". (vi) ""Supporting online social networking"" is considerably more important than ""the size of the health care market"".

(vii) ""The correct identification of a user's need and situation"" is considerably more important than ""supporting online social networking"".

(viii) ""The relaxation of the related medical laws"" is extremely more important than ""the size of the health care market"".

(ix) ""The relaxation of the related medical laws"" is slightly more important than ""the correct identification of a user's need and situation"". (x) ""The correct identification of a user's need and situation"" is considerably more important than ""the size of the health care market"".

Unobtrusiveness"" is extremely more important than ""supporting online social networking"".

(ii) ""The relaxation of the related medical laws"" is slightly more important than ""unobtrusiveness"". (iii) ""Unobtrusiveness"" is considerably more important than ""the size of the health care market"". (iv) ""Unobtrusiveness"" is slightly more important than ""the correct identification of a user's need and situation"".

(v) ""The relaxation of the related medical laws"" is considerably more important than ""supporting online social networking"". (vi) ""Supporting online social networking"" is slightly more important than ""the size of the health care market"". (vii) ""Supporting online social networking"" is slightly more important than ""the correct identification of a user's need and situation"".

(viii) ""The relaxation of the related medical laws"" is considerably more important than ""the size of the health care market"".

(ix) ""The relaxation of the related medical laws"" is slightly more important than ""the correct identification of a user's need and situation"". (x) ""The correct identification of a user's need and situation"" is extremely more important than ""the size of the health care market"". 4

(i) ""Supporting online social networking"" is as important as ""unobtrusiveness"".

(ii) ""The relaxation of the related medical laws"" is slightly more important than ""unobtrusiveness"". (iii) ""Unobtrusiveness"" is extremely more important than ""the size of the health care market"". (iv) ""Unobtrusiveness"" is slightly more important than ""the correct identification of a user's need and situation"".

(v) ""The relaxation of the related medical laws"" is slightly more important than ""supporting online social networking"". (vi) ""Supporting online social networking"" is considerably more important than ""the size of the health care market"". (vii) ""Supporting online social networking"" is as important as ""the correct identification of a user's need and situation"". (viii) ""The relaxation of the related medical laws"" is considerably more important than ""the size of the health care market"". (ix) ""The relaxation of the related medical laws"" is slightly more important than ""the correct identification of a user's need and situation"". (x) ""The correct identification of a user's need and situation"" is extremely more important than ""the size of the health care market"".

Unobtrusiveness"" is slightly more important than ""supporting online social networking"". (ii) ""Unobtrusiveness"" is considerably more important than ""the relaxation of the related medical laws"". (iii) ""Unobtrusiveness"" is slightly more important than ""the size of the health care market"". (iv) ""The correct identification of a user's need and situation"" is slightly more important than ""unobtrusiveness"".

(v) ""Supporting online social networking"" is slightly more important than ""the relaxation of the related medical laws"". (vi) ""Supporting online social networking"" is considerably more important than ""the size of the health care market"". (vii) ""Supporting online social networking"" is slightly more important than ""the correct identification of a user's need and situation"". (viii) ""The relaxation of the related medical laws"" is slightly more important than ""the size of the health care market"". (ix) ""The correct identification of a user's need and situation"" is considerably more important than ""the relaxation of the related medical laws"". (x) ""The correct identification of a user's need and situation"" is considerably more important than ""the size of the health care market"". 6 (i) ""Unobtrusiveness"" is slightly more important than ""supporting online social networking"".

(ii) ""The relaxation of the related medical laws"" is slightly more important than ""unobtrusiveness"". (iii) ""Unobtrusiveness"" is extremely more important than ""the size of the health care market"". (iv) ""Unobtrusiveness"" is slightly more important than ""the correct identification of a user's need and situation"".

(v) ""Supporting online social networking"" is slightly more important than ""the relaxation of the related medical laws"". (vi) ""Supporting online social networking"" is slightly more important than ""the size of the health care market"". (vii) ""The correct identification of a user's need and situation"" is considerably more important than ""supporting online social networking"". (viii) ""The relaxation of the related medical laws"" is slightly more important than ""the size of the health care market"".

(ix) ""The relaxation of the related medical laws"" is slightly more important than ""the correct identification of a user's need and situation"". (x) ""The correct identification of a user's need and situation"" is extremely more important than ""the size of the health care market"".

smart technology applications to mobile health care are still in their infancy, this finding was not surprising. • A parametric analysis was conducted to assess the effect of the number of experts ( P ) on the fuzzy consistency ratio ( CR ). The results are shown in Fig. 5 . In this experiment, two experts were sufficient to minimize (or optimize) CR . However, such a conclusion was based on consistency only. Involving more experts, so as to consider diversified viewpoints, was also meaningful to the practice.

• Two existing methods, FGM-fuzzy extent analysis (FEA) [57] and the FGM-alpha cut operations (ACO)-FWA method [17] , were also applied to the collected data for comparison. In the FGM-FEA method, experts' judgments were aggregated using FGM. Then, the weight of a critical factor was set to its minimal degree of being the maximum. As a result, the weight of a critical factor was estimated in a crisp value and did not need to be defuzzified. In the FGM-ACO-FWA method, experts' judgments were also aggregated using FGM. Then, the weight of a critical factor was derived using ACO. The results of applying various methods are compared in Table 9 . The results using the FGM-ACO-FWA method were close to those using the proposed methodology. However, the FGM-ACO-FWA method was not able to consider experts' unequal authority levels. In addition, it was possible to obtain weights with zero values using the FGM-FEA method. • The execution time of the proposed methodology using MAT-LAB on a personal computer (PC) with an i7-7700 CPU of 3.6 GHz and 8 GB RAM was less than 1 s, while that of the FGM-ACO-FWA method was up to 21 s, which showed the effectiveness of the proposed methodology. In addition, experts were allowed to modify their judgments in the proposed methodology, but were not allowed in the FGM-ACO-FWA method.

Existing smart technology applications to mobile health care face several challenges. First, a mobile health care system should support users in an unobtrusive, interconnected, adaptable, dynamic, embedded, and intelligent manner [24] . However, it is difficult to invisibly integrate smart technologies into our daily lives [58] . Another issue is the lack of dedicated tools. Although a number of smart technologies can be applied to assist mobile health care delivery, tools specifically devised for implementing mobile health care are still lacking [35] . In addition, the real-time availability information of a service facility is critical to the effectiveness of a mobile healthcare service. However, most service facilities are reluctant to share their scheduling commitments with each other.

There are a number of issues associated with smart phone applications. First, using a smart phone actively or passively (e.g., hearing an alert without picking up the smart phone) affects the concentration of a user and may cause safety problems while walking, driving, or working [59] . It is even more difficult for people with low or impaired vision to use smart phones, which prevents them from using the related applications [42] . This difficulty can be resolved by providing them with screen readers or allowing them to give voice or gesture commands (with the aid of a smart watch) [42] . Even when a smart phone is used safely, detecting the location of the user by using the GPS module on his/her smart phone is subject to imprecision [29] , which affects the quality of inputs to a health care app. Existing health care apps are also subjected to several difficulties [60] . For example, answering numerous questions is not always convenient to a mobile user. To address this problem, a ""short-form"" survey is usually adopted to assess the functional health and well-being of a mobile user [59] . However, addressing all the concerns of a mobile user by applying this method is impossible. This leads to misjudgment by the system server [60] . Other problems include the breach of a user's confidentiality, the conflicts of interests, malfunction, the lack of business models, and privacy issues [33] .

Reasoning is the most critical yet difficult step in a mobile health care service [ 62 , 63 ] . Conventional reasoning methods include Petri nets, mathematical programming, simulation, and

JID: HLPT [m5G; February 20, 2020; 15:16 ] ordered weighted average (OWA). Rossetti et al. [64] applied simulation to assess the benefits of using robots for transportation in a hospital. Augusto and Xie [65] formulated and solved a mixedinteger linear programming problem for planning the transportation and replenishment of medicine for the pharmacy of a hospital. Alfonso et al. [66] constructed a Petri net for modeling the mobile behaviors of blood donors. However, a Petri net is based on numerous assumptions on probability distributions and parameter independence. Chen [27] solved a fuzzy integer nonlinear programming problem to determine the no-wait path to each nearby clinic for a patient. Then, he applied OWA to determine the clinic that was most suitable for the patient. However, the method was either too theoretical or not adequately efficient for real-time applications. To solve this problem, a recent study proposed a heuristic that could be easily implemented for online applications [67] . However, the heuristic could not guarantee the global optimality of the recommendation.

Notwithstanding, several opportunities may leverage on extending the applications of smart technologies to mobile health care. For example, mobile health care systems based on apps are expected to increase. Such a trend offers clinics and hospitals opportunities to advertise themselves, elevates the users' accessibility to clinics and hospitals, and helps balance the loads on participating facilities [24] . To this end, Banos et al. [35] developed a framework named mHealthDroid that could be referred to in developing apps for assisting the delivery of mobile health care services.

Another opportunity comes from advances in cloud computing. For example, the system server of a mobile health care system can be easily developed using a platform-as-a-service (PaaS) cloud service. The most notable benefit of using cloud computing is that the system administrator does not need to acquire, set up, or maintain the system software and hardware. Some cloud service providers such as Google Cloud can help develop system servers that support mobile apps. In addition, new techniques for balancing the loads on health care facilities and resources have been proposed in the recent years. Health care facilities and resources in a mobile health care system are recommended and allocated to users through the intervention of the system server, which attempts to balance the loads on health care facilities and resources to prevent them from congestion or underuse, thereby providing opportunities to improve their performance. For example, the average waiting time in a crowded clinic can be reduced, whereas the utilization of an infrequently visited clinic can be improved. In light of this, users' requirements can be more effectively mapped to the capabilities of clinics and hospitals.

In sum, innovative methods of applying smart technologies to mobile health care remain to be ingeniously developed [68] . For example, the face of a driver can be captured and recognized to determine whether the driver is tired or drunk to initiate automatic driving for the driver's safety.

Smart technologies have been universally applied to mobile health care. However, there are still a number of difficulties hampering the effectiveness and efficiency of existing smart applications technologies to mobile health care. Identifying key factors and methods that can inform on the perceived applicability and practical innovativeness of smart technologies in mobile health care delivery are clearly some of the paramount steps in breaking down such difficulties. To this end, this study reviews the current practices of smart technologies to mobile health care. As a result, five critical factors were identified as unobtrusiveness, supporting online social networking, the relaxation of the related medical laws, the size of the health care market, and the correct identification of the need and situation of a user. Importantly, the FGM-FAHP approach is proposed to assess the relative importance of the identified critical factors. Compared with existing methods in this field, the FGM-FAHP approach is able to consider unequal experts' authority levels and incorporates a modification mechanism to enhance consistency.

Moreover, an experiment has been conducted to further examine the effectiveness of the FGM-FAHP approach. After analyzing the experimental results, the following conclusions may be drawn:

• The three most critical factors are the relaxation of the related medical laws, unobtrusiveness, and the correct identification of the need and situation of a user. • Accordingly, approximately 44%, 26%, and 15% of the budget should be allocated respectively towards achieving these three critical factors.

Depending on the target population, specific smart technology applications for mobile health can be compared using the proposed methodology in a future study. In addition, more sophisticated mechanisms needs to be designed to facilitate the aggregation or modification of expert judgments. These constitute some directions for future research.

This study was sponsored by the Ministry of Science and Technology, Taiwan, Republic of China under Grant MOST 106-2221-E-035-063-MY3.

Not required.

None declared.

",0.7421246000936502
Steps to a Sustainable Public Health Surveillance Enterprise Steps to a Sustainable Public Health Surveillance Enterprise A Commentary from the International Society for Disease Surveillance Steps to a Sustainable Public Health Surveillance Enterprise OJPHI,"At a time when populations are changing and disease outbreaks and other events of public health significance pose increasing risks to global health, economic stability, and national security, it is essential that, as a nation, we invest in the systems needed to promote and protect the public's health.","Online Journal of Public Health Informatics * ISSN 1947-2579 * http://ojphi.org * Vol. 5 

In 2002, the Institute of Medicine (IOM) reported that the public health system in the United States had a multitude of deficiencies that impact the ability to effectively conduct public health surveillance. These included outdated and vulnerable technologies; a public health workforce lacking training and reinforcements; lack of real-time surveillance and epidemiological systems; and ineffective and fragmented communications networks. 1 While considerable headway has been made since the IOM report was published, there is still evidence of a need for further improvements. A recent report by Trust for America's Health, for example, found that there are persistent gaps in the ability of state and local public health agencies to respond to events ranging from bioterrorist threats to natural disasters and disease outbreaks. 2 The question is-how can we reduce these gaps? Nationwide and globally, rapid changes in health information systems, cloud computing technologies, communications, and global connections are catalyzing a re-examination of disease surveillance as an enterprise that needs coordinated and integrated system elements. Sustainable surveillance, which we define as ongoing data collection, analysis, and application, coupled with a capability to respond to novel demands, is needed to ensure that public health agencies can perform reliably regardless of shifts in public health funding and priorities. The ISDS Sustainable Surveillance Workgroup identified the following steps to maintain and advance the public health surveillance enterprise:

1. Recognize systematic and ongoing public health surveillance as a core public health function that is essential for population health, economic stability, and national security. 2. Create and support funding mechanisms that reinforce enterprise (i.e., integrated systems), rather than categorical (i.e., disease or program specific) surveillance infrastructures and activities in order to reduce inefficient silos, leverage resources, and foster synergies. 3. Oppose further cuts to spending for surveillance activities. 4. Invest in surveillance workforce development to build competencies and improve organizational capacity to utilize technological advances in surveillance practice. 5. Advance a rigorous surveillance research and evaluation agenda that will deepen the understanding of community health, identify best practices, and provide evidence for decision-making. 

Public health surveillance is defined as, ""the systematic and ongoing collection, management, analysis, interpretation, and dissemination of information for the purpose of informing the actions of public health decision makers."" 3 In addition to providing information about the health status of our communities, surveillance is a foundation of emergency preparedness, food safety, infectious disease outbreak prevention and control, chronic disease assessments, and other key areas that protect the health, economy, and security of the public. While public health surveillance policy and practice have been indicated as priorities for policymakers at the national and global levels, [4] [5] [6] [7] questions remain about how to move forward from planning to implementation, especially in a time of critical cuts to federal funding.

Progress in health information technology (IT) and the increased use of electronic data and new data streams offer great potential for innovation in surveillance science and practice. For instance, the self-reporting of health information through social media (e.g., Twitter), as well as crowdsourcing projects such as Flu Near You (www.flunearyou.org) offer new options for collecting timely data. In addition, the Health Information Technology for Economic and Clinical Health (HITECH) Act, 8 legislated as part of the American Recovery and Reinvestment Act (ARRA) of 2009, is fueling the adoption of electronic health record (EHR) systems in the U.S. 9 In return for financial subsidies to implement EHR systems, hospitals and doctors are required to share data for public health purposes 9 with the intent to improve both population health outcomes and the quality of clinical practice. Sustainable surveillance systems have the potential to advance both of these goals. 10

The value of public health surveillance 1. Recognize systematic and ongoing public health surveillance as a core public health function that is essential for population health, economic stability, and national security.

Public health surveillance data is the foundation of public health programs and is required for a number of purposes, including: to demonstrate the size and impact of the public health problem being addressed by a program; to identify the population groups to which additional prevention efforts should be directed; to determine whether the problem is growing in size or abating; to provide feedback to data providers; and as part of an overall program evaluation strategy.

The significant health impacts and economic costs of disease outbreaks illustrate the critical importance of effective public health surveillance and rapid response, as well as the cost of inaction. 11 Table 1 provides examples of the health and financial burdens posed by some naturally occurring and intentional infectious disease outbreaks. The values reported in Table 1 do not fully reflect additional indirect costs of diseases and their potentially crippling effects on a community, nor do they address costs that are underreported/ unreported due to lack of data. Higher rates of illness, for example, can lead to lower worker productivity, 11 while premature mortality can reduce the size of the labor force, both of which have economic ramifications.

There is growing evidence that these economic and societal costs can be mitigated by surveillance systems that are stable; a stable system provides the best foundation for identifying whether the problem being addressed is getting bigger or smaller or disproportionately affecting a section of the population, etc., while still allowing flexibility to provide useful information quickly about emerging issues. The optimum mix of stability and flexibility will depend on the purpose(s) of surveillance and the particular health condition under surveillance. For example, in OJPHI the case of SARS, an effective surveillance system has the potential to decrease the size of an epidemic by one-third and the duration by 4 weeks, with significant cost savings. 25 Another study found that the early detection of an outbreak of highly infectious bacterial meningitis saved approximately $2 for every dollar invested in infectious disease surveillance. 26 Yet another evaluation of surveillance practice found that technological improvements in a sentinel influenza-like illness (ILI) surveillance system in Virginia saved over $9,500 (1,992 hours) in staff-time during the 2007-2008 influenza seasons. 27 Ongoing surveillance can also inform the design and evaluation of prevention and intervention programs in order to control the escalating costs associated with chronic diseases in the U.S. and abroad. 28 Some experts forecast that chronic disease prevention programs could save up to $48.9 billion per year by 2030, 29 while others predict applying electronic medical record implementation and networking to the prevention and management of chronic disease will exceed the currently projected $81 billion in annual savings. 30

Siloed surveillance systems are outdated, inefficient, and incapable of meeting today's demands for electronic data exchange and for the informatics capabilities needed to use the information for maximum benefit. Integrated programs and collaboration, on the other hand, facilitate the efficient management of the complex, varied, and proliferating issues and information sources that exist today. The nature of public health surveillance also lends itself to multiple-purpose approaches in that strategies for preventing and controlling diseases, such as West Nile virus, are to a great extent the same as for an influenza epidemic, a foodborne disease outbreak, or a bioterrorist attack. 31 Technology that enhances communication and data sharing across disease programs, surveillance systems, and even across jurisdictions increases the ease of obtaining and disseminating useful information to a broad audience, including public health agencies, healthcare providers, policymakers, and the general public. 6, 32 This rapid information exchange not only facilitates timely response, but can also reduce emergency room visits, hospital admissions, and even costs of care. 33 However, many health departments currently have systems that are not flexible enough to respond to changing health IT needs, which makes it difficult to deliver information when and where it is needed. 4 Disease or program-specific funding also exacerbates program vulnerability to funding and budgetary cuts. For example, when funding is earmarked for specific purposes (e.g., emergency preparedness and associated surveillance systems), and then is reduced, such as has occurred for public health emergency preparedness cooperative agreement funding through CDC in the past seven years, 34 it can undermine and reverse efforts to establish sustainable systems that serve multiple crosscutting purposes throughout public health. 

By contrast, an enterprise approach provides a cohesive framework that will better equip public health practitioners to address the challenges of processing large volumes of electronic data, and the concomitant analytical and visualization requirements. Specifically, enterprise funding supports a reliable, flexible infrastructure that can adapt to technological and information requirement changes, and allows for ongoing data collection and the integration of new data sources to advance all-hazard preparedness. A 2004 White House memo acknowledged how programmatic funding can lead to inefficiencies and redundancies in system acquisitions and usage and called for applying technological and human resources across programs. 35 By encouraging collaboration within and between departments, surveillance professionals can take advantage of shared platforms and resources to optimize data collection, analysis, storage, and dissemination, thus helping to reduce operational costs and improve efficiency. For example, collaboration could create opportunities for the effective integration of syndromic and reportable disease data for public health use. 36

A lack of consistent and sustainable funding is hampering the necessary expansion and improvement of public health surveillance systems at local, state, and national public health agencies. A 2010 survey of local health departments conducted by the National Association of City and County Health Officials (NACCHO) found that 72% of local health departments reported insufficient funding as one of their major barriers to modernizing their IT systems. 37 Health data collection systems that take advantage of recent technological advances have proven to be more cost effective and sustainable in the long-term. 38 Stable funding is essential to supporting the adoption of hardware and software systems as they become available, leading to a robust and sustainable public health surveillance infrastructure able to integrate, manage, and communicate the plethora of data necessary to generate actionable results. 39

The new age of disease surveillance requires a skilled public health workforce able to manage large volumes of increasingly complex electronic information, to understand the data flows, and to extract meaning from them. This calls for sophisticated and integrated competencies in public health informatics, epidemiology, statistics, and other areas, and the ability to present findings, draw conclusions, and make recommendations based on surveillance data. Furthermore, in addition to needing people who can effectively operate existing surveillance systems and carry out tasks (such as the onboarding process for collecting newly available EHR data) there is also demand for people who can identify and assess new opportunities for surveillance and design new systems that take advantage of these opportunities. 6 Online Journal of Public Health Informatics * ISSN 1947-2579 * http://ojphi.org * Vol. 5, No. 2, 2013

Attracting and retaining experts in these fields is especially challenging in light of the comparatively low base salaries allotted to public health workers compared to the salaries of technology-intensive positions in other sectors. 40 To align the surveillance workforce with new demands, the ISDS Sustainable Surveillance Workgroup suggests the following approaches:

• Provide training programs for existing and prospective public health workers to equip themselves with the necessary expertise and skills to work in rapidly evolving IT systems. • Promote public health careers at the primary, secondary, undergraduate, and graduate levels across disciplines. • Provide competitive salaries to recruit and retain a workforce skilled in public health surveillance and informatics.

5. Advance a rigorous research and evaluation agenda that will deepen the understanding of community health, identify best practices, and provide evidence to inform decision-making.

Research and evaluation play an important role in connecting the processes of information collection, information use for decision-making, and translation of decisions to actions and measurable outcomes. Research-based evidence and evaluation results can help to identify the limitations and benefits of different surveillance procedures for better decision-making and more effective resource allocation. Investing in research and applying the rigors of science to public health surveillance questions leads to informed decisions on how best to direct efforts and resources.

In addition, periodic evaluations of surveillance infrastructuresthe systems and people-are needed to assess return on investment and opportunities for quality improvement.

Effective and efficient surveillance systems are proven to save money and lives. The ability to detect and respond to known and emerging pathogens is central to protecting and maintaining population health. 41 The breakdown or absence of a stable public health surveillance infrastructure, on the other hand, can undermine efforts to mitigate disease outbreaks and other public health events. 31 Public health surveillance systems built on a strong infrastructure of core workforce competencies, information systems, and organizational capacity, 42 and supported by consistent and enterprise-based funding, are essential if we are to understand and respond to the real and growing threats to population health. By providing political commitment and financial support to this issue, decision makers can play an active role in advancing the health of individuals, communities, and nations.

",0.7410207284960633
Future Veterinary Diagnostics,"The development of rapid, accurate, and sensitive diagnostic methods for detecting pathogens is the basis for treating, controlling, and eradicating infectious diseases of veterinary importance. Scientific and technological advancements have revolutionized the field of veterinary diagnostics. Genome sequencing has allowed efficient, sensitive, and specific diagnostic assays to be developed based on the detection of nucleic acids. The integration of advances in biochemistry, proteomics, engineering, and medicine offers enormous potential for the rapid and accurate diagnosis of viral, microbial, genetic, and metabolic disease. In the future, polymerase chain reaction assays, microarray testing, genomic analysis, and metabolic profiling will be accomplished in a rapid, portable, sensitive, and cost-efficient manner.","S ince the discovery in 1953 by James D. Watson and Francis Crick that DNA and RNA provide the code to life, there has been a global effort to understand how the genome sequences of numerous organisms are related to health and disease.

The rapidly advancing field of diagnostics, tests used to detect a medical condition or causative pathogen responsible for an infection, will drastically change the practice of veterinary medicine in the near future. The ability to detect nucleic acids has had and still has a major impact on diagnostic processes in clinical medicine. 1 Classical pathogen detection and identification are based on culture methodology, biochemical tests, and microscopy. The culture method is still a core technology in many clinical laboratories because it can provide important information about the viability of the pathogen and its susceptibility to therapeutic agents. Likewise, the historic gold standard for diagnosing viral infection has been selective cultivation followed by electron microscopy. Recovery of the live virus remains a very slow, technically demanding, and expensive diagnostic tool.

The development of rapid, accurate, and sensitive diagnostic methods for detecting disease pathogens is fundamental for treating, controlling, and eradicating infectious disease. Rapid scientific and technological developments have revolutionized the possibilities of diagnosing and treating disease. Researchers now have tools for observation at the level of the atom, to sequence entire genomes, and to understand the molecular basis of disease. 2 Recent biotechnological developments in microtechnologies and nanotechnologies have led to the proliferation of new, rapid diagnostic tests that hold promise for the improved management and control of infectious disease in animals. Emerging technologies are rapidly being adapted to improve our ability to de-tect new etiological agents, to provide rapid, accurate diagnostic tests for previously difficult infections, and to develop new response-based paradigms for diagnostic testing. Molecular tools in particular are advancing toward laboratory-on-chip and fully integrated technologies that are capable of same-day detection and pathotyping of agents in clinical specimens. Future diagnostic technologies that are rapid, portable, low cost, and of greater sensitivity/specificity will aid the veterinarian in the early disease detection in a point-of-care setting.

Scientific advancements over the previous 2 decades have enabled the sensitive detection and characterization of bacterial and viral nucleic acids. Amplification technologies (e.g., polymerase chain reaction [PCR] , nucleic acid sequence-based amplification [NASBA] ) allow for specific detection of genetic targets. Furthermore, technological improvements in sequence determination systems allow for organisms to be fully characterized as to genotype, subtype, variant, mutants, and resistance patterns.

For the veterinary clinical laboratory, commercially standardized assays are not readily available. Most laboratories offering nucleic amplification tests (NAT)-based diagnostics use methods developed inhouse or those adapted from research publications. There is no standardization among these in-house assays or an official regulatory body to monitor quality assurance. Differences in laboratory experience and technique, primer and probe combinations, methods of nucleic acid isolation, and control standards contribute to the reproducibility and variability observed among laboratories. 1 Assays must be precise and reproducible and have positive and negative controls in place to prevent invalid test results. It is also recommended that these assays have an internal control to validate nucleic acid isolation and to test for amplification inhibitors.

In a blind study, veterinary laboratories offering NAT diagnostics for psittacine circovirus were evaluated. 3 The findings of this investigation revealed that accuracy was 100% for 2 laboratories, 95% for 2 laboratories, and 71% for 1 laboratory, whereas 1 laboratory had a test specificity of 20%. 3 The authors concluded that although results from most laboratories were accurate, both false-positive and false-negative results were reported by at least 2 laboratories. They also concluded that 1 laboratory may be generating large numbers of falsely reported results. Veterinarians must realize that the accuracy of re-sults depends completely on the accuracy of the operating standards of the individual laboratory. Laboratories should always strive for the highest standards of assay precision. Differences in assay precision make it difficult to compare data from different laboratories without defined standards.

Another consideration in NAT-based diagnostics concerns sample quality. Preservation of sample integrity is essential for accurate test results, especially if detection or quantification of RNA is desired. Unlike DNA, RNA is unstable and is readily degraded by naturally occurring endonucleases (RNases) that are present in the environment and tissues. Samples for RNA testing should be handled with gloves to prevent contamination with RNases present on the skin. Samples for RNA testing should be promptly submitted to the laboratory or stored in a freezer at Ϫ80°C until forwarded for testing. Samples may also be treated with buffers designed to minimize RNA degradation, with several commercial additives being available for this purpose, but their use varies among laboratories. It is best to consult the specific laboratory for their protocol for sample submission. Although DNA tends to be more stable than RNA, naturally occurring endonucleases that degrade DNA do exist. Test samples should be aseptically collected and refrigerated or frozen until submission. The ability of PCR to detect extremely low levels of DNA in a submitted patient sample results in the very real potential of falsepositive test results due to DNA contamination.

PCR is a technique that started as a research tool and was eventually integrated into medical diagnostic applications. In 1971, Kleppe demonstrated that DNA could be artificially replicated in vitro with a DNA polymerase enzyme. 4 Combined with methods allowing for the synthesis of DNA oligonucleotides, Kary Mullis developed the technique of PCR in 1983 and won the Nobel Prize in Chemistry for his work. 5 PCR has become one of the most widely used technologies in veterinary research and molecular diagnostics of infectious disease. The wide range of applications include the detection of DNA or RNA derived from infectious bacterial, viral, mycoplasmal, parasitic, protozoal, or fungal organisms; evaluation of specific genetic defects; and quantization of cytokines and cellular growth factors. PCR testing can be used on a wide range of samples, including tissue from biopsies or necropsies, blood, feces, tissue, soil, cerebrospinal fluid, and swabs of mucosal surfaces. PCR uses the highly specific molecular recognition ability of Watson-Crick base pairing to provide the selectivity needed for a nucleic acid probe to bind to a targeted DNA sequence and allow for its exponential amplification. Specific fragments of DNA are replicated by cyclical heating and cooling through repeated temperature stages in a thermal cycler unit (Fig 1) . Each cycle can double the amount of existing target DNA. Repeatedly amplifying the target sequence over 30 to 40 cycles theoretically allows for millions (2 30 ) of DNA copies to be produced. PCR assays are thus extremely sensitive. They are capable of detecting femtograms (10 Ϫ15 g) of target DNA sequence in a large mixture, or up to a microgram (10 Ϫ6 g) of nontarget DNA. The DNA obtained at the end of the reaction can be used for a variety of applications, including sequencing, cloning, or as a probe for in situ hybridization.

The nested PCR consists of 2 sequential PCR amplifications. The product from the first-round PCR is used as the template for a second PCR amplification. A second set of primers, internal to the first set, amplify a smaller fragment of DNA within the first-round product. In regular PCR, primers may bind to incorrect regions of the DNA, giving unexpected amplification products. The benefit of nested PCR is that increased specificity is achieved through confirmation of the correct primer binding during the first PCR amplification process. The nested PCR is more sensitive than conventional single-round qualitative PCR.

The implementation of nucleic acid technology into routine diagnostics has greatly advanced with the introduction of real-time PCR. 1 Real-time PCR amplification uses the detection and quantification of a fluorescent reporter molecule whose signal increases in proportion to the amount of target amplification product generated. Basic methods involve the detection of a DNA-binding dye, such as SYBR Green (Molecular Probes, Eugene, OR USA). The dye intercalates between double-stranded DNA formed in the PCR reaction and, when exposed to an excitation source, fluoresces. A laser scanner detects the fluorescence emitted by dye-impregnated DNA strands formed through the PCR amplification. Software traces this fluorescence as a running graph as the amplification reaction in real time (Fig 2) .

A more specific strategy for real-time assays includes the use of a confirmatory probe sequence that matches part of the amplification product. These real-time amplicon detection technologies include TaqMan (Applied Biosystems, Inc., Foster City, CA USA) or hydrolysis probes, 6 molecular beacon technology, 7 and hybridization probes. 8 Fluorescence is only detected if the probe is bound to the amplified target, thereby enhancing specificity to an already highly specific assay. These probes do not react with primer-dimers or other nonspecific products that may be generated during the PCR reaction as may occur with SYBR Green reactions. However, a welldesigned and optimized SYBR Green amplification is a very effective testing technology. Unlike TaqMan probes, the SYBR Green reporter remains intact so a melt curve analysis can be performed on the amplification product. Melt curve analysis measures the dissociation characteristics of doublestranded DNA during heating. Melt curve analysis is sensitive enough to detect single-nucleotide polymorphisms (SNP) and can distinguish between homozygous and heterozygous gene alleles by the dissociation patterns produced (Fig 3) . The advancement of this technology to produce highresolution melt analysis will provide higher sensitivity for SNP detection within entire dye-stained amplicons for genomic analysis.

The main advantage of real-time detection is that it accurately quantifies the number of RNA or DNA copies present in the test sample. Real-time technologies can measure DNA presence over a large dynamic range, providing important information regarding the magnitude of viral involvement within a test sample. This information is helpful in distinguishing clinically active infections from transient environmental exposures. Quantification also aids in the treatment of patients with chronic infectious disease and monitoring disease progression and resolution. 9 

Standard PCR techniques cannot be used to detect the presence of RNA-based viruses in clinical samples. The direct amplification of RNA in PCR systems is not possible because of the polymerase enzyme's inability to synthesize DNA from an RNA template. The inability to synthesize DNA from an RNA template is overcome by the use of a reverse-transcriptase (RT) enzyme before the PCR reaction. In RT-PCR, RNA is converted into a complementary DNA (cDNA) strand by an RNA-dependent DNA polymer-ase (reverse transcriptase). The cDNA produced is then used as a template in a conventional PCR reaction. It is important that all genomic DNA within the test sample be destroyed before the RT-PCR to prevent masking the presence of cDNA derived from the RT step.

In addition to diagnosing RNA viral infections in an individual patient, RT-PCR has been applied to monitoring disease in flocks and large groups of animals. Environmental air-sampling, referred to as ""sniffing,"" was paired with real-time RT-PCR to detect exotic Newcastle disease virus in commercial poultry operations in 2003. 10 The same approach was successfully used to detect H6N2 influenza in commercial quail. This methodology offers the advantage of detecting virus in contaminated environments by collecting airborne particulates, including virus comingled with dust and debris, without having to handle individual birds.

PCR currently is still the main target amplification technique used in both research and diagnostic laboratories. A second target amplification technique, known as NASBA, was specifically developed for the quantitative amplification of RNA targets such as retroviruses and other RNA viruses. 11,12 NASBA's main advantage is that it works at isothermic conditions and can give quicker and more sensitive results than PCR. It has been used to develop rapid diagnostic tests for several pathogenic viruses with singlestranded RNA genomes, including influenza A, 13 footand-mouth disease virus, 14 and severe acute respiratory syndrome (SARS)-associated coronavirus. 15

Loop-mediated isothermal PCR (LAMP) is a 1-step amplification reaction that amplifies a target DNA sequence with high sensitivity and specificity under isothermal conditions. LAMP uses a DNA polymerase with inner and outer primers that recognizes 6 separate regions within a target DNA sequence. The assay has high specificity because the amplification reaction only occurs when all 6 regions within the target DNA are correctly recognized by the primers. 9 LAMP exhibits less sensitivity to inhibitory substances present in biological samples than regular PCR.

LAMP has been applied to pathogens that cause food-borne diseases. Testing kits that use this technology are commercially available for detecting Salmonella spp., Legionella spp., Listeria spp., verotoxinproducing Escherichia coli, and Campylobacter spp.

Many amplification methods, including PCR, ligase chain reaction, NASBA, and strand displacement amplification, are used in commercially available diagnostic test kits. These diagnostic test kits are promoted as providing a rapid, sensitive, and specific detection of important pathogens, especially those that are not readily identified by more conventional methods. Conventional detection and identification of noroviruses are performed by electron micros-copy, which is less sensitive and more time consuming than NAT diagnostics.

Molecular-based testing for multiple pathogens in a large number of animals is expensive. The development of new variations of PCR have allowed for simultaneously performing multiple tests on a single diagnostic sample, thereby affording some savings on a per-test basis. In multiplex PCR, multiple primer pairs are used to amplify more than one gene or pathogen in a single PCR reaction. Multiplex assays must be carefully designed for specific primer selection. Primers selected must be to regions that are uniquely different and characterized by low affinity for primer-to-primer interactions.

Nucleic amplification technologies will continue to be a benchmark for pathogen detection in veterinary diagnostics. Currently, there are 3542 reference sequences for 2416 viral genomes and 40 reference sequences for viroids in GenBank, an open-access public database of nucleotide reference sequences (http://www.ncbi.nlm.nih.gov/genomes/VIRUSES/ viruses.html).

The exponential replication of DNA afforded by the PCR process allows for excellent diagnostic sensitivity and specificity. However, the instability and variability inherent in molecular enzymatic processes limit the application of this technology outside an institutional setting to a research facility or dedicated molecular laboratory. New technologies such as the lab-on-a-chip have been developed and have potential in veterinary point-of-care diagnostics. Defined as a miniaturized analytical laboratory constructed on a small chip, this technology has promising applications when combined with NAT diagnostic processes. Both PCR and LAMP assays have been used to facilitate the development of labon-a-chip-based NAT devices. 16 Such devices can offer molecular-based testing in a rapid, effective, low-cost, and disposable automatic format as a pointof-care diagnostic tool.

The concept of the DNA or oligonucleotide array began in the mid-1980s as a derivative of the Southern blot technique in which DNA fragments are linked to a substrate and probed with a known gene or target DNA sequence. The ""probe"" refers to the DNA that is immobilized on a solid support such as a glass slide or a silicon wafer, whereas ""target"" refers Future Veterinary Diagnostics to the pool of nucleic acids whose identity or abundance is being detected.

Both high-density microarrays for high-throughput screening applications and lower-density microarrays have been developed for diagnostic applications. 17 Microarray technology represents an important tool for the investigation of various areas of biology, mainly because of its ability to simultaneously detect and analyze thousands of different genes in a single trial.

A DNA microarray is a miniaturized form of dot blot in a predefined, ordered fashion at a high density. 17 It consists of an arrayed series of thousands of microscopic spots of DNA oligonucleotides, each containing picomoles (10 Ϫ12 moles) of a specific DNA sequence. The DNA spots (gene chips) are attached in an equidistant, ordered arrangement to a solid surface, such as a glass, plastic, or silicon chip. Alternative microarray platforms include polyacrylamide gel pads, microsphere beads, 3-dimensional films, and solid-state electronics. 18 Microarrays may have from a hundred to thousands of test sites that can range in size from 10 to 500 m. DNA arrays are different from other types of microarrays only because they either measure DNA or use DNA as part of the detection system.

Hybridization is performed with corresponding probes attached to the solid support. These can be cDNAs, oligonucleotides of varying length, or genomic sequences that are either radioactively or fluorescently labeled. Artificial single-stranded DNA or RNA ligands, short oligonucleotides or peptide molecules that can be generated against amino acids, drugs, proteins, and other molecules are called aptamers. 19 Aptamers bind to a specific target molecule with high affinity and selectivity ranging from the micromolar to the nanomolar level, and they can discriminate between closely related targets. 16, 20 A high-density array can be generated by applying aptamers to the substrate with robotic pins or inkjet technology, or by an in situ photolithographic synthesis process. These techniques allow for the production of a high-density chip containing thousands of short probes targeted at different loci within a single gene or multiple genomes. High-density microarrays may have up to 106 test sites in a 1 ϫ 2 cm 2 area. 17 The core principle of DNA microarrays is based on hybridization probing, the property of complementary nucleic acid sequences to specifically pair with each other by forming hydrogen bonds between nucleotide base pairs. Hybridization occurs between 2 cDNA sequences such as the nucleic acid probes immobilized on the solid surface of a chip and the mobile DNA, cDNA, or messenger RNA (mRNA) sample. A high number of complementary base pairs in a nucleotide sequence allows for tighter noncovalent bonding between the 2 strands to occur. Probetarget hybridization is usually quantified through the detection of a fluorophore, silver, or chemiluminescence-labeled target. After the hybridization step is complete, the microarray is scanned. The fluorescent tags are excited by a laser, read by a microscopic camera, and computer integrated into a digital image of the array. Software analyzes each microarray spot calculating the red-to-green fluorescence ratio and subtracting out background data. The nature and relative abundance of nucleic acid sequences in the target are determined. DNA microarrays have been used for the detection of infectious and genetic disease, cancer diagnostics, and forensic and genetic identification purposes. Microarray systems are very definitive and highly scalable because hundreds to tens of thousands of possible DNA elements can be interrogated in a single experiment.

Examination of animal and zoonotic pathogens typically focuses on 3 objectives: (1) determining the presence or absence of a pathogen; (2) determining pathogen viability; and (3) enumerating the pathogenic agents of a specific disease. DNA microarrays are well suited for determining the presence or absence of specific pathogens. DNA microarrays also permit relatively rapid interrogation of a clinical sample against thousands of genetic targets, allowing for simultaneous detection and discrimination among hundreds of pathogenic agents of veterinary interest. 18 However, DNA microarrays are not very useful in inferring pathogen viability.

The first important step in microarray-based pathogen detection is probe selection and design. Many issues associated with probe design for DNA microarrays can impact the overall fidelity of the assay and particularly the levels of specificity and sensitivity. One of the first applications of a bacterial high-density DNA probe array was for the diagnostic capability of identifying Mycobacteria spp. 21 Using RT-PCR and a DNA microarray based on the mycobacterial heat shock protein, this technology was used for the rapid identification of different mycobacterial species in mixed infections 22 and array-based genotyping of mycobacterial organisms. 23 Other applications to bacterial infections have included the diagnosis of Salmonella spp., Escherichia coli O157:H7, Mycoplasma hyopneumoniae, gene profiling of Pasteurella multocida, Mycobacterium avium subsp, Paratuberculosis spp., Rhodococcus equi, Borrelia burgdorferi, Staphylococcus aureus enterotoxin genes and Clostridium perfringens toxin genes, Bacillus anthracis, C. perfringens, Brucella spp., Bordetella pertussis and B. parapertussis, and Actinobacillus pleuropneumoniae. 24 Microarray technology has also been used to generate a ""lipopolysaccharide array"" against the outer membrane of Gram-negative bacteria. Monoclonal antibodies specific for Escherichia coli O111, E. coli O157, Francisella tularensis, and Salmonella enteric serovar Typhimurium O antigens were used to evaluate the assay. The detection limit of antibodies was reported to be 10 ng/mL, which is 100-fold more sensitive than conventional immunofluorescence assays. 24

The diagnosis of viral diseases requires the identification of the viral pathogen in the clinical sample and a subsequent correlation between the identified virus and the clinical disease. Conventional methods of viral detection include isolation and in vitro culture, and immunological methods such as direct fluorescence antibody or enzyme immunoassay. With some viral disease agents, identification is readily accomplished. Other viruses (e.g., norovirus), however, are refractory to in vitro culture. Immunological methods, dependent on the quality and availability of antibodies, may also be ineffective for identifying viruses with constantly evolving serotypes. Although PCR has revolutionized viral diagnostics by increasing detection sensitivity and specificity, the maximum number of viruses detectable in a single assay is relatively small. In diseases where different viruses are present or where no etiologic agent has been identified, the value of these conventional methods may be limited.

Unlike PCR technology where the target agent must be known to use specific test primers, microarrays can allow for the rapid diagnosis of multiple pathogenic agents in disease outbreaks and epidemics of unknown etiology. One of the first studies in multiple pathogen detection used a microarray chip to discern 2 viral pathogens, porcine reproductive and respiratory syndrome virus and foot and mouth disease virus. 25 The microarray detection device demonstrated rapid and accurate detection of these 2 specific viruses. In another study, a low-density microarray, using 15 capture-oligonucleotides targeting the conserved influenza matrix gene, accu-rately subtyped H1N1, H3N2, and H5N1 influenza A viruses in less than 12 hours. 26, 27 Researchers led by Joseph DeRisi of the University of California at San Francisco have combined genome databases of sequenced viruses with DNA microarray technology. 28 Using available sequence data from viral genomes, they designed a long oligonucleotide viral microarray. The microarray was capable of simultaneously detecting hundreds of viruses in a single clinical sample. The approach used randomized primers in an RT-PCR to amplify any viral RNA that was present in the clinical sample. The PCR product was then hybridized to the microarray comprising 1600 probes, representing nearly 140 respiratory virus genomes. Viruses that were represented on the microarray were readily detected and identified by specific hybridization to the appropriate oligonucleotides. This methodology enabled the simultaneous detection of hundreds of viruses, including essentially all respiratory tract viruses. This microarray system greatly expanded the spectrum of detectable viruses in a single assay while simultaneously providing the capability to discriminate among viral subtypes. Similarly, Sengupta and colleagues developed microarrays with 476 probes to distinguish among various influenza viruses. 29 Although initial efforts were focused on only a few hundred viruses, DeRisi and colleagues devised a microarray representing all known human, animal, aquatic, and plant viruses called the ViroChip. The most highly conserved genomic sequences from within viral families were chosen for representation on the microarray. Viruses represented included double-and single-stranded DNA viruses, retroviruses, and both positive-and negative-stranded RNA viruses. DNA fragments of virtually every virus ever discovered, about 22,000 different viral sequences, were represented on the ViroChip.

The ViroChip greatly accelerates the ability of researchers to diagnose viral infections and identify new viral epidemics within a very short time. This technology can help to rapidly identify known viruses and classify new ones based on their genetic makeup. The ViroChip has been credited with the rapid recognition of a novel coronavirus in 2003, the etiologic agent responsible for SARS. 30 Although the ViroChip represents sequences from known referenced viruses, equally important is its potential to facilitate viral discovery in diseases of unknown etiology. As new viruses evolve, they still maintain certain characteristic-conserved regions in the viral genome. By using highly conserved sequences from all known viral taxonomy, the Viro-Chip can identify similar sequences in previously Future Veterinary Diagnostics unknown agents and newly evolved viruses within recognized families.

Proventricular dilation disease (PDD) has been diagnosed in psittacine birds for the past 40 years, but the underlying etiologic cause has not been identified. Although the disease exhibits characteristics of a viral infection and numerous viral etiologies have been proposed, confirmation of a definitive agent remained elusive. In 2008, Kistler and coworkers used the ViroChip to interrogate samples from 2 PDD case/control series collected on 2 different continents. 31 Tissues from birds with confirmed PDD displayed a novel bornavirus signature in 62.5% of the PDD cases and none of the control tissues. The bornavirus-positive samples were further confirmed by virus-specific PCR testing and the complete genome sequenced. Currently, 7 genotypes of the avian bornavirus have been identified.

Microarray-based viral detection offers a powerful alternative for determination of viral subtypes. Classic serotyping of viruses is tedious and limited by availability of antisera. Conserved array elements are capable of broadly detecting many, if not all, viral serotypes. Unique hybridization patterns are often observed for different viral serotypes, which enables rapid identification.

Because of the high resolution of microarray hybridization, it will see future application in the study of viral pathogenesis and diagnosis of veterinary diseases. Microarray hybridization is a viable approach for detecting unsequenced or uncharacterized viruses and novel virus discovery. It can be used to differentiate among viral subtypes and provide important information on how a new virus relates to those that have been classified. Application of this type of information can provide clues about viral origin and possible treatment strategies. This comprehensive and unbiased analysis of viral prevalence in a given biological sample also increases the feasibility of introducing molecular testing for viruses that are easily detected using classical diagnostic testing methods. 1

When target nucleic acids are abundant, then direct microarray interrogation of pathogen targets is achievable in the absence of amplification. 18 Direct hybridization strategies involve extracting RNA or DNA from the sample and applying the material directly to a microarray without additional amplification. Direct hybridization provides the least bias in gene detection, but also has the lowest level of ana-lytical sensitivity. Another strategy bypasses these limitations and yields an extremely broad-reaching and unbiased detection strategy. It uses microarrays composed of carefully selected viral sequences, coupled to a random PCR amplification. One approach is to amplify one or more universal genes (e.g., 16S rRNA, 18S rRNA, 23S rRNA genes) and to screen for pathogen-specific polymorphisms. Greisen and coworkers were one of the first groups to amplify fragments of the 16S rRNA gene using group-specific PCR primers and, with Southern blots that had pathogenspecific probes, identified pathogenic bacteria in cerebrospinal fluid. 32 Microarrays can be used in a similar fashion to interrogate PCR products to distinguish between multiple pathogens. A second PCR strategy is to use multiplex PCR to amplify a number of discreet, pathogen-specific genetic markers that are subsequently detected on a DNA microarray.

Simultaneous detection of multiple pathogens should be viewed in the context of an entire analytical process that spans sample collection to final testing. 18 Physical differences between target organisms often require very different strategies to extract nucleic acids for analysis after which the DNA microarray can then serve as the actual detection device.

The reader is referred to the review by Wang for an encompassing review of microarray technology. 33 In the area of microarrays for microbiological applications, several general reviews are available. Specific reviews are available on DNA microarray analyses of host-pathogen interactions 34 and the use of microarrays for the molecular diagnosis of Mycobacteria spp. 35 Generalized reviews in the medical-related areas include microarrays for disease gene discovery, 36 microarrays in medicine, 37 and microarrays for molecular pathology. 38

Early methods of DNA sequencing, determining the nucleotide order of a DNA segment, gene, or genome, were costly and slow. Although the first sequence of a human gene locus was published in 1990, 39 it took 10 years to report the sequence of the entire human genome in 2000. 40 The recent development of cost-effective, high-throughput sequencing now allows for efficient, full sequencing of an organism's genome. As DNA sequence data have become more available, the field of comparative genomics has rapidly progressed. Attention has turned to generating whole genome assemblies and genomics resources for veterinary species. In July 2005, a publicly accessible annotated genome assem-bly of the domestic dog, 41 and subsequently a draft sequence of the domestic cat, 42, 43 were released. With complete genome sequences now available for the dog, cat, and horse, 39 the tools are now available to research very specific questions about animal genomes and cancer.

DNA microarrays have technologically advanced genomics research. 44 DNA microarrays are used to detect DNA or RNA (most commonly as cDNA after RT) and are referred to as gene expression analysis or expression profiling. 26 Gene expression analysis is based on the assumption that cells react to changes in their environment by increasing or decreasing transcription of appropriate genes. These changes in expression levels can be detected or quantified by using specific scientific methodology such as realtime RT-PCR and microarray technology.

The use of a distinct DNA array for gene expression profiling was first described in 1987 to identify genes whose expression was modulated by interferon. 45 Determining the level, or volume, at which a certain gene is expressed is called microarray expression analysis. Miniaturized microarrays were first applied to expression profiling in 1995. 46 In veterinary medicine, genome libraries have allowed for the development of new, high-density microarrays to facilitate whole genome and gene-targeted profiling at high resolution and throughput. 47 New, high-density microarrays have rapidly advanced our knowledge and understanding of pet and food animal genomes and have fostered the development of a rapidly growing class of molecular diagnostics focused on diagnostic testing for genetic traits encoded by genomic DNA.

Scientists can use a microarray to examine the expression of hundreds or thousands of genes within a single sample, in a single assay. Through computer analysis, the amount of mRNA bound to each site on the array is precisely measured, generating a profile of global gene expression within the cell. Once researchers have characterized the expression patterns of various genes involved in many diseases, the expression pattern of cDNA from an individual can be hybridized to determine if it matches the expression pattern of a known disease. If a gene is over expressed in a certain disease state, then more sample cDNA, compared with control cDNA, is detected. In microarray comparative genomic hybridization, the increased or decreased expression of a particular gene(s) is determined. A change in the number of copies of a particular gene may correlate to a certain disease state. Microarrays may also be used in veterinary diagnostics by comparing gene expression of infected cells or tissues with that of uninfected cells or tissues. Expression profiling can also identify genes whose expression is changed in response to disease-causing agents. 48 Revealing how multiple genes work together to produce physical and chemical responses and analysis of patterns of coordinated gene expression can help identify genes involved in the development of various diseases.

A canine-specific microarray using mRNA and expressed sequence tag sequences has been developed. 49 The elucidation of these biological networks (e.g., disease, development, nutrition) and identification of associated molecular mechanisms will allow for the development of more precise diagnostics for these processes. 49

DNA microarrays can be designed specifically for genotyping, thereby enabling them to measure single base pair changes at many thousands of points throughout a genome. DNA microarrays are useful for genotyping point mutations, single nucleotide polymorphisms (SNPs), and short tandem repeats (STRs) among alleles within or between populations. 50 SNP technology has been diversely applied in areas such as analyzing forensics, measuring disease predisposition, evaluating germ line mutations in individuals and somatic mutations in neoplasias, diagnosing infectious and genetic disease, and diagnosing cancer. Once an SNP pattern is associated with a particular disease, SNP microarrays can be used to test an individual for that disease expression pattern. A determination can be made if the individual is susceptible or at risk of developing that specific disease condition.

In sheep, diagnostic microarrays are used to determine specific SNPs that predispose animals to scrapie, a prion protein disease. Individuals with these SNPs accumulate the prion protein disease cellular protein within their central nervous system. 51

Initial microarray platforms available for veterinary oncology were genomic and cDNA microarrays generated by binding DNA fragments onto a glass or silicon surface. More recently, high-density oligonucleotide arrays, where short DNA sequences are synthesized directly on the surface of a ""chip,"" have allowed for thousands of features to be represented. DNA copy number variation, gene expression levels, and genetic alterations can be determined at a substantially increased resolution.

High-density microarrays represent a powerful tool for comparative genomic studies. Microarraybased comparative genomic hybridization merges molecular diagnostics with traditional chromosome analysis. 52, 53 Applications for high-density microarrays have been extended to many areas of medicine and medical genetics. 54 New syndromes have been discovered, and the phenotypes of existing disease conditions have been expanded.

A comparative study of canine chronic myelogenous leukemia showed that dogs diagnosed with chronic myelogenous leukemia also present with a functionally active genetic translocation previously identified in humans. 55 This study resulted in the first molecular cytogenetic test for the presence of a clinically significant genomic alteration in a veterinary cancer.

Several tests are now available for canine patients to identify oncogenes and clonal lymphocyte populations. 56 These tests can be placed into 3 broad categories: (1) detection of individual mutations in oncogenes, (2) detection of chromosomal translocations, deletions, and duplications, and (3) detection of clonality in lymphoma and leukemia through unique antigen receptor genes of B-and T-cells. Each of these testing methods has been used in veterinary medicine on an experimental basis and is now being offered by several laboratories. 56

The original emphasis on microarrays was on DNA technologies. Researchers have exploited the diversity of this technology to further extend applications to the development of protein, antibody, and carbohydrate microarrays. Macbeath and Schreiber 57 developed the first small-molecule microarray in 1999 and the first protein microarray in 2000. Soon afterward, other array types were developed, such as cell arrays, carbohydrate arrays, and proteome arrays.

Newer microarray formats that developed at the turn of the 21st century provide a host of other biomolecules that can be presented on a chip, including proteins (e.g., whole proteomes, enzymes, antibodies), 58,59 small molecules (e.g., drug-like molecules, peptides, carbohydrates), 60,61 and even whole cells and tissues for simultaneous, multiplexed experimentation. 62

Within the last decade, protein microarrays have entered the field of proteomic research. 63, 64 Pro-teomics is the term used to describe the study of proteins expressed by a genome. Technologies that had previously been established for DNA microarrays were adapted to the generation of protein arrays with glass slides being initially used as a solid support in planar microarrays. Bead arrays or liquid array systems rely on the use of different bead types that can be distinguished either by color, size, or shape. Currently, protein microarrays are highly miniaturized and parallelized solid-phase assay systems that use a large number of different capture molecules immobilized in microspots. Protein microarray technology has enormous potential for in vitro diagnostics and disease monitoring.

Protein microarray technology has enormous potential for application within the veterinary diagnostic testing arena. Antibodies in an immunoassay format have been widely used and are well established as highly sensitive tools for disease detection. With monoclonal antibody production, it is possible to produce pure and highly specific antibodies against almost any type of antigen. As a next-generation tool, protein arrays in their role as miniaturized, multiplexed immunoassays are perfectly suited for generating a maximum of diagnostically relevant information from very minute samples. 65 Protein microarrays offer unparalleled throughput, minimal reagent consumption, and sensitive simultaneous detection of multiple targets. 58 Currently, there is rapid advancement of antibody microarrays that have been developed for clinical, biothreat, and point-of-care applications. Early applications included the development of a protein microarray with 35 antibodies, which was used to subtype the 20 most common Salmonella serovars, 66 and a similar microarray was developed for E. coli. 67 Lipopolysaccharide, carbohydrate-based, and whole-cell microarrays have also been used for antibody-based detection of pathogens such as Francisella tularensis. 68, 69 Liquid array technology has been adapted to the detection of avian influenza (AI) virus antibody in commercial poultry. 70 The assay uses recombinant AI virus nucleoprotein conjugated to microspheres to detect the influenza-specific antibody. The AI virus liquid array has a 99.3% sensitivity and a 94.7% specificity. This liquid array format can theoretically detect 100 different analytes in a single assay. Therefore, this technology has the potential to simultaneously detect and subtype all influenza hemagglutinin and neuraminidase proteins, as well as antibodies specific to the other relevant avian diseases.

Protein microarrays with high pathogen proteome content offer a valuable platform for highthroughput serology. These antigen microarrays have been used to identify seropositive individuals by using the presence of serum antibodies to detect exposure to a specific pathogen. Zhu and coworkers monitored the antibody profiles of patients with SARS using protein microarrays containing 82 purified coronavirus proteins. 71 Immunoreactivity against the coronavirus nucleocapsid proteins remained high for 120 to 320 days postinfection, which provided a means to check for exposure long after infection has occurred. The integration of DNA and protein-based microarray methods may extend the range of rapid clinical diagnostic testing.

In protein expression analysis, proteins or biomarkers from complex biological samples are identified and correlated with diagnostic patterns that are unique to specific cancer and disease conditions. The main challenge in the development of biomarker diagnostics is the biological variability among patient samples as well as the large dynamic range of biomarker concentrations. Both disease and cancer biomarker discovery (oncoproteomics) has been studied extensively using proteomics. Biomarkers have been identified that correlate with bladder cancer, breast cancer, colorectal cancer, esophageal cancer, gastrointestinal stromal tumors, glioma, hepatocellular carcinoma, leukemia, lung cancer, lymphoma, nasopharyngeal carcinoma, ovarian cancer, pancreatic cancer, prostate cancer, and urothelial carcinoma. 72 Protein microarrays designed with hundreds of copies of protein-coding oligonucleotides or ""bio-barcodes"" can serve to amplify the protein target and are capable of detecting proteins down to attomolar concentrations. 73 Besides protein expression analysis, protein microarrays can also be used to determine the functional analysis of proteins, including protein interaction involving immobilized proteins or peptides, low molecular weight compounds, DNA, oligosaccharides, tissues, or cells. Protein microarrays have been developed for the detection of allergen-specific immunoglobulin E reactivity and detection of specific auto antibodies associated with autoimmune diseases. In the field of autoimmune disease diagnostics, multiplexed assays are currently entering the diagnostic market, 74 and in the future, sets of tumor marker panels may also be applied to monitor treatment therapy for specific disease processes. Immunoassay panels have been developed for fertility, cardiac disease, tumors, cytokines and growth factors, cell adhesion molecules, thyroid function, and drug residues. Virtually every biological component from diverse small molecules, macromolecules (e.g., DNA, proteins), and entire living cells have been placed on microarrays.

A biomarker is ""a characteristic that is objectively measured and evaluated as an indicator of normal biologic processes, pathogenic processes, or pharmacologic responses to therapeutic intervention."" 75 Research regarding biomarkers associated with disease has greatly advanced in both the human and veterinary medical fields. The challenge of this information is to determine which marker or combination of markers is optimal for the condition to which they are applied. Evidence from research studies suggests that using multiple markers may be superior to using single markers alone. Multiple marker combinations provide information of greater diagnostic and prognostic value than any single marker alone.

The most promising use of veterinary biomarkers are those used to evaluate cardiovascular disease in dogs and cats. [76] [77] [78] Currently, circulating natriuretic peptides are regarded as the most promising markers used in the evaluation of such patients. Other markers include those of myocyte injury typified by troponins, markers of myocyte stress (adrenomedullin), matrix metalloproteinases, markers of endothelial function, and markers of inflammation (C-reactive protein). 79 Concentrations of some of these substances are known to change in response to heart failure and the treatment of cardiac disease in veterinary patients. 76, 77 Clinical application of biomarkers in the future includes the detection of subclinical diseases, diagnosis of acute or chronic syndromes, risk determination, therapy selection, and monitoring of disease progression or response to treatment. 78 Presently, most biomarker testing takes place at dedicated laboratories, thereby increasing the expense of the test and the time needed to perform the test. The development of biosensor technology will enable biomarker testing to be accomplished in a smaller, faster, cheaper, and portable format.

A biosensor can be generally defined as a device that consists of a biological recognition system and a transducer. A biosensor is a compact analytical device that integrates a biological element on a solid-state surface, enabling a reversible biospecific interaction with an analyte. 80 The interaction of the analyte with the bioreceptor produces an effect measured by the transducer, a device used to convert the information into a measurable signal. The biological component of a biosensor can include any molecule used for biomolecular recognition. Biosensors have been de-signed using specific binding proteins, lectins, nucleic acids, membranes, whole cells, antibodies, or antibody fragments. Biosensors that detect the immunological interaction between a specific antibody and antigen are referred to as immunoreaction-based biosensors or immunosensors. 80 Some biosensors use aptamers, single-stranded DNA or RNA oligonucleotide sequences that recognize various target molecules with high affinity and specificity. These ligand-binding oligonucleotides mimic properties of antibodies in a variety of diagnostic formats.

There are 2 main types of biosensors: biocatalytic and bioaffinity-based biosensors, and they are primarily categorized on the detection principle applied. Biocatalytic sensors use enzymes to catalyze a signaling biochemical reaction, and bioaffinity-based sensors monitor the binding event itself. Transducers convert the biological interaction to a detectable signal and can be based on electrochemical (e.g., potentiometric, amperometric, conductometric), optical, microgravimetric, or thermometric (calorimetric) principles.

Biosensors have been designed using carbon nanotubes and nanoparticles for signal amplification. Combining the molecular specificity of biological recognition with an operationally simple transducer, extend these small units' diagnostic capability toward portability and ease of use. Recent developments in immunosensors have produced solid state systems that allow for rapid and continuous monitoring of antibody-antigen reactions in real time.

The specificity of the biosensor system is determined by the immobilized detection molecule. 20 Innovative strategies using biomarkers represent alternative strategies for reliable cancer testing and for detecting clinical markers predictive of cardiovascular and other diseases. Biosensors using antibodies or DNA/RNA strands can detect infectious disease agents and be used for hormone and drug assays. 20, 81 Currently, most biomarker testing takes place at a dedicated, centralized laboratory, but in the future this technology will provide a means of rapid, convenient, and economical testing at the ""point of patient care."" In cancer, there is a strong connection between early detection and positive patient outcome, thereby improving patient survival and disease prognosis in many cases. In addition to changes within the host genome, complex molecular alterations such as over or under expression of a protein can occur during the course of tumorigenesis. 82 Biosensors can detect these changes and can analyze molecular biomarkers for tumor diagnosis and classification, monitor response to treatment, and detect recurrence of the disease. Cancer biosensors have been developed in a panel format for the simultaneous detection of different cancer biomarkers. 83, 84 Because most cancer biomarkers are not specific to a particular tumor, the use of an array to detect multiple markers increases the diagnostic value of their definitive diagnosis. 85 Recently, biosensors have been developed to detect breast cancer, ovarian cancer, gastrointestinal tract carcinoma, chronic myelogenous leukemia, and prostate cancer. 20 Prostatespecific antigen is the most reliable tumor marker to detect prostate cancer in its early stages. The recently developed biosensors for prostate-specific antigen detection are sensitive to a limit of 4 pg/mL. 86 The importance of inflammatory markers in the early detection of cardiovascular disease has been demonstrated. 81 Biosensors are being developed to detect C-reactive protein, cardiac troponin, myoglobin, and natriuretic peptide in a sensitive and efficient manner. Based on a competitive immunoassay in a portable biosensor, good correlation between salivary cortisol and levels of ""free"" cortisol in serum has been demonstrated. 87

A nucleic acid biosensor is an analytical device that integrates an oligonucleotide with a signal transducer. 88 The nucleic acid probe is immobilized on the transducer and acts as the biorecognition molecule to detect DNA/RNA fragments. Several DNAbased biosensors have recently been developed for the detection of virus-related sequences and other infectious agents. 19 DNA or RNA ligands can also be synthesized to be used to detect amino acids, drugs, proteins, and other molecules. DNA or RNA ligands can bind to their targets with affinity ranging from the micromolar to the nanomolar level and can discriminate between closely related targets. 20 There is a growing need for small, fast, efficient and portable biosensors.

The analysis of thousands of small molecules (metabolites) (e.g., sugars, organic acids, amino acids, nucleotides) by means of a global approach is referred to as metabolomics. Combined with pattern recognition processes, metabolomics defines a metabolic phenotype (metabotype) in the study species. 89 This approach combines high-throughput sample analysis with computer-assisted pattern recognition techniques whereby the full metabolite profile (metabolome) of a cell, tissue, or organism is determined. Metabolomics is a powerful investigative tool for studying the biochemical effects of disease and screening for potential pharmacologic agents. Alterations in the metabolic profile often present much earlier in the course of disease than induced histopathological changes. Metabolomics can be a sensitive, early indicator of a disease process, and because metabolites are conserved across species lines, screening patterns can be applied to a wide range of species. 89 The application of metabolomics in veterinary medicine is in its infancy. Whitfield and colleagues used metabolomics to distinguish canine congenital portosystemic vascular anomalies from acquired hepatopathies. 90 Other early applications include the analysis of cadmium toxicity in rodents, 91 assessment of sublethal stress in aquatic organisms, 92 and neurotransmitter deficits in cerebral tissue from a mouse model of human batten disease. 93 Although current applications to the field of veterinary medicine are rare, metabolomics use in early preclinical safety assessment make it a rapidly developing tool in the field of drug investigations. Metabolomics enables a noninvasive systems assessment of a broad spectrum of biologic responses by an individual to the effects induced by therapeutic compounds, which could impact all stages of veterinary drug research and development.

Molecular theranostics is an emerging area in which molecular diagnostic tools are used to provide rapid (less than 1 hour), accurate, and informative diagnostic microbiology assays. Conventional microbial culture and sensitivity testing methods require at least 2 days' time because of the reliance on growth and isolation of microorganisms. Veterinarians must frequently treat patients empirically with broad-spectrum antimicrobial agents, which may not be needed or may not be effective. The common use of broadspectrum antimicrobials is associated with an increasing rate of microorganism resistance, which may complicate patient treatment. Early diagnosis and treatment of diseases reduce the risk of the patient developing long-term disease complications. For some diseases, a prompt treatment will also reduce further transmission of the disease to other animals or humans.

Assays based on the detection of nucleic acids offer enormous potential for the rapid and accurate diagnosis of microbial infections. This assay-based testing can be extended to include the detection and characterization of genes or mutations associated with antimicrobial resistance and virulence. 94 Advances in genomics and proteomics will provide the essential nucleotide or amino-acid sequence data required to design accurate assays. Efficiency will also depend on the development of rapid, simple, and efficient methods for microbial nucleic acid or protein extraction from a variety of clinical samples. Finally, rapid and specific assays, and the ability to detect all or most targeted microorganisms in multiplex target amplification systems, or signal amplification technologies, are needed. This will help future technologies reach a level of analytical sensitivity appropriate for testing directly from clinical samples without previous enrichment, thereby leading to more judicious antimicrobial usage and a concurrent reduction in antimicrobial-resistant organisms. 94 To be useful, diagnostic methods must be accurate, simple, and affordable.

",0.7405993551914392
Epidemic Surveillance Using an Electronic Medical Record: An Empiric Approach to Performance Improvement,Backgrounds: Electronic medical records (EMR) form a rich repository of information that could benefit public health. We asked how structured and free-text narrative EMR data should be combined to improve epidemic surveillance for acute respiratory infections (ARI).,"Epidemics of acute respiratory infections (ARI), whether due to influenza [1, 2] , coronaviruses [3, 4] , or other pathogens [5, 6] , could overwhelm even the most developed health care systems. It is imperative to recognize these epidemics as early as possible, as the passage of time quickly degrades the effectiveness of mitigating measures [7] .

Electronic data offer the opportunity for more timely and complete gathering of health information compared to what has historically been achieved through manual, paper-based reporting [8] . The increasingly rapid deployment of electronic medical records (EMR) [9] broadens the array of data that could be recruited for surveillance purposes [10, 11] . EMR-based surveillance could improve our response to a serious outbreak of ARI not only by allowing earlier recognition, but also by offering an efficient conduit for the information necessary to manage actual patients and to keep abreast of the evolving epidemic [12] [13] [14] . At present, however, the tantalizing potential of EMR-based surveillance remains in the making [15] [16] [17] [18] .

To gain insight on the conduct of surveillance in an EMR environment, we previously evaluated how EMR entries should be assembled to discover individuals with ARI [19] . We found that computerized free-text analyses aimed at uncovering ARI symptoms documented in outpatient clinical notes could complement diagnostic codes and other structured data to improve case detection. In this report, we asked if those EMR-enabled gains in case-detection could accelerate the discovery of ARI outbreaks. Using software to reconstitute a surveillance system operating prospectively on historical data sets, we compared alternative casedetection approaches for their ability to reduce the delay in detecting a modeled community outbreak of influenza. Our approach and results begin to chart how EMR-based information could be systematically organized to better serve public health surveillance.

The Institutional Review Boards of the Veterans Administration (VA) Maryland Health Care System and the University of Maryland approved this study. The study was granted a waiver of consent as risks were limited to information confidentiality and the work would not have otherwise been feasible, given the large number of EMR records screened for possible ARI. All EMR information was anonymized and de-identified prior to simulations and analyses, which used only daily case counts. Background: authentic counts of patients with possible ARI. Outpatients with possible ARI were identified by applying previously developed ARI case-detection algorithms (CDAs) [19] to institutional databases derived from real historical EMR entries. ARI was defined as: positive respiratory virus culture/antigen OR any two of the following symptom, of no more than 7 days duration: a) cough; b) fever or chills or night sweats; c) pleuritic chest pain; d) myalgia; e) sore throat; f) headache AND illness not attributable to a non-infectious etiology. The components and single-case detection performance of the eight (8) ARI CDAs selected for the current studies are summarized in Table 1 . CDA components included: a) provider-assigned ARI-related diagnostic codes (International Disease Classification, 9 th Revision, Clinical Modification, ICD-9) either used by the original Centers for Disease Control and Prevention (CDC) ''BioSense'' surveillance system [20] (the ''CDC ICD-9 Codes'' component in Table 1 ) or a code set previously adapted to the VA (''VA ICD-9 Codes'') [19] ; b) prescription for cough suppressants (''Cough Remedies''); c) documented body temperature of $38uC; d) computerized analysis identifying at least two symptoms from the ARI case definition in the free text of the clinical note [19] . Time series of daily ARI counts were created for each CDA (those datasets are provided as Datafiles S1, S2, S3, S4, S5, S6, S7, and S8), and served as the backgrounds into which synthetic influenza epidemics were injected.

Signal: synthetic influenza epidemic. To create a plausible ARI outbreak to be discovered by the surveillance system, we developed an epidemic model of influenza (Matlab R2008a, the Mathworks, Inc., Natick MA). The model included 30 contiguous ZIP codes centered on Baltimore, Maryland, and consisted of a coupled series of differential equations to describe the overall epidemic [21] . The susceptible population has the size and age structure described for Baltimore in the 2000 Census [22] . Agespecific death rates for each of the 20 demographic age-classes (age-class 1 = 0-4 years, age-class 2 = 5-9 years and so on) were derived from the United States Life Table Functions for the 1994 calendar year [23] . The birth rate for each age-class was obtained from Guyer et al. [24] . Model parameters were adjusted to mimic the estimated transmission and severity characteristics of the 1918 pandemic influenza in a non-immune population [25] . The proportion of cases that would be present for medical evaluation at the VA Maryland Health Care system was adjusted to reflect the age, gender, and population estimates of Baltimore veterans, over half of whom are older than 60 years old and more than 90% are male [26] . The same model-generated outbreak that was used as a common signal for all of our surveillance simulations is shown in Fig. 1 (upper panel, blue circles). Assuming that the synthetic epidemic cases would be discovered at the same rate as authentic cases, simulated cases from the epidemic model were first discounted by the sensitivity of the ARI CDA before being injected into CDA-specific background time series.

Surveillance simulations. We developed software aimed at replicating a surveillance system operating prospectively on the authentic historical background datasets described above (R v. 2.15.0, http://www.r-project.org). Starting on the day when the synthetic epidemic was injected into a CDA-specific authentic background time series, and then daily for a total of 80 days, a statistical outbreak detection method (see below) was applied in parallel to corresponding time series that included either: a) both background and epidemic cases (''Combined'' dataset; Fig Outbreak detection statistics. We used the ''early aberration reporting system'' (EARS) W2c [27, 28] statistical method to detect the injected epidemic. Daily case counts were separated a priori into two time series, one for weekdays and another for weekend and federal holidays. Using the appropriate time series for a given index day, the EARS W2c statistic is expressed as

where t is the time series index, Y (t) is the observed case count on that index day, Y Y (t) and S(t) are respectively a 7-day moving sample mean and a standard deviation calculated with a 2-day lag from the index day. The value of S(t) was replaced by 1 if S(t),1. The method signaled when the value of the W2c statistic exceeded a given threshold.

Performance measures. We computed three performance benchmarks at any given statistical alert threshold: 1) the Detection Delay, the time from the injection of the synthetic outbreak to the first true positive alarm, averaged for the 52 surveillance cycles of the evaluation year; 2) the false alarm rate (FAR), the average daily number of unique alarms issued in the background-only time series during the evaluation year; 3) the Caseload, defined as the total yearly number of cases included in the above false alarms. Corresponding Detection Delays, FARs and Caseloads were obtained for a range of statistical alert thresholds adjusted iteratively to focus on a FAR range felt to be of practical use for surveillance i.e. 0-10%. 

The activity monitoring operating characteristic (AMOC) curves [29, 30] shown in Figure 2 , upper panel, illustrate the relationship between average delay at detecting the synthetic influenza epidemic and the FAR for simulated surveillance systems that utilize one of eight (8) 

The surveillance advantages of adjusting the ICD-9 codes used to identify ARI cases can be visualized by comparing the AMOC curves obtained using a ''respiratory'' code set used by a surveillance system of national scope [20] Fig. 2 , upper panel, black circles). Note that at any given FAR, CDA 2 resulted in shorter detection delay than CDA 1 (Fig. 2 , upper panel, compare black circles with grey stars).

Case detection that relied solely on text analyses of clinical notes resulted in outbreak detection performance roughly on par with that of the VA-adapted set of ICD-9 codes (Fig. 2 , upper panel, compare green ''plus'' signs (CDA 4) to black circles (CDA 2)). When combined to ICD-9 codes using an ''OR'' logical operand, text analyses boosted both case-detection sensitivity to 97% and the area under the ROC curve (Table 1 , compare CDA 5 to CDA 2). Despite these performance gains, the [VA ICD-9 Codes OR text analysis of clinical note] case-detection approach worsened outbreak detection performance (Fig. 2 , upper panel, compare blue ''x'' signs (CDA 5) to black circles (CDA 2)). In contrast, combining text analyses of clinical notes with ICD-9 codes using an ''AND'' logical operand, which improved specificity and PPV at the expense of sensitivity and lowered the area under the ROC curve (Table 1 , compare CDA 7 to CDA 2), improved outbreak detection performance (Fig. 2 , upper panel, compare purple triangles (CDA 7) to black circles (CDA 2)).

We had previously found that selected structured EMR data, such as the documentation of fever or a prescription for cough suppressants, could further improve the test characteristics of ARI CDAs (Table 1 , compare CDA 3 to CDA 2, CDA 6 to CDA 5, and CDA 8 to CDA 7) [19] . These improvements in case detection performance did not translate into improved outbreak detection Effect of case-detection strategies on surveillance caseload Figure 2 , lower panel, illustrates AMOC curves of surveillance systems based on the ARI CDA shown in the Table 1 , this time replacing the FAR by the corresponding number of yearly cases contained in these false-alarms. We have named this variable ''caseload'' because it reflects the amount of work (phone calls, records reviews) a public health practitioner would have to perform to investigate the system's false alarms in a given year. Replacing the FAR by its corresponding caseload upheld the utility of adjusting the diagnostic code sets (Fig. 2 , lower panel, compare grey stars (CDA 2) to black circles (CDA 1)) and did not support the addition of structured EMR information about cough suppressants or fever (Fig. 2 , lower panel, compare red triangles (CDA 3) to black circles (CDA 2); teal diamonds (CDA 6) to blue ''x'' signs (CDA 5); and yellow boxes (CDA 8) to purple triangles (CDA 7)). Caseload information further emphasized the advantages of coupling ICD-9 diagnostic codes with text analyses using an ''AND'' logical operand to identify ARI cases. To wit, under our experimental conditions, a public health department willing to investigate false alerts that involved 200 cases/year would discover the influenza outbreak in 20 days with CDA 8 (Fig. 2 , lower panel, yellow boxes) compared to 33 days with CDA 2 (Fig. 2, lower panel, black circles) .

We reconstituted a surveillance system in software to evaluate the impact of alternative EMR-enabled case-detection approaches on outbreak detection. Our data suggest that text analyses seeking ARI symptoms documented in ambulatory care visit notes can be combined with provider-assigned diagnostic codes to discover a modeled influenza epidemic sooner and to reduce the number of cases contained in false-alerts. These data support our working hypothesis that information harnessed from a comprehensive EMR can improve timeliness and efficiency of public health surveillance.

If simulated data have often been used to compare statistical approaches to cluster detection [28, [31] [32] [33] [34] [35] , we could not find prior reports of the use of whole-system simulations to determine how case-detection methods affect outbreak-detection performance. The CDAs evaluated in this study were developed against a validated manual reference standard and only included EMR data elements found to contribute to ARI detection [19] . The CDAs were implemented against authentic EMR entries and thus produced backgrounds expected to mimic the noisy surveillance conditions found in the real world. In keeping with our goal to create a realistic evaluation platform, we also used an epidemic model to simulate the cases expected to present to our particular health system during an outbreak of severe influenza. Modelgenerated cases were sporadic at first but soon attained numbers large enough to be discovered by just about any surveillance method. Such an epidemic signal narrowed the time window over which alternative surveillance methods could demonstrate their superiority over one another.

Most automated surveillance systems utilize diagnostic codes to find diseased individuals. We [19] and others [36] have demonstrated that even minor adjustments of code sets to account for local practices can significantly strengthen case-detection performance. We now report that those same adjustments accelerated outbreak detection. Our data bolster the argument for providers to assign ICD-9 codes to summarize outpatient encounters, as it is done in VA medical centers, so that this coded information can quickly be made available for surveillance purposes. Our results further argue that diagnostic codes sets used for case-detection should routinely optimized during surveillance system development or expansion. In contrast, prescriptions for cough suppressants, the only structured EMR data found to contribute to ARI case detection aside from ICD-9 codes [19] , did not benefit outbreak detection. While these data suggest how to streamline CDAs for this particular use-case, structured EMR data other than diagnostic codes may very well prove useful in discovering diseases that typically cause abnormal vital signs or prompt predictable diagnostic or therapeutic interventions.

Information potentially useful to public health surveillance has long been extracted from the free text narrative of chief complaints [37] [38] [39] [40] [41] , laboratory or imaging reports [37, 38, [41] [42] [43] [44] , hospital discharge summaries [45, 46] or outpatient clinical notes [19, 47, 48] . To date however, little is known about how information extracted from free text EMR fields should be combined with structured data to accelerate outbreak detection. We had previously found that text analyses aimed at abstracting ARI symptoms typed in clinical notes [49] could improve the performance of ARI CDAs [19] . Our simulations suggest that when these text analyses were combined with structured data so as to improve case-detection accuracy, timelier outbreak discovery ensued. In contrast, when text analyses were directed toward maximizing the area under the ROC for single-case detection, outbreak detection performance deteriorated despite near-perfect case-detection sensitivity. These results seem counter-intuitive until we consider that outbreak detection depends upon a statistical rendering of the size of the epidemic signal relative to that of a baseline. With sensitivities in the 69-75% range, the highspecificity CDAs did not recognize the largest possible number of epidemic cases. Yet, with PPVs in the 50% range, they disproportionately reduced the number of false-positive background cases and improved the system's signal-to-noise ratio. Our results therefore suggest that CDA performance measures that attribute equal weight to sensitivity and specificity, such as the area under the ROC curve, may not anticipate how well a CDA will discover epidemics. Because of the complex interplay between epidemic signals, background noises and statistical processes, promising CDAs should be evaluated in a whole-system context before they are incorporated into an operational surveillance system.

Even though we expect the empiric approach to system improvement outlined this report to be broadly applicable, specific results are necessarily confined to our experimental conditions. Text analyses would indeed be expected to complement structured data differently for diseases that are not defined through symptoms only. Optimal EMR data integration may also depend on the Figure 2 . System performance using alternative case-detection methods. AMOC curves displaying epidemic Detection Delay (days) as a function of daily false alert rate (FAR) (upper panel) or yearly caseload (lower panel). Each curve represent an alternative CDA: CDA 1 (grey stars), CDA 2 (black circles), CDA 3 (red triangles), CDA 4 (green crosses), CDA 5 (blue x's), CDA 6 (teal diamonds), CDA 7 (purple triangles), CDA 8 (yellow stars). doi:10.1371/journal.pone.0100845.g002 characteristics of the epidemic, as well as on the nature and utilization of the EMR in other health care cultures. Our assumption that epidemic cases would be discovered at the same rate as authentic cases may also not hold in reality, as real epidemic cases may have a peculiar disease presentation or severity, and vigilant providers may change their coding or documentation behavior. Our results may also not represent the final word on ARI outbreak detection in our own health system, as we did not formally optimize individual system components, such as ICD-9 groupings, text analyses or statistical approaches, and have not validated our simulation findings in other medical centers.

This work highlights an empiric approach to guide the integration of complementary EMR data for the purpose of epidemic surveillance. Through modular software development, a realistic evaluation platform can be harnessed to estimate the cost and benefit of alternative system configurations, or to match detection sensitivity with available resources to investigate alerts. The platform can also be extended to help improve detection of any disease or event cluster, and may thus allow automated surveillance systems to more vigorously participate in the promotion of safe and effective healthcare practices.

Datafile S1 (CDA 1) Time series of background casecounts. 8-year background time series are provided for each of the CDA outlined in the Table 1 

",0.7397989458807186
Armed Forces Health Surveillance Center (AFHSC) and Cherokee Nation Technology Solutions,"Background: Population-based serologic studies are a vital tool for understanding the epidemiology of influenza and other respiratory viruses, including the early assessment of the transmissibility and severity of the 2009 influenza pandemic, and Middle","of humans, given uncertainties about assay performance and antibody kinetics in exposed and unexposed populations. [9] [10] [11] In addition, new immunoassays and modifications of well-established assays are increasingly being used for the detection of influenza virus strainspecific antibodies. [12] [13] [14] [15] [16] [17] [18] These issues led to the formation in 2010 of the Consortium for the Standardization of Influenza Seroepidemiology (CONSISE). 19 CONSISE is comprised of international scientists experienced in conducting seroepidemiologic studies of influenza and other emerging respiratory viruses; two working groups on epidemiology and laboratory matters were formed to provide tools to help standardize protocols and laboratory methods used (see https://consise.tghn.org/ about/working-group-projects/). The overarching goal of CONSISE is to improve the quality of data arising from influenza seroepidemiologic studies, harmonize methods used in such studies, and thereby provide better evidence for policy makers that guides rational implementation of intervention and control measures. 19 Guidelines for the reporting of the design, conduct, and results of research have been an effective tool for improving the quality and interpretability of published data. Examples include the Consolidated Standards of Reporting Trials (CONSORT) and the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE). 20, 21 These guidelines have become, in some instances, widely accepted standards for reporting of research studies, and the expectation that publications should meet these standards has helped to improve the design and conduct of studies. CONSISE has prepared the following statement, Reporting Of Sero-Epidemiologic Studies for Influenza (ROSES-I), which distills the experience of the working groups into a set of recommendations on the optimal reporting of influenza seroepidemiologic studies.

The aim of the CONSISE ROSES-I statement was to improve the quality and transparency of reporting of seasonal, avian, and pandemic influenza seroepidemiologic studies in order for the validity and generalizability of the results to be better assessed. This statement also aims to improve the design and conduct of influenza seroepidemiologic studies by proposing reporting standards that investigators should consider when designing studies. CONSISE has developed a number of protocols as guides to the design and implementation of seroepidemiologic studies, and these protocols (available at https://consise. tghn.org/articles/available-consise-influenza-protocols/) are a valuable resource that should be consulted in addition to the ROSES-I statement ( Table 1 ).

The components of the ROSES-I statement can be used as a checklist to help guide what key information should be included in the results of published seroepidemiologic studies, and can also serve as a guide to the items that need to be considered during study design and implementation. As with other reporting guidelines, this statement is not intended as a required framework that must be followed in content and format. It is also not designed as an instrument for assessing study quality, for which other instruments exist 22,23 . 

To facilitate the clear identification of studies that quantitatively measure antibodies concentration in members of a defined population in order to make inferences about exposure of that population to emerging respiratory viruses, transmission, and severity, one of the terms ""seroepidemiologic,"" ""seroepidemiology,"" ""seroprevalence,""

or ""seroincidence"" should be used in the title and/or abstract of the study, and the MeSH term ""Seroepidemiologic Studies"" should be used as a keyword [ROSES-I 1.1] ( Table 2 ).

The The term ""seroepidemiologic,"" ""seroepidemiology,"" ""seroprevalence,"" or ""seroincidence"" should be applied to the study in the title or abstract, and the medical subject heading ""Seroepidemiologic Studies"" be used when the report is of a population-based serological survey Explain how quantitative variables were handled in the analyses. If applicable, describe which groupings were chosen, and why ROSES-I 11.1: Describe the serological assay's limit of detection and how this limit is defined or calculated. Describe how samples with a result below or on the borderline of the limit were handled in the analysis ROSES-I 11.2: Describe and justify the titer or other result used to define ""seropositivity,"" or the antibody titer change or change in other assay result used to define ""seroconversion."" Avoid the term ""seroconversion"" unless referring to change from undetectable to detectable antibody level. Otherwise report the fold-rise in titer. Avoid the term ""infection"" but report ""seroprevalence at a titer of …."" 

A number of different seroepidemiologic study designs can be used to estimate various measures of virus infection risk, and different designs have different strengths and weaknesses depending on the objectives of the investigation ( Table 1 ). The methods section should begin by describing the study design, the study population, sampling procedures (e.g., random or convenience), the source of serum or plasma that was analyzed (e.g., frozen stored vs recently collected and tested), the rationale for choosing the study design, and the gen- 

For studies where close contacts of confirmed influenza cases (e.g., household contacts of confirmed cases; 

In order to permit the generalizability of the study results, it is preferable that the study population be as close as possible to the general population under study. However, this is not always feasible, and for example, many serologic studies detecting antibodies to A(H1N1) pdm09 virus infection were conducted using residual sera from blood donors 4,28 or hospitalized patients, 29 who may not be representative of the broader populations in those locations. The potential to introduce selection bias into the study should be addressed in the discussion. Any method used to infer cumulative incidence of infection among the population based on results from the study sample, for example, weighting or standardization, should be reported in sufficient detail to permit reproducibility (ROSES-I 16.2).

In addition, the confidence in the results and conclusions of any seroepidemiologic study depends, among other things, on whether the planned study sample size was sufficient to provide estimates of prevalence or incidence of infection with sufficient precision and certainty. 30 To assess whether the planned sample size was adequate, the How these titers are analyzed and interpreted can affect the results, especially if they constitute a large proportion of the results. A common convention, which is acceptable, is to consider a result below the limit of detection as a serial step below that limit; that is, if the starting antibody dilution is 10, then a value of <10 can be reported as a five for the purposes of data analysis rather than a zero or a ""not 

In cross-sectional studies, seroprevalence can be estimated by the proportion of specimens with antibody titers at or above a specific threshold, with 95% confidence intervals typically obtained using the binomial formula or the normal approximation to the binomial. If a number of additional assumptions are met, including that seroprevalence before an epidemic is very low, and almost all infected individuals have rises in convalescent antibody titers above the chosen threshold, the post-epidemic seroprevalence can provide an approximate estimate of the cumulative incidence of infection. 34 Note that seroprevalence is a proportion and not a rate.

In studies with paired sera, the cumulative incidence of virus infection can be estimated by the proportion of persons with a rise in antibody titer, traditionally a fourfold or greater rise. 31 In most studies, 95% confidence intervals are typically estimated using the binomial formula or the normal approximation to the binomial, implicitly assuming that each person can experience no more than one virus infection during the period considered. It is noteworthy to point out also that cumulative incidence of virus infection is sometimes referred to as an ""attack rate,"" although a proportion of infections may be asymptomatic (and therefore not ""attacks""), and the quantity measured is a proportion and not a rate. The term ""cumulative incidence of infection"" should therefore be preferred to ""attack rate"" in the context of serological studies.

In either case, the methods used to account for the probability of seropositivity or seroconversion if infected, and any method used to account for decay in antibody titer over time, should be reported (ROSES-I 12.2). To increase transparency of cumulative incidence of infection estimates, it is often helpful to report unadjusted estimates of the distribution of antibody titers by age group (ROSES-I 16.1).

In some studies, particularly those with more complex designs in terms of timing of serologic measurements, improved estimates of the seroprevalence at a certain point in time, or the cumulative incidence of infection over a specified time period, may be obtained by fitting observed data to a mechanistic model of transmission dynamics. 4, 35 This can account for non-independence in the data (ROSES-I 12.1).

Although serum samples are more commonly used for serologic studies, convenience sampling may only enable access to plasma. The use of anticoagulants to separate plasma has been shown to reduce the antibody titer to some influenza viruses. 36 This antigen should be antigenically equivalent to the specific virus strain to which the study population was exposed. To enable the comparison between laboratories and also aid in the development of Specifically, as the WHO recommendation for A(H5N1) viruses recommends the use of confirmatory serologic assays upon the detection of single serum positive by MN assay, any confirmatory assays used and the criteria for positivity also need to be described in the same details as above (ROSES-I 12a.13). 38, 39 Inclusion of available international standards 40, 41 facilitates the comparability of serological data.

Inclusion of the actual titers obtained from the international standards and indication whether the data are reported as raw values or international standard-adjusted values should therefore be described (ROSES-I 12a.14). 

The direct comparability of influenza seroepidemiologic studies is currently limited by a lack of standardization across such studies. 8 The 

",0.7391065611420318
BMC Public Health Establishing a nationwide emergency department-based syndromic surveillance system for better public health responses in Taiwan,"Background: With international concern over emerging infectious diseases (EID) and bioterrorist attacks, public health is being required to have early outbreak detection systems. A disease surveillance team was organized to establish a hospital emergency department-based syndromic surveillance system (ED-SSS) capable of automatically transmitting patient data electronically from the hospitals responsible for emergency care throughout the country to the Centers for Disease Control in Taiwan (Taiwan-CDC) starting March, 2004. This report describes the challenges and steps involved in developing ED-SSS and the timely information it provides to improve in public health decision-making.","groups starting in November 2004 and two clear spikes in enterovirus-like infections coinciding with the two school semesters. Using ED-SSS for surveillance of influenza-like illnesses and enteroviruses-related infections has improved Taiwan's pandemic flu preparedness and disease control capabilities.

Taiwan's ED-SSS represents the first nationwide real-time syndromic surveillance system ever established in Asia. The experiences reported herein can encourage other countries to develop their own surveillance systems. The system can be adapted to other cultural and language environments for better global surveillance of infectious diseases and international collaboration.

With the recent global concern over emerging infectious diseases (EID) and the challenges of the 2003 SARS epidemics, government health officials in SARS-affected countries have begun to consider various measures of improving their infectious disease surveillance systems [1] [2] [3] [4] . Infectious disease epidemiologists and several leading public health administrators at the Centers for Disease Control in Taiwan (Taiwan-CDC) becoming aware of the importance of early detection of EID or bioterrorism, started developing an automatic alert system. Therefore, the Automatic Syndromic Surveillance Planning Task Force Committee was created and recruited infection physicians, epidemiologists, biostatisticians, and information technology (IT) experts in July 2003 to oversee the initiation and development of Taiwan's first medical informatics-based emergency department syndromic surveillance system (ED-SSS).

To prepare for this project, we reviewed the syndromic surveillance systems of other countries and officials of health informatics at Taiwan-CDC started collaborating with the Real-time Outbreak and Disease Surveillance (RODS) Laboratory at the University of Pittsburgh to develop a real-time syndromic surveillance system for Taiwan in August 2003 [1, [4] [5] [6] [7] [8] . RODS, used during the 2002 Olympic Winter Games, is the first commonly used syndromic surveillance system in the United States and has been found to efficiently process and analyze data in a timely manner [9] [10] [11] . Together, the task force and the RODS group aimed to establish a nationwide syndromic surveillance system within six months to meet the challenges of potential avian flu outbreaks for up-coming winter seasons and other future EIDs. To gain more operational level experiences, we also visited the Department of Health in New York City, where syndromic surveillance system was established and has been in daily operation since 2001 [12] . There, the task force members observed routine workflow processes and became familiar with other practical concerns of operating an ED-SSS on a daily basis. Based on these experiences and high population density in Taiwan, we decided to create a nationwide surveillance system. To this nationwide ED-SSS, we added geographical information system (GIS) technology, meant to facilitate epidemiological investigation and feedback between data providers and decision-makers [13] .

Using the electronic data from the health information systems already in place in about eighty percent of the hospitals in Taiwan required by the National Health Insurance Payment Program and the technical support of the RODS Laboratory at the University of Pittsburgh, Taiwan's ED-SSS has been in operation since March, 2004 [14] . It is the first time in Taiwan that information technology and timely data directly from hospitals has been used with systematic approaches to facilitate public health surveillance. This report shares our experience of establishing an ED-SSS in a non-English-speaking country. It covers the process of taking into account the various needs at different levels of hospitals, discusses the stages of developing the system, and highlights the characteristics of ED-SSS data collected during the first year. The experiences reported here may benefit other countries seeking to establish or improve their own surveillance systems for infectious disease.

Initially, two Taipei City Municipal Hospitals that kept electronic files of their emergency department patients' medical information were selected as pilot sites. From these two pilot hospitals, we had learned work flow involved in the process of data transfer, format of Chinese chief complaints, practical concern of ED (such as heavy workload etc.) during and after the 2003 outbreak of SARS, and available electronic ED information from nationwide emergency care hospitals. To obtain more representative data from various geographical areas, we gradually enlisted the cooperation of 189 hospitals nationwide, all offering emergency healthcare. Because many outbreaks of EIDs require emergency health care, these emergency care-designated hospitals were required transmit their ED triage and patient data to the Taiwan-CDC electronically on a daily basis ( Figure 1 ).

Nurses at the triage stations at all participating hospitals generated the bulk of the information needed for syndromic surveillance. That information included time and date of admission, date of birth, gender, home zip-code, body temperature, triage categories and chief complaints for patients admitted to their EDs. Because the National Health Insurance requires hospitals to keep clinical data written in ICD-9 codes based on the criteria of the international classification of diseases, 9th revision, clinical modification (ICD-9-CM) for billing purposes, we were also able to collect clinical data from initial assessment of each case by an ED physician. These fundamental data were provided in either the health level-seven protocol (HL-7) format or extensible Markup Language (XML) format, if HL-7 was not used by a hospital during the time of the study. Thus, ED-SSS is capable of accepting these data of above-mentioned variables (Table 1) , including the patients' clinical and demographical information, and hospital identification numbers, in either format.

All the sentinel hospitals recruited into our system had independent MySQL servers on which their data were saved, plus a remote connecting program for automatic transfer of data. Data files generated by the 189 hospitals, including data from their triage classification systems, hospital information systems (HIS) and clinical information systems (CIS), were firstly de-identified and then transferred hourly to a Microsoft SQL Server 2000 at the Taiwan-CDC. Hyper text transfer protocol over secured sockets layer (HTTPS) or secured file transfer protocol (SFTP) was used in this process. All communication histories were recorded in a log file in the SQL server at the Taiwan-CDC and monitored daily by health informatics personnel. A program was written into the system so that each transfer attempt to the Taiwan-CDC would automatically generate an e-mail to the hospital notifying them whether the transfer had been successful or not.

Three different data storage tables were designed to process the data in the Taiwan-CDC's syndromic surveillance database. All information received is initially fed into the first table, with a serial number generated in an additional column for each case. The system picks up the data from the first table every five minutes and moves it into a second temporary table for a logic check and data cleansing. At this point, the system checks for unambiguously erroneous data, e.g., a birth date later than the admission date or other variables such as body temperatures that fall outside of reasonable ranges. The data cleansing work is accomplished through a system algorithm written with SQL commands. The cleaned-up data are transferred to the third table for further epidemiological analysis, aberration detection, and then sent them to related local public health agencies.

Although only a few variables were collected from each hospital on consecutive days, one major difficulty we had was the data presented by discontinuous data, i.e. data that sometimes be there and sometimes not. Sometimes data were repeated. To handle this problem, specific criteria of data cleansing were used for different variables, including the logic checks described above and double checks for possible presence of duplicate patient records. If data in the chief complaint field was written as ""test"" or the field was left empty or if the ICD-9 field was written as ""test"" or left empty, they were deleted before data analysis. Hospital ID, date of birth, admission time, gender, and home zip-code were used as key indicators of whether a listing is a duplicate listing and be deleted as repeated data. The system was capable of performing frequent and rapid checks of any subject of hospital identification code and time format of all time fields. It was capable of moving erroneous data to an ""error table"" for storage. Incorrectly formatted ICD-9-CM data were also moved to the error table. All deletion and removal operations were recorded in the log file for monitoring. In certain situa-tions in case possible systematic errors were found (i.e., aberrant number of ED visits on certain days or occasionally inconsistent formats of ICD-9 codes), the data examiner would contact the medical informatics officers of those specific hospitals to discuss improving data entry.

After the data cleansing, we categorized ED visits into 11 different syndromic groups important in Taiwan. There were: (1) fever, (2) respiratory, (3) skin, (4) neurological, (5) upper gastrointestinal (GI), (6) lower GI, (7) haemorrhagic, (8) influenza-like illness (ILI), (9) asthma, (10) enterovirus-related infection (EVI) syndrome, and (11) syndrome for severe illness or death. Since only about 25% of all chief complaints were written fully in English and the grouping of syndromes by chief-complaints due to Chinese language barriers would have effects on the outbreak detection ability, we first analyzed our data according to the ICD-9 coded syndrome groups [15] . Definitions of clinical syndromes were based on two different sources: (1) those associated with bioterrorism-related agents as announced by the Centers for Disease Control and Prevention (CDC) in the U.S.; and (2) those identified as important by the ED-SSS Advisory Committee in Taiwan, whose members include infectious disease physicians, emergency doctors, pediatricians, and epidemiologists [16] . For example, because the epidemics of enterovirus 71 caused severe fatal cases in Taiwan in the years of 1998, 2000 and 2001, the EVI syndrome group was considered as an important syndrome group locally (ICD-9 codes listed in Table 2 ) [17] . All patient information was de-identified and only aggregated data was used for data analysis. The protocol for this study was approved by the Research Ethical Committee (Institutional Review Board) of National Taiwan University. 20 Coxsackie carditis, unspecified 074. 21 Coxsackie pericarditis 074. 22 Coxsackie endocarditis 074. 23 Coxsackie myocarditis, Aseptic myocarditis of newborn 074. 3 Hand, foot, and mouth disease Vesicular stomatitis and exanthem 074. 8 Other specified diseases due to Coxsackie virus, Acute lymphonodular pharyngitis Although the ED-SSS data started transferring on March 10, 2004, we confined our analysis to data collected between April 1, 2004 (when the data became more stabilized) and March 31, 2005 . Data were organized using statistical programs to perform a descriptive analysis of the daily and weekly plots of different syndrome cases and obtain a baseline pattern for each syndrome in Taiwan. We initially generated the SQL commands for data querying and data grouping into the 11 different syndromic groups. To increase the sensitivity of this ED-SSS in monitoring regional patterns of these 11 syndrome groups, we categorized ED-SSS data by four different geographical areas (northern, central, southern and eastern Taiwan), based on major regional variations in the types of infec-tious diseases. In analyzing the seasonal patterns of ED visits, the correlation between the ILI syndrome and respiratory or asthma syndrome was assessed by the value of Pearson's coefficient (R). the Taiwan-CDC on a daily basis. The greatest challenge as began to develop this system was communication with different hospitals. Because different hospitals were using different information systems and inputting different data with various formats, it took long time to agree which variables and their data format should be collected. At the very beginning to build the ED-SSS, we had at first intended to collect information on a large set of epidemiologically useful variables, including occupation, travel history, family clustering, other exposure-related factors, and address for each patient to help detect possible zoonosis. However, such data were not collected during routine medical examination and care. Finally, we decided to capture only parameters usually collected by the hospitals during examination, intake and care.

During the planning phase, it was necessary to gain a full understanding of what not only just public health personnel expected but also what the medical staff at participating hospitals expected from the ED-SSS. Public health officials tended to prefer a timely and sensitive surveillance system able to detect all possible outbreaks of emerging or known infectious diseases. Mostly concerned over the limited public health resources, they wanted more evidence to prove the cost-effectiveness of ED-SSS and fewer false positive signals from pilot studies before integrating the system into routine public health surveillance workflow. On the other hand, the hospitals and their medical staff had three major expectations. First, the hospitals expected an easily operated feedback mechanism and quick feedback of useful information for better decision-making. Those who had experienced nosocomial infection of SARS during the 2003 SARS outbreak were particularly interested having analyzed information, based on their own hospital or regional/national hospital data, quickly fed back them. They believed that this would provide incentive for them to share hospital data and routinely maintain the high quality of their data for public health usage. Second, the hospitals anticipated two-way communication with public health agencies, as they frequently been requested or even forced to send data on short notice when they were too busy or too involved in emergency care. What made matters worse, despite their compliance; they had difficulty in obtaining useful feedback information from the public health agencies so that they could improve their care of patients at the time of an outbreak crisis. Third, the hospital decision-makers wanted immediate firsthand feedback, particularly with regard to control of nosocomial infection and hospital management in order that their health-care workers could be protected during regional outbreaks. Considering the expectations of both public health agencies and hospitals, we learned that the syndromic surveillance system should provide efficient means of feedback and effective two-way communication.

From April 1, 2004 to March 31, 2005 , data transmitted from the 189 hospitals on 2,692,325 ED visits were collected and stored in the Taiwan-CDC database. Initially, we appointed two computer engineers to cleanse the data by checking the log files and inform the hospitals by telephone on weekdays to correct errors or provide missing data. Then, these cleaned data on daily counts of ED visits collected from ED-SSS were analyzed. The time series plot of rough data on daily numbers of ED visits in our nationwide ED-SSS during the study period is shown in Figure 3 .

The computer system shut down twice during this period. The first time occurred from August 8 th to August 9 th , when no daily procedure was installed to monitor the quality of uploaded data. To help both hospitals and public health agencies perform routine data quality checks, we installed a computer program having check-up procedures of data quality after each data transfer from the hospital to the Taiwan-CDC for automatic quality control of data. This program records all the logs of each data sending from the participated hospitals. If Taiwan-CDC doesn't receive data from hospitals, program will send e-mail automatically to inform the personnel of health informatics in that hospital about failure sending. System maintenance personnel need to check the log daily and make a phone call to the hospital to verify successful data transfer and quality of data if there are no data transfers in two consecutive days.

Only 5.04% of hospitals failed to send ICD-9-CM data, but almost half (47.4%) failed to send chief complaints. For example, of the 239,617 sets of cleaned data received in July 2005, about 7.1% of ICD-9-CM information was lacking or filled out as 'null' and 54.82% of cases did not include chief complaints. Additionally, certain hospitals transmitted the patients' chief complaints in Chinese, further complicating the analysis. Because of these difficulties, this report focuses on the data based on ICD-9-CM diagnostic codes only. The time-series plots of the 11 syndrome groups (Figure 4 , 5) that may correlate to infectious diseases and important health problems in Taiwan (e.g., asthma has become an important pediatric problem in recent years) were analyzed.

Understanding the characteristics and patterns of numbers of ED visits over time from our established ED-SSS in Taiwan is very crucial before we set up appropriate threshold levels of different syndrome groups for outbreak detection, There was a significant difference in daily counts between weekdays and weekends, which occurred on a weekly basis. ED visits were 1.288-fold-higher on weekends than on weekdays (p < 0.001), while national During this first year study, the ED-SSS found patients throughout Taiwan were more likely to seek emergency medical services at medical centers than at district or local hospitals. Most patients visiting ED were 60 years old or older (21.46%) or below the age of 10 years old (18.55%). These two groups were also at greatest risk for various infectious diseases, especially influenza (Table 3) . Young adults between 20 to 39 years old ranked the third most frequent visitors to ED (17.43%), though traffic accidents, not infectious diseases, were the main reasons for their visits. These age distributions suggest that the newly established ED-SSS was capable of providing information for the age groups most at risk for severe cases of infectious diseases in Taiwan. Male ED patients slightly outnumbered female ED patients (male : female = 1.12:1), which approximates the general distribution of gender in Taiwan (male/female ratio = 1.10). For elderly ED patients (age > 65 years old), the male/female ratio in our sample was the same as general population (male: female = 1.10:1).

To understand the epidemiological characteristics of the eleven important syndrome groups and asthma syndrome in ED-SSS, data of their daily and weekly counts were plotted and shown in Figure 4 and 5, respectively. As can been seen in time series plots in Figure 6 , the seasonal patterns for ED visits due to respiratory syndrome and ILI syndrome were quite similar with high correlation (R = 0.98), while that for asthma syndrome, which had distinct peaks, was nonetheless not highly correlated with ILI syndrome (R = 0.78) nor respiratory syndrome (R = 0.77). Importantly, the ED-SSS was able to detect peaks of these respiratory-related syndromes even occurred in or around the summer season (July to September), though their most noticeable peaks were found during the Chinese new year holidays ( Figure 5B, 5C, 5D ). Like respiratory infection syndrome, visits due to the fever syndrome also showed another peak during the summer (July to September) ( Figure 4A ). ED visits for respiratory syndrome peaked earlier (mid-September) than those visits for ILI (mid-October) and asthma syndrome (end of September, e.g. the transition period between summer and autumn). Visits for pediatric asthma syndrome for children 12 years old and below peaked in mid-autumn, around mid-October (data not shown). Later during winter season (between November and February), there was an increase 

Respiratory Syndrom Asthma Visits In gastrointestinal (GI) syndromes, visits due to upper GI or lower GI or total GI started increasing November 2004 and peaked during the Chinese New Year holidays ( Figure  4F, 4G, 4H) . Interestingly, cases of hemorrhagic syndrome also increased slightly in the winter season ( Figure 4I ).

For those syndrome groups with severe symptoms, including skin rash, neurological symptoms and death/ coma that might be related to bioterrorism attacks, there was no significantly cyclic or seasonal patterns ( Figure 4J , 4K and 4L). One spike of syndrome cases with clinical severity (severe syndrome) appeared in mid-May, but that occurred as a result of one hospital sending duplicate data that escaped from our check algorithm ( Figure 4L ).

Taiwan has very high population density (Taiwan, 632.23 persons/km 2 ; Taipei, 9662.53 persons/km 2 ), which increases the spread of many human-to-human infectious diseases [19] . This makes the surveillance of infectious diseases very important for this island. Taiwan's ED-SSS is the first syndromic surveillance system to be implemented in Asia. It also represents the first time that Taiwan's public health agencies have attempted active nationwide surveillance. Its automatic data collection mechanism is capable of capturing comprehensive population-based health information and providing important details on current disease epidemics at the community level. The information it provides can also be used as community baseline data for further infectious disease modeling and can also improve the detection of emerging infectious diseases.

In addition to the information that the ED-SSS can provide for disease control, it can open avenues for further investigation. For example, in addition to the neurological syndrome, asthma syndrome, and syndrome for severe symptoms, there were clear and consistent weekend and holiday increases in the visits of other nine syndrome groups. Because cost of ED visits in Taiwan is not as expensive as it is in other countries, especially the United States, it is very likely that many patients seek ED medical care when local clinics are closed on weekends and holidays. It is also possible that the gathering of people on the holidays would increase the transmission of certain pathogens, particularly on cold days in closed spaces where respiratory viruses including influenza virus are easily transmitted. Therefore, future research might want to investigate the effect of holidays on the aberration detection of outbreaks and prediction of number of cases for certain infectious diseases using Taiwan's ED-SSS.

We also found differences in seasonal trends in visits due to symptoms/signs related to respiratory, influenza-like illness and asthma syndromes. Our ED-SSS found a sum-mer peak in visits for cases with influenza-like illnesses in 2004. This has seldom been found by the previous passive surveillance systems used in Taiwan. These summer cases of influenza-like illness occurred before annual vaccinations usually done in October or November. Therefore, a further longitudinal analysis of influenza-like syndrome patterns is needed to formulate the best vaccination policy on human influenza.

Another epidemiological finding from our ED-SSS was an increasing trend in visits due to gastrointestinal syndrome starting late autumn 2004. Such trends have not been detected by other infectious disease surveillance systems in Taiwan. There are two possible explanations for this finding. One reason for the increases might be related to the increased activity of certain pathogens, including rotavirus or norovirus, during winter season, as was found during the winter of 2006 in both Japan and Taiwan [20] [21] [22] [23] [24] [25] [26] [27] . Another reason might be the social habits of Taiwanese who like to dip raw meats and seafood into boiling water fondues and eat from the chafing pot during the winter season. This would increase the change that inexperienced or careless diners would consume undercook seafood or use chopsticks contaminated by raw seafood.

The findings of our ED-SSS, the first time in Taiwan to use daily rather than weekly data, suggest further directions for research into GI syndrome and many other diseases of significant interest to public health. For example, during the 1998, 2000 and 2001 enterovirus 71 epidemics, children aged 3 years and younger who were at higher risk of severe or fatal cases of the disease were identified for more effective prevention only after the occurrence of several cases of sudden deaths from weekly sentinel physician surveillance and later retrospective epidemiological data analysis on those cases when sample size became larger. Therefore, prospective monitoring of daily ongoing data of EVI syndrome in this high risk age group and early ED-SSS detection of enterovirus activities by local public health personnel might help minimize social panic among parents. Furthermore, the results on seasonal pattern of enterovirus-like infection in our ED-SSS was consistent with the previous epidemic patterns in Taiwan, again demonstrating the usefulness of ED-SSS to avoid future large-scale or severe epidemics caused by enteroviruses [18] . In summary, these initial findings suggest that it is necessary to develop algorithms capable of detecting aberrations for different syndrome groups from patients in different geographical areas of Taiwan, taking into account variations in the levels of medical care and the effect of weekends and holidays on ER visit.

The ED-SSS did not, however, reveal obvious trends in all syndrome groups. For example, it was hard to find seasonal patterns or secular trends in cases of coma/death, skin rash, or neurological symptoms -the three syndrome groups that might be useful in the detection of severe outbreaks caused by bioterrorism, e.g., anthrax, during the study period without bioterrorism attacks [28, 29] . Certainly, continuous monitoring for these syndrome groups at both local and national levels will be very helpful in detecting possible bioterrorism or EIDs in future years. Using those trends in coma/death and other syndrome groups of clinical severity or unexpected symptoms/signs, our ED-SSS data have provided directions for further research in the areas of pathogen detection, epidemiological clues, and improvement in public health policies. Therefore, future investigations have to control the weekend and holiday effects of ED visits for better aberration detection even during long holidays.

In daily public health practice to monitor the data of ED-SSS, careful verification and systematic management is needed once the aberration signals are detected. The server needs an automatic error feedback system function instead of the original use of engineers to double check for data errors would increase the efficiency and completeness of surveillance. Future efforts require closer collaboration between computer-science professionals and medical informatics personnel at the Taiwan-CDC to establish a system with the standard operating procedures (SOP) for database maintenance and to provide more continuous on-job training for both hospital users and local and central public health agencies [30] .

The major difficulty in developing our ED-SSS was diverse formats for different types of data, including categories of chief complaints, the ways to fill out ICD-9-CM codes, and even the different number of digits used in home zip codes in different participated hospitals. For example, most Taiwan hospital ED physicians/nurses EDs only write down one chief complaint, which is very different from the ED reports made by most U.S. hospitals which list all possible complaints (with text format) in English. Several participating hospitals only had a paper system for recording triage chief complaint data. A standard format for select syndromes and variables needs to be established and continuously reevaluated to improve data quality and stability of data transmission. There are needs to have more research into the chief complaints with Chinese styles, the suitability of chief complaints vs. ICD-9 codes, how to combine symptoms/signs and link data to improve sensitivity.

With regard to the current epidemics of avian influenza H5N1 in China and many other southeast Asian countries, an ED-SSS like the one we developed in Taiwan may play an important role in detecting an outbreak possibly caused by human-to-human transmission even when cluster size is small [31] [32] [33] [34] . Through early detection, ED-SSS may help minimize the adaptation of avian influenza virus to human populations. Because of the large volume of business traffic, international travelers, and workers from Southeast Asia coming to Taiwan, it has previously been difficult to do real-time surveillance for imported infectious diseases, including dengue, malaria, acquired immunodeficiency syndrome (AIDS) and SARS. However, using the ED-SSS to monitor health status at the community level may help public health decision-makers handle unexpected health threats. Because countries are so interconnected today, it is imperative that we share our health information and experiences with other countries if international health is to be guarded. Our ED-SSS has equipped Taiwan the ability to closely monitor avian influenza and other potential EIDs in Asia and worldwide. We hope that by sharing our experiences developing ED-SSS, other countries can be encouraged to develop and improve their own surveillance systems for infectious disease.

The author(s) declare that they have no competing interests.

TSJW was in charge of epidemiological data analysis, improvement of the ED -Syndromic Surveillance System, and manuscript writing. FYFS initiated the thoughts on Syndromic Surveillance for detecting emerging infectious diseases in 2003 and contributed to syndrome groupings, selection of variables for ED-SSS, and system improvement based on clinical data analysis. MYY contributed to syndrome groupings, initiating the standard format for collecting Chinese Chief-Complaints in our ED-SSS, and system improvement with regard to clinical aspects. JSJW initiated the thoughts on Syndromic Surveillance for Emerging Infectious Disease in 2003. SWL worked on computer programming on ED-SSS and health informatics for surveillance systems of infectious disease at Taiwan-CDC. KCMC was a leader and coordinator on health informatics for surveillance systems of infectious diseases at Taiwan-CDC and gave the most administrative support on systematic improvement in health informatics. CH provided statistical consultation and chose the best statistical modeling method for outbreak detection in the initiation stage of establishing the ED-SSS. JHC was the Deputy Director at Taiwan-CDC in charge of improving surveillance of infectious diseases, coordinated the 189 hospitals designated for emergency health care to participate the ED-SSS, and provided strong administrative support on ED-SSS. YTC was a research assistant of ED-SSS in charge of the data analysis and project administrative assistance. HC was the Director of Department of Health in Taipei City and participated in the task force meetings from planning to implementation of ED-SSS in the perspective of the local government. CHC was the Section Chief of Department of Health in Taipei City in charge of surveillance, prevention and control of infectious diseases in Taipei City and gave suggestions health informatics and practical concerns from the viewpoints of local government. FCRT helped set up the Real-time Outbreak and Disease Surveillance (RODS) system at Taiwan-CDC. MMW introduced our public health officials and scholars in Taiwan to the practical applications of RODS in the USA and informed us of recent progress of RODS. IJS, the former director of Taiwan-CDC, had the vision to invite scholars to discuss the improvement of infectious surveillance system in Taiwan right after the 2003 outbreak of SARS. CCK, involved in the improvement of infectious disease surveillance in Taiwan for more than twelve years, initiated research on syndromic surveillance, coordinated each trouble-shooting step as the ED-SSS was developed and implemented, and was involved the revision of the manuscript. All authors read and approved the final manuscript.

Publish with Bio Med Central and every scientist can read your work free of charge 

",0.7390339873802242
Conference report UK vaccines network: Mapping priority pathogens of epidemic potential and vaccine pipeline developments,"During the 2013-2016 Ebola outbreak in West Africa an expert panel was established on the instructions of the UK Prime Minister to identify priority pathogens for outbreak diseases that had the potential to cause future epidemics. A total of 13 priority pathogens were identified, which led to the prioritisation of spending in emerging diseases vaccine research and development from the UK. This meeting report summarises the process used to develop the UK pathogen priority list, compares it to lists generated by other organisations (World Health Organisation, National Institutes of Allergy and Infectious Diseases) and summarises clinical progress towards the development of vaccines against priority diseases. There is clear technical progress towards the development of vaccines. However, the availability of these vaccines will be dependent on sustained funding for clinical trials and the preparation of clinically acceptable manufactured material during inter-epidemic periods.","The re-emergence of Zika virus since 2007 and the 2013-2016 Ebola outbreak in West Africa highlighted the risks that epidemic infectious diseases still pose. The outbreaks stimulated a reexamination of priorities in research and development of vaccines to these diseases at national and international levels. The UK Vaccines Research and Development Network (UKVN) was set up under the instructions of the British Prime Minister in 2015 and an expert working group (WG1) was convened to map the priority pathogens capable of causing future epidemics. A subsequent meeting of the group occurred in 2017 and progress and revised priority pathogens were considered. This report summarises the findings of both meetings and reviews the international emerging disease vaccine research and development landscape. The 2013-2016 Ebola outbreak marked a paradigm shift in the attitude of governments and international organisations to high impact epidemic infectious diseases. Prior to this, disease epidemics had been dealt with on a largely reactive basis, with coordinated development of disease control strategies, vaccines and antiviral drugs occurring only in response to large scale outbreaks. Although good academic research on the trends in emergence and basic biology of outbreak diseases was often available, it almost invariably stalled at the level of demonstrating that immunogens were effective in small scale trials in animal models [1] [2] [3] . The Ebola virus outbreak has followed the pattern of intensification of research effort following major outbreaks previously seen with other diseases (such as SARS and MERS coronaviruses). For example, a search of NCBI PubMed for 'Ebola vaccine' reveals 240 original scientific reports (excluding review articles) in the 37 year period 1976-2013 compared to 612 reports in a 3.5 year period between 2014 and July 2018. However, there is some hope that there will be co-ordinated clinical research for epidemic diseases following this outbreak and, critically, technology transfer to pilot methods suitable for large scale vaccine manufacture before large outbreaks of other pathogens arise.

Although there remain questions regarding the long term sustainability of commercial vaccines for emerging (and potentially emerging) epidemic diseases, some of the factors that previously limited vaccine development beyond a small laboratory scale are now being addressed. The possibility of conducting phase II/III clinical trials during an outbreak, at least as part of a ring vaccination strategy, has been demonstrated [4] [5] [6] [7] . This has highlighted that prototype vaccine trials need to be better integrated into emergency response protocols during an outbreak [8, 9] . The caveat to this is that such trials need to be very carefully managed and communities affected must be actively engaged to prevent misunderstandings about what researchers are doing [10, 11] . Also, performing research during an outbreak presents extra challenges compared to similar research conducted on endemic diseases. Partnerships between researchers and local health authorities need to be quickly and effectively established; local regulatory and ethical approval must be granted for any vaccination trial; ideally local physicians should be recruited to deliver the vaccine and monitor patients; sometimes these factors are additionally complicated by a lack of local infrastructure (power, water, internet access) hampering storage and administration of large batches of vaccine [8, 12, 13] . A fundamental problem for funding trials for vaccines to emerging epidemic diseases with outbreak potential is that such vaccines are not commercially attractive prior to an outbreak, or during inter-epidemic periods. Given the costs and extended timeframes of developing, licensing and manufacturing a new vaccine it is understandable that the commercial priority lies with endemic diseases in wealthy countries where there is a predictable market for the vaccine every year [14] . Emerging and outbreak diseases are sporadic by definition and although outbreaks can be large there can be long periods between outbreaks and therefore there is no guaranteed market for the vaccine product. In this context, the willingness of governments and inter-governmental organisations such as WHO to support commercial scale vaccine development and to invest in establishing a bank of experimental vaccines is key to preparedness for future outbreaks. It is important that the international community develops a strategic approach to avoid duplication and ensure all gaps are covered. Obviously the United States Government and other agencies have their own unique additional objectives to address vaccines for bioterror agents. Overall, WHO and the Coalition for Epidemic Preparedness Innovations (CEPI) are in a good position to promote a cooperative approach to emerging disease vaccine development.

As with all publicly funded initiatives limited resources demand prioritization of pathogens targeted for vaccine development. In the case of the UKVN, prioritization was based on expert review of available information on diseases that represent a known or potential threat for an epidemic disease cross referenced with the state of vaccine availability for those diseases. Specific criteria for inclusion on the list included: case fatality rate (CFR) and disability burden of disease, regularity of outbreaks, evidence for geographical spread, zoonotic impact and ease of transmission from animal hosts to humans where the disease was zoonotic, potential for human-to-human transmission, availability of diagnostic platforms and existing investment and development stage of current vaccines, and finally whether there was evidence that the infection/disease could be treated effectively through another intervention.

The review panel included individuals with expertise in epidemiology and vaccine development, as well as infectious disease experts in human and animal health, and representatives of major UK funding bodies (MRC, BBSRC, Wellcome Trust, Department of Health and Social Care). The panel specifically focussed on diseases with the potential to cause high impact epidemics in humans. Animal diseases were only included in the consideration when they had substantial zoonotic potential (for example, Rift Valley Fever). Influenza A virus was excluded on the grounds that there were, and remain, separate funding routes for the development of vaccines for emerging pandemic Influenza A subtypes. A long list of epidemic diseases for which no suitable vaccine was available was devised, and subsequently reduced to a list of 14 priority pathogens by the scientific experts on the panel by a voting system ( Fig. 1 ). Based on its late stage of commercial vaccine development Dengue virus vaccine research was subsequently deprioritised. A total of £101 million (US$131 million), from the £120 M UKVN allocation, was spent on funding specific projects addressing the initial priority list and the original list was again reviewed after two years to identify any gaps or revisions necessary.

In addition to the UKVN, other organisations around the world have undertaken similar outbreak pathogen prioritisation processes. WHO generated a list of 8 priority pathogen groups in 2015, which it reviewed in 2017 and 2018 [15] . The criteria for assessing prioritisation used by WHO was based on 8 criteria (Human transmission; Medical countermeasures; Severity or case fatality rate; The human/animal interface; The public health context of the affected area; Potential societal impacts; and Evolutionary potential). This WHO blueprint has been the basis for the selection of priority diseases for vaccine development by the numerous funding agencies, including CEPI. The US National Institute of Allergy and Infectious Diseases (NIAID) also prioritises pathogens based on transmission, mortality and requirement for public health preparedness. Although it does not specifically take into account availability of vaccines, the NIAID priority A list does include many of the pathogens that are outbreak type diseases [16] . As might be expected given the similar focus of these organisations on the promotion of human health, there is considerable overlap between these lists (Fig. 2) . The bunyaviruses Rift Valley Fever and Crimean Congo Haemorrhagic Fever, filoviruses Ebola and Marburg, and the paramyxovirus Nipah, arenavirus Lassa fever, and coronavirus Middle East Respiratory Syndrome, appear on all the priority lists. Only WHO also includes Hendra and Severe Acute Respiratory Syndrome Coronavirus. UKVN includes Chikungunya and Q fever, and NIAID includes smallpox, anthrax, botulism and some new world arenaviruses (Lujo, Junin, Machupo, Guanarito and Chapare viruses) in its priority A list. Hantavirus and plague are shared between the NIAID and UKVN lists only, and the emergence of an as yet unknown disease (Disease X) is specifically accounted for in the WHO Blueprint, CEPI and UKVN lists. These differences reflect both the different priorities of the organisations compiling the lists and the uncertain nature of prioritising diseases that are not currently, or are only intermittently, a problem in different parts of the world. Very often the differences between lists is the result of small variations in the perceived risk of diseases by members of the review panel. In part, this is a consequence of the fact that surveillance data for many outbreak diseases is limited and the most important drivers contributing to outbreaks are poorly characterised. For example, the UKVN prioritised Hantavirus, plague, Chikungunya and Q fever were all on the WHO long list, while Hendra and SARS were on the UKVN long list but not the final list. Future prioritisation exercises would be facilitated by better surveillance information for all priority diseases. This would include serosurveillance data in 'at risk' populations, where they can be identified, to assess whether changes in immunity at a population level can be linked to the risk of an outbreak.

Since the onset of the West African Ebola virus outbreak in 2013 there has been some notable progress towards the control of some of the diseases on the priority lists, particularly Ebola virus. In 2015, there was no approved vaccine for Ebola virus and arguably this lack of an effective vaccine for use in emergency situations was one of the factors that contributed to the size and duration of that outbreak. While effective vaccines were produced and tested during the outbreak [4] [5] [6] 17] , which may prove important in future control, they were available too late to make a significant impact on disease control in 2013-2016 [18] . It is notable that the Democratic Republic of Congo, which has had repeated Ebola virus outbreaks since 1976, had outbreaks in both 2014 (before vaccines were available) and 2018, after vaccines had been tested in West Africa. During the 2014 outbreak in Djera there were 66 cases and 49 deaths. During the 2018 outbreak in neighbouring Bikoro, Iboko, Wangata and Ntondo by June 14 there were 66 cases but only 28 deaths (14 from confirmed Ebola cases). By the same date, 2730 people in the country had been vaccinated with the rVSV-ZEBOV vaccine using a ring vaccination strategy [19] . Additionally, for the current the current outbreak in the east of DRC over 170 000 people have been vaccinated. As always, it is particularly challenging to assess the impact of vaccination for a disease in an outbreak situation, in particular to quantify the cases that do not occur due to vaccination rather than the number of observed cases. Unlike an endemic disease, where there is a baseline disease incidence, an emerging epidemic disease is by its nature relatively unpredictable in the number of cases that will occur without intervention. During a ring vaccination trial in Guinea, 10 cases of Ebola occurred where vaccination was delayed in contrast to no cases, after 10 days, in vaccinated individuals where vaccination provision was immediate, thus providing evidence the vaccine may save lives [5] . In addition to the rVSV-ZEBOV vaccine, several other promising EBOV vaccines have undergone clinical or pre-clinical development since 2015 (Table 1) .

For the other pathogens on the UKVN priority list the progress towards clinical development of vaccines has been mixed. There is now a licensed tetravalent Dengue vaccine manufactured by Sanofi Pasteur, which is a recombinant virus based on the yellow fever 17D attenuated vaccine backbone [20, 21] . Due to concerns over antibody dependent enhancement of disease, this vaccine is only recommended for particular age groups in endemic areas who have been pre-screened for antibody responses indicating prior Dengue exposure [22] . A second attenuated tetravalent Dengue vaccine is under development by Takeda and has shown promising results in a phase 2 clinical trial [23] . There are now two licensed inactivated hantavirus vaccines which are available in South Korea (Hantavax), and China (HFRS vaccine) [24] [25] [26] , but these vaccines are not available elsewhere. Inactivated plague vaccines have been manufactured since the 1890s [27] however there are currently no vaccines of this type being marketed. In terms of new vaccines against plague, there are two candidates that have reached phase 2 clinical trials (Table 1) both are subunit vaccines based on recombinant F1 and V plague antigens. Phase 2 trials have also been completed for vaccines against Chikungunya and Rift Valley Fever virus, although the Chikungunya vaccines are likely to reach the market faster since their development is being driven by commercial organisations. The RVFV vaccines were tested by the US Army and therefore may not be immediately available for the general population [28] . It is interesting to note that new RVFV vaccines are being developed for animal vaccination but to date there have been no trials of these vaccines in humans [29] [30] [31] . Vaccines for other pathogens on the UKVN list are less well developed, immunogens for ZIKV, SARS, and CCHF have all reached phase I clinical trials. In the case of ZIKV there is also a good chance that these will be taken forward for more advanced clinical testing. Encouraging preclinical vaccine data exists for CCHFV [32] and in the 1970s the Bulgarian Ministry of Health produced a vaccine based on an inactivated CCHFV but no reliable human efficacy data exists. More recently a Turkish forma-lin inactivated vaccine has been reported to have been tested clinically in humans (Table 1 ). There is a Q fever vaccine that is available in Australia manufactured by Seqirus that is reported to be effective [33, 34] , however the vaccine is not currently licensed in other countries. For Lassa and Nipah no vaccine for use in humans has progressed beyond pre-clinical testing. However, CEPI has recently funded five large vaccine programmes for Lassa which are likely to lead to early clinical trials. One of the key observations emerging from consideration of existing and new vaccines is that differences between countries in the licencing process and the level of prior efficacy data required substantially affects the time new vaccines take to reach the market.

In terms of vaccine technologies, a range of different approaches have proven effective. Inactivated virus vaccines have been shown to be effective for a range of priority pathogens [24, 25, 35 ] and arguably have the lowest level of technology development required. The drawbacks are the safety issues related to growing and preparing hundreds of litres of highly pathogenic virus, the potential for incomplete inactivation, and the requirement for multiple doses of vaccine to achieve immune protection. Recombinant virus vaccines, where the genome of one virus is modified to express antigen(s) from another pathogen have also been very successful, with VSV, MVA, human and chimp adenoviruses, and measles all effective for the delivery of foreign antigens to stimulate strong immune responses [5, 7, 36, 37] . The use of measles vectored vaccines has the additional advantage that it may provide some additional protection against an endemic human disease (measles). Subunit and VLP vaccines for plague and Chikungunya have also been tested in clinical trials [38] . There is also some evidence that nucleic acid (DNA, mRNA) based vaccines can be immunogenic in humans for some diseases [39, 40] . Although not Table 1 Vaccines reported as tested clinically against UKVN priority diseases.* where peer reviewed results are available the reference has been included, otherwise the ClinicalTrials.gov identifier has been listed. rV = Recombinant virus vaccine, iV = Inactivated virus vaccine, iB = inactivated Bacterial vaccine, VLP = Virus like particle vaccine, Sub = Subunit vaccine, N = Nanoparticle, IgG = Therapeutic antibody, DNA = DNA vaccine, VSV = Vesicular stomatitis virus, Ad5, Ad26 = replication defective Human Adenovirus type 5 strictly vaccines, the development of therapeutic antibodies that can be used post exposure during an outbreak, such as those developed against MERS coronavirus, may be particularly useful for outbreak type diseases provided that they are effective in the field [41] .

Significant challenges remain for the control of epidemic diseases, despite more clearly defined priorities in terms of pathogen selection. In particular, early identification and reaction to an outbreak is critical [18] , there is a need to provide sustained investment in surveillance for disease outbreaks. Maps that predict outbreak risk may help to guide resource allocation with respect to infrastructure development and surveillance. The need for surveillance is further complicated by the requirement to monitor 'Disease X' [15] , the previously unknown pathogen that can have major impacts on health during an outbreak. Recent advances in sequencing technologies have led to the identification of many previously unknown viruses [42] [43] [44] . This is likely to increase even further with efforts to sequence DNA and RNA from other eukaryotes including wildlife. While most of these new viruses do not pose a threat to human health, identifying those that do in a timely manner is one of the major challenges of modern surveillance. This underlines the importance of international collaboration including with low income/endemic disease countries. Another challenge is that of implementing the Nagoya Protocol with respect to the use of samples from low and middle income countries. Previously, countries could control the physical export of samples to ensure that there was agreement on how samples could be used. With the advent of portable sequencing devices that can be used in the field [45] [46] [47] [48] [49] and gene synthesis technologies it is now possible to design and produce a new vaccine starting with only sequence data, without physical transfer of material.

Currently there are relatively few diseases caused by bacteria that appear on the priority list. This is largely related to the effectiveness of antibiotic treatments. However, the threat of emergence of a previously unknown disease must be balanced against that of emergence of antimicrobial resistance in known bacterial pathogens.

Technical challenges around unwanted adverse effects of vaccines such as those associated with dengue vaccines [21] and the development of vaccines useful in resource poor settings are real, but are likely to eventually be overcome. Other challenges are in addressing negative attitudes that result in vaccine hesitancy in populations that would benefit from routine vaccination for outbreak type diseases to protect the wider community. The question of how and whether wildlife populations should be vaccinated if there is a zoonotic disease also needs to be addressed. Also, vaccinating livestock against diseases that first infect livestock and then humans may be a highly effective way to prevent new human disease outbreaks, and the time and costs required to achieve licensure of livestock vaccines are considerably lower than for human vaccines. Vaccination of sheep, cattle and goats against RVFV, pigs against Nipah, sheep against CCHFV and camels against MERS could prevent human infections, but in the last two examples the disease does not cause significant economic losses and vaccination of livestock would be more difficult to introduce. Indeed, if the primary reason for livestock vaccination was to benefit human public health, uptake by farmers would be low unless the benefits to the community were clear and there was no commercial cost to the farmer from vaccination.

The commercial and political challenges associated with epidemic diseases are significant. Since outbreaks are sporadic, investment tends to wane between outbreaks for the establishment of emerging diseases vaccine research and development, especially progressing prototype vaccines to proof of clinical efficacy. This includes funding preclinical trials in nonhuman primates. The new WHO and CEPI initiatives may overcome this problem but it is important that this momentum towards the development of effective vaccines is maintained even in the absence of a recent outbreak. A second political challenge was highlighted in West Africa and in the more recent outbreak in DRC, it is vital that local communities are actively engaged in the vaccine testing process to avoid disinformation [10, 11] . Furthermore, it is crucial that local government and healthcare agencies are fully integrated into any disease outbreak response.

Since 2016 the arrival of CEPI has provided strong financial support for the development of MERS, Nipah and Lassa vaccines towards Phase 2 studies. With donor commitments of >US$700 million to date, CEPI still lacks the financial force of the Wellcome Trust or the Gates Foundation (both of which are CEPI donors). The US government, through civil and military programmes is the largest sponsor for vaccines on the priority list. Other significant state contributors include the UK and Norway. China and India have emerged as vaccine players, but government funding mechanisms are not transparent at this time. Corporate vaccine funding remains important, with global R&D spend estimated at up to US$7 billion (on a purchasing power parity basis), but only for commercially viable vaccines. As already discussed this is a particular issue for vaccines against epidemic diseases. Vaccine development from lab bench to registration can cost over US$1 billion, with largescale manufacturing plants costing another US$1 billion. Without guaranteed sales, there is no incentive for a company to invest in vaccine development. Only in the case of biosecurity vaccines, such as anthrax or smallpox has the US government funded both manufacture and purchase of vaccine stockpiles. There is a question over whether the international public sector can guarantee purchase and manufacture of priority list vaccines. The use of platform technologies to develop both commercial and emerging pathogen vaccines would obviate the need to build separate large-scale manufacturing plants in order to produce a stockpile of the emerging pathogen vaccine. Of note is the growth of Indian vaccine manufacture -now making more doses each year than the top four vaccine manufacturers -this is backed by supply to the Indian public sector market and growing sales to the international public sector (for example UNICEF).

Although future outbreaks may result from as yet unknown or poorly understood pathogens there is no excuse for governments not to prepare for known threats. 'Disease X' represents the hardest scenario to prepare for, however previous emerging disease outbreaks have been associated with pathogens that have already been described, or pathogens closely related to them so the exploration of platform vaccine technologies for known priority pathogens is logical. Progressing vaccine development for known threats to a field-ready state may also provide a springboard for the development of vaccines against new, related, diseases. Clearly, the time taken to develop even such modified vaccines requires that other epidemiological approaches are used as the primary focus of disease control. However, it is vital that these approaches are integrated with the testing of new vaccines. To this end, it is imperative to undertake phase I and II safety and immunogenicity studies and produce vaccine stockpiles during inter-epidemic periods so that vaccines are available for rapid deployment to compliment established outbreak control efforts and permit the collection of valuable clinical efficacy data.

",0.7387924039435707
Frontiers of transcutaneous vaccination systems: Novel technologies and devices for vaccine delivery,"Transcutaneous immunization (TCI) systems that use the skin's immune function are promising needlefree, easy-to-use, and low-invasive vaccination alternative to conventional, injectable vaccination methods. To develop effective TCI systems, it is essential to establish fundamental techniques and technologies that deliver antigenic proteins to antigen-presenting cells in the epidermis and dermis while overcoming the barrier function of the stratum corneum. In this review, we provide an outline of recent trends in the development of techniques for the delivery of antigenic proteins and of the technologies used to enhance TCI systems. We also introduce basic and clinical research involving our TCI systems that incorporate several original devices.","Infectious disease is the most common cause of death, accounting for approximately one-third of fatalities worldwide. Recent waves of transnational migration of people and materials enhanced by the development of transportation facilities, changes in social structure, and war have increased the global spread of emerging infections, such as severe acute respiratory syndrome and avian influenza virus [1, 2] . In addition, declining sanitation and the onset of drug-resistant pathogenic organisms have increased the spread of re-emerging infectious diseases, such as tuberculosis and malaria [3, 4] . Although major treatment for these infectious diseases is antibiotic administration, the only fundamental prophylaxis is vaccination for a biological preparation that improves immunity to a particular disease. Vaccine development, which has a long history, has progressed recently with the development of new approaches and technologies based on advances made in the fields of bacteriology, virology, and molecular biology.

Conventional vaccination is, however, performed mainly by injection, which has several inherent problems: pain, the need for trained personnel, associated needle-related diseases or injuries, and storage or transport issues. In some areas, vaccine coverage against infection is low due to failure in follow-up as well as a lack of trained medical personnel and facilities. The reuse of needles causes the death of at least 1.3 million people per year from hepatitis B and AIDS [5] . Thus, the development of needle-free, easy-to-use, and low-invasive vaccination methods is an urgent task. With its advantages that overcome the inherent problems of vaccination by injection, transcutaneous immunization (TCI) or intranasal immunization (INI) is now attracting attention as an alternative vaccination route.

INI, which is needle-free vaccination method, is highly expected as a hopeful vaccination procedure to stimulate both mucosal and systemic immune responses. The mucosal antigen (Ag)-specific immune response, however, is weak, thus it is necessary to develop a mucosal vaccine adjuvant to develop mucosal vaccines. The cholera toxin (CT) and heat-labile enterotoxin (LT) are potent mucosal adjuvants, but recent reports showed that a human vaccine containing inactivated influenza virus and LT as an adjuvant resulted in a very high incidence of Bell's palsy [6] . Therefore, mucosal vaccine adjuvants with high efficacy and safety for the purpose of a clinical application are necessary.

The skin has important immune functions as a proinflammatory organ [7] [8] [9] . The epidermis and dermis are highly populated by dendritic cells (DCs), which are potent Ag-presenting cells (APCs) with important immunostimulatory and migratory activities (Fig. 1) . Langerhans cells (LCs) in the epidermis and dermal DCs (dDCs) in the dermis are important for the induction of Ag-specific immune responses in the TCI system. Thus, if Ag can be efficiently delivered to LCs or dDCs resident in the epidermal layer or dermis, TCI might elicit an effective immune response. However, there is a difficulty to overcome for development of TCI system. The uppermost layer of the epidermis is the stratum corneum (SC), which consists of about 20 layers off lattened, enucleate, and keratin-filled corneocytes surrounded by lamellae of around eight lipid bilayers [10, 11] . The lipid bilayers consist Fig. 1 . Skin immune system. The skin is enriched with various immunocompetent cells such as LCs, keratinocytes, and several dDCs. Keratinocytes are mainly involved in the induction of innate immunity. LCs and dDCs capture external Ag, migrate into regional lymph nodes, present Ag to T cells, and activate Ag-specific T cells and B cells. Activated T cells and B cells migrate to each tissue and induce Ag-specific immune responses. primarily of cholesterol, free fatty acids, and ceramides. As the SC is the principal barrier to the penetration of substances, it is difficult to efficiently deliver adequate Ag to cutaneous APCs through the SC by just applying Ag onto bare skin. Therefore, in order to develop effective TCI systems, technologies must be established that promote Ag penetration through the SC.

In this review, we outline the mechanisms of the skin immune system and recent transcutaneous antigenic protein delivery techniques, technologies, and devices. Furthermore, we introduce the progress we have made in our research into the practical application of TCI in basic, preclinical, and clinical investigations.

The skin, the access site for TCI, acts not only as a physical barrier but also as an immunologic barrier and is enriched with various immunocompetent cells such as LCs, keratinocytes, and dDCs ( Fig. 1) .

In special, LCs and dDCs take important roles in induction of Agspecific immune responses. Under non-inflammatory conditions, LCs and dDCs are, for the most part, immature, meaning they have a strong endocytic capacity. When external Ag enter the skin, LCs and dDCs capture them and increase the expression of costimulatory factors, which play a role in the presentation of Ag to T cells, and CCR7 to permit the movement of APCs away from the skin and their subsequent entry into and localization within the draining lymph nodes [12] . After that, LCs and dDCs present Ag to CD4 and CD8 Tcells and activate Ag-specific T cells and B cells.

Keratinocytes also involved in induction of Ag-specific immune responses by activating the innate immune system. Keratinocytes could effectively convert exogenous stimuli into host homeostatic responses [7, 13] . In particular, they express numerous toll-like receptors on their surface or in endosomes [14] . Also, another type of receptor has been discovered on keratinocytes: nucleotide binding-domain oligomerization domain-like receptors [15, 16] . These receptors allow the keratinocytes to recognize bacterial components, namely, pattern-associated molecular patterns (PAMPs). In case external Ag do enter, keratinocytes produce cytokines and chemokines. TNF-␣ and IL-1␤ constitute the signals necessary for LCs or dDCs to migrate to a regional lymph node [17] . Like keratinocytes, LCs and dDCs express these receptors that contribute to the maintenance of an inflammatory environment [18, 19] . This inflammatory microenvironment, innate immunity, arises from the first contact with a vaccine component and contributes to different extents to the production of pro-inflammatory molecules that strongly contribute to the primary events of the adaptive immune response, that is, activation of skin-resident APCs. When vaccine Ag is administrated into skin, Ag-specific immune responses are induced by these mechanisms. Thus, the skin is clearly an attractive organ for Ag delivery to elicit immune responses.

Several types of professional APCs inhabit the healthy skin and the studies about function of skin-resident APCs involved in induction of skin immunity have been investigated (Fig. 2) . In mice, skin-resident APCs were classified into two categories; LCs in the epidermis and dDCs in the dermis. LCs and dDCs seemed to induce Th2-type and Th1-type immune responses, respectively [20, 21] . However, some studies suggested that LCs were not involved in induction of immune responses [22] . In a few years, also, reports suggesting the existence of several dDC subsets have been published in mice [23, 24] . It was generally assumed that the expression of langerin in the skin was strictly confined to LCs in the epidermis, but this view has been altered by current data indicating that a large population of langerin-positive cells corresponds to dDCs [25] [26] [27] . Classical langerin-negative dDCs express the macrophage Lipid-based vesicles Nano-bio interaction, flexible bilayer mixes with SC and disrupts it

Ags protected from external environment Disadvantage Low efficiency [84] [85] [86] [87] [88] markers CD11b, F4/80, and CX3CR1, whereas langerin-positive dDCs express CD103, CD8␣, and XCR1 without CD11b, F4/80, or CX3CR1 [28] . It was also reported that CD103-negative, CD11bnegative dDCs exist in the dermis [29] . Especially, CD103-positive dDCs but neither dermal CD103-negative dDCs nor LCs were shown to have a crucial role in the induction of Ag-specific CD8-positive T cells (Th1-type immune responses) [30] . Recent progress was made in identifying potential homologs of mouse dDC subsets by examining human dDCs. Human skin APCs also divided into two groups on the basis of localization, LCs in the epidermis and dDCs in the dermis. Epidermal LCs preferentially induced the differentiation of CD4-positive T cells secreting Th2 cell cytokines and were efficient at crosspriming naive CD8positive T cells [31] . Human dDCs can be distinguished into several subsets; CD1a/1c-positive dDC, CD14-positive dDCs, and CD1a/1cnegative, CD14-negative, CD141-positive dDCs by phenotype and function in the homeostatic and inflamed skin [31] [32] [33] [34] . Human CD1a/1c-positived dDCs and CD14-positive dDCs do not express langerin and can be classified based on their reciprocal expression of CD1a and CD14, which are thought to be equivalent to mouse CD11b-positive dDCs [32] . However, the relative contributions of these subsets to the generation of immunity or tolerance are still unclear [32, 33] . Yet, specialization of these different populations has become apparent. Human CD14-positive dDCs can promote antibody production by B cells [31] . In addition, CD1a/1cnegative, CD14-negative, CD141-positive DCs exhibit specialized cross-presenting function and express a number of markers associated with mouse CD103+ DCs [34] .

Although the immune mechanisms of the skin remain to be completely elucidated and further analyses were should be investigated, improved knowledge of the skin immune system could lead to the induction of optimal immune responses, such humoral immunity or cellular immunity, against infectious diseases.

As previously noted, the SC acts as a physical barrier against the penetration of substances into the skin. Various pharmaceutical approaches and devices have been developed to enable TCI systems to overcome the penetration barrier of the SC. In this section, the techniques, technologies, and devices used for the enhancement of TCI are reviewed (Table 1) .

Electroporation is a method to increase the permeability of the skin by applying single or multiple short-duration pulses. It has been widely used to loosen the cell surface, allowing the delivery of molecules into living cells. With high-voltage pulses (75-100 V) delivered against the skin surface, microchannels or local transport regions are created through lipid bilayer membranes including the SC [35] [36] [37] [38] [39] . Zhao et al. reported that TCI with the SL8 peptide derived from ovalbumin (OVA) and CpG oligodeoxy nucleotide as an adjuvant using electroporation could induce OVAspecific T cell responses equivalent to those induced by intradermal injection [37] , indicating that TCI using electroporation induced Ag-specific immune responses. However, this method requires power-supply equipment, thus they may be useful procedures in medical institutions but they cannot achieve an optimal ease of self-administration. In addition, disrupting SC as skin barriers may lead to secondary infection.

Iontophoresis is a method to enhance the transportation of ionic or charged molecules through a biological membrane by passing direct or periodic electric current through an electrolyte solution with an appropriate electrode polarity. This technique has been applied in the fields of transdermal drug delivery and has been shown by several groups to promote penetration of peptides or proteins such as insulin, calcitonin, or botulinum toxin through the SC [40, 41, 42] . The combination of electroporation and iontophoresis makes substance penetration even more effective [43] . From these reports, iontophoresis enhanced penetration of macro molecules through SC into skin, thus application of this method to TCI systems is expected. However, several problems about lack of convenience and risk of secondary infections remain because it requires powersupply equipment and may break cutaneous barrier.

Sonophoresis is a method to enhance substance penetration by disrupting the structure of the SC with low-frequency ultrasound. Cavitation is the formation of gaseous cavities in an ultrasoundcoupling medium upon exposure to ultrasound and involves the rapid growth and collapse of a bubble (transient cavitation) or slow oscillatory motion of a bubble (stable cavitation) in the ultrasound field. Oscillations and collapse of cavitation bubbles disorder the lipid bilayers of the SC, thereby enhancing transport [44] . Dahlan et al. have shown that TCI using low-frequency ultrasound with tetanus toxoid (TT) induced anti-TT IgG and neutralizing antibodies [45, 46] . Interestingly, Tezel et al. reported that ultrasound treatment induced LC activation and enhanced the Agspecific immune response, suggesting it acts as a physical adjuvant [47] .Although TCI using sonophoresis induced Ag-specific IgG antibody and have advantage of activating immunocompetent cells, this method require power-supply equipment and disrupt cutaneous barrier, thus they have several issues in terms of usefulness and safety.

Jet injectors are devices that use pressure to deliver substances into the skin [48] [49] [50] [51] . The first devices were multiple-use nozzle jet injectors, with which a large number of patient were vaccinated through the same fluid stream and nozzle [48, 49] . However, such devices are no longer used because of cross-contamination. Recent development efforts have resulted in disposable syringe jet injectors. Simon et al. reported a clinical study of the immunogenicity of trivalent inactivated influenza vaccine administrated by the Lec-traJet M3 ® RA disposable syringe jet injector, which was cleared for sale and use by the U.S. Food and Drug Administration in 2009 [52] . In jet injector systems, Ag-specific immune responses are induced and administration methods are simple, but ampoules are needed in the same way as in conventional injection systems, indicating the need of a cold-chain for transport and storage.

Patch formulations are one of the commonly used systems for TCI. Several groups have reported that TCI using gauze patches or adherent patches induced Ag-specific immune responses [53] [54] [55] [56] [57] . Application of a LT-containing single-ply polyester-rayon gauze patch onto human skin increased the anti-LT IgG titer in serum [53] . Although the other groups also have reported developing TCI systems for practical use and showed their safety and efficacy [54] [55] [56] [57] , these systems comprised a gauze patch as the TCI device. Because they require the gauze patch to be saturated with Ag solution just before application to the skin, such TCI systems are inconvenient and require cold storage and transportation of the Ag solution, as do conventional injectable vaccination systems. In addition, the disadvantage of patch-based TCI system is the requirement of skin preparation system (SPS) or cyanoacrylate skin surface stripping (CSSS) procedures to remove SC before patch application for improvement of Ag penetration into skin. These methods may carry a risk of increasing sensitivity to secondary infection by disrupting SC as a cutaneous barrier, which is a safety issue. Thus, the development of more easy-to-use and safer patch-based TCI system is desirable.

In our research group, we have developed a hydrogel patch as a TCI devise, which is made of safe materials that have already been applied to humans [58] [59] [60] [61] [62] and TCI formulation using a hydrogel patch was shown to induce effective immune responses to tetanus and diphtheria after application in absent of any treatment in animal models [59] . We also demonstrated its safety and efficacy by performing a clinical study of our TCI formulation for vaccination against tetanus and diphtheria in humans without disrupting SC [62] .

In our patch-based TCI system, we can prepare a TCI formulation by dropping Ag solution to a hydrogel patch and leaving out at room temperature for a while. The hydrogel patch formulation immersed with TexasRed (TR)-labeled OVA solution formed a concentrated Ag layer on its surface (Fig. 3A) , because only water in Ag solution absorbed by hydrogel polymer.

It is very important to deliver antigenic proteins to the skinresident APCs for induction of Ag-specific immune responses. Therefore, we analyzed biodistribution of Ag after transcutaneous administration by a hydrogel patch. There was marked penetration of the Ag into the epidermal layer of intact skin after 6-h application of a hydrogel patch containing TR-OVA to the auricle skin of mice (Fig. 3B) . In human and tissue-engineered skin models, a hydrogel patch also promoted the penetration of antigenic proteins through the SC [60] . Although theories of conventional transdermal drug delivery suggest that skin structure and composition do not allow for the penetration of materials larger than 500 Da [10, 11] , our transcutaneous vaccination system delivered antigenic proteins (45-150 kDa) into the epidermal layer [58, 59] . We proposed the following mechanisms for penetration of Ag into the skin. First, the concentrated antigenic proteins on the surface of the patch might generate a high concentration gradient of antigenic proteins in the skin, which is critical for producing the driving force needed to accelerate passive diffusion and distribution. This theory is supported by our observation that the distribution of TR-OVA in the epidermal layer was not simply a result of spreading the TR-OVA solution on the intact skin surface, and that the application of the filter paper immersed in Ag solution did not enhance either Ag penetration or antibody titer [58] . Second, humectation and hydration of the skin to which the hydrogel patch is applied might loosen intercellular gaps in the SC, which contributes to improve the penetration of water-soluble substances. According to our observations, Ag penetration via our patch system occurred mainly through the intercellular gaps of the SC. In fact, there are several reports that an increased water content in the SC leads to increased membrane fluidity and decreased electrical resistance [63, 64] . Although it is possible that antigenic proteins penetrate into the epidermal layer through hair follicles -there are some reports that hair follicles allow for even nanoparticles to reach the epidermal layer in skin [65] [66] [67] -our hydrogel patch enhanced Ag penetration on a tissueengineered skin without pores [60] , suggesting that this pathway contributes little to the penetration of Ag into the skin promoted by a hydrogel patch. Through a combination of these mechanisms, our patch vaccine system promoted the penetration of water-soluble macromolecular proteins into the SC.

As shown in Fig. 3C and D, yellow fluorescent spots, indicating that TR-OVA localization accorded with LC localization, were observed in merged images of an epidermal sheet and lymph node sections prepared from mice with intact skin, suggesting that LCs, which are cells critical for the induction of potent immune responses, captured antigenic proteins penetrated into the skin and migrated into the regional lymph node. Thus, Ag-capturing LCs, which migrated from the epidermal layer to regional lymph nodes, would greatly contribute to triggering and amplifying Ag-specific immune responses induced by transcutaneous vaccination using the hydrogel patch formulation.

In an animal model of tetanus and diphtheria infection, the vaccination efficacy of TCI using a hydrogel patch was evaluated. TCI using a hydrogel patch elicited toxoid-specific immune responses and the serum titer of antibody in the TCI groups were equivalent to or greater than those of the subcutaneous immunization (SCI) group ( Fig. 3E and F) . In rats vaccinated with combined TT and DT, both TT and DT-specific IgG antibodies were detected in serum as efficiently as that in rats vaccinated with each toxoid alone, suggesting that our TCI using a hydrogel patch is applicable to a combination vaccine. As mixed inoculationis now recommended in vaccination, our TCI formulation is suitable for practical use. We also demonstrated that TCI using a hydrogel patch containing TT and DT induced little adverse reactions in local and systemic toxicity assessments [61] , indicating that hydrogel patch-based TCI formulation is a non-invasive vaccination method. In addition, on the basis of IgG subclass analysis, it was suggested that our TCI using the hydrogel patch formulation predominantly elicited a Th2-type immune response rather than a Th1-type immune response [58, 59] . Further analyses are necessary to elucidate the Th2-dominant mechanism in our patch vaccination.

In our hydrogel patch-based TCI system, we can simply prepare a manageable TCI formulation like general fomentations with a concentrated Ag layer on the surface of the hydrogel patch. Our TCI system using a hydrogel patch enhanced Ag penetration into the skin and induced Ag-specific immune responses by single application onto skin surface without disrupting SC. This is superior to other patch-based TCI formulation in terms of avoiding secondary infections by breaking skin barriers.

Patch formulations, such as the hydrogel patch, are less effective at promoting penetration of particulates and insoluble Ags through the SC. Most practical vaccine Ags are in a particulate state, for example, the less virulent strains of bacteria. The development of a different TCI system that is effective for use with all Ag forms is needed. A microneedle array contains many micrometer-sized needles that can create a transport pathway large enough for proteins and nanoparticles, but small enough to avoid pain [68] [69] [70] [71] [72] [73] [74] . Microneedle arrays can penetrate the SC barrier and deliver Ag to immunocompetent cells in the skin more efficiently than other TCI systems. In addition, the use of a disposable array is suitable for selfadministration. Thus, microneedles are the most attractive devices for the development of effective TCI systems. Microneedles were first conceptualized for drug delivery in a 1976 patent [75] . Since then, several type of microneedles have been developed and they are classified into four types with respect to mechanism of action: (1) solid microneedles for pretreatment of the skin to increase permeability, (2) microneedles coated with drug that dissolves in the skin, (3) polymer microneedles encapsulating the drug that fully dissolves in the skin, and (4) hollow microneedles for infusing the drug into the skin.

Traditional microneedle arrays made from silicon, metal, stainless steel, or titanium were reported in the early stages of development, but the clinical use of microneedle arrays has faced serious obstacles because needles on microneedle arrays can fracture and remain in the skin, creating a safety issue. These conventional microneedle arrays suffer from the risk of fracture of microneedle fragments in the skin, therefore, in 2004, microneedle systems made with biocompatible or biodegradable polymers began to be developed [69] , and their superior safety has led to early clinical use. This system, however, remains the risk of breaking cutaneous barrier by insertion of microneedles into skin. In manufacture of dissolving microneedles, the technical innovation is required to allow Ag to be incorporated into the matrix of microneedle material using mild procedures that do not cause the decrease of antigenicity or compromise material strength.

Our research group has developed a dissolving microneedle array (MicroHyala ® ; MH) as a TCI device, which was fabricated using micromolding technologies with biocompatible sodium hyaluronate as the base material and this approach demonstrated effective vaccination effects comparable to those of conventional injection systems [76] [77] [78] .

We have developed a dissolving microneedle array, MH as mentioned above, made of sodium hyaluronate as the base material (Fig. 4A) . We successfully fabricated several types of MH in various forms and lengths: konide-shaped MH (needle length 200 or 300 m) and cone-shaped MH (needle length 300, 500, or 800 m) 4 g) , or intranasal application of HA (0.4 g) combined with CT (10 g) as an adjuvant twice at 4-week intervals. Two weeks after last vaccination, these mice were each infected intranasally with 6 × 10 5 PFU of the A/PR/8/34(H1N1) virus. (A) At the indicated points, sera collected from the mice were assayed for the titer of HA-specific IgG by ELISA. (B) Body weight was measured each day after infection and is presented as a percentage of the original weight before infection (day 0). (C) Six days after infection, the lungs were collected from the mice and the number of viruses in the lung homogenate was determined with a plaque assay system. Data are expressed as mean ± SE of results from (A) 13 or (B and C) 10 mice. Arrowheads indicate vaccination points. (Fig. 4A) . The microneedles on the MH were dissolved by water in the skin and thus had no danger of remaining in the skin, making our MH safer than traditional microneedle arrays made of metal or stainless steel. In fact, the microneedle tips were fully dissolved at 1 h (Fig. 4B) . Application of each MH caused only temporary skin irritation and the skin barrier function after insertion recovered immediately [76] , suggesting that the holes caused by insertion of each MH closed up quickly. These results suggest low probability of causing secondary infection by application of MH. In observation of skin sections after application of each MH containing fluorescein isothiocyanate (FITC)-silica particles, they were clearly detected (Fig. 4C) , suggesting that the MH delivered particulate Ag into the epidermis or dermis without regard for the Ag form. In addition, the MH size can be used to control the depth of Ag delivery, meaning that each MH might deliver Ag to specific skin-resident APCs, LCs in the epidermis or several dDCs in the dermis.

We examined the efficacy of vaccination with influenza hemagglutinin (HA) Ag, which is particulate Ag. In an influenza virus challenge, TCI with HA alone elicited production of HA-specific functional IgG antibody equivalent to that after intramuscular immunization (IMI) with HA alone or INI with combined HA and CT as an adjuvant (Fig. 5A) . On the other hand, little anti-HA IgA antibody was detected in the TCI and IMI groups [77] . After challenge with A/PR/8/34 influenza virus, mice in the TCI group showed no remarkable weight loss, similar to those in the IMI group and INI with CT group (Fig. 5B ). In addition, the virus titer in the lungs of the TCI group was below the detection limit (Fig. 5C) , demonstrating that our TCI system provided protection equal to that of IMI or INI with adjuvant. In INI system, mucosal vaccine adjuvants with high efficacy and safety for the purpose of a clinical application are necessary. As compared to INI system, our TCI could efficiently elicit Ag-specific vaccine effect without an adjuvant, which is an advantage of our TCI system.

In addition, the vaccination efficacy of TCI using MH was also demonstrated in tetanus, diphtheria, and malaria infection models. On the basis of these results, TCI system using MH suggested to induce Ag-specific immune responses against any vaccine Ags, such as soluble Ags, insolubleAgs, or particulate Ags, which conventional TCI system fail to do so. Thus, we can conclude that our TCI system using MH which is dissolved in the skin effectively confers protective immunity without causing serious adverse reactions in an animal model.

Conventional microneedle array made of metal or stainless steel has difficulties in clinical application because needles on microneedle arrays can fracture and remain in the skin, which is serious problem. However, the microneedles on the MH were dissolved by water in the skin and thus had no danger of remaining in the skin, indicating that TCI using MH would be attractive vaccination method in terms of both safety and efficacy.

Recent studies suggested that nanoparticles are attractive means for transcutaneous Ag delivery. By disrupting the SC as a result of the nano-bio interaction with skin lipids, antigenic proteins encapsulated in the nanoparticles can be delivered through the SC into the skin. Some researchers reported that nanoparticle vaccine compounds can penetrate via the hair follicles where there is a high density of APCs and enhanced immune responses. There are numerous nanoparticle systems available, including polymeric poly (d-l-lactic-co-glycolic acid) and poly (lactic acid) nanoparticles, biodegradable chitosan nanoparticles, and metal nanoparticles [65, [79] [80] [81] [82] [83] .

In addition, lipid-based vesicles such as liposomes, transfersomes, or niosomes have structures similar to those of biological membranes and facilitate skin penetration [84] [85] [86] [87] [88] . When mixed with SC lipids, flexible liposomes (FLs) can carry a remarkable amount of lipid mass into the skin and can, therefore, be advantageous in promoting cutaneous drug disposition after disrupting the skin barrier with their flexible bilayers [88] . It also has been reported that FLs stimulated a transcutaneous immune response by acting as an adjuvant [89] .

The design of novel formulations especially nanoscale systems, such as nanoparticles and lipid-based vesicles, can be helpful for protecting the Ag from external environment and keeping the long term activity. These properties are conductive to the application of transcutaneous vaccine. However, the development of novel nanoscale systems for TCI is limited by the low efficiency in eliciting robust immune responses.

For the diffusion of the vaccine worldwide including in developing countries, patch formulations and microneedles are more suitable because of their ease of use and efficacy. Several research groups have conducted clinical studies of TCI using patch formulations or microneedle systems in recent years ( Table 2 ). Glenn et al. first reported the results of TCI using a patch in humans [55] . Application of a patch containing LT as Ag resulted in robust LT-specific antibody responses. In addition, their group used LT to investigate patch vaccination against traveler's diarrhea in a phase II clinical trial and found that the 59 LT-patch recipients were protected against moderate-to-severe diarrhea (protective efficacy [PE] 75%) and severe diarrhea (PE 84%) [53] . LT-patch recipients who became ill had shorter episodes of diarrhea (0.5 vs 2.1 days) with fewer loose stools (3.7 vs 10.5) than recipients of placebo [55] . Since then, numerous studies of devices that serve as simple, easy-to-use, and low-invasive TCI systems have been undertaken. Etchart et al. showed that TCI of human adult volunteers with live-attenuated measles induced Ag-specific immune responses in their phase I/II clinical study [54] . Combadiere et al. demonstrated that TCI with an inactivated influenza vaccine induced a significant increase in influenza vaccine-specific CD8 responses compared with those obtained from the intramuscular route [90] . However, these TCI systems require cyanoacrylate skin surface stripping for Ag delivery into skin, which might cause skin irritation as one of the side effects.

Microneedle-based TCI systems have also been applied in clinical trials. Van Dammeet al. reported the results of a clinical study of influenza vaccination in which a hollow microneedle device (MicronJet) was used [91] . Local adverse reactions were significantly more frequent than those with intramuscular vaccination, but were mild and transient in nature. After TCI, immunogenic responses increased in humans. In addition, the safety and efficacy of several microneedle devices have been assessed in applications other than vaccination [92, 93] . In the future, more clinical studies will be conducted for needle-free, easy-to-use, low-invasive, and low-cost vaccination methods.

We performed a clinical study of our original hydrogel patch formulation containing combined TT and DT in humans ( Fig. 6A and B ) [62] . In the safety assessment to evaluate local adverse responses at 0 h and 24 h after patch removal, a TCI formulation containing TT and DT was shown not to induce local severe adverse events (Fig. 6C ). As shown in Fig. 6D , anti-TT IgG and anti-DT IgG increased (paired-t test; p < 0.01) following the first vaccination using the TCI formulation, indicating that a single application of our TCI formulation could induce an immune response in humans. We also administered a second vaccination to five subjects in whom neither antibody titer was significantly increased by the first vaccination. The IgG titers increased in a part of subjects following the second vaccination, suggesting that an additional application increases the efficacy of the TCI formulation. Antibody titers on day 365 after application of the TCI formulation were maintained at a higher level than those on day 0 in all subjects examined, although antibody titers tended to be lower on day 365 than on day 60 [62] . Conventional patch-based TCI systems require the pre-treatment of disrupting or removing SC, but our hydrogel patch achieved Ag penetration into skin without removing the SC and Ag-specific antibodies were produced in some subjects by a single application in humans, which represents a safety and efficacy advantage.

We also conducted a clinical evaluation of TCI using MH (Fig. 7) . Ag-free konide-shaped MH300, cone-shaped MH500, and coneshaped MH800 as TCI devices were applied on left brachial lateral skin (Fig. 7A ) and they caused no serious local or systemic adverse reactions (in preparation). To evaluate the efficacy of vaccination (Fig. 7B) , we used trivalent influenza HA Ags. HA-containing coneshaped MH800 induced HA-specific IgG responses against three HA Ags without severe adverse events (in preparation), indicating that our MH-based TCI system was safe and efficacious in humans.

These simple, easy-to-use, low-invasive, and effective TCI formulations might be applicable for mass treatment in the event of an outbreak and for increasing vaccination rates in developing countries. We expect that our TCI system as an innovative vaccination method will be put to practical use at an early date and greatly will contribute to decrease the mortality and morbidity by infectious diseases.

The development of vaccines, which represent the only basic prophylaxis against infectious diseases, is drawing attention worldwide. The main objective of vaccine development is the establishment of manufacturing technologies that supply safe and effective vaccine Ag rapidly and stably, but the problem of how to carry out enough vaccinations to prevent infectious diseases remains to be solved. In order to distribute the vaccine across the world to people who need it, especially those in developing countries, easy-to-use, low-cost, and low-invasive vaccination methods instead of conventional injection systems are required. TCI offers an attractive avenue for the development of needlefree prophylaxis. The main challenge to be addressed during the development of TCI systems is to ensure accurate delivery of Ag to the epidermis and dermis through the SC. As we introduced in this review, various approaches to overcome the SC barrier have been developed and basic, preclinical, or clinical studies of these approaches have been conducted.

Recent studies have demonstrated that intradermal vaccine delivery to skin-resident APCs can increase the magnitude of the immune response rather than IMI. For example, some studies evaluating intradermal delivery of influenza vaccine have suggested that dose sparing relative to IMI can be achieved [94, 95] . Nowadays, INTANZA ® /IDFlu ® is marketed as a new trivalent inactivated influenza vaccine administered by the intradermal route. Thus, TCI systems targeting the skin immune system are attractive vaccination methods that can supplant conventional IMI or SCI in terms of not only ease and safety but also efficacy.

Practical use of these easy-to-use, low-cost, low-invasive, and effective transcutaneous vaccination methods in the near future would contribute to a global countermeasure against infectious disease and would greatly benefit countries with poor vaccination rates.

Exploratory Research (23659079) from the Ministry of Education, Culture, Sports, Science, and Technology of Japan, a Grant-in-Aid from The Mochida Memorial Foundation for Medical and Pharmaceutical Research, and a Grant-in-Aid from the Tokyo Biochemical Research Foundation.

Conflict of interest: There is no conflict of interest.

",0.738081288286871
Platform technologies for modern vaccine manufacturing,"a b s t r a c t Improved understanding of antigenic components and their interaction with the immune system, as supported by computational tools, permits a sophisticated approach to modern vaccine design. Vaccine platforms provide an effective tool by which strategically designed peptide and protein antigens are modularized to enhance their immunogenicity. These modular vaccine platforms can overcome issues faced by traditional vaccine manufacturing and have the potential to generate safe vaccines, rapidly and at a low cost. This review introduces two promising platforms based on virus-like particle and liposome, and discusses the methodologies and challenges.","Vaccination continues to be a leading defense strategy against infectious pathogens. Traditional vaccines that employ whole-cell antigens to raise an immune response have been irrefutably successful in the control or localized eradication of diseases such as poliomyelitis, measles, mumps, rubella, influenza and hepatitis A and B [1] [2] [3] . Eradication of smallpox was declared in 1980 after a global immunization effort by WHO [4] . Rinderpest was the second disease globally eradicated by traditional vaccine means as declared by the World Organization for Animal Health in 2011 [5] . Despite this success, live attenuated and inactivated vaccines possess several major drawbacks. Both live attenuated and inactivated vaccines require the production of large volumes of pathogens in the form of viruses and bacteria. This lengthy culturing process contributes to a considerable lag time between antigen production and vaccine delivery. Furthermore, it demands specialized containment facilities and poses considerable risk to the operators and environment due to the infectious nature of the material [6, 7] . Despite adequate passaging to diminish virulence, live attenuated pathogens are capable of reverting to virulent strains as evidenced with simian immunodeficiency virus [8] , African horse sickness [9] and infectious bronchitis virus vaccines [10] . The genuine threat of vaccine-derived polio associated with Sabin's oral polio vaccine has hindered immunization programs worldwide [11, 12] . Inactivated polio vaccine has less of a biosafety risk to vaccine recipients as inactivated poliovirus is incapable of replication, thereby eliminating the possibility of vaccine-derived polio.

However, inactivation of microorganisms can compromise the native conformation of antigenic epitopes resulting in reduced immunogenicity [13] . Pathogens that display high levels of antigenicity owing to high mutation rates (e.g. RNA viruses such as influenza and human immunodeficiency virus [14, 15] ) or existing as multiple genotypes and serotypes (e.g. rotavirus [16, 17] , enterovirus [18] and the Group A Streptococcus [19] ) present a challenge for developing efficacious vaccines. While this is an important consideration for all vaccine manufacturing platforms, the current timescale of traditional vaccine manufacturing highlights their inadequacy.

Outbreaks of H1N1 influenza, Middle East Respiratory Syndrome, Ebola and Zika over the last decade, are timely reminders that improved modern vaccine technology is necessary to shorten the developmental and production time of vaccines. Vaccine platform technologies, the formulation of antigens of choice with a pre-defined platform base, have the potential to address vaccine manufacturing challenges such as speed, safety and efficacy. Platforms based on virus-like particle (VLP) and liposomes are discussed, with a focus on the challenges and opportunities offered by these vaccine platform technologies.

A tailorable platform that supports safe and simple manufacture of target antigens at high capacity has the potential to rapidly respond to an emerging disease. Most vaccine platform technologies consist of a platform base carrier ( Fig. 1 ) that is amendable to modularization with target antigenic components of pathogens (known as modules). Independently, these components exhibit weak immunogenicity and poor stability. To harness the immunostimulatory properties of such antigens, platform carriers are engineered and developed to enhance the antigenicity but without the infectious trait of pathogens. Such engineering also allows the production of novel vaccine candidates that cannot be obtained through traditional methods (attenuation and inactivation). Basic research to determine suitable modules with antigenic potential is a prerequisite of this modular approach, yet the use of generic platforms supports streamlined and standardized vaccine development, potentially reducing the cost of development.

A well-exploited platform is based on VLP technology. VLPs are highly ordered structures, with varying degrees of complexity, which stimulate both innate and adaptive immune responses [20, 21] . These intrinsic properties contributed to the commercialization of VLP-based vaccines against human papillomavirus (HPV), hepatitis B and E [22] [23] [24] . The self-adjuvanting properties of VLPs, due to their particulate structure and optimal size for uptake by antigen presenting cells [20, 25] , makes them an attractive tool for increasing the immunogenicity of antigens. Antigens encapsulated within VLPs can also be used as vectors for drug delivery [26] . Well reported platforms based on self-assembling proteins include HPV L1 [27] , Hepatitis B core [28] or surface antigen [29, 30] , murine polyomavirus VP1 [31, 32] and bacteriophages MS2 [33] , AP205 [34, 35] and Qb [36] . High antigen-specific antibody titers and protective efficacies have been demonstrated across a range of peptide epitopes and protein domains modularized onto these VLP platforms. As reported, a pre-existing immunity against the VLP proteins from previous exposure to the platform does not diminish the immune response against the antigenic modules [37, 38] . Mosquirix TM (RTS,S/ASO1, GlaxoSmithKline), a protein-based malaria vaccine comprising circumsporozoite protein and Hepatitis B surface antigen, has demonstrated safety and protection in children and infants in a Phase III trial [39] , and WHO has recently announced the first pilot studies in sub-Saharan Africa [40] .

Liposomes are another favorable vaccine platform owing to their natural ability to induce an immune response [41] . Composed of an aqueous core and a uni-or multilamellar phospholipid bilayer, these lipid-based vesicles have immense adaptability and parameters with relation to size, charge, lipid, adjuvant composition and antigen presentation are manipulable [42] . As a result of this versatility, liposomal-based platforms are less well-defined than VLP-based platforms. Surface charge of the vesicle is reported to be an important factor that influences the immune response [42] [43] [44] . Cationic formulations are considered the most effective tools in liposomal antigen delivery due to their ability to bind antigen presenting cells through electrostatic interactions and form antigen depots at the site of injection [45, 46] . The combination of positively charged dimethyldioctadecylammonium (DDA) with the immunostimulant, trehalose-6,6-dibehenate (TDB) was engineered for the delivery of the tuberculosis antigen, Ag85B-ESAT-6 [45] and is possibly the best characterized. DDA:TDB is also considered as a potential platform for Chlamydia vaccines [47] .

The strategy for modularizing antigenic peptide or protein module onto the platform base is the key driver for inducing the protective immune response. Maintaining both the native conformational structure of the antigenic module post modularization and the integrity of the immunostimulating platform base are of equal importance. The rules to guide vaccine design are still limited. Although computational simulation tools and structure-based vaccine design are still in their infancy, they offer alternative possibilities to traditional empirical vaccine development [48, 49] .

Modularization of chosen antigens onto VLPs is achieved through electrostatic interaction [50] , chemical conjugation or genetic fusion [51] . Electrostatic interaction requires minimal processing but these non-covalent interactions can be weak and stability is questionable. A variety of linkage chemistries suitable for chemical conjugation result in a more permanent interaction albeit this requires more complex manufacturing processes under potentially harsh conditions that may alter protein structure. Permanent and regular module display is afforded through genetic fusion, eliminating downstream processing yet insertion sites for modules

Target epitope Target pathogen + Bacteria Parasite Virus-like particle Virus Liposome Fig. 1 . Modularization of target epitopes onto VLP and liposome vaccine platforms. Antigenic modules from a variety of microorganisms may be modularized onto the surface of VLPs through electrostatic interaction, chemical conjugation or genetic fusion. In liposomes, these antigenic modules may be encapsulated into the aqueous core, adsorbed into the lipid bilayer or conjugated (both covalently or non-covalently) to the vesicle surface.

can place limitations on antigen size and may be incompatible with VLP assembly. Peptides are more amenable to VLP surface display than large protein domains although conformational structure can be compromised, ultimately affecting the quality of the immune response [52] [53] [54] . Displaying large protein domains has the added benefit of presenting multiple epitopes in the correct structure which may increase immunogenicity. However, expression of large genetically-fused antigens is a challenge owing to protein folding errors or compromised VLP formation through steric hindrance [55, 56] . To overcome these issues, strategies such as linker designs [56] , antigen titration [38, 56, 57] , split-intein conjugation [34, 58, 59] and a tandem core fusion strategy [60] are implemented to enable ease of large antigen modularization.

For liposomal vaccine platform, antigens can be encapsulated into the hydrophilic aqueous core [61, 62] , intercalated into the lipid bilayer or surface attached [63] . Successful modularization of antigens up to 150 kDa have been reported [64] [65] [66] , larger than those described for VLPs. Modularization with surface attached antigens often elicit superior immune responses in comparison to encapsulated antigens perhaps owing to intracellular processing which is possible for the latter [67] . Despite this, encapsulation protects antigens from protease degradation, facilitates longer circulation time and can generate effective immune responses [68] [69] [70] . Low encapsulation efficiency is common due to antigen loss from the vesicle during the manufacturing process which involves film extrusion and high sheer methods [71] . Incubating antigens with pre-formed liposomes in the presence of 30% v/v ethanol improves encapsulation efficiency [71, 72] and may aid a more streamlined manufacturing process whereby peptides can be encapsulated post-production. Unlike VLP technology, modules cannot be genetically fused to the carrier thus surface exposed antigens rely heavily upon bioconjugate technologies such as covalent conjugation (i.e. palmitoylation). Lipidation can compromise peptide conformation potentially resulting in altered immune responses [73] . Incorporating appropriate linkers between the module and the fatty acid to create spatial separation can address this [74] [75] [76] . As demonstrated by Lipotek Pty Ltd [77] and others [66, 78] , the use of nitrilotriacetic acid (NTA) -histidine conjugation is promising, yet this remains a relatively unexplored area of liposome technology. NTA conjugation offers the opportunity of assembling entire protein domains [66] onto pre-formed liposomes whilst removing costly purification processes. Novel liposomal platforms encapsulate immunostimulants (including diphtheria toxoid and TLR9 agonists) independent of surface attached target antigen [76, 79] . This spatial segregation of antigenic components (with the immunostimulant exposed only upon intracellular processing) has been shown to enhance target specific immune responses. Table 1 summarizes manufacturing technologies for modularization.

The long and complex vaccine development process (development, testing, regulatory) requires a huge investment of resources which includes time, facilities and money. Vaccine manufacturing processes are often customized and conducted in dedicated facilities for separate vaccines due to the characteristics of vaccine antigens and safety issues. These factors pose barriers for a fast response that is critical for controlling modern-day disease outbreaks that spread rapidly, as observed for H1N1 influenza in 2009 [80] and most recently Zika [81] . A platform approach for vaccine manufacturing ideally streamline the bioprocess, and shorten vaccine product development and delivery (time to market). Platform technologies allow the standardization of upstream and downstream processes, given that the platform base remains unchanged. Certainly, processes will need optimization with modularization of different antigenic modules, but vaccine platform technologies provide flexibility and possibility for multi-product facilities. Prior knowledge, experience and production facility setup is immensely beneficial. Merck Research Laboratories used their prior knowledge and know-hows from developing hepatitis B VLP vaccine (Recombivax) as the key decision factor when choosing to use the same host (Saccharomyces cerevisiae) for the production of HPV VLP vaccine (Gardasil) [82] . Similarly, the decision on the choice of adjuvant to formulate HPV VLP was made based on Recombivax.

The desire to lower cost of goods, thus leading to cheaper vaccines in the market has been well discussed and debated in papers and at conferences. The largest vaccine market is in developing countries, where vaccines would have a significant impact on public health, but these low-income countries face vaccine accessibility and affordability challenges. In combination with modular single-use technologies [83] , modern vaccine manufacturing based on platform technologies may potentially lower capital and operating costs, resulting in affordable vaccines.

Another benefit of platform technologies is the potential reduction of regulatory burden. The level of proof and documentation required for new antigenic module on the generic platform may lessen as regulatory authorities are well informed by regulatory track records on the platform base. In the scenario of a disease outbreak, a close collaboration with regulatory authorities may lead to fast-track development of a safe and effective vaccine for the public, against an emerging pathogen.

The benefits of VLP and liposome platform technologies are many but perhaps the most significant is their potential to generate multivalent vaccines. Vaccines designed for immunization against multiple strains of an antigenically diverse pathogen are possible through display of different modules on a single platform or formulation of multiple platform products. Future work is expected to optimize the methodologies by which modules are incorporated into each platform to ensure the success of modern vaccines.

",0.7375304555120441
Remote sensing of multiple vital signs using a CMOS camera-equipped infrared thermography system and its clinical application in rapidly screening patients with suspected infectious diseases,"Background: Infrared thermography (IRT) is used to screen febrile passengers at international airports, but it suffers from low sensitivity. This study explored the application of a combined visible and thermal image processing approach that uses a CMOS camera equipped with IRT to remotely sense multiple vital signs and screen patients with suspected infectious diseases. Methods: An IRT system that produced visible and thermal images was used for image acquisition. The subjects' respiration rates were measured by monitoring temperature changes around the nasal areas on thermal images; facial skin temperatures were measured simultaneously. Facial blood circulation causes tiny color changes in visible facial images that enable the determination of the heart rate. A logistic regression discriminant function predicted the likelihood of infection within 10 s, based on the measured vital signs. Sixteen patients with an influenza-like illness and 22 control subjects participated in a clinical test at a clinic in Fukushima, Japan. Results: The vital-sign-based IRT screening system had a sensitivity of 87.5% and a negative predictive value of 91.7%; these values are higher than those of conventional fever-based screening approaches. Conclusions: Multiple vital-sign-based screening efficiently detected patients with suspected infectious diseases. It offers a promising alternative to conventional fever-based screening.","Since the outbreak of severe acute respiratory syndrome (SARS) in 2003, infrared thermography (IRT) systems have been used as border-control devices at most major international airports to screen passengers for fever. IRT remains the gold standard for border control, because it can rapidly mass-screen infected individuals without contact. [1] [2] [3] [4] However, IRT measurements are influenced by several factors, including the environmental temperature and humidity, alcohol consumption, and the consumption of antipyretic medications. 5 Body temperature, in particular, can be modified rapidly by the consumption of antipyretic drugs, which directly affects the sensitivity of IRT.

Hence, fever-based screening using IRT suffers from low sensitivity. This paper describes a combined visible and thermal image processing approach that uses a complementary metal oxide semiconductor (CMOS) camera-equipped IRT system that may address this issue. These systems have already been installed at most major international airports, and they can remotely sense several vital signs, including body temperature and heart and respiration rates, thereby facilitating the rapid and accurate screening of people who are suspected of carrying infectious diseases.

The concept underlying vital-sign-based screening is based on the association between infections and inflammation. Inflammation causes elevations in body temperature and in heart and respiration rates; therefore, integrating vital sign monitoring increases screening accuracy. This concept was used in previous studies by the present author group to develop a novel infection screening radar system to mass-screen individuals. This system utilizes a multisensor fusion technique to remotely measure heart and respiration rates using a microwave radar, and the facial skin temperature is measured using IRT. The results from case-control studies that investigated seasonal influenza screening showed a detection accuracy that ranged from 81.5% to 98.0% using the heart and respiration rates and the facial skin temperature, which is higher than the detection accuracies of the conventional feverbased screening methods. [6] [7] [8] [9] However, the radar system used to screen for infections incorporates expensive embedded multisensor modules, namely a microwave radar, a reflective photoplethysmography sensor, and IRT, and it requires large-scale systems. Consequently, the system is not used widely. Hence, to promote the widespread use of vitalsign-based screening, focus has been placed on systems with minimum hardware requirements to achieve a system that is more suitable for real-world settings. The most reliable solution is to enhance the functionality of the conventional IRT systems that are already installed at international airports. By incorporating the latest advances in image processing techniques, these IRT systems can acquire thermal and visible images together by integrating visible and thermal cameras.

In this study, high image and temperature resolution IRT that combines visible and thermal images was used to acquire multiple vital sign measurements from facial images using remote sensing. The benefit of this approach is that it only requires a CMOS camera that is equipped with IRT rather than a large-scale system. Technical details of the system and the evaluation of its laboratorybased performance have been described in a previous publication. 10 Respiration rates are measured by monitoring the temperature changes around the nasal area that are associated with inspiration and expiration; the facial skin temperature can be determined easily from the thermal images simultaneously. The circulation of blood in the face causes tiny color changes that provide a visible facial image that can be used to determine the heart rate. 11 A multiple logistic regression function is incorporated into the system to predict the possibility of infection; hence, IRT can automatically detect infected individuals based on their vital signs, which are measured in real time. This system was tested on patients with an influenza-like illness in a clinical setting to evaluate the performance of this vital-sign-based screening approach using IRT alone.

Visible and thermal image processing method to remotely sense multiple vital signs This method has been described in detail from an imageprocessing perspective in a previous publication. 10 A CMOS camera-equipped IRT system (TVS-500; NEC/AVIO Infrared Technologies Co. Ltd, Tokyo, Japan) was used; this is the same system that is used in the quarantine station at Narita International Airport in Japan. 5 The IRT system integrates a CMOS camera with thermography to capture visible and thermal images, respectively ( Figure 1 ).

The visible and thermal images were recorded at a speed of 30 frames per second and at a resolution of 640 Â 480 pixels. The circulation of blood in the face causes tiny color changes on facial images that are undetectable with the human eye. The CMOS camera uses this information to determine the heart rate. To measure the respiration rate from the thermal images, the temperature changes that occur around the nasal area during the inspiration of cold air from the environment and the expiration of warm air from the lungs was monitored, and the respiratory waveform was extracted from the differences in each of the thermal images. This enabled the respiration rate to be determined from the breath-to-breath intervals. The facial skin temperature was measured simultaneously using the thermal images. The image acquisition and processing programs were written in LabVIEW software (National Instruments, Texas, USA). Subjects were seated in front of the IRT system at a distance of approximately 0.5 m. The IRT system displays the 'Infection' or 'Healthy' result within 10 s using the logistic regression discriminant function, which bases the output on the heart rate, respiration rate, and facial skin temperature.

This was a cross-sectional investigation that was undertaken at the Takasaka Clinic in Fukushima in Japan. The study involved 16 outpatients (11 male and five female) who visited the Takasaka Clinic with an influenza-like illness that included fever, headache, and sore throat, between January 22, 2015, and February 25, 2015. The ambient temperature (around 23.0 C) was also monitored to ensure reproducible environmental conditions. The average axillary temperature of the patient group was 37.2 C (range <36.2-< 39.0 C), and their average age was 36 years. The 22 healthy control subjects (10 male and 12 female) were students and admissions staff from Tokyo Metropolitan University, Japan, none of whom had a fever, headache, or sore throat. The average axillary temperature of the control group was 36.3 C (range <35.4-< 37.0 C), and their average age was 35 years. In this study, abnormal vital signs were defined according to the diagnostic criteria for systemic inflammatory response syndrome (SIRS), i.e., (1) body temperature >38 C or <36 C, (2) heart rate >90 bpm, and (3) respiration rate >20 breaths/min. This study was approved by the Faculty of System Design Committee on Human Research at Tokyo Metropolitan University.

Logistic regression discriminant analysis to predict the possibility of infection based on the vital signs measured

To distinguish between patients with infectious influenza and healthy control subjects, logistic regression discriminant analysis was used to establish a classification model based on the three derived vital signs. Multivariable logistic regression analysis is a well-established statistical method that is used to analyze dichotomous outcomes in clinical practice; it is flexible and robust, and enables meaningful data interpretations. 12 Moreover, logistic regression analysis is much easier to implement in a real-time classification system, and saves computation time, compared with other classification methods such as neural network computation. The logistic regression discriminant function was defined as:

where p i is the probability of the outcome of infection, b 0 is a constant, b 1 , b 2 , and b 3 are the regression coefficients corresponding to the respiration rate, heart rate, and facial skin temperature, respectively, and x 1 , x 2 , and x 3 are the three vital sign variables of the respiration rate, heart rate, and facial skin temperature, respectively.

The results from the logistic regression classification model were used to calculate the sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV) using a 2 Â 2 contingency table. 13 To avoid overfitting, a leave-one-out cross-validation was performed. The mean and standard deviation (SD) values of the three vital signs were calculated. The differences between the influenza patients and the healthy control subjects with respect to the three vital signs were evaluated using the Mann-Whitney U-test. A p-value of <0.05 was considered to indicate statistical significance.

The classification model was established using the data that described the three vital signs from the 16 influenza virus-infected patients and the 22 healthy control subjects using multivariable logistic regression. The statistically significant model is shown below:

where x 1 is the respiration rate, x 2 is the heart rate, and x 3 is the facial skin temperature. The derived logistic function, Z(x 1 ,x 2 ,x 3 ), was statistically significant (p < 0.05). The Z(x 1 ,x 2 ,x 3 ) value could be used to differentiate patients with influenza (Z ! 0) from healthy subjects (Z < 0). Figure 2 illustrates the discrimination results that were obtained by plotting the Z(x 1 ,x 2 ,x 3 ) values against the axillary temperatures of the two groups. Of the patients with influenza, 14 (red dots) are enclosed within the red ellipse and they had positive Z-values, and two patients had negative Z-values ( Figure 2 ). The 22 healthy control subjects (blue dots) enclosed within the blue ellipse had negative Z-values and none of the healthy subjects had a positive Z-value ( Figure 2) . Therefore, the sensitivity, specificity, NPV, and PPV were 87.5%, 100%, 91.7%, and 100%, respectively. The fever-based screening, for which the cut-off value for the axillary temperature was set at 37.0 C, did not detect five influenza patients (false-negative). The sensitivity of the fever-based screening was 68.7%. Table 1 presents a more detailed comparison of the patients with influenza and the healthy control subjects. The influenza patients who had higher Z(x 1 ,x 2 ,x 3 ) values had more severe symptoms, namely higher body temperatures and more elevated heart and respiration rates. The classification model determined that some patients had influenza, even if they did not have a fever. The Z(x 1 ,x 2 ,x 3 ) values could be used to evaluate the severity of infections, and they could, therefore, support the clinical risk stratification of patients. The two influenza patients who were misclassified had negative Z-values because their vital signs were normal.

The mean (SD) facial skin temperature of the influenza patients (36.4 (0.7) C) was 1.1 C higher than that of the healthy control subjects (35.3 (0.6) C). The mean (SD) heart rate of the influenza patients (86.7 (12.1) bpm) was 21.3 bpm faster than that of the healthy control subjects (65.5 (6.8) bpm). The respiration rate did not differ significantly between the influenza patients (19.8 (5) breaths/min) and the healthy control subjects (19.5 (5) breaths/ min) (Figure 3 ).

An integrated visible and thermal image processing approach is proposed for the remote monitoring of multiple vital signs using IRT, thereby enabling the rapid screening of infection in places of mass gathering. The results of this study demonstrate that the effectiveness of IRT for the screening of infection can be greatly enhanced by measuring body temperature, as well as heart and respiration rates, using IRT without any additional sensors. The high level of accuracy of the automated IRT system has a number of clinical implications that could enable the system to be used to provide primary screening of people who may be carrying infections within emergency outpatient units or quarantine stations. Moreover, this system saves time, because considerable amounts of time are required to investigate false-positive subjects when systems have low sensitivity levels and NPVs.

This technology also opens up new opportunities for controlling the spread of infections. For example, the present study was conducted at the Takasaka Clinic in Fukushima prefecture, which is one of the three prefectures that were most affected by the 2011 earthquake and tsunami in Japan. 14 The risk of contracting infectious diseases, particularly influenza, increased after the earthquake and tsunami in Fukushima, and healthcare Table 1 Comparisons between the patients with influenza and the healthy control subjects.

Zðx 1 ; x 2 ; x 3 Þ value (Z ! 0)

x 2 Respiration rate (bpm) x 2 Heart rate (bpm) Figure 3 . Mean (standard deviation) values were calculated for the three vital signs. The differences between the influenza patients and the healthy control subjects with respect to heart rate, respiration rate, and facial skin temperature were assessed. NS, not significant. professionals and medical facilities were severely affected by the disaster. 15, 16 In such settings, an automated IRT system could distinguish between individuals who are and are not carrying infections, thereby alleviating the workload of healthcare professionals. Therefore, the proposed integrated visible and thermal image processing approach may be a promising pre-examination technique in disaster settings. Limitations of the present study mostly pertain to data samples (16 patients and 22 healthy control subjects), which can be considered sufficient for evaluation by the CMOS camera-equipped IRT system for vital-sign measurement. However, the data samples are small for training in a logistic regression classification model. To refine the performance of the logistic regression classification model implemented in the IRT system, field testing with larger and completely random subject populations will be conducted in realworld settings. Moreover, to guarantee the accuracy of heart and respiration rate measurement by the IRT system, the authors are now working on the development of an automatic real-time human face tracking algorithm using visible and thermal images. Detecting and tracking human faces can significantly reduce motion artifacts, thereby extracting stable heartbeat and respiration signals. The face tracking algorithm can also be expanded to multi-person tracking, i.e., more than two human faces can be monitored simultaneously to avoid a 'human traffic jam' in places of mass gathering such as airports.

In summary, the feasibility of using IRT to remotely sense multiple vital signs and to rapidly and accurately screen patients who are suspected of carrying infectious diseases has been demonstrated, and it appears that this is a very promising approach that will provide an alternative to conventional feverbased screening.

The authors state that they have no conflicts of interest to declare.

",0.7362218521401941
Web-based participatory surveillance of infectious diseases: the Influenzanet participatory surveillance experience,"To overcome the limitations of the state-of-the-art influenza surveillance systems in Europe, we established in 2008 a European-wide consortium aimed at introducing an innovative information and communication technology approach for a web-based surveillance system across different European countries, called Influenzanet. The system, based on earlier efforts in The Netherlands and Portugal, works with the participation of the population in each country to collect real-time information on the distribution of influenza-like illness cases through web surveys administered to volunteers reporting their symptoms (or lack of symptoms) every week during the influenza season. Such a large European-wide web-based monitoring infrastructure is intended to rapidly identify public health emergencies, contribute to understanding global trends, inform data-driven forecast models to assess the impact on the population, optimize the allocation of resources, and help in devising mitigation and containment measures. In this article, we describe the scientific and technological issues faced during the development and deployment of a flexible and readily deployable web tool capable of coping with the requirements of different countries for data collection, during either a public health emergency or an ordinary influenza season. Even though the system is based on previous successful experience, the implementation in each new country represented a separate scientific challenge. Only after more than 5 years of development are the existing platforms based on a plug-and-play tool that can be promptly deployed in any country wishing to be part of the Influenzanet network, now composed of The Netherlands,","Infectious diseases remain a serious medical burden all around the world, with 15 million deaths per year estimated to be directly related to infectious diseases [1] . The emergence of new diseases such as human immunodeficiency virus/AIDS and SARS, and the rise of the new influenza strains H1N1, H5N1, and H7N9, as well as other respiratory pathogens such as the novel coronavirus Middle East respiratory syndrome coronavirus, represent a few examples of the global problems facing public health and medical science researchers. Even though pathogens such as the SARS coronavirus and H5N1 had a limited impact in terms of mortality, their sudden appearance has shown how abruptly health emergencies on a global scale can arise.

In recent times, our ability to control epidemic outbreaks has been, to a high degree, facilitated not only by advances in modern science (new cures, new drugs, and cooperative infrastructures for disease control and surveillance), but also by the worldwide spread of new technologies such as computers and smartphones, which allow more than 2 billion persons worldwide to have access to the Internet. This represents an unprecedented opportunity to collect and communicate health-related information in real time and with high geographical resolution, including individuals who do not have contact with the healthcare system. Real-time surveillance data are crucial for rapidly identifying public health emergencies, optimizing the allocation of resources to respond to them, and devising mitigation and containment measures.

Although existing disease surveillance systems (predominantly general practitioner (GP)-based but also involving laboratory-based reporting, mandatory notifications, etc.) have a fundamental role in monitoring and understanding the spread of communicable diseases, they also have several important limitations. One of the major issues is that, for diseases such as influenza-like illness (ILI), only an unknown proportion of all infected individuals see a doctor. In addition, consultations frequently occur with a considerable delay, taking place only when a complication has occurred or a doctor's certificate is required (for example, in Sweden such a certificate is not required until after 1 week of absence from work). Other issues concern: the time delay in data reporting and aggregation; the lack of information on the patterns of household transmission; the lack of uniform standards for clinical definitions, which may vary considerably between countries and even between reporters (European Influenza Surveillance Network). Furthermore, age-stratified rates of physician consultation may vary widely with different healthcare and health insurance systems. Healthcare-seeking behaviour can change unpredictably during an epidemic, making extrapolations of those statistics to the general population uncertain.

To overcome the above limitations in the existing infectious diseases surveillance systems, with a focus on ILI, we proposed an innovative information and communication technology approach based on Web2.0 tools. Starting from the pivotal and successful experiences with Internet-based monitoring systems in The Netherlands and Portugal [2] [3] [4] , the Influenzanet consortium undertook the challenge of deploying an innovative real-time and interlinked surveillance system across multiple European countries.

The first successful example of using the web for public health purposes dates back to 2003, when, in The Netherlands and Belgium, a Dutch scientific communication project was initiated by the small company Science in Action. The purpose was to inform the general population about influenza by means of a self-reporting Internet platform in Dutch (http://www.degrotegriepmeting.nl), where volunteers in The Netherlands and Belgium (the Flemish part) could answer socio-demographic, medical and behavioural questions, and report their influenzarelated symptoms each week. User participation was achieved through targeted communication and recruitment. A positive side effect of this project was the collection of almost real-time data about influenza, including from individuals who did not visit a doctor when ill. Knowledge of volunteers' postal codes allowed the real-time collection of data at the postal code level. Thanks to a vigorous communication campaign using both online and offline media, the project attracted 20 000 users during this first season.

The possibility of collecting such detailed data about influenza cases from such a wide audience attracted the interest of the Gulbenkian Institute of Science Epidemiology group in Portugal, who implemented their own platform in Portuguese, and deployed the system at the beginning of the 2005 influenza season (http://www.gripenet.pt). In Portugal, during the first 2 years of the project, approximately 5000 volunteers were attracted [3] . On the basis of this success, the computational epidemiology group of the ISI Foundation in Turin, Italy decided to deploy a similar platform during the winter of 2007, to carry out influenza surveillance with the aim of informing computational models for studying the spread of ILI on a large scale. Data collected by these four platforms have been evaluated [3, 4] (Paolotti et al., 3rd International ICST Conference on Electronic Healthcare for the 21st Century, 2010, Abstract no. 30), and the estimated seasonal influenza incidence curves have been found to be in good agreement with those from the former European Influenza Surveillance Scheme.

Since 2009, the four Internet-based monitoring systems have formed the foundation of a European-wide network of platforms that became the key instrument for providing real-time disease incidence for the epidemic forecast infrastructure developed by the EU-funded research project EPIWORK (http://www.epiwork.eu). The proposed network of web platforms, called Influenzanet (http://www.influenzanet. eu), saw, for the first time, the collaboration of epidemiologists, public health practitioners and modellers with the aim of collecting epidemiological data in real time through the contributions of Internet volunteers self-selected from among the general population. The epidemiology teams devised reference standard questionnaires for different diseases to collect unified data across European countries (presented in the supporting information). ILI has been the focus in the early deployment of the system, but the final goal is to considerably enlarge the portfolio of diseases and health conditions monitored. In June 2009, the plans to export the platform to other European countries were accelerated by the unfolding of the H1N1 pandemic in the northern hemisphere. A web system very similar to the Italian platform was rapidly deployed in the UK (https://flusurvey.org.uk/) to capture the whole progress of the H1N1 pandemic, with the involvement of the mathematical modelling group at the London School of Hygiene and Tropical Medicine. This work allowed researchers to detect the differences in healthcare-seeking behaviour between different population groups and changes over time, with significant implications for estimates of total case numbers and the case-fatality rate [5, 6] .

The extension of the web platform in each country represented a separate scientific challenge and research problem, as the countries have different population stratifications and geographical distributions. The Influenzanet prototypical web platform implementation had to take into account these differences and design specific mapping/representation solutions, as well as find the optimal granularity for the data acquisition and statistical sampling in the different countries. It also had to provide easy management of content, news, and info-graphics, and provide a user-friendly interface for the volunteers to access the surveys aimed at collecting epidemiological data.

The data collection revolves around two main surveys: a background survey and a symptoms survey. The background questionnaire is completed at registration, and contains questions about the user's postal code, gender, birth date, household, level of education, employment, chronic health conditions, smoking habits, etc. The symptoms questionnaire is a weekly survey with a list of symptoms that the users employ to describe their health status.

The platform is equipped with a 'Reminder', i.e. a weekly email sent to users to remind them to fill in the symptoms survey. These features were included in a plug-and-play implementation to lessen the burden of deploying the platform in new countries, and to let the local teams concentrate on the communication and dissemination needed to attract participants, instead of having to deal with technical problems. The new Influenzanet system started to be deployed in October 2010. The localization, translation, testing and fine-tuning activities took place between October 2010 and February 2011. This powerful and flexible new platform has been successfully adopted by all of the 'old' countries (The Netherlands, Belgium, Portugal, the UK, and Italy), and, thanks to its plug-and-play features, it was deployed, in December 2011, in Sweden by the Swedish Institute for Infectious Disease Control, and by the Institut National de la Sant e et de la Recherche M edicale in France. In both cases, in <2 months, the platform saw the enrolment of several thousands of participants from all over the country. The system has since been successfully deployed in Spain at the Instituto de Biocomputacion y F ısica de Sistemas Complejos, at the University of Saragoza, in September 2012, in Denmark at the Statens Serum Institut in Copenhagen, and in Ireland at the College of Engineering & Informatics, National University of Ireland, Galway in August 2013.

All of the national platforms upload their data to a centralized database, which provides uniform epidemiological real-time data from >50 000 volunteers in ten European countries.

The Influenzanet network for influenza surveillance in Europe contains ten countries: The Netherlands, Belgium, Portugal, Italy, the UK, Sweden, France, Spain, Ireland, and Denmark. Identical questionnaires are implemented with questions about respiratory symptoms, access to and utilization of care and self-medication, uptake of vaccines, attitudes to influenza vaccines, and absenteeism. The questionnaires can be used to monitor ILI in the community in real time, to track vaccine effectiveness [3, 4, 7, 8] , and to estimate risk factors for ILI (not shown here).

In the ten countries, epidemiological data are collected with the participation of a self-selected cohort followed over the influenza season. The success of the data collection strongly depends on the extent of participation of the volunteers involved in the projects. The determinants of participation and the representativeness of the self-selected cohorts of participants in the various countries have been explored elsewhere [9] . In order to assess how the web-based systems for ILI surveillance can be a useful addition to the GP-based system, we present the cross-year and cross-country incidence data, which can be readily compared and easily collected, once the platforms are in place. The national platforms collect data about the health status of the active cohort volunteers during the whole duration of the influenza season. Participants are considered to be active if they have completed, on average, at least one symptoms questionnaire every 3 weeks since the registration. To define ILI, the following ILI case definition is used, which resembles the WHO guidelines [10] : a sudden onset of fever, namely, a measured body temperature of ≥38°C, accompanied by headache or muscle pain, and accompanied by cough or a sore throat. The date of fever onset is used as the date of ILI onset.

In Fig. 1 These incidence curves can be compared with the incidence rates as estimated from ILI consultations in the sentinel GP network (http://ecdc.europa.eu/en/activities/surveillance/ EISN); this comparison shows very good agreement in the peak time. With different definitions of ILI cases and active participants, the incidence has the same trend, with a slight difference in the peak amplitude, but not in the timing (not shown). A more detailed figure for each country can be downloaded at https://dl.dropboxusercontent.com/u/8418871/ influenzanet_supplementary.pdf.

In a few cases, such as Portugal in 2005 and Spain in 2012, the incidence curves are not perfectly congruent with the sentinel GP network data, because, for both countries, it was the first season, and the number of volunteers contributing to the data collection was very low at the beginning of the season, and rapidly increased during the unfolding of the influenza epidemics, leading to a noisy signal.

It is worth mentioning that the answers of Influenzanet participants, when they were asked whether, during an ILI episode, they consulted a GP, showed wide variability [3, 4, 7] (Paolotti et al., 3rd International ICST Conference on Electronic Healthcare for the 21st Century, 2010) (Fig. 2) in the various countries, showing that, more often than not, the web platform can be of great advantage in collecting data about ILI cases that would otherwise go undetected by GP surveillance. The results show clearly the reliability and complementary relevance of the participatory platform data.

The Influenzanet network of web-based platforms presented here highlights the benefit of using the Internet to carry out large-scale real-time surveillance for common diseases such as influenza. Influenzanet relies on a flexible and readily deployable web tool that is capable of coping with the requirements of different countries regarding the data collection method, either during a public health emergency or during an ordinary influenza season. Internet surveillance of healthcare usage can be used to complement traditional surveillance. 

",0.7354706624418168
Monitoring Disease Trends using Hospital Traffic Data from High Resolution Satellite Imagery: A Feasibility Study,"Challenges with alternative data sources for disease surveillance include differentiating the signal from the noise, and obtaining information from data constrained settings. For the latter, events such as increases in hospital traffic could serve as early indicators of social disruption resulting from disease. In this study, we evaluate the feasibility of using hospital parking lot traffic data extracted from high-resolution satellite imagery to augment public health disease surveillance in Chile, Argentina and Mexico. We used archived satellite imagery collected from January 2010 to May 2013 and data on the incidence of respiratory virus illnesses from the Pan American Health Organization as a reference. We developed dynamical Elastic Net multivariable linear regression models to estimate the incidence of respiratory virus illnesses using hospital traffic and assessed how to minimize the effects of noise on the models. We noted that predictions based on models fitted using a sample of observations were better. The results were consistent across countries with selected models having reasonably low normalized root-mean-squared errors and high correlations for both the fits and predictions. The observations from this study suggest that if properly procured and combined with other information, this data source could be useful for monitoring disease trends.","S atellite imagery has been used to derive estimates of land use, vegetation index, human and vector population distribution for risk assessment, mapping and forecasting of diseases such as Hantavirus pulmonary syndrome (HPS), malaria, dengue, Lyme, and Rift Valley fever [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] . These studies have exemplified that if properly analyzed, high-resolution satellite imagery data can be extremely useful for understanding disease spread and implementation of control activities. Remote sensing using satellites has existed as far back as the 1960s and 70s. In contrast, in the last ten to twenty years, numerous studies have advanced several nontraditional data streams as tools to supplement public health surveillance systems. These non-traditional data sources (e.g., social media, micro-blogs, online news reports, and web searches and reservations) [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] appear to be most suitable for surveillance of diseases with seasonal trends (e.g., influenza, dengue and foodborne diseases) and short incubation periods 20 . However, most surveillance systems based on these data streams depend on the existence of disease reports, mentions of disease-related terms or access to digital diseaserelated documents. In the case of an emerging infectious disease, the disease signal available through some of these channels might be relatively low due to limitations in public health infrastructure and access to the Internet, thereby limiting (external) real-time monitoring efforts. Other indicators of social disruption such as the number of patients at a hospital with an undiagnosed infection could serve as proxies for early detection of emerging disease outbreaks. Unfortunately, such data are not easily accessible due to bureaucratic, privacy, security and infrastructural reasons.

Data on hospital traffic extracted from satellite imagery of hospital parking lots could serve as an indicator of hospital attendance and could be useful as an estimator of disease activity. In this study, we evaluate the feasibility of using hospital traffic as a possible proxy for detecting influenza and other respiratory illnesses (hereafter referred to as influenza-like illness (ILI)) in Latin American countries. Similar approaches have been used to study and predict hospital admissions due to seasonal diseases 25 , predict hospital occupancy 26 and to study patterns of hospital use 27 . We estimate hospital traffic based on the number of cars at a hospital parking lot and non-parking lot spaces relative to parking lot size. Data from the Pan American Health Organization (PAHO) is used as a reference for ILI activity. Similar to influenza (and other seasonal respiratory virus) surveillance systems in the United States and several other countries, the release of ILI data to PAHO can be delayed by weeks 28 . The data is also usually updated several weeks after the initial release. This implies that public dissemination of the number of cases due to an emerging outbreak can be delayed by several weeks (or even months) due to delays in reporting, and retrospective updating of case information. The purpose of this study is therefore to present an initial assessment of the use of hospital traffic data in these countries for estimating and predicting disease activity. There are two aims in this study: (1) introduce a new data resource (i.e. high-resolution satellite imagery of hospital traffic) for disease surveillance and (2) evaluate the impact of recency (defined as the most recent data observations) in dynamical multivariable linear models for modeling and predicting ILI data from PAHO based on estimates of hospital parking lot occupancy.

After elimination of unsuitable images (example shown in Figure 1 ), the satellite imagery data consisted of 26, 15 and 13 hospitals for Mexico, Argentina and Chile respectively. We considered four recorded variables (the numbers of vehicles in the parking lot, on the street, and along the hospital border, and the occupancy or fill rate), thereby resulting in 104, 60 and 52 variables respectively. There were 2890 satellite images from January 2010 to May 2013, and all images were used in the analysis. The mean and median numbers of parking lot spaces by country were as follows: Mexico (mean 195, median 155), Argentina (144, 112) and Chile (159, 91).

The mean weekly parking lot occupancy rate is shown in Figure 2 . Based on the monthly ILI activity and average number of cars in the parking lot, peaks in parking lot volume appeared to either precede or follow peaks in percent ILI in some cases. For example, ILI activity peaked in the months of September, June and July for 2010, 2012 and 2013 respectively for Chile. In contrast, hospital peak occupancy months were August, March and May respectively. Similarly, for Mexico, hospital peak occupancy was observed in September, May and February, while ILI activity peaked in August, December and January. The trends observed for Argentina were not as consistent. Note that the influenza season typically runs from May to October, and October to May in the Southern and Northern hemispheres respectively. So for most years, for each of the countries, the peak occupancy month fell within the influenza season.

Recency, Fits and Predictions. In Table 1 , we present various values of recency (defined as the most recent data observations given by n 2 t to n, where t is the recency value and n is the current time point) and the resulting normalized root mean squared error (RMSE) and Pearson correlation coefficient between the fitted/predicted values and the percent ILI from PAHO for Chile, Argentina and Mexico. In most cases, the normalized RMSE agreed with the Pearson correlation coefficients. Based on the recency values considered, high correlations between the ILI and fitted/predicted data corresponded to low RMSE values. In addition, smaller recency values appeared to achieve the best fits and predictions based on the correlation and normalized RMSE. The highest correlation and lowest RMSE value pair for the model fits was observed separately at recency values of 4, (4 and 5) and (4 and 5) for Chile, Mexico and Argentina. Note, the model fitted using all the observed data from the initial to the current week ( Figure 3 ) had the lowest correlation and the highest normalized RMSE for the fitted models. The model fitted with a recency value of 4 ( Figure 4 ) had a better fit compared to the model shown in Figure 3 . This was consistent across all countries. The fitted models with fewer data points captured the peaks and ILI trend better than the model based on all observations. The correlations and normalized RMSE appeared to depreciate with long-term predictions. number of hospitals, suggesting there was more data available. However, although Chile had the least number of hospitals, the RMSE and correlations were sometimes better than that for Mexico. This suggests that the performance of the models could partially be explained by the quality of the data and differences in trends across countries rather than the number of observations/images used. Similar observations were made for recency values less than 10.

Hospital Variables. Occupancy for each hospital was represented by the fill rate, number of vehicles in the parking lot, on the street, and along the hospital border. At each week, the elastic net model selected between one and four variables. The number of cars in the parking lot appeared to be the dominant variable (i.e. most significant model coefficient) across all countries. For example, the number of cars in the parking lot of a general care hospital located in the Arica and Parinacota Region had the most significant coefficient for Chile for most weeks when the entire set of observations ( Figure 3 ) and also when the most recent set of observations were used in fitting (as shown in Figure 4 ). The second most significant coefficient for most weeks was the fill rate of a hospital located in the Metropolitan Region of Chile. The fill rate was also the second most significant variable for the models developed for Mexico and Argentina. Similar to Chile, the hospitals with significant coefficients were located in urban regions specifically, Mexico City for Mexico and Buenos Aires, Ushuaia, and Mendoza for Argentina. The location of the most dominant hospitals in urban areas could be partially explained by the increased likelihood of owning a car in an urban/metropolitan region compared to a rural region. For all three situations, there was no statistical significant evidence (P 5 0.391 to 0.9141) to suggest that hospital parking lot usage was different during and after these disasters.

Our models for influenza and other respiratory viruses using hospital traffic data for select hospitals in Chile, Argentina and Mexico, appear to perform well in capturing the trends present in the data within a reasonable range of error. We used a dynamical Elastic Net approach, which implies that models were fit at each week enabling a dynamical estimation of coefficients and selection of hospital variables that best capture current ILI trends. The models were compared to percent ILI data from PAHO. Ministries of Health and National Influenza Centers of PAHO member states provide the data. The data release is sometimes delayed by a few weeks and data is also retrospectively updated. Therefore, information on current respiratory viruses activity can be delayed by several weeks. Alternative data sources that could serve as early proxies for disease activity are especially useful for monitoring emerging infectious disease outbreaks 29 . For instance, information extracted from satellite images can be processed and available within a few days. The multivariable models for percent ILI from PAHO based on hospital traffic appear to capture the overall trend and peaks in most instances. However, this seems to depend on the number of recent observations used in developing the models since in most cases, using all observations from the initial to the current week results in spuri-ous peaks and troughs, which leads to higher error rates in both the fits and prediction. One possible reason for these artificial peaks and troughs is that data for each hospital were available at irregular intervals due to the fact that the data was archived and some observations were lost because of factors such as tree cover, building shadow, and construction. Real-time surveillance that involves tasking satellites to take images at particular times of the day would eliminate some of the inconsistency in the data. Images can be taken at multiple times of the day for specific hospitals that best capture disease trends in each country.

In addition, there are expected discrepancies in vehicle ownership when comparing rural versus urban dwellers. So estimating hospital traffic based on the number of cars in the parking lot might not be suitable for rural regions. In addition, parking lots for hospitals in rural areas might be more exposed compared to lots for hospitals in metropolitan regions which might have multiple levels, with only the top level revealed. This could lead to a disproportionate sample of hospitals from urban areas. There are also limitations in the surveillance data used as a reference for ILI activity. Although estimated percent ILI was given for each week, the data available from PAHO has missing values for some viruses. In addition, we also fail to account for other factors that could impact hospital occupancy such as natural disasters (e.g. hurricanes), and social unrest (e.g., riots), and the hospital's distance from a metropolitan region due to lack of data. Although there were some significant correlations between the hospital traffic data and social unrest, defining the duration and scope of impact is challenging. While many projects seek to identify or predict those events through the use of social media or news reports, finding a comprehensive list that can be matched to hospital locations was beyond the scope of this project. Including a flawed list in the model would likely result in extensive misclassification of a binary variable, with the primary concern being false negative values. However, these are variables that ought to be carefully considered in future studies. Additionally, comparing the fits and predictions, the recency values for the best correlation and RMSE pairs are different. More work is needed to procure satellite data that could best capture the data trends. As with most studies using non-traditional sources of data for disease modeling or predictions, a measure of noise is present. Hence the recency approach might be suitable for developing models in such situations. Recency allows the model to focus on the most recent observations for fitting and predictions. Recent observations of disease incidence are expected to provide the most precise indication of future disease activity/trends. In addition, the most recent observations of the hospital occupancy rates are expected to have the most significant correlation with current disease activity. If satellites are targeted and values recorded more frequently, the sample size would be larger and fewer images would be eliminated during processing.

Other approaches such as syndromic surveillance (e.g., school absenteeism, calls to nurse hotlines, over-the-counter and prescription medication sales) can also be useful for monitoring disease activity in data and resource poor regions 30 . These data sources can supplement limitations in disease surveillance systems by providing early indications of changes in disease and mortality trends. These data sources can also be used in combination with satellite imagery data to improve early detection of disease outbreaks.

The concept of tracking hospital traffic, as an early indicator of disease outbreak especially in the context of limited data availability is promising based on this initial study. However, our study also suggests that if such data sources are to be used as proxies for disease activity, the data procurement needs to be well defined such that the highest quality of data is obtained.

Hospital Traffic Data. We obtained archived high resolution satellite imagery (average resolution of about 70 cm) data of hospital parking lots from Remote Sensing Metrics (RS Metrics), a company that performs quantitative analysis on highresolution satellite imagery data for various applications 31 . RS Metrics constructed a comprehensive list of hospitals and other healthcare institutions with parking lots for each country (Mexico, Chile and Argentina) using online hospital lists, hospital ranking lists, Google Earth/Google Maps, and Bing Maps. This resulted in a comprehensive list of approximately 120 hospitals and health care facilities for each of the countries (see Supplementary Table 1) . Supplementary Table 1 includes information on type of health facility (hospital or other), health care provider (private or government), location (rural or urban), number of beds (if available) and hospital ranking (if available). Upon initial analysis (not presented), we limited the hospital list to: (i) non-specialty (or general care) hospitals and eliminated specialty hospitals (such as psychiatric hospitals, and surgical clinics) and research centers based on information provided on each hospital (or health entity) website; (ii) hospitals with more than forty parking spaces to increase the chance of detecting significant anomalies in hospital traffic.

For each hospital, RS Metrics performed automated data extraction by first delineating hospital premises, parking lot borders and street parking in different colors as shown in Figure 1A . Images with tree cover, building shadow (e.g., Figure 1B ), construction and other factors that present difficulties in defining the contours were excluded since this could lead to over-or under-counting of the number of vehicles. After delineation, the company used a standard approach for processing images for all clients. This involved a combination of Automated Feature Extraction (AFE) software, manual counting and quality control, and workflow management software to count the number of cars and parking spaces. Please note that the process of data analysis was independent of the image selection process.

The dataset used in analysis consisted of the date and time of each image; the hospital's name and geographic location (including the address, latitude, and latitude); the numbers of vehicles in the parking lot, on the street, and along the hospital border; the number of parking lot spaces and the occupancy or fill rate Weather Data. In addition to disease, weather, social unrest and natural disasters are other factors that could influence hospital traffic. We obtained temperature, absolute humidity and precipitation data from the Global Data Assimilation System (GDAS).

The data was extracted in GRIB format from http://ladsweb.nascom.nasa.gov/ at a one-degree latitude/longitude resolution for each of the countries -Chile, Mexico and Argentina. These weather covariates were selected because they can influence decisions on car usage and studies have shown associations between absolute humidity and onset of influenza epidemics 32, 33 . Each of the meteorological covariates was averaged at a weekly level and time-series were constructed from January 2010 to May 2013.

Civil Unrest and Natural Disasters. The civil unrest data was extracted from openly available data sources (e.g., government reports, social media (such as Twitter) and newspaper reports). The dataset had been used by Doyle et al. 34 in a project aimed at producing real-time detailed forecasts of future events. The civil unrest events included planned protests and riots. Due to the scope of project reported in Doyle et al. 34 the data was limited to November 2012 to May 2013. We used Pearson correlation to evaluate any associations between frequency of civil unrest reports and trends in hospital traffic. Natural disasters may include earthquakes, hurricanes, floods and fires. Although the exact time and location of an earthquake or hurricane landfall may be precisely determined, definition of the duration and scope of impact is more challenging. To evaluate potential associations between natural disasters and the hospital traffic data, we selected three major natural disasters for Mexico and assessed differences in mean hospital parking lot occupancy four weeks before, during and immediately following the event using the Two Sample Welch T-test. We focused on Mexico since it had the largest data sample.

Multivariable Regression Model. We developed multivariable linear regression models to estimate and predict weekly percent ILI for Mexico, Chile and Argentina.

Hospital occupancy reflected by each of the variables (fill rate, number of vehicles in the parking lot, on the street, and along the hospital border) for each hospital is represented as a single explanatory variable x i . PAHO percent of hospital/clinic visits with ILI (hereafter referred to as percent ILI) is the dependent variable y, a i are the coefficients and the normally distributed error term is given by e. The number of variables n varies since the number of hospitals varies by country. We used the Elastic Net regularization and variable selection method 35 to select the hospital variables that best captured the trend in the ILI data. The elastic net estimator is given by:

The elastic net combines the properties of the Least Absolute Shrinkage and Selection Operator (LASSO) and Ridge regression procedures. When a equals to 0 and 1, (2) equates to the Ridge and LASSO estimators respectively. The LASSO procedure minimizes the sum of squared errors subject to a bound on the sum of the absolute values of the coefficients 36 . Ridge regression has a grouping effect, whereby it tends to select all correlated variables. The elastic net combines these two properties such that it tends to select and average the coefficients of highly correlated predictors if any of the variables within the group is selected. The procedure performs well for studies were the number of covariates is greater than the number of observations (p ? n). In such a situation, the number of selected variables can be greater than the number of observations. We make use of this property by fitting models to different sample sizes as later discussed.

Correlations between hospital variables differed by country, which could require different values for a. Models for all three countries were fit with a at 0.8 after exploring values between 0.5 and 0.9. At each data observation (i.e., each week), each of the model coefficients are updated so as to continuously select a subset of variables that provides the best model fit. This results in a diversity of hospital variables used in the model at each week. The model selected by elastic net for each week was used in one and two step-ahead predictions of the weekly percent ILI.

Since the data was extracted from a historical archive and not based on targeting satellites to specific locations, and due to the elimination of images deemed unsuitable, the data had some missing observations. These missing observations were filled using the last known value. To improve the prediction and reduce the impact of noise in our models, we fitted models using a range of previous values (henceforth referred to as recency). We defined recency as the most prior weeks of data given by n 2 t to n, where t is the recency value and n is the current week. This can be illustrated as follows. Let recency equals to t and S represent the complete training set:

Then the recency sample is defined as:

S t~Xn{t , y n{t ð Þ , Á Á Á , X n , y n ð Þ f g ð4Þ

We considered a range of small and large recency values. Given that we expect recent changes in parking lot usage to correlate with recent changes in disease activity, we selected values that were between 4, 5, 6, and 7 weeks so the number of observations for each covariate was at least five. We later assessed whether similar observations could be made if the analysis focused on the last three, six and twelve months of data. For consistency, across all recency values, the initial model was fitted starting from the third week in 2011 and the model fits and predictions were compared based on the normalized root mean squared error (RMSE) and the Pearson correlation coefficient (r). The first set of models solely used hospital parking lot occupancy variables as covariates. The second set of models considered both the hospital parking lot data and meteorological covariates. Model parameters were estimated using a ten-fold cross validation approach and the models were implemented using the glmnet package in the R statistical software.

",0.7347090555261385
Communicable Disease Surveillance Ethics in the Age of Big Data and New Technology,"Surveillance is essential for communicable disease prevention and control. Traditional notification of demographic and clinical information, about individuals with selected (notifiable) infectious diseases, allows appropriate public health action and is protected by public health and privacy legislation, but is slow and insensitive. Big data-based electronic surveillance, by commercial bodies and government agencies (for profit or population control), which draws on a plethora of internet-and mobile device-based sources, has been widely accepted, if not universally welcomed. Similar anonymous digital sources also contain syndromic information, which can be analysed, using customised algorithms, to rapidly predict infectious disease outbreaks, but the data are nonspecific and predictions sometimes misleading. However, public health authorities could use these online sources, in combination with de-identified personal health data, to provide more accurate and earlier warning of infectious disease eventsincluding exotic or emerging infections-even before the cause is confirmed, and allow more timely public health intervention. Achieving optimal benefits would require access to selected data from personal electronic health and laboratory (including pathogen genomic) records and the potential to (confidentially) re-identify individuals found to be involved in outbreaks, to ensure appropriate care and infection control. Despite existing widespread digital surveillance and major potential community benefits of extending its use to communicable disease control, there is considerable public disquiet about allowing public health authorities access to personal health data. Informed public discussion, greater transparency and an ethical framework will be essential to build public trust in the use of new technology for communicable disease control.","Like it or not, we are under constant, state-sanctioned surveillance (Hanley 2017) , which is officially ""justified"" on the grounds of national security, crime prevention, road safety or public service improvement. Unofficially, retailers, goods and service providers and advertisers monitor our preferences, behaviours and habits, for commercial gain-drawing on data provided by us, sometimes voluntarily, 1 but often unwittingly. Masses of ""anonymous"" data about population movements, financial transactions and leisure activities are mined, from surveillance cameras, travel cards, smartphones and tablets, wearable devices, internet searches, online orders, credit card use and social media. These data are analysed, compared, integrated and traded without our explicit consent. Surveillance has a long history, but modern technology has revolutionised the accessibility, scope and speed of data collection and analysis.

""Big Data"" refers to the rapidly escalating volume, complexity, variety and speed of data acquisition. Big Data analytics is ""the process of collecting, organising and analysing large data sets, to discover patterns and generate useful, actionable information"" (Garattini et al. 2017 ). There are risks and benefits associated with Big Data analytics (Davis and Patterson 2012) , but little public understanding of what they are, what they depend on and how, if at all, individuals can influence their use. We often assume that data are anonymous, because they do not contain primary identifiers, such as our name or unique (e.g. social security or healthcare) number. Or we are told they have been de-identified, by removal of primary identifiers and other personal data such as address or date of birth. However, experts agree that data de-identification is, at best, provisional because of the plethora of other ""anonymous"" data and metadata that can be linked, to re-identify individuals (Lubarsky 2017) . Even the most sensitive personal data that banks, government agencies, healthcare providers or insurance companies assure us are secure can be accidentally ""lost"", deliberately leaked, sold or hacked, with loss of privacy, identity or funds (Tanner 2017) .

Big Data is used very effectively in marketing and some types of scientific research, such as meteorology, but relatively little, so far, in healthcare, partly because of privacy concerns and partly because data are often not digital ). However, health service administrative data, patient medical records and laboratory reports are rapidly being digitised. In the microbiology laboratory, for example, infectious disease research and diagnostics have been transformed by nucleic acid-based pathogen identification and genome sequencing, which are faster, more informative and more amenable to digitisation than traditional culture-based ""cottage industry"" methods (Gilbert 2002) .

In this paper, we discuss the benefits, risks and ethical implications, for individuals and the community, of the intersecting roles of healthcare digitisation, pathogen genomics and Big Data analytics. Less attention has been paid to the application of Big Data to pathogen genomics than to its application to human genomics but, despite some major differences, there are parallels between them with respect to potential benefits and ethical, legal and social implications (Mattick 2018; Mittelstadt and Floridi 2016; Middleton 2018) . We argue that an ethical framework is needed, to guide the use of new technologies in communicable disease surveillance and control.

Hospitals, general practitioners, diagnostic services, pharmacies, health insurance companies and government healthcare agencies already store personal health data electronically. Most people have several separate records, created by different agencies for different purposes. A universal ""cradle-to-the grave"" personal electronic health record (EHR), incorporating all personal heath data in a single repository, would have many potential advantages. It would provide immediate access to actionable information in an emergency and could be updated by, and shared among, authorised healthcare providers, as required; it would alert prescribers to drug allergies and interactions, prevent unnecessary investigations and, by the use of personalised decision support systems, provide enhanced diagnostic, therapeutic and prognostic information.

Despite potential benefits, relatively few countries have successfully implemented universal EHRs. Denmark's system, which is one of the most advanced in the world (Rothstein 2008) , has reportedly failed to realise its potential to improve healthcare service delivery, in part because of failure to develop common technical standards for health information exchange (Kierkegaard 2013) . Wherever such systems have been proposed, there are unresolved controversies about privacy, effects on doctor/patient relationships and trust, whether individuals can opt out, who controls or can have access to data and when consent is required, how to protect data from unauthorised use or accidental loss (Fairweather and Rogerson 2001) , and use of ""de-identified"" data for ethically approved research. Many of these issues were widely canvassed in recent debates about Australia's (limited) My Health Record system (Bragge and Bain 2018; Gillespie 2018) . A universal EHR system could have major benefits for healthcare research and delivery. For example, ""de-identified"" aggregate data can be used to monitor the use, outcomes and quality of health services and inform improvements or generate new knowledge about disease epidemiology, such as spatiotemporal distribution or socioeconomic, environmental or ""lifestyle"" risk factors. This information would support the development and evaluation of new treatments, preventive strategies or decision support systems and help to address social determinants of disease (Hunter 2018; Marmot 2001) . Potential benefits would be attenuated if a substantial proportion of the population were excluded or refused to participate, because of concerns about privacy and data security, or if individual consent were required for access to deidentified data.

Data safety and security cannot be absolutely guaranteed, even with the best technical standards. It is claimed that re-identification is straightforward (Teague et al. 2017 ) (depending on the de-identification methods used); data can be hacked (BBC World-Asia 2018) and, occasionally, data custodians betray public trust by selling data for financial gain, to pharmaceutical, insurance or software companies (Tanner 2017; Naughton 2017) . The risks and potential consequences of misuse are likely to be minimal, if optimal technical standards are applied, but there is often little publicly available information on which to base an informed judgement. Nevertheless, universal EHRs could provide substantial benefits for patient safety and health resource allocation. In the following sections, we argue that linking EHRs to person-specific pathogen genomic data (and/or that of relevant nonhuman animal or environmental pathogens) would enhance the timeliness, precision and effectiveness of public health responses to infectious diseases emergencies.

""Surveillance serves as the eyes of public health"" (Fairchild et al. 2007) or ""the finger on the pulse of the health of a community"" (Lee et al. 2012) . The WHO defines surveillance as the ""…systematic ongoing collection, collation and analysis of data for public health purposes and the timely dissemination of public health information for assessment and public health response as necessary""(WHO, n.d). Communicable disease surveillance dates back to, at least, the nineteenth century. Its purpose is to identify and provide appropriate care of people affected by diseases of public health importance and their immediate contacts; prevent the spread of disease; and detect, investigate and control outbreaks. Recent infectious disease outbreaks and pandemics have demonstrated its continuing importance (Box 1 (Heymann and Rodier 2004) . Despite China's delayed outbreak report, a massive global effort, led by WHO and GOARN, rapidly identified a novel coronavirus (SARS CoV) as the cause. They documented modes of transmission, nosocomial infections, risk factors and a high mortality, which enabled WHO to develop evidence-based guidance for diagnosis, management, hospital infection control, quarantine and travel. Within 6 months, the global spread of SARS had ceased, albeit only after it had spread to 29 countries on six continents, caused 8437 cases (of which 92% were in China) and 813 deaths, and cost the global economy an estimated US$54 billion (Knobler et al. 2004 ). Similar delays in recognition and reporting of the 2013-2014 Ebola virus disease outbreak in West Africa led to unprecedented cross-border transmission and, ultimately, > 28,000 cases and 11,000 deaths-mostly in the three affected countries-before it was eventually brought under control by a massive, coordinated international effort (Koch 2016a).

These outbreaks illustrate not only the serious risks for the source countries, of inadequate surveillance and delayed outbreak detection, but also the benefits of prompt public health intervention once outbreaks have been recognised. The SARS (2003) and H1N1 influenza (2009) pandemics highlighted the importance of effective communicable disease surveillance, for national and international health security. The WHO International Health Regulations (IHR) (WHO 2008) provide a legal framework for disease detection and response. Many countries still do not comply with the IHR, but WHO has increased their efforts to encourage and support implementation, especially in low-and middle-income countries. In most high-income countries, communicable disease surveillance has been in operation for decades and credited with rapid detection of novel disease incursions, such as Hendra virus in Brisbane, 1994 (Selvey et al. 1995 , and West Nile virus in New York, 1999 (Sejvar 2003) , and national monitoring of pandemic influenza 2009 (NSW Public Health Network 2009) as well as outbreak control. Methods used range from reports of unusual or suspicious index cases by astute (medical or veterinary) clinicians to mandatory notification of diseases of public health importance, syndromic surveillance and digital epidemiology.

Notification of communicable diseases, to central public health authorities, allows coordinated public health action. Detection and investigation of outbreaks require prompt, accurate laboratory diagnosis and follow-up of affected individuals. This means that personal information-names, ages, addresses and relevant medical data-is reported to public health authorities, often without the person's knowledge or consent. Patients and their contacts are questioned to determine the likely sources and the extent of the outbreak. Patients are treated, if necessary, and may be isolated, while they remain infectious, to prevent further spread; contacts, or occasionally whole communities, may be quarantined (Koch 2016b) , especially if preventive measures, such as vaccination or antibiotic prophylaxis, are unavailable.

These interventions are intrusive and have been a source of controversy, since the late nineteenth century, when physicians objected to tuberculosis surveillance, which they claimed would encroach on the sanctity of the patient-doctor relationship (Shrady 1897) . However, throughout most of last century, there has been widespread public acceptance of name-based communicable disease surveillance of selected (prevalent, serious and/or preventable) infectious diseases, based on a generally well-founded assumption of privacy-i.e. that information will be conveyed only to those who need to know. The benefits include development of evidence-based infection prevention, disease control and health service planning strategies and the ability to monitor disease epidemiology (Fairchild et al. 2017 ) and inform development of vaccines and antimicrobials.

Collection and storage of personal data for communicable disease control are usually protected by public health and privacy legislation. Until recently, it has been relatively inefficient. Conventional paper-based disease notification, by mail or fax, and ""shoeleather"" outbreak investigations are slow. Hard copy records are ""protected by chaos"" (Rothstein 2008 ) and difficult to access for unauthorised, inappropriate or even legitimate use, such as approved research. Culture-based laboratory diagnosis and referral of isolates to a reference laboratory for strain typing takes days or weeks. By then, important epidemiological information (such as food history) is often lost or forgotten and the outbreak is likely to have spread. Recently, faster, more accurate pathogen identification and strain typing methods, including whole-genome sequencing (WGS) (Köser et al. 2012a) , and automated laboratory reporting have improved the timeliness of outbreak detection and provided more accurate microbiological information for public health action.

Over the past 20 years, increasingly sophisticated genotypic methods have been developed for pathogen strain typing but, until recently, most have been expensive, timeconsuming and/or not particularly discriminatory and mainly used for retrospective outbreak investigation. However, newer strain typing methods have been used routinely, for several years, to improve detection of outbreaks due to, for example, Mycobacterium tuberculosis (Merker et al. 2017) , foodborne pathogens, such as Salmonella Enteritidis and its numerous serovars (Campioni et al. 2015) , and nosocomial pathogens such as methicillin-resistant Staphylococcus aureus (O'Sullivan 2006) . Pathogen whole-genome sequencing (WGS) is the ultimate strain typing method. Recently, it has and is continuing to become faster, less expensive and more informative than other genotypic methods and is being introduced into routine use in public health laboratories (Ashton et al. 2016; Satta et al. 2017; Inns et al. 2017; Gurjav et al. 2016) . It promises to dramatically improve the accuracy and speed of pathogen identification, antimicrobial resistance (AMR) profiling, biological risk prediction, outbreak identification and pathogen tracking (ECDC 2016; Quainoo et al. 2017) , with definitive discriminatory power.

The value of WGS lies in the fact that microbial genomes change over time due to point mutations of nucleotides-i.e. single nucleotide polymorphisms (SNPs)-which occur at different rates between species and between different regions of the genome. 2 Epidemiologically related isolates (i.e. from the same outbreak) are identical or different from each other by small numbers of SNPs, whereas differences between unrelated isolates are much larger. As the pathogen spreads from person to person, differences between the outbreak isolates' genomes increase, as SNPs accumulate, but remain small, compared with those between unrelated genomes. By comparing genomes of outbreak isolates with each other and with a reference strain of the same pathogen, one can infer the approximate date of onset and sequence of transmission events (Dudas et al. 2017) , from the number and positions of SNPs. WGS of related isolates can confirm (or exclude) individual transmission events (Arnold et al. 2016), reveal who infected whom, whether there are gaps in the order of transmission (Köser et al. 2012b) , which may indicate unrecognised cases or asymptomatic carriers. If relevant isolates are available, it can identify an index case or common source (e.g. food) of widely dispersed cases (Inns et al. 2017) , and it can identify ""superspreaders"" (Stein 2011)individuals who infect a disproportionately large number of other people.

Some of this information can be discovered by traditional epidemiological investigation, but generally, only much more slowly and with greater difficulty. People may not know, may forget or not wish to reveal what, where or with whom they ate or were in contact, days or weeks before. Implicating a place (restaurant, food processing plant, farm), person or animal as the source of an outbreak has potentially serious medicolegal, commercial or international trade implications (Stasiewicz et al. 2015; Lüth et al. 2018) and genomic data are more objective and, hence, more convincing than epidemiological data. Nevertheless, these different types of information are complementary and both are needed to validate results.

Additional technical developments are needed before the benefits of WGS can be fully realised, but they are feasible or already in progress. They include greater automation and standardisation of quality control methods, bioinformatics tools and algorithms, for analysis and interpretation, and networking of laboratory databases to allow real-time monitoring for outbreak detection (Kwong et al. 2015) .

Surveillance and outbreak investigation has always required the use of personal data. The much greater precision of WGS raises new questions of consent and unanticipated harm, but despite its increasing use in public health and hospital laboratories (Quainoo et al. 2017; Azarian et al. 2015) , there has been little discussion of these issues. Unlike public health surveillance, nosocomial outbreak investigations are not protected by legislation and there is no clarity about privacy protection or the need for informed consent when the use of WGS is extended into new domains. In the following case study (Box 2), we outline some ethical issues raised by WGS of stored isolates for retrospective investigation of a hospital outbreak and suggest that, while the benefits would have been greater if WGS had been available at the time, the ethical dilemmas would have been even more challenging.

Box 2 A high stakes neonatal intensive care unit outbreak investigation 3 (Pinto et al. 2013) Two very premature infants aged 9 and 11 days, respectively, who were nursed in the same room of a neonatal intensive care unit (NICU), developed fulminating methicillin-resistant Staphylococcus aureus (MRSA) sepsis and died within 2 days of each other, despite appropriate treatment. Routine genotyping showed they were infected with the same rare MRSA strain. Possible sources were vaginal colonisation of one mother, transmitted to her infant at birth and then to the other infant; or unrecognised colonisation of a patient, staff member or visitor transmitted, by direct or indirect contact, to both infants. NICU staff and patients were screened for MRSA colonisation. Although screening of staff is controversial and rarely indicated, they readily agreed, after being assured of confidentiality. Routine admission and weekly follow-up screening of new inpatients was implemented. No patient or staff member was identified as carrying the MRSA outbreak strain by initial screening, but over the next 7 months, 13 additional infants became colonised, indicating that nosocomial transmission was continuing, despite enhanced infection control measures. Several of these infants were already colonised with the MRSA outbreak strain within hours of delivery, by caesarean section. Therefore, operating suite and NICU staff (again) were screened. One colonised NICU staff member was assumed to have acquired it from a colonised infant, whom she was nursing and no one was identified as a likely source. However, soon after this, another NICU staff member attended the hospital emergency department, with an infected leg abrasion, from which the MRSA outbreak strain was isolated. She was treated and returned to work. Subsequently, for several months, no newly MRSA-colonised infants were identified; the outbreak was apparently over. This raised the possibility that this latter NICU staff member had been the an unwitting source or vector of ongoing transmission. Her screening swabs had been negative, but sites of MRSA colonisation other than the nasal mucosa (the only site swabbed) are not uncommon. The outbreak strain reappeared in the NICU 7 months later and, subsequently, was isolated from patients in the emergency department and other hospital wards, most of whom had some contact with the NICU. WGS was not available at the time of the outbreak, but it was performed, 5 years later, on stored MRSA outbreak isolates to determine, if possible how this unusual, highly virulent MRSA strain was introduced. What/who were the source and/or vector(s) of continued transmission? Was the reappearance of the outbreak strain, after seven months, due to ongoing transmission or a new introduction?

WGS of stored outbreak isolates, for retrospective outbreak investigation, is likely to provide some answers to epidemiological questions, but before it is done, several important ethical questions should be considered: a) Should informed consent for WGS be obtained from individuals from whom isolates were obtained (or their carers), despite logistical difficulties and emotional risks? Individual consent was given, at least implicitly, for collection of specimens for diagnosis or screening and whatever laboratory procedures were in routine use at the time, but WGS is a new procedure, and its results will have potentially significant implications, for affected individuals. b) If WGS identifies an individual as a likely source or vector of nosocomial transmission, how would this reflect on her infection control practices or affect her future employment? What would be the psychological effects of discovering that she had transmitted a pathogen to vulnerable patients in her care (even if no blame were attributed to her by others)? Should she be told? c) What are the potential medico-legal implications for the hospital, if a staff member were identified as a vector of pathogen transmission? Transient contamination of other staff members' hands was likely but unverifiable. d) Who owns microbial isolates and the information they contain-the people from whose clinical samples they were isolated or the laboratory, which isolated and characterised them?

Questions like these are even more relevant now that the use of WGS is increasing, not only for research or retrospective outbreak investigation but, potentially, for routine pathogen identification, AMR testing and prospective hospital infection control (O'Sullivan et al. 2012 ).

It is predicted that WGS will replace conventional culture-based pathogen identification and AMR testing, in public hospital and private diagnostic laboratories, within a few years (Kwong et al. 2015) . Pathogen identification and sequencing, directly from clinical specimens, without the need for culture or prior knowledge of the target pathogen (clinical metagenomics 4 ), is likely to be possible in the future. When it is, sequence-based pathogen identification and AMR testing results will be available within hours. This will allow appropriate antimicrobial therapy for a bacterial or fungal infection, to be started much sooner than is currently possible or avoided altogether, if a viral infection is identified. Either way, outcomes will be better, with fewer drug side effects, improved antimicrobial stewardship, less AMR and less frequent pathogen transmission. There are still major barriers to routine culture-based WGS or cultureindependent metagenomics, but decreasing costs, faster sequencing, the potential to identify virulence or AMR markers and polymicrobial infections and/or to predict 4 Culture-independent pathogen identification, using nucleic acid detection such as polymerase chain reaction (PCR) has been used for years, but has not replaced culture, when the likely pathogen(s) is/are uncertain or AMR testing and/or strain typing are required. Metagenomics allows unbiased, detection and sequencing of all micro-organisms in a sample, including, potentially, unexpected, rare, novel, unculturable and/or multiple pathogens.

outbreaks with a single assay, make them attractive options for clinical microbiology (Forbes et al. 2017; Gardy and Loman 2018) .

Human genomic data and Big Data analytics are already being combined for personalised diagnosis and treatment (""precision medicine"") of some diseases (Brittain et al. 2017) . The use of pathogen WGS or metagenomic data will expedite the introduction of precision medicine in infectious diseases. Application of Big Data analytics to (i) demographic, clinical and lifestyle risk factors (from EHR); (ii) genetic and immunological predictors of risk and response to therapy (from personal genomic data); and (iii) pathogen virulence and AMR profiles (from WGS data) would allow individually tailored antimicrobial therapy (drug, dose, route of administration, duration) and improved outcomes. Infectious diseases differ from most others in that the speed of diagnosis and time to initiation of appropriate therapy (within hours, for life-threatening sepsis) is the major determinant of the outcome, not only for the affected individual but also for her immediate contacts and the wider community. Rapid identification is obviously essential to ensure prompt, appropriate care and isolation of a patient with a potentially fatal meningococcal or an imported pan-resistant Gram-negative bacterial infection; extremely drug-resistant tuberculosis; or an exotic, high impact viral infection, e.g. Ebola or pandemic influenza. It is also important to allow timely application of appropriate hospital infection control and public health interventions, such as vaccination, antimicrobial prophylaxis and/or quarantine of contacts, to prevent or limit a dangerous outbreak.

Many benefits for individual patients and their (known) contacts could be achieved by the use of individual EHRs, alone, but personalised treatment regimens and outcome monitoring also depend on the use of algorithms derived from analysis of aggregate data from large numbers of EHRs. Big Data analytics can not only develop but also validate and improve diagnostic, treatment and decision support algorithms and, potentially, identify previously unrecognised prognostic information such as clinical or genetic markers of susceptibility to infection, an excessive immunological response or of being a ""superspreader"". Thus, the community benefits of an EHR system, for disease prevention and research, would depend on widespread-ideally universalcommunity participation (not unlike vaccination).

Moreover, to realise the additional benefits of using pathogen WGS data for communicable disease surveillance will require access to and analysis of aggregate WGS data in national or international laboratory networks, linked to clinical and epidemiological databases. EHRs of individuals, whose isolates are identified as part of an outbreak, would be scanned for common risk factors, contacts or exposure to environmental sources, of which the individuals, themselves, may be unaware, to expedite outbreak investigation. Automatic access to, at least, selected personal EHR data, without the need for individual consent, will be essential, since the effectiveness of public health interventions depends on speed. However, data linkage could disclose information that individuals may want to remain private. The trade-off has implications for the effectiveness of surveillance systems because, as ""utility increases privacy decreases"" (Lubarsky 2017) . The challenge is to ensure an appropriate balance between individual risk and community benefit.

The use of pathogen WGS or metagenomics for surveillance, as outlined above, is highly specific, but also selective and insensitive. Infected (and infectious) people, who are asymptomatic or have mild symptoms, who do not seek or have access to medical care or for whom laboratory tests are not ordered, may unwittingly spread infection to others. Syndromic surveillance can ""capture"" undiagnosed infections and is commonly used to complement other methods. One important source of syndromic data is medical encounters such as emergency department or office visits (Henning 2004; Muscatello et al. 2005) . Automated monitoring of coded, de-identified data, analysed in almost real-time, allows health authorities to rapidly identify the onset, trace the spread and identify trends, over time, of outbreaks, e.g. of diarrhoeal or respiratory diseases; it can provide early warning of seasonal, or even pandemic, influenza, or track adverse reactions to antimicrobial drugs or vaccines (Salathé 2016) . Although syndromic surveillance contains high levels of background ""noise"", its strengths are timeliness and pattern recognition.

Digital epidemiology refers to the study of disease patterns using digital data (Gardy and Loman 2018; Salathé et al. 2012) . For communicable disease epidemiology, data may include conventional syndromic surveillance, as outlined above, as well as, for example, participatory surveillance systems, to which volunteers report symptoms online (Guerrisi et al. 2016 ); calls to hospital or nurse help lines; ambulance dispatch requests; health insurance claims; and laboratory, pharmacy or EHR records. They can also include any of the myriad data collected for unrelated purposes that may (or may not) reflect disease activity, such as school and work absenteeism rates, over-thecounter drug sales, social media posts (Salathé et al. 2012; Charles-Smith et al. 2015; Tang et al. 2018 ) and internet searches (Ginsberg et al. 2009 ). Data from animal and environmental health sources can help to identify emerging infectious disease risks.

Applying Big Data analytics to diverse and underutilised data, often from otherwise hidden populations, could enhance outbreak detection and our understanding of infectious disease epidemiology on a global scale, rather than the local or national focus of conventional surveillance. The greater scope, diversity and geographic range of data sources would increase the potential for acquisition of new knowledge, modelling of disease outbreaks, trends and related human behaviour and, ultimately, for improved global control, reduced morbidity and mortality from communicable diseases and greater health security. But there are also potential, largely unknown and poorly defined, risks.

One risk is the use of flawed methods for disease prediction or modelling. The accuracy and reliability of different types of data, and the methods used to analyse them, are highly variable, e.g. references to ""flu"", on Facebook, ""flu-like illness"" in an experienced GP's case notes or ""influenza A virus identified"" in a laboratory report could apply to the same, or to very different, conditions. The limitations of novel surveillance systems were illustrated by the demise of Google Flu Trends, which was based on Google searches related to ""flu"" or symptoms or behaviours interpreted as likely to be due to flu. It was predicted, but ultimately failed, to identify the onset of seasonal influenza 2 weeks in advance of conventional surveillance (Ginsberg et al. 2009; Lazer et al. 2014) . Its failure was attributed to the lack of transparency of methods, over-fitting of data and failure to account for changing search behaviours (Lazer and Kennedy 2015) .

Another risk is that many data sources do not capture basic demographics (age, sex, ethnicity) and infants, the elderly and economically disadvantaged groups-who are most at risk from communicable diseases-are likely to be under-represented . Methods developed to minimise bias and validate results in conventional epidemiological research, such as sampling protocols and case definitions, cannot easily be applied (Lee et al. 2016) .

The harmful effects, of inaccurate or exaggerated outbreak predictions or modelling, include economic impacts on trade, tourism and health services, social consequences of unnecessary public fear and loss of trust in public health authorities. Appropriately skilled, multidisciplinary development teams could anticipate and mitigate these risks. Peer review and ethical oversight of methods and validation of results by comparison with conventional data would help to prevent them-albeit at increased cost of program development and maintenance.

Perhaps, the greatest risk, however, is public fear, ignorance and mistrust. Illinformed media scrutiny and political risk aversion could prevent or delay the incorporation of de-identified personal health data into Big Data-based public health surveillance, despite the benefits. Including them would complement and help to validate less-specific, less-reliable data and mitigate the risks. Even the best available de-identification methods, and optimal levels of data security around legitimate use of identified personal data for outbreak investigations, may not placate these fears. However, well-designed public education and consultation initiatives, supported by suitable privacy regulation and regulation of standards, would improve trust. On the other hand, if important data were, theoretically, available but omitted, public health authorities would attract public, media and political censure if they failed to prevent or limit an infectious disease emergency because they failed to utilise or respond to intelligence, whatever the source, which could have predicted it.

There is an urgent need for open and informed discussion about the ethical implications, quality and safety of Big Data-based use of varied types of data, including personal and pathogen genomic data, in communicable disease control. Questions for discussion-assuming the most reliable methods of data capture and analysis are in use-might include, but not be limited to: a) What is the personal significance of a pathogen isolated from an individual, considering the highly specific and sensitive personal information that its WGS can reveal? Should it be subject to a similar degree of privacy protection as that of her personal (human) genome? b) Given the public benefits (accuracy, reliability, sensitivity, specificity, timeliness) of using as many and varied types of data as possible, in communicable disease surveillance, outbreak investigation and research, should the use of de-identified personal data from EHRs (which may include personal and/or pathogen genomic data) be permitted, and in what circumstances? Do individuals have a moral obligation to participate? Should individual consent be required and if so in what circumstances? c) What level of probability and reliability, based on Big Data analytics, should be required before issuing a public alert about an impending infectious disease emergency-e.g. due to a previously unknown emerging or highly virulent, transmissible or drug-resistant pathogen-considering the need to balance the risks of delay against those of a false alarm? (Would a 30% risk of a dangerous infectious disease emergency be morally equivalent to a 30% risk of a category 4 tropical cyclone, for example?) d) What would be an ethical response to information, based on reliable Big Data analytics that some populations are at increased risk of infection and/or of becoming a risk to others? Considering the risk of stigmatisation, discrimination, loss of autonomy (e.g. mandatory testing) and/or restriction of liberty (e.g. quarantine), who should be told (the affected group; health professionals; the general public)?

Answers to these and other questions would inform development of an ethical framework, for future communicable disease control, which, we argue, is needed because recent technological innovations have raised new ethical issues. A framework would guide development of policies to optimise benefits and minimise risks, protect vulnerable populations and build public trust in and support for public health action in an infectious disease emergency.

Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.

",0.733985090148268
Malware and Disease: Lessons from Cyber Intelligence for Public Health Surveillance,"Malicious software and infectious diseases are similar is several respects, as are the functional requirements for surveillance and intelligence to defend against these threats. Given these similarities, this article compares and contrasts the actors, relationships, and norms at work in cyber intelligence and disease surveillance. Historical analysis reveals that civilian cyber defense is more decentralized, private, and voluntary than public health in the United States. Most of these differences are due to political choices rather than technical necessities. In particular, political resistance to government institutions has shaped cyber intelligence over the past 30 years, which is a troubling sign for attempts to improve disease surveillance through local, state, and federal health departments. Information sharing about malware is also limited, despite information technology being integral to cyberspace. Such limits suggest that automation through electronic health records will not automatically improve public health surveillance. Still, certain aspects of information sharing and analysis for cyber defense are worth emulating or, at the very least, learning from to help detect and manage health threats.","D iscourse about cybersecurity is riddled with biological analogies and metaphors. We describe malicious software in terms of ''viruses'' and ''worms,'' in part because some of their similarities are significant for security. Most important, contagion of either kind can threaten the community at large. Infectious diseases can spread through travel and trade, as illustrated by the Ebola outbreak in West Africa, Middle East respiratory syndrome, and the H1N1 pandemic, among countless other examples. Like the human ecosystem, the internet is another kind of global network, and malware ranging from Conficker and Zeus to Stuxnet has spread through cyberspace in recent years.

To the extent that malware and infectious diseases are analogous threats, they may warrant similar solutions. In particular, several policy analysts claim that ''cyber public health'' and ''cyber hygiene'' would be improved by creating a ''cyber CDC,'' mirroring the US Centers for Disease Control and Prevention (CDC), or even a ''cyber WHO'' modeled on the World Health Organization. [1] [2] [3] [4] [5] [6] [7] [8] ''Both public health and cybersecurity aim to achieve a positive state (health or security) in a loosely affiliated but highly interdependent network.'' 9(p77) While the details differ, some of the functional requirements for achieving cyber and health security are similar as well. Among others, these functions include collecting data about threats to the population (eg, malware or disease), analyzing this information, and reporting to inform action-in a word, surveillance.

What insight does cybersecurity offer for improving public health surveillance, given their family resemblance? Can public health learn from its younger, digital cousin? To answer these questions, this article compares disease surveillance with what is now called cyber intelligence, specifically as it relates to malicious software. The research methodology is described first. Important similarities between malware and infectious disease are identified next, followed by the functional similarities between disease surveillance and cyber intelligence. The organizational histories of these fields are then compared and contrasted at the local, national, and international levels.

Despite roughly analogous threats and functional requirements, there are striking differences in how these fields are organized. Cyber intelligence is more decentralized, private, and voluntary than disease surveillance in the United States. These differences inform at least 3 lessons for public health policy: (1) the prevailing politics of the time have changed to the detriment of health agencies inside government; (2) electronic health records will not automatically improve disease surveillance; and (3) the CDC could adopt aspects of information sharing and analysis from cyber defense to help improve its image and data exchange. The logic of each lesson is examined in the final discussion.

This article provides a qualitative, historical analysis of cyber intelligence and disease surveillance. It compares and contrasts the actors, relationships, and norms at work in each field. The empirical evidence draws on more than a dozen semistructured interviews with practitioners in public health and cyber defense (30 to 120 minutes in length, tape recorded or written notes, most not for attribution), as well as archival research and existing literature (grey and peer reviewed). While the primary focus is on the United States, international cooperation is also addressed.

Cybersecurity would be important for public health even if malware and disease were unrelated. The confidentiality, integrity, and availability of health information are vital to this profession and threatened by security challenges ranging from medical data breaches to ransomware targeting hospitals. Of course, it is important to note that malicious software and infectious diseases are not the same. Malware is computer code processed by digital electronics, whereas diseases and their hosts are biological organisms. Malware is not lethal, while infectious diseases kill millions of people each year. Usually, malware is created with malicious intent (as its name implies), but the vast majority of infectious diseases are naturally occurring and lack hostile motives (with the rare exception of biological weapons). 10 Nevertheless, these threats are similar in several ways that are significant for security. First and foremost, both malware and infectious disease can be contagious and widely spread. Transmissibility varies: Not all cyber attacks are broadcast or intended to spread beyond a specific computer, and not all infectious diseases are contagious. Yet, the potential for spread means that the threat to any individual or computer depends in part on the health of the overall population or network. This interdependence also means that securing either kind of system through intelligence or surveillance depends in part on local, national, and international cooperation.

Second, both malware and disease have nonkinetic effects that can be difficult to detect. 11 This difficulty complicates surveillance and source attribution because symptoms can be delayed, hidden, or nonspecific, leaving victims and vectors unaware. Detection, diagnosis, and treatment can therefore require specialized knowledge and tools:

Just as medical researchers have to study a virus before they can create a vaccine, antivirus makers must capture a computer virus, take it apart and identify its ''signature''unique signs in its code-before they can write a program that removes it. 12 Here again, cooperation is helpful for developing these countermeasures. For example, antivirus companies now share malware samples to improve detection, and the vaccine industry has long benefited from the virus samples shared through the Global Influenza Surveillance and Response System (GISRS). 13, 14 Both the advantages of cooperation and difficulties of detection are compounded by the variety of malware and disease. New pathogens are always emerging (eg, through antigenic drift and shift in influenza). New malware is also common. According to Symantec, ''there were more than 317 million new pieces of malware created'' in 2014, ''meaning nearly one million new threats were released into the wild each day.'' 15(p7) Finally, some of the specialized knowledge and tools used to detect and treat malware and disease are dual use. This means that they can be used for both benefit and harm, or, alternatively, for both military and civilian applications. Cyber intelligence and disease surveillance are both conducted by military and civilian agencies because malware and disease are both interpreted as threats to national security. 16 Furthermore, knowing how to detect malware can be used to help remove it or to hide the code and cause harm. Disease surveillance is rarely thought of in terms of potential harm, since the information it provides is intended to help reduce morbidity and mortality. But this information can also be used to stigmatize and discriminate against individuals and groups, as well as restrict travel and trade. Fear of misuse has fueled resistance to disease surveillance, including popular opposition to name-based reporting of HIV and government opposition to reporting outbreaks such as SARS and H5N1. [17] [18] [19] One consequence of these dual-use dilemmas is that cooperation is not inevitable, even when it would be advantageous for cyber or health security.

Just as malware and infectious disease are roughly analogous threats, so too are certain aspects of acquiring, interpreting, and communicating information about them. According to the CDC, ''Public health surveillance is the ongoing, systematic collection, analysis, interpretation, and dissemination of data regarding a health-related event for use in public health action.'' 20 Similar terminology is used to define ''cyber intelligence''-namely, ''the acquisition and analysis of information to identify, track, and predict cyber capabilities, intentions, and activities that offer courses of action to enhance decision making.'' 21(p2) Malware is one kind of cyber capability subject to intelligence, as infectious disease is one kind of health-related event-along with noncommunicable diseases, injuries, medical services and devices, and the like-subject to surveillance, biosurveillance, or ''health intelligence.'' 22 And both fields differ from espionage. Following revelations by Edward Snowden, associating the word ''cyber'' with ''intelligence'' or ''surveillance'' may bring to mind warrantless wiretapping, bulk collection of metadata, and other controversial programs conducted by the National Security Agency (NSA). But clandestinely gathering intelligence for counterterrorism differs from collecting, analyzing, and sharing information with consent to improve internet security, just as disease surveillance differs from spying.

As shown in Table 1 , similar kinds of information can be collected, analyzed, and shared through cyber intelligence and disease surveillance. First, information about malware includes the source code (human readable) and binary files (machine readable). These files can be summarized as signature or hash functions to help automate detection; they can also be analyzed to identify other indicators of compromise and clues about their author or origin. Second, data on susceptible, infected, and exposed hosts can provide information about vectors and vulnerable computers in the population. Third, cyber intelligence, like disease surveillance, relates to action. Information about defensive action includes the actors involved, as well as how they detect, contain, and remediate threats using patched or recon-figured software, firewalls, blacklists, access controls, and other countermeasures.

To be clear, computer forensics is not the same as shoe leather epidemiology. At an abstract level, however, cyber intelligence is sufficiently similar to disease surveillance that comparing their respective histories and policies can provide useful insight into each field. Since cyberspace is a relatively new domain, the organizations that have been created to defend it are more sensitive to recent trends in politics and technology than many older institutions in public health. Therefore, cyber intelligence may serve as a sentinel case for the forces that stand to reshape disease surveillance in the years ahead.

Comparative Analysis: Local, National, and International Organizations

The basic structure of disease surveillance at the local, national, and international levels is familiar to many public health professionals. But neither the organizations involved nor their future should be taken for granted. These actors, relationships, and norms look different from those in cyber intelligence, despite similar threats and functional requirements. In addition, politics have had a profound impact on both fields, and the prevailing politics have changed over time.

American public health consisted mostly of local, uncoordinated, and episodic sanitation and quarantine measures until the bacteriological revolution. [23] [24] [25] These local roots are still evident in the design of many health departments across the United States. The modern practice of public health surveillance also reflects important political compromises, quite distinct from the germ theory of disease.

Local laws about infectious disease date back to the colonial era, but the proliferation of modern regulations for surveillance coincided with the new science of bacteriology in the 1880s and 1890s. For instance, one of the first bacteriological laboratories in the United States was 27(p3) Bacteriology ''brought professionalism to disease control,'' and laboratory diagnosis provided these professionals with newfound ''status and authority.'' 17(p3) However, the political power afforded by this status and authority was insufficient to overcome countervailing interests. ''Public health . sought to expand the role of government as a guardian against disease,'' 17(p256) but many physicians in private practice opposed surveillance as a ''demand for unpaid labor'' and threat to their clinical authority. 17(p13) To win or defuse conflicts of interest over reporting tuberculosis and venereal disease at the turn of the century, public health officials-including outspoken proponents of surveillance such as Hermann Biggs-conceded that they would not encroach on the health care provided through private practice in exchange for notification.

This concession was a political compromise. While it was expedient, limiting the actions that might otherwise have been informed by disease surveillance is hard to justify in terms of maximizing technical efficiency. Nevertheless, this compromise was one of many, and it endures to this day as a practical constraint on the relationship between disease surveillance and local intervention.

Germ theory revolutionized biology, but cyberspace is manmade and thus somewhat less mysterious (eg, malware was never blamed on miasma or treated with leeches, although scam and snake oil solutions are not uncommon). John von Neumann theorized about self-replicating automata as early as 1949, and Fred Cohen coined the term ''computer virus'' in 1984, years before malware became a common threat. 28, 29 Instead of a new scientific theory, organizations for civilian cyber defense emerged in response to communication and coordination problems highlighted by the Morris worm.

In November 1988, the Morris worm exploited flaws in the Unix operating system to quickly spread, infecting and slowing about 10% of the 60,000 to 80,000 computers that constituted the entire internet at that time. A few weeks later, another hack compromised the US military's network. Postmortem reviews of these incidents concluded that response was hampered by limited communication and coordination. 30 To address these problems, the Defense Advanced Research Projects Agency (DARPA) funded the Software Engineering Institute at Carnegie Mellon University to establish the Computer Emergency Response Team Coordination Center (CERT/CC).

Naming the CERT/CC a ''coordination center'' did not mean that the collection, analysis, or dissemination of data about malware and software vulnerabilities were centrally coordinated. The CERT system was a decentralized approach to cyber intelligence. From the beginning, ''other agencies and constituencies were encouraged to create and sustain their own teams.'' 31(p19) These teams typically serve narrow and discrete constituencies (eg, a single company), unlike the local, municipal, and state health departments that serve the entire population in a given location. The result is a loosely coordinated ''mosaic of hundreds of independently operating CERTs across the world.'' 32(p92) The CERT/CC is not a government agency, in contrast to the health departments that conduct most disease surveillance. Nor can it make or enforce rules. ''From the very beginning,'' according to one founder of the CERT/CC (interview October 2015), ''DARPA stressed . that we had no authority.'' Moreover, little capacity for cybersecurity was developed inside local, municipal, or state governments. Even now, fewer than half of local law enforcement agencies have a cybercrime unit. 33 Therefore, unlike other threats to public health and safety, the only recourse that most malware victims have is through a commercial service provider, such as an antivirus vendor like Symantec or McAfee. In sum, in addition to being more decentralized, the United States chose a more private and voluntary approach to cyber defense compared to public health at the local level.

Disease surveillance in the United States is a patchwork of state systems, developed through a long political process rather than an inevitable response to contagion. Congress created the US Marine Hospital Service in 1798, but control was not centralized until John Woodworth was appointed the first Surgeon General in 1871. Woodworth ''adopted a military model for his medical staff'' that was later institutionalized as the commissioned corps of the Public Health Service (PHS). 34 He also aimed ''to expand the service into a national quarantine agency'' and ''gather information about the spread of contagious diseases throughout the world.'' 23(p162) Despite political resistance, the Marine Hospital Service eventually gained control over quarantine through bureaucratic empire-building within the federal government.

Here, as elsewhere, federal authority can be seen as constrained by the Tenth Amendment to the Constitution or empowered by the General Welfare Clause and the Commerce Clause. 35 Either way, deference to states' rights remained the norm for disease surveillance. The PHS appointed epidemiologists to serve in state health departments in 1914, but the federal government did not establish an agency dedicated to disease surveillance until 1946. 27 

Even then, when the CDC was built on a wartime campaign against malaria, its bureaucratic survival was not assured. ''The CDC sometimes encountered opposition from better-established wings of the Public Health Service that resented the intrusion of a newcomer,'' and ''the National Institutes of Health [NIH] . also resented the upstart institution in Atlanta,'' 36(p28) so ''the possibility that CDC would be dismantled and merged with NIH was very real.'' 36(p49) To survive, the CDC sought to distinguish its fieldwork in service to the states from research at NIH. Similarly, Alexander Langmuir cited the threat of biological weapons during the Korean War to justify CDC's Epidemic Intelligence Service, which was also used to support state and local health officials. 11 Service to the states therefore became gospel at CDC, helping it become an authoritative federal agency while still deferring to states' rights. Prompted by Langmuir at CDC, the Council of State and Territorial Epidemiologists (CSTE) created an annual list of notifiable diseases in 1951. Reporting through the National Notifiable Disease Surveillance System is voluntary and somewhat variable; however, notification is compulsory at the state level, and, ''in practice, states do notify the CDC.'' 35(p1038) Disease surveillance is conducted by other federal agencies as well (eg, the Department of Agriculture, the Food and Drug Administration, and the Department of Defense), but CDC is the dominant authority in this field. It is also the biggest source of public health grant money, and this funding gives the CDC considerable power over the state health departments that it serves (interview with local public health official and former EIS officer, July 2015).

There is no national institution for cyber intelligence with the same authority and resources as the CDC. Instead, the CERT system spread alongside a global marketplace-now worth almost $100 billion-for cybersecurity products and services. These products and services include various forms of threat data and analysis, as well as hardware and software to limit the damage caused by cyber attacks (eg, patches, antivirus, firewalls, encryption, intrusion detection and prevention systems, etc).

How effective is this organizational ecosystem? Paradoxically, while the internet has drastically decreased the cost of communication, limited information is shared about threats to information technology. 2,3,37-39 Reporting is often ad hoc and idiosyncratic, depending more on interpersonal trust than institutional affiliation. Data collection and analysis are also balkanized. 40 Unlike epidemiology, where there is some consensus over CSTE case definitions, different vendors and CERTs use different nomenclature to name malware and count its frequency. Consequently, ''comparing incident statistics across teams is difficult and often meaningless,'' which diminishes the value of what little information is shared. 31(p83) Proposals for the federal government to establish authoritative standards and agencies have gained little traction. In 1989, the General Accounting Office (GAO) argued that internet security required a focal point. It proposed that the Office of Science and Technology Policy (OSTP) establish this focal point, but to no avail. 30 In 1991, the National Research Council recommended that the government help establish a nongovernmental organization to serve a similar function-again, with no apparent impact. 41 CERT/CC even pitched a similar proposal in 1997, which, by implication, was a concession that the CERT/CC had failed to establish itself as the single clearinghouse for cyber intelligence. 42 In 1998, President Bill Clinton issued Presidential Decision Directive 63 (PDD-63), 43 calling for ''A Public-Private Partnership'' to reduce the vulnerability of critical infrastructure to cyber attack. PDD-63 encouraged the private sector to create an Information Sharing and Analysis Center (ISAC), modeled on the CDC: As ultimately designed by private sector representatives, the ISAC may emulate particular aspects of such institutions as the Centers for Disease Control and Prevention that have proved highly effective. . 43 However, unlike the CDC (ie, a federal agency with police powers, informed by compulsory reporting at the state level), this ISAC was to be private and voluntary. Moreover, ''the single ISAC-the one PDD-63 calls for-was never built'' (interview with former US National Security Council staff, September 2015). Rather than establish a single clearinghouse, building separate ISACs became a business opportunity. 44 At least 19 sector-specific ISACs were subsequently established for financial services, energy, healthcare, and other parts of critical infrastructure. While reasonable, this outcome was not inevitable, especially since variation inside a given sector (eg, small and medium enterprises versus multinational corporations) may be just as significant for security as variation across sectors (eg, finance versus energy or healthcare).

The Department of Homeland Security (DHS) was created in the aftermath of September 11, and it built capacity for cyber intelligence through the United States Computer Emergency Readiness Team (US-CERT) and the National Cybersecurity and Communications Integration Center (NCCIC). In the future, the NCCIC or US-CERT may come to resemble the CDC. Yet, the CDC started out with hundreds of employees. 45 US-CERT started out with a few dozen people and a small room, ''with one table and one chair in one corner'' (interview with CERT/CC staff, October 2015). 46 Today, the CDC provides more than $3 billion per year in grants for domestic public health, along with more than $400 million for global health. 47, 48 The Homeland Security Grant Program provides more than $1 billion to help states counter terrorism and other hazards, but most of this money is spent on physical rather than cyber security (interview with SMITH former MS-ISAC staff, November 2015). In addition, US-CERT was intended to play an international role, and yet it focuses on domestic agencies, with less global engagement than initially imagined (interview with former US-CERT staff, September 2015). Analogous functions and threats aside, national cyber intelligence is structured differently from disease surveillance.

Less than a year after the Morris worm, the CERT/CC helped the US Department of Energy and the National Air and Space Administration respond to the Wank worm, an early attempt at antinuclear hacktivism that protested the plutonium-powered Galileo spacecraft in 1989. Initial signs pointed to France and then Australia, and this incident fueled interest in international information sharing. 49 CERT/CC therefore helped create the Forum of Incident Response and Security Teams (FIRST) in 1990.

FIRST provides ''access to best practices, tools, and trusted communication.'' 50 It started out as a small club (except for 1 French team, the other founding members were all American). 31 It has since grown into an international forum for more than 300 national and industry teams from around the world. Given this international membership, FIRST can be compared with the World Health Organization. As with comparisons between CERTs, ISACs, and the CDC, however, FIRST is no WHO.

Unlike the WHO, FIRST did not build on previous international organizations. FIRST was the first of its kind. In contrast, the WHO was preceded by several international organizations and false starts, including the Pan American Sanitary Bureau (PASB), the Office International d'Hygiene Publique, and the Health Organization of the League of Nations. The structure of WHO reflects the politics of this legacy. For example, PASB leveraged its existing status to maintain autonomy and resist full integration into the WHO in 1948, a political precedent that helped other regional offices lock in similar arrangements and authorities. 51, 52 Furthermore, FIRST did not build on international law. In contrast, WHO administers the International Health Regulations (IHR), the origins of which trace back to the 19th century. After decades or more of noncompliance with these legally binding regulations, the IHR were revised in 2005. The revised IHR require member states to report all public health events of international concern, and they authorize WHO to acknowledge unofficial information about these events. In theory, WHO can use this authority to ''name and shame'' noncompliance (even though it rarely does so, bowing instead to political pressure from member states, and noncompliance persists). 52, 53 However, ''FIRST exercises no authority over the organization and operation of individual member teams.'' 54 It does not pretend to be an operational agency or wield international law.

FIRST merely consists of a small board, a few committees, and an annual conference. It is dwarfed by WHO, which has a $4 billion budget and oversight of surveillance through programs such as GISRS and the Global Public Health Intelligence Network (GPHIN). As small as FIRST appears in this comparison, it is still more prominent than other international organizations involved with cyber intelligence. For instance, the International Multilateral Partnership Against Cyber Threats (IMPACT) is supposedly ''modelled after the Centers for Disease Control and Prevention . to enhance the global community's capabilities in dealing with cyber threats.'' 55 But IMPACT has little following or credibility in advanced industrialized countries (interview with former AusCERT staff, May 2015). Here again, this international ecosystem looks very different from disease surveillance.

The institutional differences between disease surveillance and cyber intelligence are striking. On balance, cyber defense is more private, voluntary, and decentralized than public health. Which field is more effective is beyond the scope of this study, and ''there is no simple relationship between central control and effective surveillance.'' 22(p1071) Nevertheless, the chorus of criticism surrounding cyber defense suggests that public health may have the advantage. Either way, the history of cyber intelligence provides at least 3 important lessons for disease surveillance.

First, both fields reflect political choices at least as much as any technological imperative. For example, in public health, the power and interests of physicians in private practice constrain the potential for local intervention informed by disease surveillance. The CDC's service-or federal deference-to states is a political choice as well, as is the patchwork of different state systems. ''US public health law and policy are inescapably political,'' and they have been ''from their very inception.'' 56(p46) The same is true for global health governance.

Cyber intelligence is also shaped by politics, particularly the American ideology of ''anti-statism.'' 57 Ideological opposition to government institutions and interventions was ascendant during the 1980s and 1990s, at the same time that cyberspace and network threats were coming online. Anti-statism was bipartisan: The debate was less over whether to cut government and more over how much to cut. Thus, it was politically infeasible to provide cyber intelligence as a public good or service through government. This ideology helps explain why civilian cyber defense is now more decentralized, private, and voluntary than public health. Things could have been different. After all, the internet was born on public infrastructure with military funding. The United States could have built a focal point for cyber intelligence inside the government, like the CDC. For better or worse, it could have worked through an intergovernmental organization such as the International Telecommunications Union, which, like the WHO, is part of the United Nations (UN). Instead, the United States chose to rely on CERTs, ISACs, FIRST, and other private sources for cyber intelligence. The 1980s and 1990s were not the 1940s, when the United States had been willing to build government agencies at home (eg, CDC) and abroad (eg, the UN and the WHO). The ideological pendulum had swung towards anti-statism, and cyber intelligence reflects this shift in the marketplace of ideas.

Cyber intelligence therefore provides a cautionary tale for public health. As underfunded and uncoordinated as disease surveillance may be, it could be worse. If the CDC did not already exist, it would be difficult to build this kind of government agency in this day and age. Building the WHO would be even harder. Path dependency suggests that the CDC and the WHO will survive, but they should not be taken for granted. More important, if these agencies are going to help improve disease surveillance, then their policies and actions must be politically sophisticated, given the prevailing ideology in the United States. Without this kind of leadership, progress is unlikely.

A second lesson to learn from cyber intelligence is that information technology does not guarantee information sharing. Electronic records are native to the internet, as are the means to share them (email, listserv, web portals, etc). But information sharing about malware and other cyber threats remains limited, which, in turn, limits how much ''big data'' is available and amenable to analysis. Sometimes, critical information is not even shared with potential victims. For example, IP addresses infected by the Conficker worm are routinely identified without informing the users of these compromised systems or removing the malware. 13 This practice may break with sound medical ethics for diagnosing and treating disease (although it pales in comparison to the infamous Tuskegee experiments). Either way, the lack of information sharing is not due to any lack of information technology.

Therefore, as revolutionary as information technology and electronic health records may be for disease surveillance, they are no panacea. The technical challenges of moving from manual to automated data management are easy when compared to the political barriers to sharing information through interoperable systems. These barriers range from conflicts between public and private interests to privacy and liability concerns derived from the dual use of data, all of which have contributed to the balkanization of cyber intelligence.

Public health officials should work to avoid similar fragmentation. To this end, it is important to recognize how the design and implementation of electronic records are not merely functions of technical efficiency but rather choices with distributional consequences (ie, there are winners and losers). One potential advantage for public health agencies is that disease surveillance is ''hierarchical and relatively simple in structure.'' 58(p2) This structure places CDC and the Department of Health and Human Services (HHS) in a strong bargaining position to promote open standards for information sharing (including common nomenclature and data types). If harmonized, these standards can create network effects-namely, positive externalities-that substantially increase the volume and value of information available. So, in addition to modernizing the National Electronic Disease Surveillance System (NEDSS), the CDC and HHS should aggressively push the development and adoption of data standards, as well as ''meaningful use'' criteria for electronic health records to include public health. 59 

Finally, while cyber intelligence leaves much to be desired, some aspects of this field may be worth replicating. After all, the relatively hierarchical system whereby state health departments report information to the CDC is not above reproach. The CDC can be criticized for being a black box into which information flows ''but does not come back.'' Ironically, this criticism echoes what some cybersecurity experts say about sharing information with intelligence agencies like the NSA. Given that the politics of American anti-statism resist increasing federal administration of public health surveillance, alternative approaches for improving collection, analysis, and reporting also warrant consideration.

Because ISACs for cyber intelligence failed to actually emulate the CDC, one alternative approach would be for the CDC to sponsor an ISAC for public health surveillance. 60 The CDC Office of Public Health Scientific Services could seed this public-private partnership, which might look like the National Health Information Sharing and Analysis Center (NH-ISAC) for cybersecurity in the healthcare sector. Ideally, it would combine the lessons learned from NH-ISAC with experience from CDC's pilot fusion center, BioPHusion. 61 Likewise, private sector participation may help this ISAC avoid some of the problems suffered by the DHS National Biosurveillance Integration Center. 62 The goals would be consistent with the CDC's surveillance strategy. 63 The ISAC could help make surveillance ''more adaptable to the rapidly changing technology landscape'' by allowing tech vendors and users to engage in an operational environment on an ongoing basis. 63 It could help make surveillance ''more versatile in meeting demands for expanding knowledge about evolving threats to health'' 63 by providing a sanctioned forum for stakeholders to share and SMITH evaluate heterogeneous data alongside internet-based surveillance (GPHIN, HealthMap, ProMED, etc), official sources (eg, NEDSS, Epi-X), and similar systems. The ISAC could also help make surveillance ''more able to meet the demands for timely and .

[specific] information''especially demands from outside the CDC-by broadening participation. 63 Of course, the devil is in the details, and an ISAC cannot replace surveillance by local, state, and federal authorities (a lesson that may be dawning on cybersecurity experts). This article provides a foundation for further research, and an ISAC for public health surveillance may be found to complement the existing architecture. Even if the CDC rejects this particular approach, it can still learn from the successes and failures of cyber intelligence. This field is in touch with information technology, by definition. It is sensitive to contemporary politics, lacking the longer legacy of public health. Therefore, while cyber intelligence and disease surveillance are not the same, the threats and functional requirements are too similar to ignore when crafting better policy.

",0.7339552036488267
Targeting Transmission Pathways for Emerging Zoonotic Disease Surveillance and Control,"We used literature searches and a database of all reported emerging infectious diseases (EIDs) to analyze the most important transmission pathways (e.g., vector-borne, aerosol droplet transmitted) for emerging zoonoses. Our results suggest that at the broad scale, the likelihood of transmission occurring through any one pathway is approximately equal. However, the major transmission pathways for zoonoses differ widely according to the specific underlying drivers of EID events (e.g., land-use change, agricultural intensification). These results can be used to develop better targeting of surveillance for, and more effective control of newly emerged zoonoses in regions under different underlying pressures that drive disease emergence.","E merging infectious diseases (EIDs) have significant public health and economic impacts and are increasing in frequency (Brahmbhatt 2005 , Jones et al. 2008 . Nearly twothirds of EIDs are zoonotic and three-quarters of those originate in wildlife ( Jones et al. 2008) , thus targeted disease surveillance may be useful to optimize prevention and control measures and reduce the threat of future zoonotic EIDs (Karesh et al. 2005 , Vrbova et al. 2010 . Recent approaches to disease surveillance and control have largely been disease specific and reactive in nature, tackling pathogens after they have already emerged (Childs and Gordon 2009) . Proactive approaches include pathogen discovery in wildlife to identify potential zoonoses , Lipkin 2013 , however the number of microbes in wildlife remaining to be discovered is likely large and the causes and dynamics of transmission from wildlife to human are poorly understood ( Jones et al. 2008 , Childs and Gordon 2009 . As pathogens continue to emerge from wildlife, a better understanding of the ways through which transmission could potentially occur is needed. Because pathogens with different transmission pathways may require very different prevention and control strategies, understanding the relative importance of each pathway for a given pathogen is essential.

Zoonoses can be transmitted from wildlife to humans by a range of routes (or pathways), yet research exploring the role of transmission pathways in past EID events has not been consolidated. Here, we analyze transmission pathways of all known previously emerging zoonoses. Our results suggest that the relative importance of different transmission pathways varies by EID driver (e.g., land use change, bushmeat consumption, climate, and weather). This suggests that targeting pathogen discovery and surveillance programs to different transmission pathways will increase our capacity to identifying important pathogens, either known or unknown.

We identified all unique zoonotic pathogens (n = 183) from a published database of 335 emerging infectious disease ''events'' (the original case or cluster of cases representing an infectious disease emerging in human populations for the first time) from 1940 to 2004 ( Jones et al. 2008 ). This database includes EID events caused by newly evolved strains of pathogens, novel pathogens that have entered human populations for the first time, and pathogens that have likely been established in humans historically, but that have recently increased in incidence ( Jones et al. 2008) . We excluded emergence events that arose through the evolution of antimicrobial drug resistance and those caused by newly evolved strains of known pathogens (e.g., multidrug-resistant tuberculosis and chloroquine-resistant malaria), as well as those attributed to human susceptibility to infection (disease outbreaks that are secondary to immunodeficiency diseases), leaving 148 records in our database.

We then conducted a systematic literature search to identify all documented animal-to-human transmission pathways for each pathogen and the relative contribution of each when more than one transmission route was described. Transmission pathways were identified through a comprehensive search in the ISI Web of Science online database, performing a topic search using the string ''[pathogen name]? AND *transmission* AND (route* OR pathway*) from 1940 to present. The use of this combination of key words allowed for the identification of all documented pathways. Pathogen synonyms were included in the search. Supplementary references, including World Health Organization (WHO) and Centers for Disease Control and Prevention (CDC) fact sheets, were used to provide additional information about transmission routes from animals to humans. We identified five broad but distinct transmission pathway categories: (1) Direct contact (skin-to-skin contact; scratches; animal bites; contact with body fluids, organs, and tissues; direct large droplet exposure); (2) airborne transmission (via dust particles and airborne small droplets); (3) vector-borne (by biting or mechanical transfer by arthropods); (4) oral transmission (consumption of contaminated food or water); and (5) contaminated environment or fomite (indirect contact with soil or vegetation, contact with water, indirect transmission by contaminated inanimate objects). Where no transmission route was documented, this information was assumed to be unknown, and these pathogens were excluded from the analysis (n = 3), leaving a total of 145 records in our database (Supplemental Table 1 ).

To determine whether zoonotic diseases are more likely to be transmitted through specific pathways, we first assigned all transmission pathways per pathogen equal weighting as per methods published in Taylor et al. (2001) and calculated the total proportion of zoonotic EIDs transmitted by each pathway at a broad scale. However, for many pathogens, some modes of transmission are more frequently implicated than others. To account for this, we then assigned all transmission pathways per pathogen different weightings based on how likely transmission is to occur through that particular pathway-either ''likely'' or ''less likely'' (Supplemental Table 1 ).

Each pathogen in the analysis received a total value of 1. For those pathogens that could only be transmitted through one pathway (e.g., West Nile), that pathway received the full value of 1 (assuming transmission would occur via that pathway 100% of the time). However, for organisms that could be transmitted by more than one pathway (e.g., rabies), any pathway that is documented in the literature as likely to occur (e.g., rabies infection via a bite or scratch) received a weight of 0.9, while the remaining value was split equally between the less common pathways (e.g., rabies infection via airborne transmission).

Likewise, if the literature indicated that transmission through a particular pathway was rare or unlikely to occur, this pathway received a weight of 0.1 (assuming that transmission via that pathway was likely to occur less than 10% of the time), and the remaining value of 0.90 was split equally among the other pathway(s). If a pathogen could be transmitted via multiple ''unlikely'' or ''rare'' pathways, the value of 0.1 was split equally between those pathways. In the literature, rare or unlikely routes of transmission generally refer to those that have been demonstrated in experimental laboratory studies and/or where only a handful of human cases have been documented (e.g., four cases of rabies attributed to the airborne pathway, two documented suspected cases of tick-borne Q fever; Supplemental Table 1) .

For example, for the bacteria Coxiella burnetii (Q fever), the most common mode of transmission reported is airborne transmission via inhalation of aerosols from contaminated soil or animal waste. More rare modes of transmission to humans include tick bites and ingestion of unpasteurized milk or dairy products (Anderson et al. 2013) . For this pathogen, we assigned the common pathway, the airborne pathway, a probability of 0.9, whereas the two less common pathways were assigned a probability of 0.05 each (Supplemental Table 1 ).

We then examined which transmission pathways were most likely to occur within various EID drivers, as defined in Smolinski et al. (2003) and Morse (1995) and modified by Jones et al. (2008) . These drivers are largely environmental, ecological, political, economic, and social forces, functioning on a range of different scales, which facilitate the expansion and adaptation of a pathogen to a new niche (Smolinski et al. 2003 ). The first classification of these drivers was published in 1992 by the Institute of Medicine (IOM). This report identified six factors in the emergence of infectious diseases including: (1) Human demographics and behavior, (2) technology and industry, (3) economic development and land use, (4) international travel and commerce, (5) microbial adaptation and change, and (6) breakdown of public health measures. Seven additional drivers were added in a followup IOM report in 2003 (Smolinski et al. 2003) including: ''human susceptibility to infection,'' ''climate and weather,'' ''changing ecosystems,'' ''poverty and social inequity,'' ''war and famine,'' ''lack of political will,'' and ''intent to harm.'' To calculate which pathways were associated with each EID driver, we split the data into subsets by driver, summed the weights for each transmission category, and divided the sum by the total number of EID events per driver.

Last, to determine whether transmission routes differ significantly by disease driver, we used a permutation t-test (Hothorn et al. 2006 (Hothorn et al. , 2008 to compare all possible pairwise disease drivers (121 possible combinations). To minimize the probability of detecting false positives (i.e., decrease Type I error) we selected a rejection alpha of 0.10 (Quinn and Keough 2002) . This level of significance was selected given the inherent reporting bias of EID data (e.g., developed countries tend to report more than other countries) and the low sample size for some of the drivers (e.g., Bushmeat n = 4).

In a previous study, Taylor et al. (2001) included transmission route as a potential risk factor for human disease emergence. In their analysis, if an organism could be transmitted by more than one transmission pathway, all were included with equal weighting. They found that zoonotic diseases were more likely to be transmitted by vectors (by biting or mechanical transfer by arthropods) or indirect contact (via food or an environmental reservoir). In accordance with their results, we found that when all pathways in our analysis received an equal weighting, zoonotic diseases were more likely to be transmitted by the same two pathways that Taylor identified-oral transmission and by vectors (Supplemental Table 1 ; Fig. 1 ). Because some pathogens could be transmitted by more than one pathway, we found that, at a broad scale, 42% of all zoonotic pathogens were transmitted through oral transmission, 42% via vector-borne, 36% by airborne transmission, 29% by direct contact, and 24% via contact with a contaminated environment or fomite.

When broken down by pathogen type, the majority of zoonotic EIDs were viral and bacterial in origin, with smaller proportions of helminth, fungi, and prion-origin diseases. For viruses, the vector-borne route of transmission was the most common, followed by airborne transmission and direct animal contact. Very few viral EIDs were transmitted through the foodborne pathway through exposure to a contaminated environment, or via fomites. For bacteria, transmission was most likely to occur through the foodborne, contaminated environment, and direct-contact pathways. Fewer bacterial EIDs were transmitted through the airborne and vector-borne pathways. The majority of rickettsial and protozoal infections were likely transmitted through vectors, whereas the direct contact and airborne pathways were most relevant for fungal diseases. Last, the oral transmission pathway was most relevant for helminth and prion-driven diseases.

When ranked by primary EID driver; land-use change, agricultural industry change, and international travel and commerce are globally the top three drivers of zoonoses ( Fig. 1) . At the broad scale, our results indicate that all pathways are approximately equally common, yet when stratified by EID driver, the relevant transmission pathways vary greatly (Fig. 2) .

Results from the permutation t-tests indicate that transmission pathways differ significantly between many disease drivers (Table 1) . For example, zoonotic diseases attributed to land-use change were more likely to be transmitted via the vector-borne pathway (52.5%), followed by direct animal contact (23.8%), the airborne pathway (19.8%), and a smaller proportion from the contaminated environment (2%) and oral transmission pathways (2%; Fig. 2 ). For food industry change, the most relevant pathway is oral transmission (58%) (Fig. 2) .

An important challenge to developing effective prevention and control strategies for zoonotic EIDs is identifying the relevant transmission routes between reservoir hosts and humans. In many cases, primary pathways for transmission are identified during outbreak investigations after a disease has already emerged. However, it is likely that many more novel transmission pathways remain undescribed for pathogens that pose a potential human health risk. The recent emergence of a number of pandemic zoonoses (e.g., severe acute respiratory syndrome [SARS] , pandemic influenza H1N1), zoonotic viruses of pandemic potential (e.g., Middle East respiratory syndrome [MERS] coronavirus), and those of regional concern (e.g., Ebola virus), in addition to the increasing frequency of EID events ( Jones et al. 2008) , make the targeting of surveillance programs to early stages of emergence a crucial tool for combating pandemics (Morse et al. 2012 ). Our analysis also provides us with insights into which transmission pathways are important in regions where different drivers of EIDs predominate, thus allowing for more targeted prevention measures and surveillance approaches. This approach may have value for targeted surveillance of pathogens that are known to emerge through different pathways. For instance, although Nipah virus (NiV) has been attributed to preferential feeding by Pteropus bats on human-cultivated fruit or other plant products, as was seen with NiV in Malaysia (transmitted via mangoes fed to pigs) and Bangladesh (via date palm sap) (Chua et al. 2000 , the mechanism through which the disease emerged differed between countries. In Malaysia, agricultural intensification led to the planting of mango trees directly adjacent to intensively managed pig populations, attracting fruit bats to the area. This activity resulted in the initial spillover of NiV from Pteropus bats into intensively managed pig populations, leading to subsequent outbreaks to human via direct contact with live pigs (Chua 2003 , Epstein et al. 2006 , Pulliam et al. 2012 . In Bangladesh, the most frequently implicated transmission pathway from animals to people is via the oral transmission pathway, specifically ingestion of fresh date palm sap contaminated by bat excreta (Hughes et al. 2009 , Olival et al. 2013 . By targeting this and other potential routes of food contamination, surveillance efforts and control measures may better minimize the risk of zoonotic disease emergence.

A more apparent example highlighting the differences in transmission routes between drivers is land-use change versus climate and weather. Land-use change is the leading driver for emerging zoonoses (Fig. 1) and is likely to increase in the future (Smolinski et al. 2003 , Patz et al. 2004 , Murray and Daszak 2013 . Our results suggest that disease emergence in regions under pressure from land use change has most often occurred through the vector-borne pathway and direct animal contact. Thus, effective control measures in regions of active land use change could be focused on ensuring or enhancing vector control (e.g., larval control through environmental management, use of mosquito nets, etc.), preventing transmission through direct animal contact (e.g., use of personal protective equipment, hand washing), and reducing risk of airborne transmission (e.g., use of mask or light cloth to prevent inhalation of infectious agents in high-risk occupations or in areas where land disruption activities are occurring).

Infectious diseases driven by climate and weather are more likely to be transmitted via the oral transmission and vector-borne pathways. The relevance of these particular pathways to this driver can be explained by examining the events in our database linked to climate and weather. For instance, there are 10 events/pathogens that are associated with climate and weather including five Vibro spp., Coccidioides immitis (Valley fever), and several vector-borne diseases, including Murray Valley encephalitis virus, Sindbis virus, St. Louis encephalitis virus, and Zika virus. Previous work has shown climate-related increases in sea surface temperature and sea level can lead to a higher incidence of food-borne (shellfish poisoning) and waterborne infectious diseases, highlighting the importance of the oral transmission pathway. In fact, recent studies examining climate change impacts on human and animal health found a positive correlation between the prevalence of Vibro spp., including V. cholerae, and increasing sea surface temperature. Similarly, changes in climate have also been shown to affect disease transmission of many vector-borne diseases by shifting the vector's geographic range, increasing reproductive and biting rates and by shortening the incubation period (Vezzulli et al. 2013 , Patz et al. 2005 . Although this finding is not surprising, it does allow us to identify strategies to reduce oral transmission (e.g., food safety measures) and vector-borne transmission in regions with changing climates.

In summary, these findings provide us with a novel approach to identifying and understanding all the ways by which a pathogen might spill over from nature into human hosts. These results may be particularly useful to target the growing number of efforts to discover and characterize new pathogens before they emerge in human populations, coupled with efforts that identify potential zoonotic pathogen reservoirs and specific high-risk behavior in a region. Our results could be compiled into a catalogue of relevant transmission pathways and reservoirs. Targeting pathogen discovery through this approach could tell us new information about how likely a newly discovered potentially zoonotic pathogen is to emerge. Public health efforts could develop control strategies that focus on behavioral change in high-risk populations to minimize exposure to potential reservoirs and target relevant transmission pathways (e.g., personal protective behaviors for direct animal contact pathway, vector control targeted at larval mosquitos, use of bed nets, etc., for the vector-borne pathway). This approach will allow zoonotic disease surveillance to shift toward a more preemptive strategy and use more targeted public health interventions to prevent zoonotic disease spillover and emergence. This table shows only statistically significant combinations, sorted from highly significant (0.016) to less significant (0.087). We selected an alpha value of rejection of 0.10 (90% level of confidence). Analysis was performed using the coin library in the R statistics program (Hothorn et al. 2006 (Hothorn et al. , 2008 .

LOH ET AL.

",0.7326664247263094
Diagnostic System for Rapid and Sensitive Differential Detection of Pathogens,"Naturally emerging and deliberately released pathogens demand new detection strategies to allow early recognition and containment. We describe a diagnostic system for rapid, sensitive, multiplex discrimination of microbial gene sequences and report its application for detecting 22 respiratory pathogens in clinical samples.","Naturally emerging and deliberately released pathogens demand new detection strategies to allow early recognition and containment. We describe a diagnostic system for rapid, sensitive, multiplex discrimination of microbial gene sequences and report its application for detecting 22 respiratory pathogens in clinical samples.

increasingly important to clinical management and public health. Methods to directly detect nucleic acids of microbial pathogens in clinical specimens are rapid, sensitive, and may succeed when culturing the organism fails. Clinical syndromes are infrequently specific for single pathogens; thus, assays are needed that allow multiple agents to be simultaneously considered. Current multiplex assays employ gel-based formats in which products are distinguished by size, fluorescent reporter dyes that vary in color, or secondary enzyme hybridization assays. Gelbased assays are reported that detect 2-8 different targets with sensitivities of 2-100 PFU or <1-5 PFU, depending on whether amplification is carried out in a single or nested format, respectively (1) (2) (3) (4) . Fluorescence reporter systems achieve quantitative detection with sensitivity similar to that of nested amplification; however, their capacity to simultaneously query multiple targets is limited to the number of fluorescent emission peaks that can be unequivocally resolved. At present, up to 4 fluorescent reporter dyes can be detected simultaneously (5, 6) . Multiplex detection of up to 9 pathogens has been achieved in hybridization enzyme systems; however, the method requires cumbersome postamplification processing (7) .

To address the need for sensitive multiplex assays in diagnostic molecular microbiology, we created a polymerase chain reaction (PCR) platform in which microbial gene targets are coded by a library of 64 distinct Masscode tags (Qiagen Masscode technology, Qiagen, Hilden, Germany). A schematic representation of this approach is shown in Figure 1 . Microbial nucleic acids (RNA, DNA, or both) are amplified by multiplex reverse transcription (RT)-PCR using primers labeled by a photocleavable link to molecular tags of different molecular weight. After removing unincorporated primers, tags are released by UV irradiation and analyzed by mass spectrometry. The identity of the microbe in the clinical sample is determined by its cognate tags.

As a first test of this technology, we focused on respiratory disease because differential diagnosis is a common clinical challenge, with implications for outbreak control and individual case management. Multiplex primer sets were designed to identify up to 22 respiratory pathogens in a single Mass Tag PCR reaction; sensitivity was established by using synthetic DNA and RNA standards as well as titered viral stocks; the utility of Mass Tag PCR was determined in blinded analysis of previously diagnosed clinical specimens.

Oligonucleotide primers were designed in conserved genomic regions to detect the broadest number of members for a given pathogen species by efficiently amplifying a 50-to 300-bp product. In some instances, we selected established primer sets; in others, we used a software program designed to cull sequence information from GenBank, perform multiple alignments, and maximize multiplex performance by selecting primers with uniform melting temperatures and minimal cross-hybridization potential (Appendix Table, available at http://www.cdc. gov/ncidod/eid/vol11no02/04-0492_app.htm). Primers, synthesized with a 5′ C6 spacer and aminohexyl modification, were covalently conjugated by a photocleavable link to Masscode tags (Qiagen Masscode technology) (8, 9) . Masscode tags have a modular structure, including a tetrafluorophenyl ester for tag conjugation to primary amines; an o-nitrobenzyl photolabile linker for photoredox cleavage of the tag from the analyte; a mass spectrometry sensitivity enhancer, which improves the efficiency of atmospheric pressure chemical ionization of the cleaved tag; and a variable mass unit for variation of the cleaved tag mass (8, (10) (11) (12) primer sets are labeled with distinct molecular weight tags. Thus, amplification of a microbial gene target produces a dual signal that allows assessment of specificity. Gene target standards were cloned by PCR into pCR2.1-TOPO (Invitrogen, Carlsbad, CA, USA) by using DNA template (bacterial and DNA viral targets) or cDNA template (RNA viral targets) obtained by reverse transcription of extracts from infected cultured cells or by assembly of overlapping synthetic polynucleotides. Assays were initially established by using plasmid standards diluted in 2.5-µg/mL human placenta DNA (Sigma, St. Louis, MO, USA) and subjected to PCR amplification with a multiplex PCR kit (Qiagen), primers at 0.5 µmol/L each, and the following cycling protocol: an annealing step with a temperature reduction in 1°C increments from 65°C to 51°C during the first 15 cycles and then continuing with a cycling profile of 94°C for 20 s, 50°C for 20 s, and 72°C for 30 s in an MJ PTC200 thermal cycler (MJ Research, Waltham, MA, USA). Amplification products were separated from unused primers by using QIAquick 96 PCR purification cartridges (Qiagen, with modified binding and wash buffers). Masscode tags were decoupled from amplified products through UV light-induced photolysis in a flow cell and analyzed in a single quadrapole mass spectrometer using positive-mode atmospheric pressure chemical ionization (Agilent Technologies, Palo Alto, CA, USA). A detection threshold of 100 DNA copies was determined for 19 of 22 cloned targets by using a 22-plex assay ( Table 1) .

Many respiratory pathogens have RNA genomes; thus, where indicated, assay sensitivity was determined by using synthetic RNA standards or RNA extracts of viral stocks. Synthetic RNA standards were generated by using T7 polymerase and linearized plasmid DNA. After quantita-tion by UV spectrometry, RNA was serially diluted in 2.5µg/mL yeast tRNA (Sigma), reverse transcribed with random hexamers by using Superscript II (Invitrogen, Carlsbad, CA, USA), and used as template for Mass Tag PCR. As anticipated, sensitivity was reduced by the use of RNA instead of DNA templates ( Table 1 ). The sensitivity of Mass Tag PCR to detect live virus was tested by using RNA extracted from serial dilutions of titered stocks of coronaviruses (severe acute respiratory syndrome [SARS] and OC43) and parainfluenzaviruses (HPIV 2 and 3) . A 100-µL volume of each dilution was analyzed. RNA extracted from a 1-TCID 50 /mL dilution, representing 0.025 TCID 50 per PCR reaction, was consistently positive in Mass Tag PCR.

RNA extracted from banked sputum, nasal swabs, and pulmonary washes of persons with respiratory infection was tested by using an assay panel comprising 30 gene targets that represented 22 respiratory pathogens. Infection in each of these persons had been previously diagnosed through virus isolation, conventional nested RT-PCR, or both. Reverse transcription was performed using random hexamers, and Mass Tag PCR results were consistent in all cases with the established diagnosis. Infections with respiratory syncytial virus, human parainfluenza virus, SARS coronavirus, adenovirus, enterovirus, metapneumovirus, and influenza virus were correctly identified (Table 2 and to RNA obtained from banked samples of cerebrospinal fluid and brain tissue that had been previously characterized by conventional diagnostic RT-PCR. Two of 3 cases of West Nile virus encephalitis were correctly identified. Eleven of 12 cases of enteroviral meningitis were detected representing serotypes CV-B2, CV-B3, CV-B5, E-6, E-11, E-13, E-18, and E-30 (data not shown).

Our results indicate that Mass Tag PCR is a sensitive and specific tool for molecular characterization of microflora. The advantage of Mass Tag PCR is its capacity for multiplex analysis. Although the use of degenerate primers (e.g., enteroviruses and adenoviruses, Appendix Table and Table 1 ) may reduce sensitivity, the limit of multiplexing to detect specific targets will likely be defined by the maximal primer concentration that can be accommodated in a PCR mix. Analysis requires the purification of product from unincorporated primers and mass spectroscopy. Although these steps are now performed manually, and mass spectrometers are not yet widely distributed in clinical laboratories, the increasing popularity of mass spectrometry in biomedical sciences and the advent of smaller, lower-cost instruments could facilitate wider use and integrated instrumentation. In addition to developing additional pathogen panels, our continuing work is focused on optimizing multiplexing, sensitivity, and throughput. Potential applications include differential diagnosis of infectious diseases, blood product surveillance, forensic microbiology, and biodefense.

",0.732406820134717
Clinical Communications SARS AND ITS IMPACT ON CURRENT AND FUTURE EMERGENCY DEPARTMENT OPERATIONS,"e Abstract-A long-standing concern for international spread of new, virulent pathogens became a reality with the advent of Severe Acute Respiratory Syndrome (SARS). This respiratory syndrome, caused by a coronavirus, spread rapidly across 30 nations since its first recognition in late 2002. SARS has presented the greatest recent threat to U.S. public health, and has come at a time when purposeful introduction of pathogens by terrorists is also of heightened concern. SARS has forced the international medical establishment to reexamine how best to manage such incidents.","For many years, public health experts have predicted the advent and rapid transit of epidemics via international travel and commerce. A long-standing concern for the introduction of new pathogens to the United States became real during the migration of the West Nile Virus in the summer of 2002, and was accentuated by the unexpected emergence of Severe Acute Respiratory Syndrome (SARS). Since the first cases in the Guangdong province of China in November 2002, and Vietnam, Hong Kong, and Canada in February 2003, SARS has been reported in nearly 30 different countries (1) . As of October 2003, SARS has afflicted more than 8000 individuals and caused 774 deaths worldwide (2) . In the United States, there have been as many as 164 cases, but no reported fatalities thus far (3) . SARS is the first new disease to necessitate involuntary quarantine measures in the United States since 1983 (4, 5) .

As the emergence and progression of SARS and similar epidemics have occurred rapidly, printed medical journals have been increasingly challenged to keep pace with developments, and health care professionals are now increasingly utilizing the World Wide Web for up-to-date information (6) . SARS also underscores the need for a more coordinated effort from the medical community, including increased communication, integration of multiple departments and services, and an enhanced system of biosurveillance (7) .

Though SARS has been on the decline in human populations since July 2003, there is growing concern that the syndrome may reemerge as colder weather sets in (8, 9) . Because Emergency Departments (EDs) are primary portals of access to health care, Emergency Physicians will be responsible for the identification and safe management of new SARS cases, and thus need education about the condition. The emergence of SARS suggests that the ED may need to be an increased resource in public health policy and management.

Two laboratories have identified the infectious agent of SARS independently as a coronavirus (10) . The members of Coronaviridae are large, enveloped RNA viruses that cause diseases in humans and domestic animals. Disease caused by a coronavirus was first described in 1931; human coronaviruses were first identified in the 1960s (11) . Little attention previously had been paid to these pathogens due to their relatively innocuous nature. Previously described species include HCV-229E and HCV-OC43, both of which cause respiratory infections. Although coronaviruses primarily infect respiratory mucosa, they also have been demonstrated in gastrointestinal mucosa (12) . This may explain the gastrointestinal symptoms noted in nearly 20% of SARS cases (13) .

A key pathogenic feature of Coronaviridae is the high frequency of RNA recombination, which allows for rapid mutation and evolution (14) . Human coronaviruses are also relatively resilient, retaining infectivity for as long as 6 days in suspension and 3 h after drying on surfaces (15) . Previous coronavirus-associated respiratory diseases have incidences ranging from 5.1-18%, and, in many cases, are associated with preexisting pulmonary or cardiac disease (16 -18) . Mortality from such infections has been associated with advanced age and immunocompromised status (19, 20) .

The incubation period of SARS was initially estimated at 2 to 7 days (21), however, some cases support an interval of up to 16 days between exposure and symptoms; the median incubation period in the Hong Kong case series published by Lee et al. was 6 days (13). As with most viral illnesses, clinically differentiated SARS is preceded by a prodrome of nonspecific symptoms. SARS cases were initially defined by four criteria that were subsequently modified by the Centers for Disease Control and Prevention (CDC) on July 18, 2003 (Table 1) (21, 22) .

Common features of the prodrome include chills, rig- Temperature of Ͼ 100.4°F (Ͼ 38°C), and One or more clinical findings of respiratory illness (e.g., cough, shortness of breath, difficulty breathing, or hypoxia), and radiographic evidence of pneumonia, or respiratory distress syndrome, or autopsy findings consistent with pneumonia or respiratory distress syndrome without an identifiable cause Epidemiologic criteria Travel (including transit in an airport) within 10 days of onset of symptoms to an area with current or recently documented or suspected community transmission of SARS, or Close contact within 10 days of onset of symptoms with a person known or suspected to have SARS infection Laboratory criteria Confirmed Detection of antibody to SARS-associated coronavirus (SARS-CoV) in a serum sample, or Detection of the SARS-CoV RNA by RT-PCR confirmed by a second PCR assay, by using a second aliquot of the specimen and a different set of PCR primers, or Isolation of SARS-CoV Negative

Absence of antibody to SARS-CoV in a convalescent-phase serum sample obtained Ͼ28 days after symptom onset. Undetermined

Laboratory testing either not performed or incomplete Case classification Probable case: meets the clinical criteria for severe respiratory illness of unknown etiology and epidemiologic criteria for exposure; laboratory criteria confirmed or undetermined. Suspect case: meets the clinical criteria for moderate respiratory illness of unknown etiology, and epidemiologic criteria for exposure; laboratory criteria confirmed, negative, or undetermined Exclusion criteria A case may be excluded as a suspect or probable SARS case if:

An alternative diagnosis can fully explain the illness. The case has a convalescent-phase serum sample (i.e., obtained Ͼ 28 days after symptom onset) that is negative for antibody for SARS-CoV. The case was reported on the basis of contact with an index case that was subsequently excluded as a case of SARS, provided other possible epidemiologic exposure criteria are not present.

* As of July 18, 2003 (17) .

ors, myalgias, headache, and malaise. Less common to the prodrome are mild respiratory symptoms, nausea, vomiting, and diarrhea. Rash and lymphadenopathy have not yet been reported (13, 21) . Three to seven days after the onset of prodromal symptoms, the lower respiratory phase develops, heralded by dyspnea, non-productive cough, and hypoxia. In severe cases, respiratory failure develops, necessitating mechanical ventilation, and causing mortality in approximately 9% of cases thus far. The mortality rate for patients admitted with SARS is related to age and comorbidities, with the elderly and those with diabetes or cardiac disease at increased risk (13, 23) .

Patients presenting with compatible symptoms should be questioned about possible travel to endemic regions; barring close contact with a SARS-suspect patient, this is the current sine qua non of diagnosis, though this may change as cases become more widespread. Radiographic findings may be non-specific or absent (21) . The CDC Preliminary Clinical Description included focal interstitial infiltrates that progress to generalized interstitial infiltrates, and late consolidation (21), whereas the Lee et al. Hong Kong series reported early, focal airspace consolidation in a high proportion of patients, with all patients developing airspace opacities at some point during the illness (13) . A recent retrospective review of chest radiographs in confirmed SARS cases revealed an initial abnormality in 78.3% of cases. The most suggestive findings included peripheral location of infiltrates, progression from unilateral focal to bilateral or multifocal airspace opacities, and lack of cavitation, lymphadenopathy, and pleural effusions (24) . As the case definition also includes a reference to Acute Respiratory Distress Syndrome (ARDS) without clear source, any compatible chest radiograph in the timeframe of an ongoing epidemic should be considered consistent with a diagnosis of SARS (22) .

Laboratory findings available in the acute setting are non-specific. Many patients will manifest a moderate leukopenia and/or thrombocytopenia (21) . Of the leukocyte lines, lymphocytes tend to be the most prominently suppressed (13, 21) . More specific laboratory evidence of SARS may be sought in the form of serum antibodies to the SARS-associated coronavirus in the serum, or by detection of viral RNA in respiratory samples by reverse transcriptase polymerase chain reaction (25) . Although several international companies are developing rapid laboratory tests, these assays are not currently available for use in ED decision-making. Recent evidence also suggests that mutation of the SARS virus could impact laboratory identification of cases as well as the utility of a single vaccine (26) .

Overall, it seems prudent to recommend that patients meeting clinical criteria for SARS have pulse oximetry, chest radiograph, and a complete blood count with differential included in their ED workup. Though the findings of these tests are not diagnostic, they may be sup-portive of a diagnosis of SARS, and thereby may mandate serological testing, which could be done once the patient is in isolation.

Due to the recent nature of the SARS outbreak, there has not been sufficient time to identify an effective treatment. Several existing treatment regimens have met with anecdotal success. Ribavirin has been the most studied agent among these, however, there is not enough conclusive data to support its widespread use (27) . Human interferons have shown early promise as either a primary or adjunctive therapy, but further testing is needed (28) . Large-scale prospective research studies have yet to be completed. Furthermore, as with HIV, the rapid mutation of the coronavirus and the evolution of several strains may frustrate the development of a specific vaccine for some time (21) . As a result, current treatment for SARS consists primarily of supportive care, including mechanical ventilation as needed.

Previously, exposure to airborne pathogens has been a minor concern due to the relatively low rate of tuberculosis and a lack of significant untreatable airborne illnesses. SARS has now forced a change in thinking. Due to its relatively high infectivity, SARS may present a significant risk to the pre-hospital provider (29) . Assisted ventilation, endotracheal intubation, and close providerpatient proximity in the back of ambulances account for this increased risk (30) . Even before the arrival of SARS, some first responders were reluctant to provide mouthto-mouth assisted ventilations, even with a barrier device (31) . The potential presence of an infection like SARS can be expected to make first responder intervention less likely. Current CDC guidelines recommend full personal protective equipment (PPE), outlined below, for the transport of any known SARS patient. Unfortunately, this diagnosis may not be known during the initial evaluation (32) . Further research is needed toward developing respiratory devices to protect the pre-hospital provider while allowing for adequate airway management.

During peak ED flow, ""well-appearing"" patients often experience delays before full triage assessment. In a SARS patient, this delay potentially allows the patient to contaminate the waiting room and infect others. As suggested by Schull and Redelmeier, the presence of a single unrecognized SARS patient in a crowded ED may have created the epicenter of an outbreak (33) . During a period of SARS outbreak or concern, triage protocols should be modified so that any patient with a respiratory complaint or fever is immediately asked about travel to areas with community transmission of SARS as well as any exposure to a patient with suspected or probable SARS (34) .

Placing signs at all ED entrances with instructions to go directly to the triage nurse if either of these contact criteria are met may further reduce the risk of waiting room contamination. If either of those conditions is met, the patient should then put on a surgical mask and be moved immediately to a negative pressure isolation room, if available. Registration and assessment may then continue after ED personnel employ CDC-recommended PPE: gown, gloves, N95 respirator, and eye protection (35) . Strict hand washing also must be exercised (34) . Health care providers with PPE can then proceed in determining if the patient meets criteria for SARS (22) . All used PPE should be discarded in biohazard receptacles and all reusable equipment should be properly sanitized (32) . Once disposition of a SARS patient is completed, a terminal cleaning of the room should be done with standard hospital disinfectant (36) . The decision to involuntarily quarantine a patient for suspected SARS will require serious consideration as well as some adjustments to normal ED operations. The Department of Health and Human Services (HHS) has granted involuntary quarantine authority to the U.S. Coast Guard, U.S. Customs Service, and individual state health departments (5). This authority was exercised for a tourist with SARS symptoms who was involuntarily held for 10 days in the State of New York as well as for an airline passenger returning to Minnesota from the Philippines through San Francisco (37, 38) . The HHS guidelines were then inactivated in June 2003, due to the cessation of new human cases (39) . The power to quarantine rests with government officials, but because the ED is the primary portal of entry to most hospitals, it is logical to surmise that the ED would become a major site of any quarantine action should the guidelines need to be reactivated.

Another difficulty in managing SARS patients in the ED is how to protect the health care team while caring for the critically ill patient. Nebulizer treatments and the coughing the patient produces increase the production of infectious droplets (32) . Isolation rooms are not universally available. When available, they often do not allow close observation, and may be too small for urgent interventions such as central venous access or airway control. As such, they may not be practical for a critically ill SARS patient. At present, the most prudent solution is to place the patient in an appropriate resuscitation room as far from patient flow as possible, use of full PPE for all involved personnel, and post-resuscitation cleaning as outlined above. In the future, there may be an increasing trend toward respiratory isolation rooms with telemetry and video monitoring capabilities.

Perhaps the most significant issue for Emergency Medicine with regards to the SARS outbreak is the potential for a mass casualty situation and subsequent overwhelming of ED resources. With current CDC guidelines, the key discriminator that separates a patient with potential SARS from a multitude of other ED patients with respiratory complaints is the travel or exposure history. If widespread community transmission occurs in the United States, then it will become exceedingly difficult to separate out the SARS cases. Likely this will be combined with a much greater number of patients presenting without SARS but with similar symptoms and heightened concern for what their symptoms represent. One solution may be placing all patients with any historical concern for SARS in any available isolation measure, even if this means possibly mixing those with and without SARS. Other less attractive alternatives include more stringent triage guidelines, which may prolong the movement of suspected patients out of the waiting room, or the use of isolation rooms throughout the hospital for triage to minimize the mingling of patients, which may contaminate other sections of the hospital. Any overwhelming or shutdown of a hospital, as has already occurred in Beijing, will increase the burden on the remaining local EDs as well as adversely affect those patients who will have to seek their medical care elsewhere (40) .

Although many details about SARS transmission are not yet known, thus hindering the development and implementation of specific actions, new general measures should be considered and adopted. One of the lessons learned from Hong Kong is the importance of a strong and coordinated response by the health care community (41) . Individual health departments, Emergency Medical Services (EMS) base stations, and individual hospitals should form local protocols to conserve resources as well as to minimize confusion and error during an actual event. Prehospital protocols should direct potential SARS cases to the hospitals best able to evaluate and treat these cases. Other factors to consider in forming protocols include: availability of isolation rooms in the ED and hospital, Intensive Care Unit (ICU) capabilities, diversion contingency plans, inventories and supply of PPE and medications, periodic interdepartmental tabletop exercises, regular review of the hospital disaster plan, crowd and media control, security, notification of public health and government officials, and new laboratory capabilities as they become available. 

Over the past year, SARS has adversely affected millions of people and cost international economies hundreds of billions of dollars. It has frustrated public health officials and stymied research efforts. Recent successes have occurred in controlling the SARS epidemic despite the lack of specific details; primarily through general health measures and coordinated public health policies. Although the United States has thus far escaped the brunt of the SARS outbreak, the experiences of other affected countries have raised numerous concerns about our ability to respond to this rapidly emerging infection. SARS is both a considerable ongoing threat and a harbinger of the increasing incidence, scope, and virulence of emerging infections that the American public and emergency personnel will continue to face in the 21 st century.

",0.7317110971521712
New technologies in computer-aided drug design: Toward target identification and new chemical entity discovery,"In the postgenomic era, computer-aided drug design (CADD) has considerably extended its range of applications, spanning almost all stages in the drug discovery pipeline, from target identification to lead discovery, from lead optimization to preclinical or clinical trials.","Drug research and development (R & D) is comprehensive, expensive, time-consuming and full of risk. It is estimated that a drug from concept to market would take $12 years and cost more than US$800 million on an average [1] . Several new technologies have hence been developed and applied in drug R & D to shorten the research cycle and to reduce the expenses. Computer-aided drug design (CADD) is one of such evolutionary technologies [2] .

Having emerged as a quantitative structure-activity relationship (QSAR) analysis in the early 1960s, the concept of CADD has evolved very quickly, especially in the recent decade as an unprecedented development of structural biology and computer capabilities. CADD technologies including molecular modeling and simulation have become promising in drug discovery. Recently, CADD has even been used in designing highly selective ligands for a certain target that shares very similar structures with many proteins, which is difficult to be done by other methods. One such example is the rational design of selective inhibitors of p90 ribosomal protein S6 kinase [3] . In the postgenomic era, owing to the dramatic increase of small molecule and biomacromolecule information, CADD tools have been applied in almost every stage of drug R & D, greatly changing the strategy and pipeline for drug discovery [2] . As indicated in Fig. 1 , CADD, from its traditional application of lead discovery and optimization, has extended toward two directions: upstream for target identification and validation, and downstream for preclinical study (ADMET prediction) . In this review, we highlight some recent advances of CADD technologies; emphases are put on computational tools for target identification and new chemical entity discovery.

Target identification and validation is the first key stage in the drug discovery pipeline (Fig. 1) . However, identification and validation of druggable targets from among thousands of candidate macromolecules is still a challenging task [4] . Numerous technologies for addressing the targets have been developed recently. Genomic and proteomic approaches are the major tools for target identification. For example, a proteomic approach for identification of binding proteins for a given small molecule involves comparison of the protein expression profiles for a given cell or tissue in the presence or absence of the given molecule. This method has not been proved very successful in target discovery because it is laborious and time-consuming [5] . Therefore, complementary to the experimental methods, a series of computational (in silico) tools have also been developed for target identification. They can be cataloged into sequence-based approach and structure-based approaches.

Sequence-based approach contributes to the processes of target identification by providing functional information about target candidates and positioning information to biological networks. For those diseases caused by external pathogens such as bacteria and viruses, unique targets might be found in the pathogens by comparing functional genomics from humans with the corresponding genomics from pathogens [6] . For example, Dutta et al. used a subtractive genomic method to analyze the completed genome of Helicobacter pylori (H. pylori) and identified a set of genes that are likely to be essential to the pathogen but are absent in humans [7] . In theory, it is possible to recognize all the targets in the pathogen in this way; whereas for endogenous diseases, targets could be discovered by analyzing the differences of genomics between normal and abnormal tissues. A good example of this issue is that several novel steroid targets were identified by combinative use of bioinformatics and functional analysis of hormone response elements [8] .

Structure-based approach that has shown promise in recent years is to use computational methods to find putative binding proteins for a given compound from either genomic or protein databases, and to subsequently use experimental procedures to validate the computational result [9] . One such computational approach, which is the reverse of docking a set of ligands into a given target, is to dock a compound with a known biological activity into the binding sites of all the three-dimensional (3D) structures in a given protein database. Protein 'hits' identified in this manner can then serve as potential candidates for experimental validation. Accordingly, this approach is referred to as reverse docking (or inverse docking) [10, 11] . The general procedure of target identification by using reverse docking integrating with biological technologies is shown in Fig. 2 . It includes four steps: reverse docking of a small molecule to select hit proteins; hit proteins postprocessing of through bioinformatic analysis to select candidates; experimentally validating by using biochemical and/or cellular assays; and finally, if it is possible and necessary, determining the X-ray crystal (or NMR) structures of the small molecule-protein complexes to verify the target at the atomic level. This approach requires a sufficient number of known protein structures covering a diverse range of drug targets (Fig. 2) . The protein structures are usually selected from the protein data bank (PDB) [12] or constructed with protein structure prediction method. Using a subset of PDB as an example, Paul et al. successfully recovered the corresponding targets of four unrelated ligands with the help of reverse docking method [13] .

A reverse docking web server, Target Fishing Dock (TarFis-Dock), was also developed for identifying new drug targets [11] . For this server, a potential drug target database (PDTD) was constructed. The target proteins collected in PDTD were selected from the literatures and from several online Figure 1 . Drug discovery pipeline vs. computer-aided drug design (CADD) tools. CADD tools have been initially developed for lead optimization (such as QSAR) and then expanded for lead discovery (such as virtual screening). Now CADD tools have quickly extended toward both upstream and downstream directions along the drug discovery pipeline. In upstream direction, bioinformatics and reverse docking methods are usually used for target identification; once a target is identified, in silico methods are also developed to predict its 3D structures before experimental determination; computational methods can be used to predict target druggability and to design compounds before further experiments. In downstream direction, in silico ADMET prediction and physiologically based pharmacokinetic simulations can be conducted to model the preclinical test, which is usually integrated into in silico lead discovery stage to reduce the costs.

databases, such as DrugBank. The structures of proteins were from PDB. Recently, TarFisDock has been proved as a tool of great potential value for identifying the target of anti-H. pylori natural product [14] . Colonization of the human stomach by the bacterium H. pylori is a major causative factor for gastrointestinal illnesses and gastric cancers. However, discovery of anti-H. pylori agents is a difficult task because of lack of mature protein targets. Therefore, identifying new molecular targets for developing new drugs against H. pylori is obviously necessary. The in-house potential drug target database was searched by using the reverse docking tool TarFisDock, taking the active natural product (Fig. 2a ) discovered by anti-H. pylori screening as a probe. Homology search revealed that among the 15 candidates discovered by reverse docking, only diaminopimelate decarboxylase and peptide deformylase (PDF) have homologous proteins in the genome of H. pylori. Enzymatic assay demonstrated the natural product and one of its derivatives are the potent inhibitors against the H. pylori PDF (HpPDF) with IC 50 values of 10.8 and 1.25 mM, respectively ( Fig. 2e ). X-ray crystal structures of HpPDF and the complexes of HpPDF with the natural product and its analog were also determined ( Fig. 2f ), demonstrating at the atomic level that HpPDF is a potential target for screening new anti-H. pylori agents.

The advantage of reverse docking is obvious: in addition to identifying target candidates for active compounds, it is also possible to identify potential targets responsible for toxicity and/or side effects of a drug supposing that the target database contains all the possible targets [15] . However, reverse docking still has certain limitations. The major one is that the protein entries in the protein structure databases, like the PDB, are not enough for covering all the protein information of disease-related genomes. The second one is that this approach has not considered the flexibility of proteins during docking simulation. These two aspects will produce negative false. Another limitation is that the scoring function for reverse docking is not accurate enough, which will produce positive false [11] . One tendency to overcome these shortages were expressed, and the binding affinities and/or inhibition or activation activities of the probes to the target hits were determined. In this way, the target candidates can be selected for further functional validation. (f) The crystal structure of natural product-target candidate (HpPDF) complex. This means that the binding between the natural product and target candidate was verified at the atomic level. The images of the proteins were generated using the PyMol program (http://pymol.sourceforge.net/).

www.drugdiscoverytoday.com is to develop new docking programs including protein flexibility and accurate scoring function. Another tendency is to integrate sequence-based and structure-based approaches [4] .

Drug discovery and development in the past 100 years has been performed only against approximately 500 targets; and in the same period, about 20,000,000 organic compounds including natural products have been synthesized or isolated. However, the use of organic chemicals in drug discovery seems to be out of favor because the existing targets have not been screened by all the available compounds. In addition to this, the completion of the human genome suggests that there are 600-1500 druggable targets for drug intervention to control human diseases [16] . Therefore, it is believable that a large number of new drugs, at least many leads or hits, are hiding in the existing chemical mine. However, digging out this source is a hard task. Collecting all the existing compounds and screening them randomly are extremely unpractical, because it is intolerably expensive and timeconsuming although virtual screening shows a dawning to satisfy this requirement [17] . Indeed, recent promising advancement in virtual screening has demonstrated the efficiency of this approach in discovering lead (active) compounds. Virtual screening enriched the hit rate (defined as the number of compounds that bind at a particular concentration divided by the number of compounds experimentally tested) by about 100-1000-fold over random screening. Accordingly, virtual screening has been involved in the pipeline of drug discovery as a practical tool [18, 19] . Nevertheless, as mentioned above, all hits produced from virtual screening are existing compounds or old drugs, that is, virtual screening can only find the new medical usages for the existing compounds or drugs. What big pharmas and medicinal chemists are seeking is new chemical entities (NCEs), which can be strictly protected by the compound patents. There are at least two kinds of CADD methods for NCE discovery, de novo drug design [20] [21] [22] and combinatorial library design [23] .

The de novo drug design does not start from a database of complete molecules but aims at building a complete molecule from molecular bricks ('building blocks') to chemically fill the binding sites of target molecule [24] . The complete chemical entries could be constructed through linking the 'building blocks' together, or by growing from an 'embryo' molecule with the guidance of evaluation of binding affinity. The 'building blocks' could be either atoms or fragments (functional groups or small molecules). But using atoms as 'building blocks' is thought to be inefficient, therefore, it is seldom used nowadays. In the fragment linking approach, the binding site is mapped to identify the possible anchor points for functional groups. These groups are then linked together, and they form a complete molecule. In the sequential-growing approach, the molecule grows in the binding site controlled by an appropriate search algorithm, which evaluates each growing possibility with a scoring function. Different from docking-based virtual screening, fragment-based de novo design can perform sampling in the whole compound space, obtaining novel structures that are not limited in available databases. But the quality of a growing step strongly depends on the previous steps. Any step chemically going wrong would lead to an unacceptable result. For the fragment linking approach, choosing linkers to connect fragments together as complete structures is a problem. The most remarkable drawback of this approach might be the synthetic accessibility of the designed structures.

The advent of combinatorial chemistry is one of the most exciting developments in medicinal chemistry in the last decade. Coupled with automation technologies and highthroughput screening (HTS), it offers great potential for discovering new drug leads. This technology allows thousands or even millions of compounds to be synthesized at the same time. However, many products in the huge library are redundant. It also does not make sense to validate and assay millions of compounds. In addition to this, it was found that the large number of compounds synthesized did not result in the remarkable increase in drug candidates though the number of compounds synthesized and screened has increased by several orders of magnitude [25] . Initially, the focus of combinatorial library design was on selecting diverse sets of compounds on the assumption that maximizing diversity would result in a broad coverage of bioactivity space and hence would maximize the chances of finding drug leads. The creation of diversity through compound libraries has been a central claim and task of combinatorial chemistry since its inception. Suggestions and assumptions on how to assess diversity have been studied during the last decade. To synthesize a chemical library with reasonable size and considerable hit rate, 3D structural information and properties of a studied target should be taken into consideration to filter out redundant compounds [23] . Thus, the critical challenges are firstly to select sets of fragments that have the best potential to be parts of new drug leads for a given target; and secondly to set up proper criteria for product judgment (screening). To overcome the first challenge, three types of virtual libraries have been suggested. They are focused libraries, targeted libraries and primary screening libraries. A focused library is built on the basis of a lead molecule or pharmacophore and is geared toward one particular molecular target. A targeted library is designed for finding drug leads against specific targets. A primary screening library is a large combinatorial library used to randomly find new hits or to design novel scaffolds. To solve the second problem, druglikeness (ADMET) and structural diversity have been introduced into library design to reduce its size and increase its efficiency.

Adopting the advantages of focused library and targeted library, as well as integrating technologies of docking-based virtual screening and druglike (ADMET) analysis, a targetfocused library design method was developed, based on which a software package, called LD1.0, was also developed [23] . The flowchart of LD1.0 is shown in Fig. 3 . Starting with the structures of hits and therapeutic target, the overall skeleton of potential ligands is schematically split into several fragments according to the interaction mechanism and the physicochemical properties of the binding site. Individual fragment library is constructed for each fragment, taking into account the binding features of the fragments to the binding site. Finally, target-focused libraries on the studied target are constructed with the judgments from structural diversity, druglikeness (ADMET) profiles and binding affinities [23] . During the target-focused library design, library-based genetic algorithm was applied to optimize the focused library, and the newly developed druglikeness filter was used to predict the druglike profile of the library [26] . Molecular docking approach was employed to predict the binding affinities of the library molecules with the target.

The quality of fragment libraries is critical to the final focused library. There are at least three ways to construct the fragment libraries [23] . Extracting fragments from known drugs or ligands (inhibitors or activators) of the studied target is an effective approach for collecting building blocks. Homology proteins usually share similar structural features and characteristics, especially at the binding site or active site. Therefore, the ligands for different targets belonging to the same family should share some common fragments. Thus, the fragments for constructing target-focused library could be designed by referring to the structures of the ligands of the homology proteins of the target. Also, fragments could be isolated from the active hits produced through primary screening (HTS and virtual screening) .

The efficiency of the strategy for target-focused library design and screening has been demonstrated by our recent example of discovering human cyclophilin A (CypA) inhibitors [27, 28] . By employing docking-based virtual screening in conjunction with chemical synthesis and bioassay, 14 binders of CypA were discovered, and four of them showed high CypA PPIase inhibition activities with IC 50 values of 2.5-6.2 mM [27] . To discover new chemical entities of CypA inhibitors with more potent activities, a target-focused library was designed based on the structures of the 14 hits and their binding modes to CypA by using the program LD1.0. The binding modes indicated that the small molecular CypA binders can be divided into three parts: part A interacts with the small pocket of CypA (pocket A), part B is located in the large pocket (pocket B) and part L is a linker between A and B, interacting with the 'saddle' pocket between sites A and B (Fig. 4) . LD1.0 selected 5 fragments for part A, 17 fragments Fragments for target-focused library design can be selected according to the interaction modes between primarily screened hits (binders) and target. Accordingly, these fragments are leadlike building blocks for constructing library. When the fragments are connected into whole molecules, they are optimized again for the interaction poses and binding affinities by using docking. This image was generated using the PyMol program. Remarkably, both the binding affinity and the inhibitory activity of the most potent compound increased $10 times than that of the most active compound discovered in the first cycle of discovery [28] .

The technological progress of CADD brought a paradigm change to both pharmas and research institutions: it was now possible to obtain appropriate hits within several weeks because of the contribution of CADD [29] . Traditionally, structure-based and pharmacophore techniques and QSAR are major tools for CADD. Parallel to the development of combinatorial chemistry and HTS since more than a decade ago, several new technologies, such as library design, virtual screening, druglike analysis and ADMET prediction, have become important tools in the computer-aided discovery of new drugs. In the coming future, in addition to improving individually existing CADD techniques, such as increasing the accuracy and effectiveness of virtual screening, one major tendency of CADD technology development will be to integrate computational chemistry and biology together with chemoinformatics and bioinformatics. This will leading to a new topic known as pharmacoinformatics, which will impact the pharmaceutical development process and increase the success rate of development candidates [30] . Another tendency is that CADD technologies have been entering into the functional genomic studies and target identification in particular. After the completion of the human genome and numerous pathogen genomes, efforts are underway to understand the role of gene products in biological pathways and human diseases and to exploit their functions for the sake of discovering new drug targets [31] . Small and cell-permeable chemical ligands are used increasingly in genomic approaches to understand the global functions of genomes and proteomes. This approach is referred to as chemical biology (or chemogenomics) [32] . As such, reverse docking can be referred to as computational chemical biology, which has been proven to be an effective way in finding clues of new targets [11] [12] [13] [14] [15] . On the contrary, the CADD techniques like virtual screening and library design can also be used to design small molecule probes for illuminating the molecular mechanisms underlying biological processes through altering or perturbing the functions of target proteins by inhibiting or activating their normal functions [17, 32] .

In postgenomic era, the concept of computer-aided drug design (CADD) has extended from lead discovery to target identification, from lead optimization to preclinical or clinical trials. Two approaches for in silico target identification: sequence-based and structure-based. In sequence-based approach, bioinformatic methods are applied to analyze and compare multiple sequences and identify potential targets from scratch; whereas in structure-based approach, reverse-docking methods might be helpful to identify target candidates for active compounds. At least two in silico strategies for the discovery of NCE: de novo drug design and combinatorial library design, especially target-focused library design, which has been demonstrated by our recent example of discovering human cyclophilin A inhibitors. Druglikeness or ADMET properties can be considered in library design.

",0.7315250135522492
One Health concept for strengthening public health surveillance and response through Field Epidemiology and Laboratory Training in Ghana,"The lack of highly trained field epidemiologists in the public health system in Ghana has been known since the 1970s when the Planning Unit was established in the Ghana Ministry of Health. When the Public Health School was started in 1994, the decision was taken to develop a 1 academic-year general MPH course. The persisting need for well-trained epidemiologists to support the public health surveillance, outbreak investigation and response system made the development of the Field Epidemiology and Laboratory Training Programme (FELTP) a national priority. The School of Public health and the Ministry of Health therefore requested the technical and financial assistance of the United States Centers for Disease Control and Prevention (CDC) in organizing the Programme. The collaboration started by organizing short courses in disease outbreak investigations and response for serving Ghana Health Service staff. The success of the short courses led to development of the FELTP. By October 2007, the new FELTP curriculum for the award of a Masters of Philosophy in Applied Epidemiology and Disease Control was approved by the Academic Board of the University of Ghana and the programme started that academic year. Since then five cohorts of 37 residents have been enrolled in the two tracks of the programme. They consist of 12 physicians, 12 veterinarians and 13 laboratory scientists. The first two cohorts of 13 residents have graduated. The third cohort of seven has submitted dissertations and is awaiting the results. The fourth cohort has started the second year of field placement while the fifth cohort has just started the first semester. The field activities of the graduates have included disease outbreak investigations and response, evaluation of disease surveillance systems at the national level and analysis of datasets on diseases at the regional level. The residents have made a total of 25 oral presentations and 39 poster presentations at various regional and global scientific conferences. The Ghana FELTP (GFELTP) has promoted the introduction of the One Health concept into FELTP. It hosted the first USAID-supported workshop in West Africa to further integrate and strengthen collaboration of the animal and human health sectors in the FETP model. GFELTP has also taken the lead in hosting the first AFENET Center for Training in Public Health Leadership and Management, through which the short course on Management for Improving Public Health Interventions was developed for AFENET member countries. The GFELTP pre-tested the Integrated Avian Influenza Outbreak and Pandemic Influenza course in preparation for introducing the materials into the curriculum of other FELTP in the network. The leadership positions to which the graduates of the program have been appointed in the human and animal Public Health Services, improvement in disease surveillance, outbreak investigation and response along with the testimony of the health authorities about their appreciation of the outputs of the graduates at various fora, is a strong indication that the GFELTP is meeting its objectives.","At the request of the Ghana Ministry of Health, the University of Ghana established the School of Public Health (SPH) in October 1994. This is a 1year course in general public health which awards a Master of Public Health (MPH) degree. The SPH was one of the beneficiaries of the Rockefeller Foundation support to the network of Public Health Schools Without Walls (PHSWOW) in the Africa Region [1] .

Graduates of the SPH were found to meet the expectations of the Ministry of Health, as they took up leadership roles at district level. It was however, realized that a cadre of highly-trained epidemiologists with competencies and skills in applied epidemiology and disease control was needed to manage the existing complex of public health emergencies and emerging and re-emerging diseases, such as Severe Acute Respiratory Syndrome(SARS) and Avian influenza. During the early stages of implementation of the Global Programme for the Control of Malaria, HIV/AIDS and Tuberculosis, the lack of highly trained field epidemiologists became more apparent as the demand for expert management, interpretation and use of disease surveillance data increased. Unfortunately, the MPH Programme did not make provisions for the training of this cadre of professionals. A process was initiated to establish a Field Epidemiology and Laboratory Training Program (FELTP) to address the identified need. The Ghana FELTP (GFELTP) evolved from an initial collaboration with the United States (U.S.) Centers for Disease Control and Prevention (CDC), through cooperative agreements with the SPH. Activities supported by this cooperation included organization of short courses on disease surveillance, outbreak investigations and response. More than 60 serving district health staff (frontline health workers) and MPH graduates benefited from these short courses over a three-year (2003) (2004) (2005) period [2] . Parts of the short course materials were later incorporated into the MPH curriculum of the School of Public Health. When, in 2005, the decision was taken to start an FELTP, the task of designing the curriculum was spearheaded by the faculty under the guidance of staff of CDC including staff from the Sustainable Management Systems Development Program (SMDP) [3] .

The FELTP curriculum was adapted from CDC's core FETP curriculum [3] . GFELTP graduates receive a Master of Philosophy (MPhil) in Applied Epidemiology and Disease Control upon completing all university requirements. In addition, graduates receive certificate of competency in field epidemiology. The program was approved by the University Academic Board and the National Accreditation Board in 2007. The program started with an initial cohort of three physicians, one laboratory scientist and one veterinarian. In keeping with the ""One health"" concept, to mitigate the increasing threat of outbreaks of zoonotic diseases and to further strengthen the laboratory's key role in public health surveillance and response in the country, the trainees/residents were selected from serving staff nominated by the Ghana Health Service, Ministry of Health (physicians and laboratory scientists) and the Veterinary Service Directorate, Ministry of Food and Agriculture (veterinarians).

The vision of GFELTP is to improve the health of the people in Ghana. The mission is to contribute to addressing Ghana's public health needs and priorities through training and service provision in applied epidemiology and public health laboratory management.

The objectives of GFELTP are to: 1) Strengthen public health capacity by developing a cadre of health professionals with applied skills in applied epidemiology and laboratory management; 2) Contribute to research activities on priority public health problems; 3) Improve national capacity to respond to public health emergencies such as disease outbreaks, natural disasters and unusual public health events including those that could be a result of chemical or bioterrorism; 4) Strengthen national surveillance systems through a team approach (physicians, laboratory scientists and veterinarians); 5) Improve communications and networking of public health practitioners in the country and throughout the Africa Region.

The GFFELTP is a 2 calendar-year programme with about 30% course work and 70% field work covering two tracks (i.e., the epidemiology track and the laboratory track). During the first academic year, residents study core courses that cut across the two tracks in the first semester. In the second semester, residents take courses in each of the prescribed track (i.e., epidemiology for medical and veterinary professionals or laboratory for laboratory scientists) and some selected electives to make up for the required 36 credits for the course work. In addition, residents are required to be involved in 16 weeks of field activities made up of 8 weeks at the end of the first semester to undertake evaluation of surveillance systems of selected diseases and 8 weeks at the end of the second semester for analysis of available large datasets on diseases at national or regional levels. In the second year, residents develop their research topics under the guidance of their academic supervisors and mentors. A further requirement is the organization of at least one seminar prior to going for the field work. Ten months of the second year are devoted to field practice and collection of data while providing services to the district/region of assignment. The last two months are used for data analysis and write up of theses. During the 2-year period of training especially when on field postings, residents of the programme join the staff of the Ghana Health Service and Veterinary Service Directorate to investigate and respond to disease outbreaks and public health emergencies. Being mid-career professionals in public service, the residents sometimes lead these investigations, conduct public health interventions and present written and verbal reports to stakeholders with support of their supervisors and mentors.

Five cohorts have so far been admitted into the residency programme. The breakdown is as shown in Table 1 . The distribution of residents by professional background and sex is shown in Figure 1 and As part of the collaboration between the Disease Surveillance Department(DSD) and the School of Public Health (SPH), a needs assessment to determine the gaps in disease surveillance with emphasis on disease outbreak investigations and response, data analysis and interpretation and capacity development was conducted in fivedistricts in Ghana, namely Asuogyaman, Ketu, Kassena-Nankana, Wassa West and Berekum Districts. Needs assessment tools were developed and discussed by DSD and SPH at an orientation before the exercise commenced. The reports from these assessments were compiled into a composite document for implementation of sensitization workshops for the districts. The workshop had 19 participants, made up of Disease Control Officers, Nurses, Statistician, Midwives, and Medical Superintendent at the Volta River Authority (VRA) Hospital, Medical Assistant and District Director of Health Services. The general objective of the workshop was to give health workers in the district the appropriate knowledge and skills in identifying cases of priority diseases and also process the data and use it for public health action. In addition, core stakeholders such as district assembly members, immigration and custom officers, teachers, information officers, the police and the media were also involved.

The specific objectives were to enable participants to: detect priority diseases, analyze and interpret data on priority diseases, investigate and respond to suspected outbreaks, be prepared for disease epidemics, investigate and respond to other priority diseases, supervise and provide feedback and be able to monitor and evaluate IDSR implementation.

The workshop employed methods including presentations on Integrated Disease Surveillance and Response training modules, role-playing, group work and field exercises. Similar workshops were organized in Ketu, Upper East and Berekum districts. As a result of these workshops, participating districts reported improvements in their disease detection, investigation and public health response.

Major activities undertaken by GFELTP residents over the years are summarized as follows:

Disease outbreak investigations: A total of 23 disease outbreak investigations were conducted by GFELTP residents between 2007 and 2011. These include outbreaks on meningitis, influenza (type A), human rabies, food borne diseases, measles, gastrointestinal diseases, Yellow fever, pertussis, cholera and herpes B. The investigation of an outbreak of herpes B virus infection in May 2011 in Techiman and adjoining districts of central Ghana reported this virus as the probable cause of zoonotic encephalitis in Ghana for the first time.The large number of disease outbreak investigations and the timely response that residents of the programme have been able to carry out alongside other Ghana Health Service or Veterinary Service staff to date have appreciably enhanced disease surveillance and response capacity in the country. In particular, the role that GFELTP team of physicians, veterinarians and laboratory scientists played in the investigation and response to the AI outbreak in Ghana in 2007, the multiple outbreaks of rabies in 2009 -2011, and the monkey-associated herpes-B encephalitis outbreak in 2011 demonstrated the great value of the One Health concept and the multi-disciplinary team approach which the GFELTP has adopted.

Disease surveillance and field studies: As part of end-of-year one field requirements, 31 evaluations of various disease surveillance systems were conducted between 2008 and 2011. They included both communicable and non-communicable diseases. Residents have also analyzed available large datasets for 28 selected diseases at the regional health directorates. Cape Town, South Africa (December, 2010): At the 6 th TEPHINET Global Scientific Conference, Nine GFELTP residents presented three orals and six poster presentations. One of them, Ms. Joyce Der, a cohort-II laboratory track resident was the overall winner in the oral presentation category. She presented the epidemiological and laboratory investigation of a food poisoning outbreak at a popular urban-area food center in the Eastern Region of Ghana.

Accra-Ghana, December, 2005: GFELTP hosted the 1 st AFENET Regional Scientific Conference following the birth of AFENET in August of the same year in Accra Ghana [4] . The residents made five oral presentations and six poster presentations.

Kampala, Uganda-December 2007: Nine presentations were made by GFELTP residents at the 2 nd AFENET Regional Scientific Conference in Kampala. Four were orals and five were poster presentations.

Mombasa, Kenya-August 2009: A total of 15 posters and 6 oral presentations were made at the 5 th TEPHINET African Regional/3 rd AFENET Scientific Conference by GFELTP residents. One of them, Dr Paul Polkuu, a veterinarian and cohort II epidemiology track resident received the runner-up award for the best poster presentation. The presentation was on the investigation of an Influenza-like Illness (ILI) outbreak at a coeducational high school in the Eastern Regional mountains of Ghana. 

A paper by a cohort-II epidemiology track resident ""Community-wide outbreak of cholera following unhygienic practices by small-scale unregistered gold miners, East-Akim District, Ghana -2010"" was accepted for publication by the Ghana Medical Journal in September, 2011 Four public health articles by residents have been published in two veterinary bulletins and two national daily newspaper columns

In addition to SPH faculty members, selected Regional Directors of Health Services and District Directors of Health Services were oriented from the start of GFELTP to serve as supervisors and mentors for residents at various field sites. In May 2009, a Resident Advisor was appointed for GFELTP. Since then, in collaboration with the Ghana Health Service Public Health Division, he has conducted periodic rounds of visits to residents' Field Sites. The aims of the visits are to 1).provide mentorship, supervision and tutoring to residents during their field trainings, 2) conduct local stakeholders' feedback and public health consensus seminars and 3) conduct program advocacy and sensitization meetings with key stakeholders at regional and district levels. Multiple visits have been made to the Eastern, Central, Brong-Ahafo, Greater Accra Northern, Upper West, Upper East, Volta and Western regions. There have been 10 regional stakeholders' seminars where residents made presentations on projects they undertook in various regions or districts to stakeholders from the community, Ghana Health Service and Veterinary Services Directorate. These fora provided opportunities for feedback, inter-sectoral discussions leading to consensus on public health action and sharing of information on GFELTP activities and opportunities. This novel approach of collaborative training and service at the local level has enhanced public health decision, action and GFELTP visibility at the health system frontline level.

The GFELTP has hosted three workshops on Improving Management of Public Health Interventions. This followed an introductory course to train proposed trainers in 2008. The trainers were Deputy Directors in charge of Public Health at Regional level in Ghana. There were 17 participants and the training was facilitated by CDC, Ghana Health Service (GHS) and GFELTP staff. The first workshop was held from June 22-July 17, 2009 and was targeted at health practitioners in the African Sub-Region. Twenty-two health officials from four African Countries attended the course. Out of the 22 participants, 19 were Ghanaians, 1Kenyan, 1 Tanzanian and 1 Ugandan. All 19 Ghanaian participants were staff from the Ghana Health Service. The course was divided into four modules. These four modules were designed to touch on all aspects of health management. Uniqueness of the course was that during the four-week period, participants presented project proposals on management of public health interventions at the beginning of the course. They were helped to develop the proposals and implement them over the subsequent three months after the course. All participants were visited by a facilitator once during the three months of implementation. The Ghanaian participants came back for a day to present the results of what they implemented before receiving their certificates. The regional participants were visited by the coordinator and the AFENET focal person for the course in their various countries. Participants made their presentations at a meeting of stakeholders before they were awarded their certificates.

The workshop with the field component was evaluated six months after the first four-week IMPHI course ended. The goal of the evaluation was to determine whether the four-week training led to application of skills on the job as outlined in the curriculum and program objectives. It was a joint evaluation by CDC-SMDP and stakeholders at the School of Public Health in Ghana.

Six months after the 4-week IMPHI course ended all 12 participants who were interviewed for this evaluation reported implementing a change in management practice at their places of work. Only one participant interviewed could not provide any hard evidence for any of the changes she implemented. 

In collaboration with the USAID/STOP AI Programme, the GFELTP, in May 2010, pre-tested a newly developed set of modules on Integrated Avian Influenza Outbreak Response and Pandemic Influenza in a special two-week training workshop. The purpose of the workshop was to determine the usefulness of these modules in the African setting, with a view of introducing these modules in other FELTPs. The GFELTP has since then adapted materials from the modules into the GFELTP curriculum, and it has been organized yearly with facilitators from the Veterinary Services, School of Veterinary Medicine, National Disaster Management Organization (NADMO) and SPH.

GFELTP collaboration with the Veterinary Services Directorate, Ministry of Food and Agriculture in Ghana has led to the strengthening of the regional epidemiology capacity of the service. Two GFELTP graduates currently serve as the regional veterinary epidemiologists in the Brong-Ahafo and Upper West regions. Two others are awaiting appointment letters to serve as regional epidemiologists in the Central and Volta regions. Similarly, the Ghana Health Service is finalizing formal plans to deploy the GFELTP graduates to fill such positions in the regions. Currently, two of the graduates serve as deputy national program managers for malaria and non-communicable diseases respectively, one as deputy head of the national public health and reference laboratory, three as district directors of health service and de facto regional epidemiologists ( Table 2) .

A steering committee made up of representatives from stakeholders (MOH, GHS, Veterinary Services, Laboratory Services, NADMO, CDC, Noguchi Medical Research Institute and SPH) steers the management of the GFELTP to achieve the objectives of the programme. The Committee is chaired by MOH/GHS and it meets every quarter. The meetings are well documented and shared with all members/partners. The committee follows up plans and recommendations through designated members with support of GFELTP secretariat.

A Matrix Tool for FELTP Assessment was used to do an internal evaluation of the programme and the result presented to the GFELTP Steering Committee. The Ghana FELTP has also gone through assessment by AFENET and awarded Quality Assurance Certificate for 2010

Placement of Graduates Table 2 shows the placement of graduates, pre and post certification.

The genesis and evolution of GFELTP is an example of a national identification of a workforce capacity need and the use of multi-sectoral collaboration with international technical and financial assistance to institutionalize indigenous capacity development in applied epidemiology. The SPH at the University of Ghana is a well-established constituent member of the College of Health Sciences of the University. The MPH program which is the flagship program of the School is thriving well with enrolment from Ghana, the African Region and beyond. The GFELTP was developed as a special program based in the Epidemiology Department of the School. The contribution of the FELTP to the strengthening of the epidemiology curriculum of the MPH program in the SPH has been acknowledged by both graduates and the Ministry of Health at several of the School's annual dissemination forum. The current policy of the Ministry of Health and the Veterinary Services Directorate of deploying graduates of the GFELTP in strategic posts in the national public health service clearly shows the appreciation of the competencies and skills of the graduates.

The outputs of the residents of the GFELTP have demonstrated the scientific rigor that has characterized the field investigations and dissertations that have been produced. Two of the members of the initial cohort have submitted their upgraded dissertations for the award of PhD in epidemiology as of 2011. The emphasis on scientific writing and communication has also reflected in the oral and poster presentations that residents from the program have made in Regional and Global Scientific Conferences. The graduates of the program have all returned to positions with an evolving career structure that is likely to motivate them to remain in the public health service. As part of the new public health institute model facilitated by the international association of public health institutes (IANPHI) initiative in Ghana, the Ghana Health Service is developing a core public health technical or expert team career path that uses the GFELTP graduates to fill the critical role of epidemiologists at the subnational and national levels as well as along specific disease control or public health program lines. Crossover to public health administration track at the top of the path is an option and defined promotion track in keeping with the national public health service policy has been proposed. There is ample evidence of improved public health surveillance and response as well as evidence-based decision making taking place in the National Health Service following the joint evaluation of surveillance systems, disease dataset analyses, outbreak investigations, public health interventions with more regular reports, information sharing and periodic stakeholders' public health seminars at all levels. There has been a definite strengthening of the public health workforce and increased networking between programs in Ghana and with other countries [5] . The prospect of increasing support from the local stakeholders should see increasing enrolment in the program as demonstrated by the 2011 enrollment of nine service professionals, the highest number so far of the five cohorts. This should hasten the attainment of the vision and mission of the program.

The major challenge of the GFELTP has been the slow follow up on pledges of the major national stakeholders of the programme in honouring their funding commitments as specified in the Memorandum of Understanding (MOU). This has resulted in limiting the number of qualified residents that could have been admitted into the programme. But from testimonies that all stakeholders have given on various occasions about the value they place on the service provided by graduates of the programme, it is expected that their support should be forthcoming. At the formal public launching and 1 st certification ceremony of the programme on 02 June 2011, the Minister of Health and the Director of Veterinary Services both emphasized their new policy of utilizing the graduates of the GFELTP in strategic positions in the public health system of the country in order to improve the response to existing public health threats and the emerging zoonotic diseases. These pronouncements encourage our optimistic view regarding the programme sustainability based on continuing support from these key indigenous stakeholders.

There is no doubt that the establishment of the SPH and the subsequent addition of the GFELTP in Ghana has contributed significantly to addressing the competency and skills needs of the public health workforce. This is evidenced by the large number of disease outbreak investigations and the timely response that residents of the programme have been able to carry out to date. In particular, the role that GFELTP team of physicians, veterinarians and laboratory scientists played in the investigation and response to the AI outbreak in Ghana in 2007 demonstrated the great value of the One Health concept and the team approach, which the GFELTP has adopted. The unique feature of the GFELTP that permits trainees to provide service to the Public Health Service even while still in training has made the outputs of the trainees well appreciated by relevant employers. Consequently, the demand for the course has been growing. As more local stakeholders' support come on board, it is expected that larger numbers of trainees will be admitted into the programme in order to respond to increasing challenges of growing complex of public health emergencies in the country and the Sub-region.

Wurapa F, Afari E, Ohuabunwo C, Sackey S: Contributed to development and design of the concept, writing the article and providing important intellectual content, reviewed several drafts and final approval of the version to be published. Clerk C, Kwadje S, Yebuah N, Amankwa J, Amofah G, Appiah-Denkyira E: Contributed to revising the article for important intellectual content and factual content from perspective of service partners, and approval of the version to be published. 

",0.7308572077373702
"The Global Emerging Infection Surveillance and Response System (GEIS), a U.S. government tool for improved global biosurveillance: a review of 2009","The Armed Forces Health Surveillance Center, Global Emerging Infections Surveillance and Response System (AFHSC-GEIS) has the mission of performing surveillance for emerging infectious diseases that could affect the United States (U.S.) military. This mission is accomplished by orchestrating a global portfolio of surveillance projects, capacity-building efforts, outbreak investigations and training exercises. In 2009, this portfolio involved 39 funded partners, impacting 92 countries. This article discusses the current biosurveillance landscape, programmatic details of organization and implementation, and key contributions to force health protection and global public health in 2009.","Despite optimism in the 1960s that mankind had conquered infectious diseases, the world has repeatedly confronted the reality of its continued vulnerability. Two landmark Institute of Medicine (IOM) reports outlined these vulnerabilities [1, 2] . Recent events emphasize the wisdom of these documents, and the fact that the global community must unite to address emerging infectious diseases.

The first of two IOM reports, released in 1992, highlighted the potential role of Department of Defense (DoD) overseas laboratories in addressing the vulnerabilities of emerging infections. DoD has a long history of medical research and development, much of which has been performed through a network of overseas laboratories. Although their geographic locations have changed through time, five laboratories were in operation in 2009: Cairo, Egypt; Nairobi, Kenya; Bangkok, Thailand; Lima, Peru; and Jakarta, Indonesia in 2009 (Figure 1 ) [3] . Historically, the role of these laboratories was limited almost exclusively to the research and development of products, such as vaccines, antimicrobials or diagnostics, that would benefit the health of DoD forces throughout the world. Surveillance for infectious diseases, however, was minimal. Between 1992 and 1996, numerous documents and communications within DoD recognized the need for global emerging infection surveillance initiatives leveraging these overseas laboratories, and emphasized the commitment of DoD to these endeavors.

In 1996, the Executive Office of the President of the United States issued a Presidential Decision Directive (NSTC-7) stating that current capabilities were inadequate to protect the U.S. or global public health communities from emerging infectious disease (EID) threats [4] . DoD was again specifically noted among various federal agencies as having global presence and expertise that could be leveraged to help improve worldwide EID surveillance and preparedness. With these events, the DoD Global Emerging Infections Surveillance and Response System (DoD-GEIS) was established, thereby expanding the mission of DoD to address threats posed to the U.S. and other nations by newly emerging and reemerging infectious diseases. This was a timely development: The next decade brought SARS, West Nile virus and avian influenza, to name a few, and more recently, the H1N1 influenza virus emerged in 2009 as a pandemic threat.

In 2008, DoD-GEIS became a Division of the Armed Forces Health Surveillance Center (AFHSC) by direction of the deputy secretary of defense [5] . This move centralized DoD-wide healthcare surveillance initiatives with domestic and overseas laboratory surveillance efforts. In 2009, AFHSC-GEIS provided direction, funding and oversight to a network of 39 partners (Table 1) at approximately 500 sites. Ninety-two countries were impacted with either active surveillance, capacity-building initiatives or participation in training exercises ( Figure 1 ). This paper will summarize implementation of this global DoD laboratory surveillance network and its contributions in 2009, and discuss potential for the future as the U.S. government becomes increasingly proactive in global biosurveillance.

In addition to AFHSC-GEIS, many other DoD, U.S. government and U.S. nongovernmental organizations engage in surveillance or capacity-building activities throughout the world [6, 7] . In 2009, the U.S. Agency for International Development (USAID) spent more than $1.7 billion on health and over $1.4 billion on humanitarian assistance [8] . Fiscal year 2009 appropriations by the U.S. Congress totaled $33.7 million for the Centers for Disease Control and Prevention's (CDC) Global Disease Detection Program, the principal and most visible CDC program for developing and strengthening global public health capacity to rapidly identify and contain disease threats from around the world. The total budget for CDC's global health programs in fiscal year 2009-including the Global AIDS Program, Global Immunization Program, Global Malaria Program and others-was $308.8 million [9] . The U.S. Department of State's Biological Engagement Program (BEP) received congressional appropriations of $27 million in fiscal year 2009 to engage scientists internationally on issues related to disease surveillance and detection, biosafety and biosecurity. The U.S. Department of Agriculture (USDA) addresses animal health surveillance in the U.S., but is also engaged internationally in capacity building, research and biological control, and outbreak response, with a focus on identifying and evaluating biological agents that could impact global commerce of agricultural products [10] . USDA is also the official U.S. representative to the World Organisation for Animal Health (OIE).

Through Defense Health Program funding, the assistant secretary of defense for health affairs provides $52 million annually to AFHSC-GEIS. The assistant to the secretary of defense for nuclear and chemical and biological defense programs recently embraced emerging infections as a threat to national security, placing global surveillance also within the scope of that organization [11] . Implemented largely through the Defense Threat Reduction Agency, historically that organization's focus has been threat-agent reduction and containment in the former Soviet Union. Authorization to extend globally and beyond threat agents is in process and will be conducted in part through the agency's Cooperative Biological Engagement Program. This is likely to result in an additional infusion of [12] . Most drugs licensed for the treatment of malaria were also products of DoD research and development [13, 14] . AFHSC-GEIS surveillance provides baseline infectious disease risk data that directly influences priorities and viable geographic locations for the conduct of various projects within the MIDRP. Much of the justification for engagement by the U.S. government in this work falls under the category of ""health diplomacy."" The meaning of ""global health diplomacy"" can be controversial, but a commonly accepted definition by the University of California at San Francisco is ""political change activity that meets the dual goals of improving global health and maintaining and improving international relations abroad, particularly in conflict areas and resource-poor environments.""

The involvement of DoD partners throughout the world in implementing this program can clearly be seen as serving a global health diplomacy role. By conducting surveillance and capacity building and assisting with training and outbreak investigations, all integrated into the functions and capabilities of host-country agencies, relationships are forged and trust is developed. International relations abroad are improved. Other DoD organizations work in this broad field of health diplomacy, but less directly in active biosurveillance.

Funding avenues and oversight for these different U.S. government health and surveillance initiatives are independent of each other, and coordination is complex. In a recent publication, the Center for Strategic and International Studies commented that with expanding efforts, agencies should leverage the existing successful programs, and seek a ""unity of effort."" [15] . The release in November 2009 of the National Strategy for Countering Biological Threats (Presidential Policy Directive-2) also emphasizes the need for coordination: ""No single stakeholder can fully address the challenge of biological threats on its own"" [16] . This document uses similar terminology as many of the mid-1990s documents that resulted in the development of DoD-GEIS.

The global laboratory assets of DoD have long been recognized as valuable platforms from which to conduct biosurveillance. Each laboratory is ""sponsored"" in-country by either the Ministry of Defense or Ministry of Health. In addition, close working relationships exist with other components of the host and neighboring countries' governments and academic institutions. Leveraging and empowering these relationships is a formula for success with expanded activities. Maintaining personnel at these military laboratories has also proven sustainable over time, when other U.S. government programs found this to be difficult. DoD's unique ability to provide valuable logistical support is a factor, as is its global integrated health care system meeting the health needs of uniformed families throughout the world that can help determine exposures and risk. The synergy between this system and the DoD laboratory system is becoming clear now that both organizations exist at AFHSC.

Another reason for DoD engagement in these endeavors lies in DoD's mission to ""deter war and protect the security of our country"" [17] . Combat aggressors are but one threat to our security. In the words of James Baldwin, novelist and civil rights activist, ""The most dangerous creation of any society is the man who has nothing to lose."" Endemic diseases in many resource-poor settings are a cause of instability. Each year, more than 1.6 million people die from diarrheal disease, 800,000 from malaria and 20,000 from dengue fever [18] [19] [20] . This burden of known endemic diseases imposes an economic toll and resulting instability. In contrast, emerging infections, whether naturally occurring or the result of human introductions, can result in social unrest and instability on a scale quite out of proportion to the level of risk they introduce [21] . Though new agents have the potential for high morbidity and mortality, fear can have an even greater impact. One example is the severe acute respiratory syndrome (SARS) that rapidly spread around the world in 2003. By midyear, 8,098 individuals were known to have been infected with SARS, resulting in 774 deaths. In the scope of international infectious diseases, this toll on human life was minor. However, the economic impact is estimated at between $40 billion and $52 billion [22] . Likewise, 17 infections and five deaths were attributed to the intentional anthrax attacks in 2001. These small numbers do not adequately speak to the crippling disruption of services or huge economic losses incurred. According to a recent IOM report, ""Global health and national security are inexorably intertwined"" [23] .

Considering these facts, the enormous importance of early identification and mitigation of infectious disease threats is a critical component of a national defense strategy to ""deter war and protect the security of our country.""

The GEIS system functions on a model of ""priority pillars"" and ""strategic steps"" (Figure 2 ). The priority infectious disease pillars include respiratory, gastrointestinal, febrile and vector-borne, antimicrobial-resistant, and sexually transmitted infections. The strategic steps include surveillance and response; training and capacity building; research, innovation and capacity building; and communication of value added. Through integrated implementation of the strategic steps, a comprehensive yet flexible program is created which recognizes the needs of host and partner countries.

Funding for global surveillance initiatives in 2009 was approximately $52 million; $40 million of this was for pandemic/avian influenza initiatives (respiratory pillar), with the remainder available for surveillance in the other EID pillars. In preparation for distribution of these funds, a request for proposals was circulated among partner laboratories in the third quarter of fiscal 2008. A total of 198 proposals were received and evaluated by an internal review board of AFHSC staff. Each proposal was evaluated based on a) potential to fill a critical gap in public health programs, b) likelihood of tri-service or DoD-wide benefits, c) facilitation of timely public health actions, d) responsiveness to critical operational theater or regional needs, e) quality of epidemiology and science, f) leveraging of existing strengths, and g) accessibility of nonfiscal resources needed for execution. In addition, prior performance of the requesting organization and principal investigator was taken into consideration. Proposals were ranked based on scores received, and a cutoff level for funding was assigned based on score and available funding. An external review board, not associated with AFHSC-GEIS and representing all three major uniformed services, reviewed overall funding decisions and provided recommendations. Finally, GEIS and AFHSC directors were briefed and given the opportunity for input. Of the 198 proposals received, full or partial funding was available for the top-ranked 66 percent (130 of 198 proposals), and 56 percent of requested funding was allocated.

Communication within and outside the network was conducted in a variety of ways: required quarterly reports, monthly conference calls with awarded partners, consolidated DoD influenza reports (with variable frequency from daily to weekly during the emerging 2009 H1N1 pandemic), site visits with program reviews, peerreviewed publications, and presentations at multiple DoD and civilian international conferences. Results were reported only with local-host or partner-country notification and concurrence. In general, the information requested and shared by the GEIS network was aggregate in nature. GEIS does not archive extensive data sets from partners or host countries. Analysis and interpretation is largely done by the partner conducting the work, in collaboration with the host country, and with ultimate consideration of national sovereignty and transparency in the process. The central coordination of this global DoD surveillance system afforded multiple opportunities for enhanced utilization of partner capabilities, as well as concise information sharing with other DoD organizations and external agencies ( Table 2 ). The many examples share a central theme of leveraging global visibility and connecting needs with capabilities.

Communication with the World Health Organization (WHO) and CDC is a priority, with a DoD liaison positioned in both organizations to facilitate bilateral information exchange. The value added to these two organizations by the GEIS network is clear in the examples of the WHO reference laboratory status of Naval Medical Research Unit Number 3 (NAMRU-3) in Cairo, Egypt, and U.S. Army Medical Research Unit-Kenya (USAMRU-K). Both laboratories were highly leveraged in training and laboratory capacity building during the 2009 H1N1 pandemic [24] . Numerous influenza contributions to the WHO's Global Influenza Surveillance Network through CDC is another example. These contributions have resulted in numerous examples of viruses isolated by DoD's surveillance network being used as reference strains and the virus seed strain for seasonally available influenza vaccines [25, 26] .

This global DoD surveillance network should not and does not operate in a vacuum. A review of the DoD-GEIS influenza programs by IOM in 2007, conducted after the first year that the network received avian influenza/pandemic influenza (AI/PI) supplemental funds, commented: ""DoD-GEIS should further strengthen its coordination and collaboration on pandemic influenza … with all U.S. partners … These partners include HHS [U.S. Department of Health and Human Services], CDC,…"" [27] . The rapid communication to CDC of the novel H1N1 strains identified by two GEIS partner laboratories before any other public health laboratory (see Table 3 ) is evidence of the implementation of this recommendation. Though funded partners clearly understand the need for timely processing of samples and expeditious communication, it must be continually reinforced throughout the global surveillance network. Personnel turnover is high, and communication of these ongoing needs is a priority.

In its entirety, this special supplement of BioMed Central outlines many of the extensive accomplishments of the global GEIS partner network in 2009. Tables 3 and 4 outline the ""Top 10 accomplishments of the global network,"" and the ""Top 10 specific localized accomplishments.""

Another metric for success is the number of publications in peer-reviewed journals and presentations given by network partners. An accurate count is difficult because the independent network partners leverage funding from various sources for their initiatives. Nevertheless, 112 manuscripts associated with projects partly or wholly supported by AFHSC-GEIS were published in 2009; the number of poster sessions and presentations at various public and private conferences was far higher. Broadly speaking, 33 peer-reviewed publications encompassed febrile and vector-borne infections and other infectious diseases; 25 were in the realm of respiratory infections, including influenza; 19 described emerging infections; 18 were associated with malaria; nine were about gastrointestinal infection; seven described antimicrobial-resistant organisms; and one was related to sexually transmitted infections. Though populations under surveillance were often a mixture of military and civilian, 28 of these publications were directly related to U.S. or foreign military populations.

These numbers attest to the scientific rigor with which partners conduct their work, their ability to leverage funding to create a relatively balanced portfolio covering all five pillars of infectious disease threats of military importance, and their emphasis on military populations.

The WHO International Health Regulations, established in 1969, were originally intended to identify several specific diseases of concern (plague, yellow fever, cholera and smallpox) among travelers entering a given country. The events of the past few decades have made it clear that a new paradigm was needed to minimize the global impact of an emerging pandemic and its toll on human life. To this end, the International Health Regulations (2005), or IHR (2005), were formally adopted by the WHO 58th World Health Assembly on May 23, 2005 , and took effect on June 15, 2007 [28] . The focus of these new guidelines changed from specific diseases of concern to any event that could be considered a ""public health emergency of international concern."" Assessments of current capabilities in countries throughout the world were completed in 2009, and compliance with minimum standards of detection and reporting is required by 2012. Building local capability and infrastructure for compliance is the clear goal in IHR (2005) , and the regulations acknowledge and encourage countries and organizations that are able to assist resource-poor countries in their compliance process.

Considerable coordination and communication with in-country ministries, academic institutions and other in-country government assets is done by AFSHC-GEIS global partners. However, collaboration and capacity building conducted by DoD partners is being reexamined to comply with a broader U.S. government response, the National Strategy for Countering Biological Threats, and the IHR (2005) framework. The White House National Security staff is playing an active role in this U.S. government coordination. By conducting our program in coordination with this whole of US Government, then our capacity building, outbreak assistance and facilitating in-country diagnostic capabilities with host countries will meet the objectives of all by a) reinforcing amiable relationships between host-country Table 3 Top 10. Supported partners tested more than 72,000 respiratory samples, of which more than 17,000 (24 percent) were influenza-positive and more than 10,000 (15 percent) were novel A(H1N1). government public health assets and DoD partners; b) developing the capability to report ""public health emergencies of international concern,"" whereby the entire global community and DoD learns, and world preparations to minimize impact can proceed in a unified and transparent manner; and c) improving DoD's situational awareness through close, transparent, trusting relationships with host countries, even if an actual public health emergency of international concern does not occur.

As briefly discussed in the biosurveillance landscape section of this paper, many U.S. government organizations are becoming involved in global biosurveillance. The mission of DoD's overseas laboratories necessitates continued engagement with in-country public health authorities. However, with rapidly increasing involvement of other U.S. government agencies, a unique niche that U. S. uniformed officers throughout the world can and should expand engagement is with their global uniformed counterparts. In many cases, militaries are the major providers of health care in their countries, with abilities that far exceed their civilian programs. Despite political agendas, remarkable progress in facilitating open lines of communication can occur when two researchers or public health professionals, regardless of cultural or economic background, establish mutual rapport for a mutual interest: optimal health of their uniformed service members.

Although many military-to-military lines of communication and collaboration currently exist (Table 5) , another mechanism AFHSC used to facilitate increased activities in 2009 began with an expanded relationship with the International Committee of Military Medicine (ICMM). ICMM was established in 1921 by Belgian and U.S. medical officers (Commander Medical Officer Jules Voncken and Captain William Bainbridge) after World War I ""revealed the importance of closer cooperation between armed forces medical services worldwide"" [29] . With 104 member countries, ICMM is an unbiased, transparent organization with the goals of maintaining and strengthening the bonds between all medical services of member states, promoting medico-military scientific activities, and developing and participating in humanitarian operations.

Because of its unbiased membership policy, ICMM is the only military organization with a formal in-force memorandum of agreement with WHO. Through direct engagements or indirect facilitation and empowerment with ICMM, opportunities are being explored to work with foreign militaries, to further facilitate IHR (2005) compliance, and to facilitate force health protection and global public health in concert with WHO. Joint initiatives include co-sponsoring a forum titled ""Emerging 

U.S. DoD has a long and impressive history of infectious disease research and product development. The GEIS program was developed at a time of need by DoD-sponsored U.S. and overseas research laboratories. The wisdom of establishing improved global DoD EID surveillance capabilities is reinforced by numerous contributions to global outbreaks, most recently the 2009 H1N1 pandemic. The greatly increased interest by other DoD organizations and the U.S. government as a whole also reinforces this wisdom.

For optimal preparedness, surveillance is an ongoing process, not one that is implemented only in times of public health emergency. Sustaining these programs also avoids negative perceptions by foreign governments of U.S. involvement only with the ""surveillance priority du jour."" The right mix of empowering surveillance activities with capacity building is important to mitigate perceptions of taking but not giving. With the framework of current U.S. government guidelines, such as the National Strategy for Countering Biological Threats and IHR (2005), the world is closer than ever to truly working together on surveillance and control of infectious diseases without consideration of borders. 

",0.7307371149967872
CDC Support for Global Public Health Emergency Management RESPOND,"Recent pandemics and rapidly spreading outbreaks of infectious diseases have illustrated the interconnectedness of the world and the importance of improving the international community's ability to effectively respond. The Centers for Disease Control and Prevention (CDC), building on a strong foundation of lessons learned through previous emergencies, international recognition, and human and technical expertise, has aspired to support nations around the world to strengthen their public health emergency management (PHEM) capacity. PHEM principles streamline coordination and collaboration in responding to infectious disease outbreaks, which align with the core capacities outlined in the International Health Regulations 2005. CDC supports PHEM by providing in-country technical assistance, aiding the development of plans and procedures, and providing fellowship opportunities for public health emergency managers. To this end, CDC partners with US agencies, international partners, and multilateral organizations to support nations around the world to reduce illness and death from outbreaks of infectious diseases.","R ecent public health events, such as the 2016 Zika outbreak and 2009 influenza A(H1N1) pandemic, have illustrated the interconnectedness of the world and the importance of global health security. Outbreaks of new and highly infectious diseases that start in remote parts of the world can quickly spread to large, urban populations. When Ebola virus disease appeared in Nigeria in 2014, what could have been an explosion of cases was quickly contained, in part because of prior emergency management investment by the government of Nigeria, with assistance from the US Centers for Disease Control and Prevention (CDC) and other organizations. Nigeria's ability to use public health emergency management (PHEM) principles to rapidly detect and respond proved invaluable in quickly and effectively stopping the spread of Ebola throughout the country and illustrates the effect of a strong PHEM program (1, 2) .

In 2004, in response to the changing landscape of public health emergencies, the World Health Organization (WHO) led an effort, with support from CDC and other international organizations, to update the International Health Regulations (IHR), leading to adoption of the IHR 2005 (3) (Figure 1 ). According to WHO, ""One of the most important provisions in the IHR is the obligation for all States Parties to establish core capacities to detect, assess, notify and report events, and to respond to public health risks and emergencies"" (4). All member countries had until 2012 to conduct self-assessments and report their progress to WHO. In 2014, WHO, CDC, and other partners launched the Global Health Security Agenda (GHSA) to further advance national capacities to rapidly detect, respond to, and control public health emergencies and thereby comply with IHR 2005 (5) . Although many countries were able to manage small outbreaks within their borders, the introduction of new diseases and the increased spread of disease from international travel exposed the need for a more purposeful and streamlined approach to manage these public health emergencies.

In the same timeframe that IHR 2005 was being written, CDC began to build its own preparedness and response program as a direct result of the increasing risk for public health threats and increased terrorism around the world (6). Using foundational emergency management principles, including the Incident Management System (IMS), CDC established its first permanent Emergency Operations Center (EOC) in 2003 and activated it soon after for the agency's response to the 2003 outbreak of severe acute respiratory syndrome (7) . Since then, CDC has aimed to strengthen its emergency management program through exercises and responses to meet industry emergency response standards and, in 2013, became the first federal agency to receive full accreditation from the Emergency Management Accreditation Program (8) . Building on a strong foundation of lessons learned through previous emergencies, national accreditation, international recognition, and technical expertise, CDC has established itself as a world leader in PHEM and begun to help other entities strengthen their capacity. CDC, as outlined in its Global Health Strategy (2012-2015), now collaborates ""with host country governments and partner organizations to strengthen health security by improving the ability of countries to prepare for and respond to disease threats on a global scale"" (9) .

CDC's global footprint has grown considerably during the past 2 decades. As of 2016, CDC has 342 staff stationed in ≈50 countries and ≈40 staff detailed to international organizations and is supported by ≈1,368 locally employed staff from host countries (10) . Starting in 2009, CDC hired local emergency coordinators in Guatemala, Kenya, Egypt, Kazakhstan, Thailand, and China. As CDC's EOC became increasingly involved in managing public health responses, and the role of the emergency coordinators evolved, CDC began to focus on assisting host country ministries of health with institutionalizing emergency preparedness and response activities. The objectives were to train on IMS and risk communication, complete public health capacity assessments, develop emergency preparedness plans, conduct tabletop exercises, and advise about EOC facility development. Through these efforts, CDC laid the foundation for further technical assistance.

The CDC emergency coordinators have been a valuable asset in this endeavor by providing technical knowledge in emergency preparedness and cultural understanding of the local contexts. In particular, during 2011-2015, the emergency coordinator based at the CDC Central America Regional Office supported the Risk Management Departments of 8 ministries of health in the Central American Region through the Council of Ministries of Health of Central America cooperative agreement. The development of public health emergency response plans and the development of EOCs led to ≈3,800 hours of training to ≈400 staff from 9 countries in Central and South America. As a result of these collaborations with CDC, Central America is better prepared to manage public health emergencies.

CDC's growing role providing PHEM technical assistance coincided with countries' self-assessments for the 2012 deadline to report on progress toward achieving core capacities outlined in IHR 2005. During this time, requests increased to CDC for PHEM technical assistance, and CDC began to provide short-term, in-country emergency preparedness trainings and to host international delegations at the CDC EOC. During 2008-2011, the number of incountry delegations visiting CDC and learning about the US national-level PHEM program increased by 41%. CDC continues to host delegations and collaborates with local partners in Atlanta to enable visitors to observe PHEM at federal, state, and local levels.

Because of the benefits gained through the visits to CDC, several countries expressed interest in comprehensive fellowship opportunities to learn how CDC manages public health emergencies. In 2011, through a cooperative agreement between CDC, the Chinese Center for Disease Control and Prevention, and the National Health and Family Planning Commission of the People's Republic of China, CDC hosted fellows from China for a year-long study tour. As Chinese institutions became more advanced in their S184

Emerging Infectious Diseases • www.cdc.gov/eid • Vol. 23, Supplement to December 2017 plans and training, they sent staff for shorter fellowships to be embedded with CDC emergency management teams and receive specialized training in the areas of emergency plan development, EOC management and operations, and exercises and evaluation. The interest of sending international public health staff to CDC to learn about public health emergency preparedness and response continued to grow, and in 2013 CDC established the Public Health Emergency Management Fellowship (PHEMF) program in Atlanta to build PHEM capacity among members of the international public health community through residential training and mentorship. Fellows complete a comprehensive, standardized study program in core emergency management functions that includes operations, planning, risk communications, and logistics. They observe CDC EOC responses and conduct site visits to improve their familiarity with PHEM in the field. The program enables fellows to interact with, and learn from, stakeholders of CDC's emergency management system, including federal, state, and local partners.

The PHEMF curriculum is guided by a global PHEM Core Competency Model, currently in development, which encompasses 7 competencies: leadership, emergency management frameworks, emergency management functions, emergency management communication, partnership and collaboration, training development and facilitation, and evaluation. With mentorship from CDC subject matter experts, fellows apply their learning to develop a personalized toolkit of products to be used by their ministries of health. Specific products in the toolkits may include standard operating procedures (SOPs), draft all-hazards or hazard-specific plans, or Web-based systems for EOC messaging.

By December 2016, CDC had trained 39 fellows from 25 countries. As leaders within their respective organizations, returning fellows facilitate the expansion of PHEM within their countries and have assumed key roles as leaders and managers of emergency response units in Africa, Asia, and the Middle East.

In the years leading up to the 2012 self-assessment on IHR 2005 capacities, worldwide need increased for guidelines and standards for building PHEM capacity. In fact, by June 2012, only ""42 of 193 [21.76%] States Parties declared that they had met their core capacity requirements"" (4), and most countries had requested a 2-year extension. To fill this need, CDC and other international organizations focused their global technical assistance on countries' IHR 2005 requirements. However, after 2 years of such support, in 2014, WHO reported that ""at the time of…[the] second Review Committee meeting, 64 States Parties [33.16%] had indicated that they met the minimum core capacity standards"" (4), which indicated that additional effort was needed.

Ongoing reviews of countries' PHEM capabilities demonstrated a lack of clear international guidelines for program implementation. Therefore, WHO, with support from CDC, the Chinese Center for Disease Control and Prevention, the European Centre for Disease Prevention and Control, and others, established the Public Health Emergency Operations Centre Network (EOC-NET), which strives to identify best practices in PHEM and promote EOC capacity-building activities in member states (11). As a member of EOC-NET, in 2013 CDC supported WHO's systematic review of EOCs and technical consultations through 4 working groups that aimed to develop guidance and standards for building, maintaining, and managing Public Health EOCs (PHEOCs).

The direct result of the EOC-NET work was publication of the Framework for a Public Health Emergency Operations Centre in 2015 as a first step in creating internationally recognized minimum common standards for PHEOCs. The Framework ""outlines the key concepts and essential requirements for developing and managing… a PHEOC in order to achieve a goal-oriented response to public health emergencies and unity of effort among response agencies"" (12) . These guidelines provide a framework for public health emergency managers and practitioners to build the core capacity elements necessary for effective responses to public health emergencies.

In 2013, CDC partnered with the ministries of health in Uganda and Vietnam as part of the Global Health Security Demonstration Project to show what public health capacity could be developed in 6 months with a concentrated commitment of technical assistance and resources. Although each country faced unique hazards and challenges, both projects focused on 3 main areas: 1) strengthening laboratory systems, 2) improving information gathering and sharing, and 3) developing a highly functioning PHEOC (13, 14) . CDC worked with the countries to develop SOPs and provide emergency management training for their PHEOC staff. At the culmination of the project, each country underwent a series of drills to demonstrate its increased capacity in the 3 target areas and showed significant improvements.

In 2014, the US government signed on to a 5-year commitment to support GHSA, an international collaboration among partner nations and international organizations intended to serve as a roadmap for countries to reach the capacities outlined in IHR 2005 (5) . The goal of GHSA is to prepare nations around the world to more quickly and effectively detect and respond to infectious disease threats to reduce morbidity and mortality and prevent the global spread of disease (15) . Eleven GHSA Action Packages are organized around Prevent, Detect, and Respond (16), and CDC's strong foundation in PHEM has positioned the agency to provide effective technical assistance for the Respond 1 Action Package: Emergency Operations Centers.

The Respond 1 goal is for a country to have ""a PHEOC functioning according to minimum common standards and trained EOC staff capable of activating a coordinated emergency response within 120 minutes of the identification of a public health emergency"" (16) . Although the Action Package goal highlights the need for each country to have a functioning PHEOC, what the Global Health Security Demonstration Project showed was that a fully coordinated response can be accomplished only through a comprehensive PHEM program. Through GHSA, CDC provides technical assistance to 17 countries in 3 areas: training and mentoring of PHEM staff; reinforcing sufficient PHEOC infrastructure; and developing streamlined systems, including plans, SOPs, and connections with other ministries of health (Figure 2 ).

At the same time CDC began to support GHSA, the Ebola virus disease outbreak struck West Africa. Using the lessons learned in the Global Health Security Demonstration Project, in September 2014, CDC provided emergency management technical assistance to develop PHEOCs in Guinea, Liberia, and Sierra Leone and the surrounding countries (17) . The Ebola outbreak substantially weakened the already limited public health systems in the 3 affected countries.

CDC's emergency management assistance focused on developing IMS goals and objectives, coordinating infrastructure improvements for increased collaboration, assisting with logistics, and training staff on PHEM principles. The progress in these 3 countries demonstrated that ""rudimentary emergency management capacities can be rapidly established in countries with the application of focused technical assistance"" (17) . The response to this outbreak provided CDC with a unique opportunity to understand and overcome the challenges of providing technical assistance to countries with limited emergency management capacity, which would inform the approach for GHSA. Security series (18) (19) (20) ; the WHO EOC Framework (10); and industry-specific standards, such as the National Fire Protection Association 1600 Standard on Disaster/Emergency Management and Business Continuity/Continuity of Operations (21) . CDC then worked with ministries of health to customize work plans based on the country's baseline capacity and 5-year strategic goals for a PHEOC. Through collaboration with in-country partners, CDC assists countries with public health threat and hazard identification and risk assessments; design of PHEOC policies, protocols, and guidelines; strategic and operational plans; planning for the physical design of a PHEOC; training PHEOC staff; and developing and executing exercises to validate activities. The effect of this work has been demonstrated in multiple ways. For example, in Cameroon, 33 Ministry of Health staff received basic PHEM training and participated in a follow-up exercise, and 26 participated in a workshop to develop and validate 11 priority SOPs for the PHEOC. A PHEMF graduate served as the incident manager for an influenza A(H5N1) outbreak and applied newly acquired skills in IMS to coordinate and manage the Ministry of Health's emergency response (22) . In 2016 alone, Cameroon has seen a decrease in the time it takes to activate the PHEOC from 8 weeks (cholera outbreak) to 1 week (Lassa fever outbreak) to 24 hours (H5N1 outbreak), and coordination between animal and human health stakeholders has substantially improved.

Senegal needed a PHEM program early in the West Africa Ebola outbreak when an Ebola-positive person traveled from Guinea to the capital, Dakar. Since that time, emergency management has improved substantially through development of a PHEOC with support from CDC, the DOD's Defense Threat Reduction Agency (DTRA), and USAID. The Ministry of Health has trained permanent PHEOC staff, developed plans and procedures, and participated in 2 simulation exercises. The PHEOC assets also have been linked to national systems in public health surveillance, laboratory, human resources, and other sectors through joint strategic planning and simulation exercises. Both Cameroon and Senegal are emerging as regional PHEM leaders and are leading initiatives to share resources and exchange lessons learned from emergency responses with other West Africa countries. Of the 17 GHSA countries, 16 have received in-country CDC technical assistance and completed data collection for planning emergency management technical assistance, and 12 have held in-country CDC emergency management trainings.

Emergency management principles strive to streamline coordination and collaboration. Therefore, CDC works with US agencies and international partners to reach the goals of IHR 2005. CDC coordinates with DOS, DOD, USAID, and others to leverage resources and partnerships to expand emergency management technical assistance to countries. For example, since 2012, CDC has partnered with the DOS Biosecurity Engagement Program (BEP) to support biosecurity-related emergency preparedness. In India, CDC collaborated with the National Centre for Disease Control to develop a national-level PHEOC and is further strengthening emergency management capacity by developing PHEOCs at the state and district levels in Tamil Nadu. This network of PHEOCs will help India be better prepared to respond to biosecurity/biosafety threats. In Jordan, CDC partners with DTRA and DOS in 2 separate initiatives; through BEP, CDC has been working with the Ministry of Health Crisis Management Unit to develop a national-level PHEOC and provide training for Ministry of Health staff on the principles of emergency management. In addition, CDC has partnered with DTRA to bring different entities of the government of Jordan together for emergency preparedness planning, EOC training, and exercises focusing on civil-military coordination during humanitarian crises and health emergencies. These activities can streamline emergency management across Jordan's government.

CDC's partnerships and technical assistance also extend to large multilateral organizations and entities. CDC participates in WHO-led initiatives as subject matter experts in Joint External Evaluations, which assess a country's capacity to prevent, detect, and respond to public health threats (23) , and partners with WHO to conduct GHSA activities. CDC also provides the African Union with emergency management training and technical assistance in developing a continent-level PHEOC. CDC partners with other nations' public health organizations, such as Public Health England and Public Health Agency of Canada, to leverage technical and language expertise and has joined with Emory University (Atlanta, GA, USA) through its Rollins School of Public Health Center for Humanitarian Emergencies to help develop the next generation of public health practitioners in humanitarian emergencies, emergency preparedness, and response (24) .

Countless examples throughout the past few years have shown that diseases know no borders and can rapidly spread across land and sea. Increasing the international community's ability to rapidly and effectively respond to public health threats ensures the broader global health security of all people. In resource-limited environments, emergency response is centered on achieving the biggest public health impact. PHEM components, like preparedness plans, SOPs, and EOCs, contribute to faster and more efficient responses during emergencies which enable a greater reduction in morbidity and mortality.

Successes have occurred in capacity development in some countries; however, challenges and limitations remain to building PHEM capacity around the globe. Although the WHO EOC Framework provides guidelines for countries on how to build a PHEM program, each country faces unique circumstances and challenges in implementing these programs. Laws, policies, and authorities vary substantially, and because PHEM is still a relatively new concept for most developing countries, high-level support must be cultivated. Countries with limited financial and human resources must prioritize planning for the most immediate and dire threats, and preparedness planning can seem an unaffordable luxury of time and resources. CDC and other international partners have provided technical assistance and resources, but transitioning to managing public health emergencies through a PHEOC still requires major commitment and input from a ministry of health to progress from having functionality to being fully operational. It is often expensive for countries to dedicate staff to work only on PHEM without drawing resources from other parts of the ministry of health. Efforts to strengthen PHEM capacity must build on existing emergency response structures. Any augmentation of technology and infrastructure also should improve nonemergency capability to be sustainable and effective.

The ability to detecting and respond locally to public health threats has been needed for generations. However, as the world becomes more interconnected, countries are realizing an increased need to prepare for public health threats from across the globe. Furthermore, global health security relies on the capacity of all countries to comply with IHR 2005 and rapidly detect, respond to, and control public health emergencies. This realization has increased the demand for PHEM technical assistance in building countries' sustainable capacity. Although CDC has focused on providing PHEM technical assistance to a select number of countries through programs such as the GHSA and partners such as BEP and DTRA, the need for PHEM assistance exceeds current support. The Zika outbreak in Central and South America is just one example of how nations outside these programs are susceptible to threats previously limited only to countries on other continents.

Providing technical assistance to countries during an outbreak or public health emergency is important; however, CDC encourages countries to invest in preparing for emergencies by highlighting the effect of an effective PHEM program on a response. CDC is focusing on regional workshops to bring together neighboring countries for trainings to increase communication and collaboration to leverage expertise across nations with similar threats and hazards.

Continued close collaboration and partnership across the US government and with international organizations will enable more to be accomplished through leveraging individual institutional strengths. CDC aims to standardize the PHEM approach to respond to public health emergencies by continuing to assist WHO, through initiatives such as EOC-NET, to create global standards for PHEM. With this effort, CDC aims to reach its goal of saving lives and protecting people while making the world a safer place from disease outbreaks and other public health threats.

",0.730657236396659
